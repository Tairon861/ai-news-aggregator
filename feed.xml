<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 10 Sep 2025 01:37:06 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>Breaking the networking wall in AI infrastructure (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/breaking-the-networking-wall-in-ai-infrastructure/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Two white line icons on a gradient background transitioning from blue to pink. From left to right: icon representing a set of gears; an icon representing three connected nodes each containing a user icon." class="wp-image-1148762" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Memory and network bottlenecks are increasingly limiting AI system performance by reducing GPU&amp;nbsp;utilization&amp;nbsp;and overall efficiency,&amp;nbsp;ultimately preventing&amp;nbsp;infrastructure from reaching its full potential&amp;nbsp;despite enormous investments.&amp;nbsp;At the&amp;nbsp;core&amp;nbsp;of this challenge is a fundamental trade-off in the communication technologies used for memory and network interconnects.&lt;/p&gt;



&lt;p&gt;Datacenters typically deploy two types of physical cables&amp;nbsp;for&amp;nbsp;communication between&amp;nbsp;GPUs.&amp;nbsp;Traditional copper links&amp;nbsp;are power-efficient and&amp;nbsp;reliable,&amp;nbsp;but&amp;nbsp;limited to&amp;nbsp;very short&amp;nbsp;distances&amp;nbsp;( 2 meters)&amp;nbsp;that&amp;nbsp;restrict their use&amp;nbsp;to within a single&amp;nbsp;GPU&amp;nbsp;rack. Optical&amp;nbsp;fiber&amp;nbsp;links&amp;nbsp;can&amp;nbsp;reach&amp;nbsp;tens of meters,&amp;nbsp;but&amp;nbsp;they&amp;nbsp;consume far more&amp;nbsp;power&amp;nbsp;and fail up to 100 times&amp;nbsp;as often as&amp;nbsp;copper. A&amp;nbsp;team working across&amp;nbsp;Microsoft&amp;nbsp;aims&amp;nbsp;to&amp;nbsp;resolve&amp;nbsp;this trade-off&amp;nbsp;by&amp;nbsp;developing&amp;nbsp;MOSAIC,&amp;nbsp;a novel optical link technology&amp;nbsp;that&amp;nbsp;can provide&amp;nbsp;low power and cost, high reliability, and long reach (up to 50 meters)&amp;nbsp;&lt;em&gt;simultaneously&lt;/em&gt;.&amp;nbsp;This approach leverages a hardware-system co-design and adopts&amp;nbsp;a wide-and-slow design with hundreds of parallel low-speed channels using&amp;nbsp;microLEDs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The fundamental trade-off&amp;nbsp;among&amp;nbsp;power, reliability, and reach&amp;nbsp;stems from&amp;nbsp;the&amp;nbsp;&lt;em&gt;narrow-and-fast&lt;/em&gt;&amp;nbsp;architecture&amp;nbsp;deployed&amp;nbsp;in&amp;nbsp;today’s copper and optical links,&amp;nbsp;comprising&amp;nbsp;a few channels&amp;nbsp;operating&amp;nbsp;at&amp;nbsp;very high&amp;nbsp;data rates. For example,&amp;nbsp;an&amp;nbsp;800 Gbps link&amp;nbsp;consists of eight 100 Gbps channels.&amp;nbsp;With&amp;nbsp;copper links, higher channel speeds lead to greater signal integrity challenges, which limits their reach.&amp;nbsp;With optical&amp;nbsp;links,&amp;nbsp;high-speed transmission is inherently inefficient, requiring power-hungry laser drivers and&amp;nbsp;complex electronics&amp;nbsp;to compensate for transmission impairments. These challenges&amp;nbsp;grow&amp;nbsp;as speeds increase&amp;nbsp;with&amp;nbsp;every&amp;nbsp;generation&amp;nbsp;of networks.&amp;nbsp;Transmitting at high speeds also pushes the limits of optical components, reducing&amp;nbsp;systems&amp;nbsp;margins&amp;nbsp;and increasing failure rates.&amp;nbsp;&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/h2&gt;
				
								&lt;p class="large" id="ai-testing-and-evaluation-learnings-from-science-and-industry"&gt;Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;These limitations force systems designers to make unpleasant&amp;nbsp;choices,&amp;nbsp;limiting the scalability of AI infrastructure.&amp;nbsp;For example,&amp;nbsp;scale-up networks connecting AI accelerators at&amp;nbsp;multi-Tbps&amp;nbsp;bandwidth&amp;nbsp;typically&amp;nbsp;must&amp;nbsp;rely on&amp;nbsp;copper links&amp;nbsp;to meet&amp;nbsp;the&amp;nbsp;power budget,&amp;nbsp;requiring&amp;nbsp;ultra-dense racks that&amp;nbsp;consume&amp;nbsp;hundreds of kilowatts&amp;nbsp;&lt;em&gt;per rack&lt;/em&gt;. This creates significant challenges in cooling&amp;nbsp;and&amp;nbsp;mechanical design,&amp;nbsp;which constrain&amp;nbsp;the practical scale of these networks and end-to-end performance. This imbalance&amp;nbsp;ultimately&amp;nbsp;erects&amp;nbsp;a&amp;nbsp;&lt;em&gt;networking wall&lt;/em&gt;&amp;nbsp;akin&amp;nbsp;to the&amp;nbsp;&lt;em&gt;memory wall&lt;/em&gt;, in&amp;nbsp;which CPU speeds have outstripped memory speeds, creating performance bottlenecks.&lt;/p&gt;



&lt;p class="has-text-align-left"&gt;A technology offering copper-like power efficiency and reliability over long distances can overcome this networking&amp;nbsp;wall,&amp;nbsp;enabling&amp;nbsp;multi-rack&amp;nbsp;scale-up domains and unlocking&amp;nbsp;new architectures. This is a highly active R&amp;amp;D area, with many candidate technologies currently being developed across the industry.&amp;nbsp;In&amp;nbsp;our recent&amp;nbsp;paper,&amp;nbsp;&lt;em&gt;“MOSAIC: Breaking the Optics versus Copper Trade-off with a Wide-and-Slow Architecture and MicroLEDs”&lt;/em&gt;, which received the Best Paper award at ACM SIGCOMM&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, we present&amp;nbsp;one such promising&amp;nbsp;approach&amp;nbsp;that is&amp;nbsp;the result of a multi-year collaboration between Microsoft Research,&amp;nbsp;Azure, and M365.&amp;nbsp;This&amp;nbsp;work is&amp;nbsp;centered around&amp;nbsp;an optical&amp;nbsp;wide-and-slow architecture, shifting from a small number of high-speed serial channels towards&amp;nbsp;hundreds of parallel low-speed channels.&amp;nbsp;This&amp;nbsp;would be impractical&amp;nbsp;to realize with today’s copper and optical technologies because of&amp;nbsp;i)&amp;nbsp;electromagnetic interference challenges in high-density copper cables and ii) the&amp;nbsp;high cost&amp;nbsp;and power consumption of lasers&amp;nbsp;in optical links,&amp;nbsp;as well as the increase in packaging complexity.&amp;nbsp;MOSAIC overcomes these issues by&amp;nbsp;leveraging&amp;nbsp;directly modulated&amp;nbsp;microLEDs, a technology originally developed for&amp;nbsp;screen&amp;nbsp;displays.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;MicroLEDs&amp;nbsp;are significantly smaller than traditional LEDs (ranging from a few to tens of&amp;nbsp;microns) and, due to their&amp;nbsp;small size,&amp;nbsp;they&amp;nbsp;can be modulated at several Gbps.&amp;nbsp;They&amp;nbsp;are manufactured in large arrays,&amp;nbsp;with over half a million&amp;nbsp;in a small physical footprint for high-resolution displays&amp;nbsp;like&amp;nbsp;head-mounted devices or smartwatches. For example, assuming 2 Gbps per&amp;nbsp;microLED&amp;nbsp;channel, an 800 Gbps MOSAIC link can be realized by using a 20×20&amp;nbsp;microLED&amp;nbsp;array, which can fit in less than 1 mm×1 mm&amp;nbsp;silicon&amp;nbsp;die.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;MOSAIC’s&amp;nbsp;wide-and-slow&amp;nbsp;design&amp;nbsp;provides four core benefits.&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Operating&amp;nbsp;at low speed improves power efficiency&amp;nbsp;by&amp;nbsp;eliminating&amp;nbsp;the need for&amp;nbsp;complex&amp;nbsp;electronics&amp;nbsp;and&amp;nbsp;reducing optical power requirements.&lt;/li&gt;



&lt;li&gt;By&amp;nbsp;leveraging&amp;nbsp;optical transmission (via&amp;nbsp;microLEDs),&amp;nbsp;MOSAIC&amp;nbsp;sidesteps&amp;nbsp;copper’s reach issues, supporting distances up to 50 meters,&amp;nbsp;or&amp;nbsp;&amp;gt; 10x&amp;nbsp;further&amp;nbsp;than copper.&lt;/li&gt;



&lt;li&gt;MicroLEDs’&amp;nbsp;simpler structure&amp;nbsp;and temperature insensitivity&amp;nbsp;make them more reliable than lasers. The parallel nature of&amp;nbsp;wide-and-slow&amp;nbsp;also&amp;nbsp;makes it easy to add redundant channels, further increasing reliability, up to two orders of magnitude higher than optical links.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;The&amp;nbsp;approach is also scalable, as higher aggregate speeds (e.g.,&amp;nbsp;1.6&amp;nbsp;Tbps&amp;nbsp;or 3.2&amp;nbsp;Tbps) can be achieved by increasing the number of&amp;nbsp;channels and/or raising per-channel speed&amp;nbsp;(e.g., to 4-8 Gbps).&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Further,&amp;nbsp;MOSAIC is fully compatible with today’s pluggable transceivers’ form&amp;nbsp;factor&amp;nbsp;and it provides a drop-in replacement for today’s copper and optical cables, without requiring any changes to existing server and network infrastructure.&amp;nbsp;MOSAIC is protocol-agnostic, as it simply relays bits from one endpoint to another without&amp;nbsp;terminating&amp;nbsp;or inspecting the connection&amp;nbsp;and, hence,&amp;nbsp;it’s&amp;nbsp;fully compatible with today’s protocols (e.g.,&amp;nbsp;Ethernet, PCIe, CXL).&amp;nbsp;We are currently working with our suppliers to&amp;nbsp;productize&amp;nbsp;this technology and&amp;nbsp;scale&amp;nbsp;to mass production.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While&amp;nbsp;conceptually simple, realizing this architecture posed a few key challenges&amp;nbsp;across the stack, which&amp;nbsp;required&amp;nbsp;a multi-disciplinary team with&amp;nbsp;expertise&amp;nbsp;spanning across integrated photonics, lens design, optical transmission, and&amp;nbsp;analog&amp;nbsp;and digital design.&amp;nbsp;For example, using individual&amp;nbsp;fibers&amp;nbsp;per channel would be prohibitively complex and costly due to the&amp;nbsp;large number&amp;nbsp;of channels. We addressed this by employing imaging&amp;nbsp;fibers,&amp;nbsp;which are typically used for medical applications (e.g., endoscopy).&amp;nbsp;They&amp;nbsp;can support thousands of cores&amp;nbsp;per&amp;nbsp;fiber, enabling multiplexing&amp;nbsp;of&amp;nbsp;many channels within a single&amp;nbsp;fiber.&amp;nbsp;Also,&amp;nbsp;microLEDs&amp;nbsp;are a less pure light source&amp;nbsp;than lasers,&amp;nbsp;with&amp;nbsp;a larger beam shape (which complicates&amp;nbsp;fiber&amp;nbsp;coupling) and&amp;nbsp;a broader spectrum (which&amp;nbsp;degrades&amp;nbsp;fiber&amp;nbsp;transmission due to chromatic dispersion).&amp;nbsp;We tackled these issues through&amp;nbsp;a novel&amp;nbsp;microLED and&amp;nbsp;optical lens design,&amp;nbsp;and&amp;nbsp;a power-efficient&amp;nbsp;analog-only electronic back&amp;nbsp;end, which does not require any expensive digital signal processing.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Based on our current estimates, this approach can save&amp;nbsp;up to 68% of power, i.e., more&amp;nbsp;than 10W per cable while reducing failure rates by up to 100x. With global annual shipments of optical cables&amp;nbsp;reaching into&amp;nbsp;the tens of millions, this translates to over 100MW of power savings per year,&amp;nbsp;enough to power more than 300,000 homes. While these immediate gains are already significant, the unique combination of low power consumption, reduced cost, high reliability, and long reach opens up exciting new opportunities&amp;nbsp;to rethink&amp;nbsp;AI&amp;nbsp;infrastructure from network and cluster architectures to compute and memory designs.&lt;/p&gt;



&lt;p&gt;For example,&amp;nbsp;by&amp;nbsp;supporting&amp;nbsp;low-power,&amp;nbsp;high-bandwidth connectivity at long reach,&amp;nbsp;MOSAIC&amp;nbsp;removes the need for ultra-dense racks and&amp;nbsp;enables&amp;nbsp;novel network topologies, which would be impractical today. The resulting redesign could&amp;nbsp;reduce&amp;nbsp;resource fragmentation and&amp;nbsp;simplify&amp;nbsp;collective optimization.&amp;nbsp;Similarly,&amp;nbsp;on the&amp;nbsp;compute&amp;nbsp;front,&amp;nbsp;the ability&amp;nbsp;to&amp;nbsp;connect&amp;nbsp;silicon&amp;nbsp;dies at low power over long distances&amp;nbsp;could&amp;nbsp;enable&amp;nbsp;resource&amp;nbsp;disaggregation, shifting from today’s&amp;nbsp;large,&amp;nbsp;multi-die packages to&amp;nbsp;smaller, more cost-effective, ones.&amp;nbsp;Bypassing packaging area constraints would also make it possible to drastically increase&amp;nbsp;GPU&amp;nbsp;memory&amp;nbsp;capacity and bandwidth,&amp;nbsp;while&amp;nbsp;facilitating&amp;nbsp;adoption of&amp;nbsp;novel memory technologies.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Historically, step changes in network technology have unlocked entirely new classes of applications and workloads. While our SIGCOMM paper provides&amp;nbsp;possible future&amp;nbsp;directions, we hope this work sparks broader discussion and collaboration across the research and industry communities.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Two white line icons on a gradient background transitioning from blue to pink. From left to right: icon representing a set of gears; an icon representing three connected nodes each containing a user icon." class="wp-image-1148762" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Memory and network bottlenecks are increasingly limiting AI system performance by reducing GPU&amp;nbsp;utilization&amp;nbsp;and overall efficiency,&amp;nbsp;ultimately preventing&amp;nbsp;infrastructure from reaching its full potential&amp;nbsp;despite enormous investments.&amp;nbsp;At the&amp;nbsp;core&amp;nbsp;of this challenge is a fundamental trade-off in the communication technologies used for memory and network interconnects.&lt;/p&gt;



&lt;p&gt;Datacenters typically deploy two types of physical cables&amp;nbsp;for&amp;nbsp;communication between&amp;nbsp;GPUs.&amp;nbsp;Traditional copper links&amp;nbsp;are power-efficient and&amp;nbsp;reliable,&amp;nbsp;but&amp;nbsp;limited to&amp;nbsp;very short&amp;nbsp;distances&amp;nbsp;( 2 meters)&amp;nbsp;that&amp;nbsp;restrict their use&amp;nbsp;to within a single&amp;nbsp;GPU&amp;nbsp;rack. Optical&amp;nbsp;fiber&amp;nbsp;links&amp;nbsp;can&amp;nbsp;reach&amp;nbsp;tens of meters,&amp;nbsp;but&amp;nbsp;they&amp;nbsp;consume far more&amp;nbsp;power&amp;nbsp;and fail up to 100 times&amp;nbsp;as often as&amp;nbsp;copper. A&amp;nbsp;team working across&amp;nbsp;Microsoft&amp;nbsp;aims&amp;nbsp;to&amp;nbsp;resolve&amp;nbsp;this trade-off&amp;nbsp;by&amp;nbsp;developing&amp;nbsp;MOSAIC,&amp;nbsp;a novel optical link technology&amp;nbsp;that&amp;nbsp;can provide&amp;nbsp;low power and cost, high reliability, and long reach (up to 50 meters)&amp;nbsp;&lt;em&gt;simultaneously&lt;/em&gt;.&amp;nbsp;This approach leverages a hardware-system co-design and adopts&amp;nbsp;a wide-and-slow design with hundreds of parallel low-speed channels using&amp;nbsp;microLEDs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The fundamental trade-off&amp;nbsp;among&amp;nbsp;power, reliability, and reach&amp;nbsp;stems from&amp;nbsp;the&amp;nbsp;&lt;em&gt;narrow-and-fast&lt;/em&gt;&amp;nbsp;architecture&amp;nbsp;deployed&amp;nbsp;in&amp;nbsp;today’s copper and optical links,&amp;nbsp;comprising&amp;nbsp;a few channels&amp;nbsp;operating&amp;nbsp;at&amp;nbsp;very high&amp;nbsp;data rates. For example,&amp;nbsp;an&amp;nbsp;800 Gbps link&amp;nbsp;consists of eight 100 Gbps channels.&amp;nbsp;With&amp;nbsp;copper links, higher channel speeds lead to greater signal integrity challenges, which limits their reach.&amp;nbsp;With optical&amp;nbsp;links,&amp;nbsp;high-speed transmission is inherently inefficient, requiring power-hungry laser drivers and&amp;nbsp;complex electronics&amp;nbsp;to compensate for transmission impairments. These challenges&amp;nbsp;grow&amp;nbsp;as speeds increase&amp;nbsp;with&amp;nbsp;every&amp;nbsp;generation&amp;nbsp;of networks.&amp;nbsp;Transmitting at high speeds also pushes the limits of optical components, reducing&amp;nbsp;systems&amp;nbsp;margins&amp;nbsp;and increasing failure rates.&amp;nbsp;&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/h2&gt;
				
								&lt;p class="large" id="ai-testing-and-evaluation-learnings-from-science-and-industry"&gt;Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;These limitations force systems designers to make unpleasant&amp;nbsp;choices,&amp;nbsp;limiting the scalability of AI infrastructure.&amp;nbsp;For example,&amp;nbsp;scale-up networks connecting AI accelerators at&amp;nbsp;multi-Tbps&amp;nbsp;bandwidth&amp;nbsp;typically&amp;nbsp;must&amp;nbsp;rely on&amp;nbsp;copper links&amp;nbsp;to meet&amp;nbsp;the&amp;nbsp;power budget,&amp;nbsp;requiring&amp;nbsp;ultra-dense racks that&amp;nbsp;consume&amp;nbsp;hundreds of kilowatts&amp;nbsp;&lt;em&gt;per rack&lt;/em&gt;. This creates significant challenges in cooling&amp;nbsp;and&amp;nbsp;mechanical design,&amp;nbsp;which constrain&amp;nbsp;the practical scale of these networks and end-to-end performance. This imbalance&amp;nbsp;ultimately&amp;nbsp;erects&amp;nbsp;a&amp;nbsp;&lt;em&gt;networking wall&lt;/em&gt;&amp;nbsp;akin&amp;nbsp;to the&amp;nbsp;&lt;em&gt;memory wall&lt;/em&gt;, in&amp;nbsp;which CPU speeds have outstripped memory speeds, creating performance bottlenecks.&lt;/p&gt;



&lt;p class="has-text-align-left"&gt;A technology offering copper-like power efficiency and reliability over long distances can overcome this networking&amp;nbsp;wall,&amp;nbsp;enabling&amp;nbsp;multi-rack&amp;nbsp;scale-up domains and unlocking&amp;nbsp;new architectures. This is a highly active R&amp;amp;D area, with many candidate technologies currently being developed across the industry.&amp;nbsp;In&amp;nbsp;our recent&amp;nbsp;paper,&amp;nbsp;&lt;em&gt;“MOSAIC: Breaking the Optics versus Copper Trade-off with a Wide-and-Slow Architecture and MicroLEDs”&lt;/em&gt;, which received the Best Paper award at ACM SIGCOMM&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, we present&amp;nbsp;one such promising&amp;nbsp;approach&amp;nbsp;that is&amp;nbsp;the result of a multi-year collaboration between Microsoft Research,&amp;nbsp;Azure, and M365.&amp;nbsp;This&amp;nbsp;work is&amp;nbsp;centered around&amp;nbsp;an optical&amp;nbsp;wide-and-slow architecture, shifting from a small number of high-speed serial channels towards&amp;nbsp;hundreds of parallel low-speed channels.&amp;nbsp;This&amp;nbsp;would be impractical&amp;nbsp;to realize with today’s copper and optical technologies because of&amp;nbsp;i)&amp;nbsp;electromagnetic interference challenges in high-density copper cables and ii) the&amp;nbsp;high cost&amp;nbsp;and power consumption of lasers&amp;nbsp;in optical links,&amp;nbsp;as well as the increase in packaging complexity.&amp;nbsp;MOSAIC overcomes these issues by&amp;nbsp;leveraging&amp;nbsp;directly modulated&amp;nbsp;microLEDs, a technology originally developed for&amp;nbsp;screen&amp;nbsp;displays.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;MicroLEDs&amp;nbsp;are significantly smaller than traditional LEDs (ranging from a few to tens of&amp;nbsp;microns) and, due to their&amp;nbsp;small size,&amp;nbsp;they&amp;nbsp;can be modulated at several Gbps.&amp;nbsp;They&amp;nbsp;are manufactured in large arrays,&amp;nbsp;with over half a million&amp;nbsp;in a small physical footprint for high-resolution displays&amp;nbsp;like&amp;nbsp;head-mounted devices or smartwatches. For example, assuming 2 Gbps per&amp;nbsp;microLED&amp;nbsp;channel, an 800 Gbps MOSAIC link can be realized by using a 20×20&amp;nbsp;microLED&amp;nbsp;array, which can fit in less than 1 mm×1 mm&amp;nbsp;silicon&amp;nbsp;die.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;MOSAIC’s&amp;nbsp;wide-and-slow&amp;nbsp;design&amp;nbsp;provides four core benefits.&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Operating&amp;nbsp;at low speed improves power efficiency&amp;nbsp;by&amp;nbsp;eliminating&amp;nbsp;the need for&amp;nbsp;complex&amp;nbsp;electronics&amp;nbsp;and&amp;nbsp;reducing optical power requirements.&lt;/li&gt;



&lt;li&gt;By&amp;nbsp;leveraging&amp;nbsp;optical transmission (via&amp;nbsp;microLEDs),&amp;nbsp;MOSAIC&amp;nbsp;sidesteps&amp;nbsp;copper’s reach issues, supporting distances up to 50 meters,&amp;nbsp;or&amp;nbsp;&amp;gt; 10x&amp;nbsp;further&amp;nbsp;than copper.&lt;/li&gt;



&lt;li&gt;MicroLEDs’&amp;nbsp;simpler structure&amp;nbsp;and temperature insensitivity&amp;nbsp;make them more reliable than lasers. The parallel nature of&amp;nbsp;wide-and-slow&amp;nbsp;also&amp;nbsp;makes it easy to add redundant channels, further increasing reliability, up to two orders of magnitude higher than optical links.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;The&amp;nbsp;approach is also scalable, as higher aggregate speeds (e.g.,&amp;nbsp;1.6&amp;nbsp;Tbps&amp;nbsp;or 3.2&amp;nbsp;Tbps) can be achieved by increasing the number of&amp;nbsp;channels and/or raising per-channel speed&amp;nbsp;(e.g., to 4-8 Gbps).&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Further,&amp;nbsp;MOSAIC is fully compatible with today’s pluggable transceivers’ form&amp;nbsp;factor&amp;nbsp;and it provides a drop-in replacement for today’s copper and optical cables, without requiring any changes to existing server and network infrastructure.&amp;nbsp;MOSAIC is protocol-agnostic, as it simply relays bits from one endpoint to another without&amp;nbsp;terminating&amp;nbsp;or inspecting the connection&amp;nbsp;and, hence,&amp;nbsp;it’s&amp;nbsp;fully compatible with today’s protocols (e.g.,&amp;nbsp;Ethernet, PCIe, CXL).&amp;nbsp;We are currently working with our suppliers to&amp;nbsp;productize&amp;nbsp;this technology and&amp;nbsp;scale&amp;nbsp;to mass production.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While&amp;nbsp;conceptually simple, realizing this architecture posed a few key challenges&amp;nbsp;across the stack, which&amp;nbsp;required&amp;nbsp;a multi-disciplinary team with&amp;nbsp;expertise&amp;nbsp;spanning across integrated photonics, lens design, optical transmission, and&amp;nbsp;analog&amp;nbsp;and digital design.&amp;nbsp;For example, using individual&amp;nbsp;fibers&amp;nbsp;per channel would be prohibitively complex and costly due to the&amp;nbsp;large number&amp;nbsp;of channels. We addressed this by employing imaging&amp;nbsp;fibers,&amp;nbsp;which are typically used for medical applications (e.g., endoscopy).&amp;nbsp;They&amp;nbsp;can support thousands of cores&amp;nbsp;per&amp;nbsp;fiber, enabling multiplexing&amp;nbsp;of&amp;nbsp;many channels within a single&amp;nbsp;fiber.&amp;nbsp;Also,&amp;nbsp;microLEDs&amp;nbsp;are a less pure light source&amp;nbsp;than lasers,&amp;nbsp;with&amp;nbsp;a larger beam shape (which complicates&amp;nbsp;fiber&amp;nbsp;coupling) and&amp;nbsp;a broader spectrum (which&amp;nbsp;degrades&amp;nbsp;fiber&amp;nbsp;transmission due to chromatic dispersion).&amp;nbsp;We tackled these issues through&amp;nbsp;a novel&amp;nbsp;microLED and&amp;nbsp;optical lens design,&amp;nbsp;and&amp;nbsp;a power-efficient&amp;nbsp;analog-only electronic back&amp;nbsp;end, which does not require any expensive digital signal processing.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Based on our current estimates, this approach can save&amp;nbsp;up to 68% of power, i.e., more&amp;nbsp;than 10W per cable while reducing failure rates by up to 100x. With global annual shipments of optical cables&amp;nbsp;reaching into&amp;nbsp;the tens of millions, this translates to over 100MW of power savings per year,&amp;nbsp;enough to power more than 300,000 homes. While these immediate gains are already significant, the unique combination of low power consumption, reduced cost, high reliability, and long reach opens up exciting new opportunities&amp;nbsp;to rethink&amp;nbsp;AI&amp;nbsp;infrastructure from network and cluster architectures to compute and memory designs.&lt;/p&gt;



&lt;p&gt;For example,&amp;nbsp;by&amp;nbsp;supporting&amp;nbsp;low-power,&amp;nbsp;high-bandwidth connectivity at long reach,&amp;nbsp;MOSAIC&amp;nbsp;removes the need for ultra-dense racks and&amp;nbsp;enables&amp;nbsp;novel network topologies, which would be impractical today. The resulting redesign could&amp;nbsp;reduce&amp;nbsp;resource fragmentation and&amp;nbsp;simplify&amp;nbsp;collective optimization.&amp;nbsp;Similarly,&amp;nbsp;on the&amp;nbsp;compute&amp;nbsp;front,&amp;nbsp;the ability&amp;nbsp;to&amp;nbsp;connect&amp;nbsp;silicon&amp;nbsp;dies at low power over long distances&amp;nbsp;could&amp;nbsp;enable&amp;nbsp;resource&amp;nbsp;disaggregation, shifting from today’s&amp;nbsp;large,&amp;nbsp;multi-die packages to&amp;nbsp;smaller, more cost-effective, ones.&amp;nbsp;Bypassing packaging area constraints would also make it possible to drastically increase&amp;nbsp;GPU&amp;nbsp;memory&amp;nbsp;capacity and bandwidth,&amp;nbsp;while&amp;nbsp;facilitating&amp;nbsp;adoption of&amp;nbsp;novel memory technologies.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Historically, step changes in network technology have unlocked entirely new classes of applications and workloads. While our SIGCOMM paper provides&amp;nbsp;possible future&amp;nbsp;directions, we hope this work sparks broader discussion and collaboration across the research and industry communities.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/breaking-the-networking-wall-in-ai-infrastructure/</guid><pubDate>Tue, 09 Sep 2025 14:00:00 +0000</pubDate></item><item><title>Apple Intelligence: Everything you need to know about Apple’s AI model and services (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/09/apple-intelligence-everything-you-need-to-know-about-apples-ai-model-and-services/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If you’ve upgraded to a newer iPhone model recently, you’ve probably noticed that Apple Intelligence is showing up in some of your most-used apps, like Messages, Mail, and Notes. Apple Intelligence (yes, also abbreviated to AI) showed up in Apple’s ecosystem in October 2024, and it’s here to stay as Apple competes with Google, OpenAI, Anthropic, and others to build the best AI tools.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-apple-intelligence"&gt;What is Apple Intelligence?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2792508" height="383" src="https://techcrunch.com/wp-content/uploads/2024/06/wwdc24-Apple-intelligence-AI-for-the-rest-of-us-e1718051510774.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Cupertino marketing executives have branded Apple Intelligence: “AI for the rest of us.” The platform is designed to leverage the things that generative AI already does well, like text and image generation, to improve upon existing features. Like other platforms including ChatGPT and Google Gemini, Apple Intelligence was trained on large information models. These systems use deep learning to form connections, whether it be text, images, video or music.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The text offering, powered by LLM, presents itself as Writing Tools. The feature is available across various Apple apps, including Mail, Messages, Pages and Notifications. It can be used to provide summaries of long text, proofread and even write messages for you, using content and tone prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Image generation has been integrated as well, in similar fashion — albeit a bit less seamlessly. Users can prompt Apple Intelligence to generate custom emojis (Genmojis) in an Apple house style. Image Playground, meanwhile, is a standalone image generation app that utilizes prompts to create visual content that can be used in Messages, Keynote or shared via social media.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple Intelligence also marks a long-awaited face-lift for Siri. The smart assistant was early to the game, but has mostly been neglected for the past several years. Siri is integrated much more deeply into Apple’s operating systems; for instance, instead of the familiar icon, users will see a glowing light around the edge of their iPhone screen when it’s doing its thing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;More importantly, new Siri works across apps. That means, for example, that you can ask Siri to edit a photo and then insert it directly into a text message. It’s a frictionless experience the assistant had previously lacked. Onscreen awareness means Siri uses the context of the content you’re currently engaged with to provide an appropriate answer.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Leading up to WWDC 2025, many expected that Apple would introduce us to an even more souped-up version of Siri, but we’re going to have to wait a bit longer.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“As we’ve shared, we’re continuing our work to deliver the features that make Siri even more personal,” said Apple SVP of Software Engineering Craig Federighi at WWDC 2025. “This work needed more time to reach our high-quality bar, and we look forward to sharing more about it in the coming year.”&lt;/p&gt;&lt;p&gt;This yet-to-be-released, more personalized version of Siri is supposed to be able to understand “personal context,” like your relationships, communications routine, and more. But according to a Bloomberg report, the in-development version of this new Siri is too error-ridden to ship, hence its delay.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At WWDC 2025, Apple also unveiled a new AI feature called Visual Intelligence, which helps you do an image search for things you see as you browse. Apple also unveiled a Live Translation feature that can translate conversations in real time in the Messages, FaceTime, and Phone apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Visual Intelligence and Live Translation are expected to be available later in 2025, when iOS 26 launches to the public.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-when-was-apple-intelligence-unveiled"&gt;When was Apple Intelligence unveiled?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;After months of speculation, Apple Intelligence took center stage at WWDC 2024. The platform was announced in the wake of a torrent of generative AI news from companies like Google and Open AI, causing concern that the famously tight-lipped tech giant had missed the boat on the latest tech craze.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Contrary to such speculation, however, Apple had a team in place, working on what proved to be a very Apple approach to artificial intelligence. There was still pizzazz amid the demos — Apple always loves to put on a show — but Apple Intelligence is ultimately a very pragmatic take on the category.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple Intelligence isn’t a standalone feature. Rather, it’s about integrating into existing offerings. While it is a branding exercise in a very real sense, the large language model (LLM) driven technology will operate behind the scenes. As far as the consumer is concerned, the technology will mostly present itself in the form of new features for existing apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We learned more during Apple’s iPhone 16 event in September 2024. During the event, Apple touted a number of AI-powered features coming to its devices, from translation on the Apple Watch Series 10, visual search on iPhones, and a number of tweaks to Siri’s capabilities. The first wave of Apple Intelligence is arriving at the end of October, as part of iOS 18.1, iPadOS 18.1, and macOS Sequoia 15.1.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The features launched first in U.S. English. Apple later added Australian, Canadian, New Zealand, South African, and U.K. English localizations. Support for Chinese, English (India), English (Singapore), French, German, Italian, Japanese, Korean, Portuguese, Spanish, and Vietnamese will arrive in 2025. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-who-gets-apple-intelligence"&gt;Who gets Apple Intelligence?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="iPhone 15 Pro Max in natural titanium, being held, showing the back of the phone" class="wp-image-2602043" height="453" src="https://techcrunch.com/wp-content/uploads/2023/09/iPhone-15-Pro-31.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Darrell Etherington&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The first wave of Apple Intelligence arrived in October 2024 via iOS 18.1, iPadOS 18, and macOS Sequoia 15.1 updates. These updates included integrated writing tools, image cleanup, article summaries, and a typing input for the&amp;nbsp;redesigned Siri experience. A second wave of features became available as part of iOS 18.2, iPadOS 18.2, and macOS Sequoia 15.2. That list includes Genmoji, Image Playground, Visual Intelligence, Image Wand, and ChatGPT integration.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These offerings are free to use, so long as you have one of the following pieces of hardware:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;All iPhone 16 models&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPhone 15 Pro Max (A17 Pro)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPhone 15 Pro (A17 Pro)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPad Pro (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPad Air (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPad mini (A17 or later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;MacBook Air (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;MacBook Pro (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iMac (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mac mini (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mac Studio (M1 Max and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mac Pro (M2 Ultra)&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, only the Pro versions of the iPhone 15 are getting access, owing to shortcomings on the standard model’s chipset. Presumably, however, the whole iPhone 16 line will be able to run Apple Intelligence when it arrives.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-does-apple-s-ai-work-without-an-internet-connection"&gt;How does Apple’s AI work without an internet connection?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2792348" height="383" src="https://techcrunch.com/wp-content/uploads/2024/06/wwdc24-apple-intelligence-private-cloud-compute-02.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When you ask GPT or Gemini a question, your query is being sent to external servers to generate a response, which requires an internet connection. But Apple has taken a small-model, bespoke approach to training. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The biggest benefit of this approach is that many of these tasks become far less resource intensive and can be performed on-device. This is because, rather than relying on the kind of kitchen sink approach that fuels platforms like GPT and Gemini, the company has compiled datasets in-house for specific tasks like, say, composing an email. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That doesn’t apply to everything, however. More complex queries will utilize the new Private Cloud Compute offering. The company now operates remote servers running on Apple Silicon, which it claims allows it to offer the same level of privacy as its consumer devices. Whether an action is being performed locally or via the cloud will be invisible to the user, unless their device is offline, at which point remote queries will toss up an error.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-apple-intelligence-with-third-party-apps"&gt;Apple Intelligence with third-party apps&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="OpenAI and ChatGPT logos" class="wp-image-2763309" height="383" src="https://techcrunch.com/wp-content/uploads/2024/05/OpenAI-and-ChatGPT.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Didem Mente/Anadolu Agency / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;A lot of noise was made about Apple’s pending partnership with OpenAI ahead of the launch of Apple Intelligence. Ultimately, however, it turned out that the deal was less about powering Apple Intelligence and more about offering an alternative platform for those things it’s not really built for. It’s a tacit acknowledgement that building a small-model system has its limitations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple Intelligence is free. So, too, is access to ChatGPT. However, those with paid accounts to the latter will have access to premium features free users don’t, including unlimited queries. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT integration, which debuts on iOS 18.2, iPadOS 18.2, and macOS Sequoia 15.2, has two primary roles: supplementing Siri’s knowledge base and adding to the existing Writing Tools options.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the service enabled, certain questions will prompt the new Siri to ask the user to approve its accessing ChatGPT. Recipes and travel planning are examples of questions that may surface the option. Users can also directly prompt Siri to “ask ChatGPT.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Compose is the other primary ChatGPT feature available through Apple Intelligence. Users can access it in any app that supports the new Writing Tools feature. Compose adds the ability to write content based on a prompt. That joins existing writing tools like Style and Summary.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We know for sure that Apple plans to partner with additional generative AI services. The company all but said that Google Gemini is next on that list.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-can-developers-build-on-apple-s-ai-models"&gt;Can developers build on Apple’s AI models?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;At WWDC 2025, Apple announced what it calls the Foundation Models framework, which will let developers tap into its AI models while offline.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This makes it more possible for developers to build AI features into their third-party apps that leverage Apple’s existing systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For example, if you’re getting ready for an exam, an app like Kahoot can create a personalized quiz from your notes to make studying more engaging,” Federighi said at WWDC. “And because it happens using on-device models, this happens without cloud API costs&amp;nbsp;… We couldn’t be more excited about how developers can build on Apple intelligence to bring you new experiences that are smart, available when you’re offline, and that protect your privacy.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-when-is-siri-getting-its-next-overhaul"&gt;When is Siri getting its next overhaul?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Apple is expected to unveil a new-and-improved Siri experience in 2026, which is already a bit late compared to competitors. It may come as a blow to Apple, but in order to speed up development, they may have no choice but to partner with an outside company to power the new Siri. Apple has been rumored  to be in advanced talks with Google, its primary smartphone hardware competitor.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If you’ve upgraded to a newer iPhone model recently, you’ve probably noticed that Apple Intelligence is showing up in some of your most-used apps, like Messages, Mail, and Notes. Apple Intelligence (yes, also abbreviated to AI) showed up in Apple’s ecosystem in October 2024, and it’s here to stay as Apple competes with Google, OpenAI, Anthropic, and others to build the best AI tools.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-apple-intelligence"&gt;What is Apple Intelligence?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2792508" height="383" src="https://techcrunch.com/wp-content/uploads/2024/06/wwdc24-Apple-intelligence-AI-for-the-rest-of-us-e1718051510774.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Cupertino marketing executives have branded Apple Intelligence: “AI for the rest of us.” The platform is designed to leverage the things that generative AI already does well, like text and image generation, to improve upon existing features. Like other platforms including ChatGPT and Google Gemini, Apple Intelligence was trained on large information models. These systems use deep learning to form connections, whether it be text, images, video or music.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The text offering, powered by LLM, presents itself as Writing Tools. The feature is available across various Apple apps, including Mail, Messages, Pages and Notifications. It can be used to provide summaries of long text, proofread and even write messages for you, using content and tone prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Image generation has been integrated as well, in similar fashion — albeit a bit less seamlessly. Users can prompt Apple Intelligence to generate custom emojis (Genmojis) in an Apple house style. Image Playground, meanwhile, is a standalone image generation app that utilizes prompts to create visual content that can be used in Messages, Keynote or shared via social media.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple Intelligence also marks a long-awaited face-lift for Siri. The smart assistant was early to the game, but has mostly been neglected for the past several years. Siri is integrated much more deeply into Apple’s operating systems; for instance, instead of the familiar icon, users will see a glowing light around the edge of their iPhone screen when it’s doing its thing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;More importantly, new Siri works across apps. That means, for example, that you can ask Siri to edit a photo and then insert it directly into a text message. It’s a frictionless experience the assistant had previously lacked. Onscreen awareness means Siri uses the context of the content you’re currently engaged with to provide an appropriate answer.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Leading up to WWDC 2025, many expected that Apple would introduce us to an even more souped-up version of Siri, but we’re going to have to wait a bit longer.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“As we’ve shared, we’re continuing our work to deliver the features that make Siri even more personal,” said Apple SVP of Software Engineering Craig Federighi at WWDC 2025. “This work needed more time to reach our high-quality bar, and we look forward to sharing more about it in the coming year.”&lt;/p&gt;&lt;p&gt;This yet-to-be-released, more personalized version of Siri is supposed to be able to understand “personal context,” like your relationships, communications routine, and more. But according to a Bloomberg report, the in-development version of this new Siri is too error-ridden to ship, hence its delay.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At WWDC 2025, Apple also unveiled a new AI feature called Visual Intelligence, which helps you do an image search for things you see as you browse. Apple also unveiled a Live Translation feature that can translate conversations in real time in the Messages, FaceTime, and Phone apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Visual Intelligence and Live Translation are expected to be available later in 2025, when iOS 26 launches to the public.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-when-was-apple-intelligence-unveiled"&gt;When was Apple Intelligence unveiled?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;After months of speculation, Apple Intelligence took center stage at WWDC 2024. The platform was announced in the wake of a torrent of generative AI news from companies like Google and Open AI, causing concern that the famously tight-lipped tech giant had missed the boat on the latest tech craze.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Contrary to such speculation, however, Apple had a team in place, working on what proved to be a very Apple approach to artificial intelligence. There was still pizzazz amid the demos — Apple always loves to put on a show — but Apple Intelligence is ultimately a very pragmatic take on the category.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple Intelligence isn’t a standalone feature. Rather, it’s about integrating into existing offerings. While it is a branding exercise in a very real sense, the large language model (LLM) driven technology will operate behind the scenes. As far as the consumer is concerned, the technology will mostly present itself in the form of new features for existing apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We learned more during Apple’s iPhone 16 event in September 2024. During the event, Apple touted a number of AI-powered features coming to its devices, from translation on the Apple Watch Series 10, visual search on iPhones, and a number of tweaks to Siri’s capabilities. The first wave of Apple Intelligence is arriving at the end of October, as part of iOS 18.1, iPadOS 18.1, and macOS Sequoia 15.1.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The features launched first in U.S. English. Apple later added Australian, Canadian, New Zealand, South African, and U.K. English localizations. Support for Chinese, English (India), English (Singapore), French, German, Italian, Japanese, Korean, Portuguese, Spanish, and Vietnamese will arrive in 2025. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-who-gets-apple-intelligence"&gt;Who gets Apple Intelligence?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="iPhone 15 Pro Max in natural titanium, being held, showing the back of the phone" class="wp-image-2602043" height="453" src="https://techcrunch.com/wp-content/uploads/2023/09/iPhone-15-Pro-31.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Darrell Etherington&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The first wave of Apple Intelligence arrived in October 2024 via iOS 18.1, iPadOS 18, and macOS Sequoia 15.1 updates. These updates included integrated writing tools, image cleanup, article summaries, and a typing input for the&amp;nbsp;redesigned Siri experience. A second wave of features became available as part of iOS 18.2, iPadOS 18.2, and macOS Sequoia 15.2. That list includes Genmoji, Image Playground, Visual Intelligence, Image Wand, and ChatGPT integration.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These offerings are free to use, so long as you have one of the following pieces of hardware:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;All iPhone 16 models&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPhone 15 Pro Max (A17 Pro)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPhone 15 Pro (A17 Pro)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPad Pro (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPad Air (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPad mini (A17 or later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;MacBook Air (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;MacBook Pro (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iMac (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mac mini (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mac Studio (M1 Max and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mac Pro (M2 Ultra)&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, only the Pro versions of the iPhone 15 are getting access, owing to shortcomings on the standard model’s chipset. Presumably, however, the whole iPhone 16 line will be able to run Apple Intelligence when it arrives.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-does-apple-s-ai-work-without-an-internet-connection"&gt;How does Apple’s AI work without an internet connection?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2792348" height="383" src="https://techcrunch.com/wp-content/uploads/2024/06/wwdc24-apple-intelligence-private-cloud-compute-02.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When you ask GPT or Gemini a question, your query is being sent to external servers to generate a response, which requires an internet connection. But Apple has taken a small-model, bespoke approach to training. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The biggest benefit of this approach is that many of these tasks become far less resource intensive and can be performed on-device. This is because, rather than relying on the kind of kitchen sink approach that fuels platforms like GPT and Gemini, the company has compiled datasets in-house for specific tasks like, say, composing an email. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That doesn’t apply to everything, however. More complex queries will utilize the new Private Cloud Compute offering. The company now operates remote servers running on Apple Silicon, which it claims allows it to offer the same level of privacy as its consumer devices. Whether an action is being performed locally or via the cloud will be invisible to the user, unless their device is offline, at which point remote queries will toss up an error.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-apple-intelligence-with-third-party-apps"&gt;Apple Intelligence with third-party apps&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="OpenAI and ChatGPT logos" class="wp-image-2763309" height="383" src="https://techcrunch.com/wp-content/uploads/2024/05/OpenAI-and-ChatGPT.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Didem Mente/Anadolu Agency / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;A lot of noise was made about Apple’s pending partnership with OpenAI ahead of the launch of Apple Intelligence. Ultimately, however, it turned out that the deal was less about powering Apple Intelligence and more about offering an alternative platform for those things it’s not really built for. It’s a tacit acknowledgement that building a small-model system has its limitations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple Intelligence is free. So, too, is access to ChatGPT. However, those with paid accounts to the latter will have access to premium features free users don’t, including unlimited queries. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT integration, which debuts on iOS 18.2, iPadOS 18.2, and macOS Sequoia 15.2, has two primary roles: supplementing Siri’s knowledge base and adding to the existing Writing Tools options.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the service enabled, certain questions will prompt the new Siri to ask the user to approve its accessing ChatGPT. Recipes and travel planning are examples of questions that may surface the option. Users can also directly prompt Siri to “ask ChatGPT.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Compose is the other primary ChatGPT feature available through Apple Intelligence. Users can access it in any app that supports the new Writing Tools feature. Compose adds the ability to write content based on a prompt. That joins existing writing tools like Style and Summary.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We know for sure that Apple plans to partner with additional generative AI services. The company all but said that Google Gemini is next on that list.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-can-developers-build-on-apple-s-ai-models"&gt;Can developers build on Apple’s AI models?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;At WWDC 2025, Apple announced what it calls the Foundation Models framework, which will let developers tap into its AI models while offline.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This makes it more possible for developers to build AI features into their third-party apps that leverage Apple’s existing systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For example, if you’re getting ready for an exam, an app like Kahoot can create a personalized quiz from your notes to make studying more engaging,” Federighi said at WWDC. “And because it happens using on-device models, this happens without cloud API costs&amp;nbsp;… We couldn’t be more excited about how developers can build on Apple intelligence to bring you new experiences that are smart, available when you’re offline, and that protect your privacy.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-when-is-siri-getting-its-next-overhaul"&gt;When is Siri getting its next overhaul?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Apple is expected to unveil a new-and-improved Siri experience in 2026, which is already a bit late compared to competitors. It may come as a blow to Apple, but in order to speed up development, they may have no choice but to partner with an outside company to power the new Siri. Apple has been rumored  to be in advanced talks with Google, its primary smartphone hardware competitor.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/09/apple-intelligence-everything-you-need-to-know-about-apples-ai-model-and-services/</guid><pubDate>Tue, 09 Sep 2025 14:51:52 +0000</pubDate></item><item><title>Adapting to new threats with proactive risk management (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/09/1123083/adapting-to-new-threats-with-proactive-risk-management/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Hitachi Vantara&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;In July 2024, a botched update to the software defenses managed by cybersecurity firm CrowdStrike caused more than 8 million Windows systems to fail. From hospitals to manufacturers, stock markets to retail stores, the outage caused parts of the global economy to grind to a halt. Payment systems were disrupted, broadcasters went off the air, and flights were canceled. In all, the outage is estimated to have caused direct losses of more than $5 billion to Fortune 500 companies. For US air carrier Delta Air Lines, the error exposed the brittleness of its systems. The airline suffered weeks of disruptions, leading to $500 million in losses and 7,000 canceled flights.&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large"&gt;&lt;img alt="alt" class="wp-image-1123086" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MIT_HitachiVantura_V5_072225Cover.png?w=1555" width="1555" /&gt;&lt;/figure&gt;  &lt;p&gt;The magnitude of the CrowdStrike incident revealed just how interconnected digital systems are, and the extensive vulnerabilities in some companies when confronted with an unexpected occurrence. “On any given day, there could be a major weather event or some event like what happened…with CrowdStrike,” said then-US secretary of transportation Pete Buttigieg on announcing an investigation into how Delta Air Lines handled the incident. “The question is, is your airline prepared to absorb something like that and get back on its feet and take care of customers?”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;Unplanned downtime poses a major challenge for organizations, and is estimated to cost Global 2000 companies on average $200 million per year. Beyond the financial impact, it can also erode customer trust and loyalty, decrease productivity, and even result in legal or privacy issues. &lt;/p&gt;  &lt;p&gt;A 2024 ransomware attack on Change Healthcare, the medical-billing subsidiary of industry giant UnitedHealth Group—the biggest health and medical data breach in US history—exposed the data of around 190 million people and led to weeks of outages for medical groups. Another ransomware attack in 2024, this time on CDK Global, a software firm that works with nearly 15,000 auto dealerships in North America, led to around $1 billion worth of losses for car dealers as a result of the three-week disruption. &lt;/p&gt; 
 &lt;p&gt;Managing risk and mitigating downtime is a growing challenge for businesses. As organizations become ever more interconnected, the expanding surface of networks and the rapid adoption of technologies like AI are exposing new vulnerabilities—and more opportunities for threat actors. Cyberattacks are also becoming increasingly sophisticated and damaging as AI-driven malware and malware-as-a-service platforms turbocharge attacks.&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1123091" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MITTR2025_HItachiSocials2.png" /&gt;&lt;/figure&gt;  &lt;p&gt;To prepare for these challenges head on, companies must take a more proactive approach to security and resilience. “We’ve had a traditional way of doing things that’s actually worked pretty well for maybe 15 to 20 years, but it’s been based on detecting an incident after the event,” says Chris Millington, global cyber resilience technical expert at Hitachi Vantara. “Now, we’ve got to be more preventative and use intelligence to focus on making the systems and business more resilient.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Download the report.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Hitachi Vantara&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;In July 2024, a botched update to the software defenses managed by cybersecurity firm CrowdStrike caused more than 8 million Windows systems to fail. From hospitals to manufacturers, stock markets to retail stores, the outage caused parts of the global economy to grind to a halt. Payment systems were disrupted, broadcasters went off the air, and flights were canceled. In all, the outage is estimated to have caused direct losses of more than $5 billion to Fortune 500 companies. For US air carrier Delta Air Lines, the error exposed the brittleness of its systems. The airline suffered weeks of disruptions, leading to $500 million in losses and 7,000 canceled flights.&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large"&gt;&lt;img alt="alt" class="wp-image-1123086" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MIT_HitachiVantura_V5_072225Cover.png?w=1555" width="1555" /&gt;&lt;/figure&gt;  &lt;p&gt;The magnitude of the CrowdStrike incident revealed just how interconnected digital systems are, and the extensive vulnerabilities in some companies when confronted with an unexpected occurrence. “On any given day, there could be a major weather event or some event like what happened…with CrowdStrike,” said then-US secretary of transportation Pete Buttigieg on announcing an investigation into how Delta Air Lines handled the incident. “The question is, is your airline prepared to absorb something like that and get back on its feet and take care of customers?”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;Unplanned downtime poses a major challenge for organizations, and is estimated to cost Global 2000 companies on average $200 million per year. Beyond the financial impact, it can also erode customer trust and loyalty, decrease productivity, and even result in legal or privacy issues. &lt;/p&gt;  &lt;p&gt;A 2024 ransomware attack on Change Healthcare, the medical-billing subsidiary of industry giant UnitedHealth Group—the biggest health and medical data breach in US history—exposed the data of around 190 million people and led to weeks of outages for medical groups. Another ransomware attack in 2024, this time on CDK Global, a software firm that works with nearly 15,000 auto dealerships in North America, led to around $1 billion worth of losses for car dealers as a result of the three-week disruption. &lt;/p&gt; 
 &lt;p&gt;Managing risk and mitigating downtime is a growing challenge for businesses. As organizations become ever more interconnected, the expanding surface of networks and the rapid adoption of technologies like AI are exposing new vulnerabilities—and more opportunities for threat actors. Cyberattacks are also becoming increasingly sophisticated and damaging as AI-driven malware and malware-as-a-service platforms turbocharge attacks.&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1123091" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MITTR2025_HItachiSocials2.png" /&gt;&lt;/figure&gt;  &lt;p&gt;To prepare for these challenges head on, companies must take a more proactive approach to security and resilience. “We’ve had a traditional way of doing things that’s actually worked pretty well for maybe 15 to 20 years, but it’s been based on detecting an incident after the event,” says Chris Millington, global cyber resilience technical expert at Hitachi Vantara. “Now, we’ve got to be more preventative and use intelligence to focus on making the systems and business more resilient.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Download the report.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/09/1123083/adapting-to-new-threats-with-proactive-risk-management/</guid><pubDate>Tue, 09 Sep 2025 15:00:00 +0000</pubDate></item><item><title>NVIDIA Partners With AI Infrastructure Ecosystem to Unveil Reference Design for Giga-Scale AI Factories (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-factories-reference-design/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/inference-corp-blog-ai-factory-1280x680-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At this week’s AI Infrastructure Summit in Silicon Valley, NVIDIA’s VP of Accelerated Computing Ian Buck unveiled a bold new vision: the transformation of traditional data centers into fully integrated AI factories.&lt;/p&gt;
&lt;p&gt;As part of this initiative, NVIDIA is developing reference designs to be shared with partners and enterprises worldwide — offering an NVIDIA Omniverse Blueprint for building high-performance, energy-efficient infrastructure optimized for the age of AI reasoning.&lt;/p&gt;
&lt;p&gt;Already, NVIDIA is collaborating with scores of companies across every layer of the stack, from building design and grid integration to power, cooling and orchestration.&lt;/p&gt;
&lt;p&gt;It’s a natural evolution for the company, scaling beyond chips and systems into a new class of industrial products — so complex and interconnected that no single player can build them alone.&lt;/p&gt;
&lt;p&gt;NVIDIA, along with a deep bench of industrial and technology partners, is reactivating decades of infrastructure expertise to build this new class of AI factories.&lt;/p&gt;
&lt;p&gt;Among those partners, Jacobs serves as the design integrator, helping to coordinate the physical and digital layers of the infrastructure to ensure seamless orchestration.&lt;/p&gt;
&lt;p&gt;The embodiment of the reference design will be a digital twin of the AI factory. This digital twin integrates the IT systems inside the data center with the operational technology for power and cooling systems inside and outside the data center.&lt;/p&gt;
&lt;p&gt;The new initiative expands the digital twin to integrate local power generation, energy storage systems, cooling technology and AI agents for operations.&lt;/p&gt;
&lt;p&gt;Longtime collaborators in power and cooling — Schneider Electric, Siemens and Vertiv — have been instrumental in shaping resilient, high-efficiency environments tailored for AI-scale workloads.&lt;/p&gt;
&lt;p&gt;Siemens and Siemens Energy plays a critical role in on-premises power delivery, supporting the need for rapidly deployable, continuous power to meet the gigawatt-scale energy demands of these facilities. GE Vernova collaborates in power generation and electrification to the rack.&lt;/p&gt;
&lt;p&gt;These companies, along with a growing ecosystem of specialists in infrastructure design and simulation, and orchestration — including Cadence, Emerald AI, E Tech Group, phaidra.ai, PTC, Schneider Electric with ETAP, Siemens and Vertech — are helping NVIDIA activate a system-level transformation.&lt;/p&gt;
&lt;p&gt;At the heart of this vision lies a fundamental challenge: how to optimize every watt of energy that enters the facility so that it contributes directly to intelligence generation.&lt;/p&gt;
&lt;p&gt;In today’s data center paradigm, buildings are often designed independently of the compute platforms they house, leading to inefficiencies in power distribution, cooling and system orchestration.&lt;/p&gt;
&lt;p&gt;NVIDIA and its partners are flipping that model.&lt;/p&gt;
&lt;p&gt;By designing the infrastructure and technology stack in tandem, the company enables true system-level optimization — where power, cooling, compute and software are engineered as a unified whole.&lt;/p&gt;
&lt;p&gt;Simulation plays a central role in this shift.&lt;/p&gt;
&lt;p&gt;Companies will be able to share simulation-ready assets, allowing designers to model components in Omniverse using AI factory digital twins even before they’re physically available.&lt;/p&gt;
&lt;p&gt;These digital twins not only optimize AI factories before they’re built — they also help manage them once they’re operational.&lt;/p&gt;
&lt;p&gt;By adopting the OpenUSD framework, the simulation platform can accurately model every aspect of a facility’s operations, from power and cooling to networking infrastructure. This open and extensible approach allows for the creation of physically accurate assets, which in turn leads to the design of smarter, more reliable facilities.&lt;/p&gt;
&lt;p&gt;And the complexity doesn’t stop at the facility walls.&lt;/p&gt;
&lt;p&gt;AI factories must be plugged into broader systems — power grids, water supplies and transportation networks — that require careful coordination and simulation throughout their lifecycle to ensure reliability and scalability.&lt;/p&gt;
&lt;p&gt;This work has already begun.&lt;/p&gt;
&lt;p&gt;Earlier this year, NVIDIA introduced an Omniverse Blueprint for AI factory digital twins. This blueprint connects platforms like Cadence and ETAP, allowing partners to plug in their core tools to model gigawatt-scale facilities before a single physical AI factory site has even been selected.&lt;/p&gt;
&lt;p&gt;More recently, the company expanded its ecosystem with integrations from Jacobs, Siemens and Siemens Energy, enabling unified simulation of power, cooling and networking systems.&lt;/p&gt;
&lt;p&gt;When this blueprint is complete next year, it will allow partners to plug into the system via application programming interfaces and simulation-ready digital assets, enabling real-time collaboration and orchestration across the entire lifecycle — from design to deployment to operation.&lt;/p&gt;
&lt;p&gt;Thanks to this work, where traditional facilities operated in isolation, AI factories will be designed for composability, resilience and scale.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Call to Action:&lt;/b&gt; Join developers, industry leaders, and innovators at NVIDIA GTC Washington, D.C., to explore the latest breakthroughs in AI infrastructure and learn from expert sessions, hands-on training, and partner showcases.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/inference-corp-blog-ai-factory-1280x680-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At this week’s AI Infrastructure Summit in Silicon Valley, NVIDIA’s VP of Accelerated Computing Ian Buck unveiled a bold new vision: the transformation of traditional data centers into fully integrated AI factories.&lt;/p&gt;
&lt;p&gt;As part of this initiative, NVIDIA is developing reference designs to be shared with partners and enterprises worldwide — offering an NVIDIA Omniverse Blueprint for building high-performance, energy-efficient infrastructure optimized for the age of AI reasoning.&lt;/p&gt;
&lt;p&gt;Already, NVIDIA is collaborating with scores of companies across every layer of the stack, from building design and grid integration to power, cooling and orchestration.&lt;/p&gt;
&lt;p&gt;It’s a natural evolution for the company, scaling beyond chips and systems into a new class of industrial products — so complex and interconnected that no single player can build them alone.&lt;/p&gt;
&lt;p&gt;NVIDIA, along with a deep bench of industrial and technology partners, is reactivating decades of infrastructure expertise to build this new class of AI factories.&lt;/p&gt;
&lt;p&gt;Among those partners, Jacobs serves as the design integrator, helping to coordinate the physical and digital layers of the infrastructure to ensure seamless orchestration.&lt;/p&gt;
&lt;p&gt;The embodiment of the reference design will be a digital twin of the AI factory. This digital twin integrates the IT systems inside the data center with the operational technology for power and cooling systems inside and outside the data center.&lt;/p&gt;
&lt;p&gt;The new initiative expands the digital twin to integrate local power generation, energy storage systems, cooling technology and AI agents for operations.&lt;/p&gt;
&lt;p&gt;Longtime collaborators in power and cooling — Schneider Electric, Siemens and Vertiv — have been instrumental in shaping resilient, high-efficiency environments tailored for AI-scale workloads.&lt;/p&gt;
&lt;p&gt;Siemens and Siemens Energy plays a critical role in on-premises power delivery, supporting the need for rapidly deployable, continuous power to meet the gigawatt-scale energy demands of these facilities. GE Vernova collaborates in power generation and electrification to the rack.&lt;/p&gt;
&lt;p&gt;These companies, along with a growing ecosystem of specialists in infrastructure design and simulation, and orchestration — including Cadence, Emerald AI, E Tech Group, phaidra.ai, PTC, Schneider Electric with ETAP, Siemens and Vertech — are helping NVIDIA activate a system-level transformation.&lt;/p&gt;
&lt;p&gt;At the heart of this vision lies a fundamental challenge: how to optimize every watt of energy that enters the facility so that it contributes directly to intelligence generation.&lt;/p&gt;
&lt;p&gt;In today’s data center paradigm, buildings are often designed independently of the compute platforms they house, leading to inefficiencies in power distribution, cooling and system orchestration.&lt;/p&gt;
&lt;p&gt;NVIDIA and its partners are flipping that model.&lt;/p&gt;
&lt;p&gt;By designing the infrastructure and technology stack in tandem, the company enables true system-level optimization — where power, cooling, compute and software are engineered as a unified whole.&lt;/p&gt;
&lt;p&gt;Simulation plays a central role in this shift.&lt;/p&gt;
&lt;p&gt;Companies will be able to share simulation-ready assets, allowing designers to model components in Omniverse using AI factory digital twins even before they’re physically available.&lt;/p&gt;
&lt;p&gt;These digital twins not only optimize AI factories before they’re built — they also help manage them once they’re operational.&lt;/p&gt;
&lt;p&gt;By adopting the OpenUSD framework, the simulation platform can accurately model every aspect of a facility’s operations, from power and cooling to networking infrastructure. This open and extensible approach allows for the creation of physically accurate assets, which in turn leads to the design of smarter, more reliable facilities.&lt;/p&gt;
&lt;p&gt;And the complexity doesn’t stop at the facility walls.&lt;/p&gt;
&lt;p&gt;AI factories must be plugged into broader systems — power grids, water supplies and transportation networks — that require careful coordination and simulation throughout their lifecycle to ensure reliability and scalability.&lt;/p&gt;
&lt;p&gt;This work has already begun.&lt;/p&gt;
&lt;p&gt;Earlier this year, NVIDIA introduced an Omniverse Blueprint for AI factory digital twins. This blueprint connects platforms like Cadence and ETAP, allowing partners to plug in their core tools to model gigawatt-scale facilities before a single physical AI factory site has even been selected.&lt;/p&gt;
&lt;p&gt;More recently, the company expanded its ecosystem with integrations from Jacobs, Siemens and Siemens Energy, enabling unified simulation of power, cooling and networking systems.&lt;/p&gt;
&lt;p&gt;When this blueprint is complete next year, it will allow partners to plug into the system via application programming interfaces and simulation-ready digital assets, enabling real-time collaboration and orchestration across the entire lifecycle — from design to deployment to operation.&lt;/p&gt;
&lt;p&gt;Thanks to this work, where traditional facilities operated in isolation, AI factories will be designed for composability, resilience and scale.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Call to Action:&lt;/b&gt; Join developers, industry leaders, and innovators at NVIDIA GTC Washington, D.C., to explore the latest breakthroughs in AI infrastructure and learn from expert sessions, hands-on training, and partner showcases.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-factories-reference-design/</guid><pubDate>Tue, 09 Sep 2025 15:00:29 +0000</pubDate></item><item><title>NVIDIA Blackwell Ultra Sets the Bar in New MLPerf Inference Benchmark (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/mlperf-inference-blackwell-ultra/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/grace-corp-blog-gb300-nvl72-1280x680-r1-2.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Inference performance is critical, as it directly influences the economics of an AI factory. The higher the throughput of AI factory infrastructure, the more tokens it can produce at a high speed — increasing revenue, driving down total cost of ownership (TCO) and enhancing the system’s overall productivity.&lt;/p&gt;
&lt;p&gt;Less than half a year since its debut at NVIDIA GTC, the NVIDIA GB300 NVL72 rack-scale system — powered by the NVIDIA Blackwell Ultra architecture — set records on the new reasoning inference benchmark in MLPerf Inference v5.1, delivering up to 45% more DeepSeek-R1 inference throughput compared with NVIDIA Blackwell-based GB200 NVL72 systems.&lt;/p&gt;
&lt;p&gt;Blackwell Ultra builds on the success of the Blackwell architecture, with the Blackwell Ultra architecture featuring 1.5x more NVFP4 AI compute and 2x more attention-layer acceleration than Blackwell, as well as up to 288GB of HBM3e memory per GPU.&lt;/p&gt;
&lt;p&gt;The NVIDIA platform also set performance records on all new data center benchmarks added to the MLPerf Inference v5.1 suite — including DeepSeek-R1, Llama 3.1 405B Interactive, Llama 3.1 8B and Whisper — while continuing to hold per-GPU records on every MLPerf data center benchmark.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Stacking It All Up&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Full-stack co-design plays an important role in delivering these latest benchmark results. Blackwell and Blackwell Ultra incorporate hardware acceleration for the NVFP4 data format — an NVIDIA-designed 4-bit floating point format that provides better accuracy compared with other FP4 formats, as well as comparable accuracy to higher-precision formats.&lt;/p&gt;
&lt;p&gt;NVIDIA TensorRT Model Optimizer software quantized DeepSeek-R1, Llama 3.1 405B, Llama 2 70B and Llama 3.1 8B to NVFP4. In concert with the open-source NVIDIA TensorRT-LLM library, this optimization enabled Blackwell and Blackwell Ultra to deliver higher performance while meeting strict accuracy requirements in submissions.&lt;/p&gt;
&lt;p&gt;Large language model inference consists of two workloads with distinct execution characteristics: 1) context for processing user input to produce the first output token and 2) generation to produce all subsequent output tokens.&lt;/p&gt;
&lt;p&gt;A technique called disaggregated serving splits context and generation tasks so each part can be optimized independently for best overall throughput. This technique was key to record-setting performance on the Llama 3.1 405B Interactive benchmark, helping to deliver a nearly 50% increase in performance per GPU with GB200 NVL72 systems compared with each Blackwell GPU in an NVIDIA DGX B200 server running the benchmark with traditional serving.&lt;/p&gt;
&lt;p&gt;NVIDIA also made its first submissions this round using the NVIDIA Dynamo inference framework.&lt;/p&gt;
&lt;p&gt;NVIDIA partners — including cloud service providers and server makers — submitted great results using the NVIDIA Blackwell and/or Hopper platform. These partners include Azure, Broadcom, Cisco, CoreWeave, Dell Technologies, Giga Computing, HPE, Lambda, Lenovo, Nebius, Oracle, Quanta Cloud Technology, Supermicro and the University of Florida.&lt;/p&gt;
&lt;p&gt;The market-leading inference performance on the NVIDIA AI platform is available from major cloud providers and server makers. This translates to lower TCO and enhanced return on investment for organizations deploying sophisticated AI applications.&lt;/p&gt;
&lt;p&gt;Learn more about these full-stack technologies by reading the NVIDIA Technical Blog on MLPerf Inference v5.1. Plus, visit the NVIDIA DGX Cloud Performance Explorer to learn more about NVIDIA performance, model TCO and generate custom reports.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/grace-corp-blog-gb300-nvl72-1280x680-r1-2.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Inference performance is critical, as it directly influences the economics of an AI factory. The higher the throughput of AI factory infrastructure, the more tokens it can produce at a high speed — increasing revenue, driving down total cost of ownership (TCO) and enhancing the system’s overall productivity.&lt;/p&gt;
&lt;p&gt;Less than half a year since its debut at NVIDIA GTC, the NVIDIA GB300 NVL72 rack-scale system — powered by the NVIDIA Blackwell Ultra architecture — set records on the new reasoning inference benchmark in MLPerf Inference v5.1, delivering up to 45% more DeepSeek-R1 inference throughput compared with NVIDIA Blackwell-based GB200 NVL72 systems.&lt;/p&gt;
&lt;p&gt;Blackwell Ultra builds on the success of the Blackwell architecture, with the Blackwell Ultra architecture featuring 1.5x more NVFP4 AI compute and 2x more attention-layer acceleration than Blackwell, as well as up to 288GB of HBM3e memory per GPU.&lt;/p&gt;
&lt;p&gt;The NVIDIA platform also set performance records on all new data center benchmarks added to the MLPerf Inference v5.1 suite — including DeepSeek-R1, Llama 3.1 405B Interactive, Llama 3.1 8B and Whisper — while continuing to hold per-GPU records on every MLPerf data center benchmark.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Stacking It All Up&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Full-stack co-design plays an important role in delivering these latest benchmark results. Blackwell and Blackwell Ultra incorporate hardware acceleration for the NVFP4 data format — an NVIDIA-designed 4-bit floating point format that provides better accuracy compared with other FP4 formats, as well as comparable accuracy to higher-precision formats.&lt;/p&gt;
&lt;p&gt;NVIDIA TensorRT Model Optimizer software quantized DeepSeek-R1, Llama 3.1 405B, Llama 2 70B and Llama 3.1 8B to NVFP4. In concert with the open-source NVIDIA TensorRT-LLM library, this optimization enabled Blackwell and Blackwell Ultra to deliver higher performance while meeting strict accuracy requirements in submissions.&lt;/p&gt;
&lt;p&gt;Large language model inference consists of two workloads with distinct execution characteristics: 1) context for processing user input to produce the first output token and 2) generation to produce all subsequent output tokens.&lt;/p&gt;
&lt;p&gt;A technique called disaggregated serving splits context and generation tasks so each part can be optimized independently for best overall throughput. This technique was key to record-setting performance on the Llama 3.1 405B Interactive benchmark, helping to deliver a nearly 50% increase in performance per GPU with GB200 NVL72 systems compared with each Blackwell GPU in an NVIDIA DGX B200 server running the benchmark with traditional serving.&lt;/p&gt;
&lt;p&gt;NVIDIA also made its first submissions this round using the NVIDIA Dynamo inference framework.&lt;/p&gt;
&lt;p&gt;NVIDIA partners — including cloud service providers and server makers — submitted great results using the NVIDIA Blackwell and/or Hopper platform. These partners include Azure, Broadcom, Cisco, CoreWeave, Dell Technologies, Giga Computing, HPE, Lambda, Lenovo, Nebius, Oracle, Quanta Cloud Technology, Supermicro and the University of Florida.&lt;/p&gt;
&lt;p&gt;The market-leading inference performance on the NVIDIA AI platform is available from major cloud providers and server makers. This translates to lower TCO and enhanced return on investment for organizations deploying sophisticated AI applications.&lt;/p&gt;
&lt;p&gt;Learn more about these full-stack technologies by reading the NVIDIA Technical Blog on MLPerf Inference v5.1. Plus, visit the NVIDIA DGX Cloud Performance Explorer to learn more about NVIDIA performance, model TCO and generate custom reports.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/mlperf-inference-blackwell-ultra/</guid><pubDate>Tue, 09 Sep 2025 15:00:44 +0000</pubDate></item><item><title>Smart ring maker Oura’s CEO addresses recent backlash, says future is a ‘cloud of wearables’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/09/smart-ring-maker-ouras-ceo-addresses-recent-backlash-says-future-is-a-cloud-of-wearables/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/oura-ceo-hale.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Oura CEO Tom Hale is trying to set the record straight about the smart ring maker’s partnership with the Department of Defense (DoD) and data miner Palantir, which is used by defense, intelligence, and law enforcement agencies in the United States and elsewhere. At the Fortune Brainstorm Tech conference on Monday, Hale’s interview started off with a bang with his outright denial that the company was sharing user data with the government.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There was a lot of misinformation about this,” he said, referring to the numerous influencer-driven reports that led to a viral backlash against the health tracker. Oura’s rings collect information about users’ heart rates, sleep, body temperature, movement, menstrual cycles, and more.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Hale had already gone online to address the misleading reports and subsequent PR backlash, assuring users in his first-ever TikTok video that the company didn’t sell their data to third parties “without your explicit consent.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, he explained that the DoD program Oura is involved in requires the company to run its enterprise solution in a separate, secure environment and that the government does not have access to users’ Oura health data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hale reiterated these points on Monday, saying, “For the record, we will never share your data with anyone unless you direct us to do it. We will never sell your data to anyone ever.” He said the reports spreading online that Oura partnered with the U.S. government to share user data were “simply not true,” and he’s thankful the outrage had begun to calm down. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, he attempted to clear up confusion over the company’s relationship with Palantir, saying that calling it a “partnership” was “a bit of a strong sell.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, Hale explained that Oura had acquired a company last year that had a SaaS (software-as-a-service) relationship with Palantir — meaning a business contract rather than a data-sharing agreement. That relationship was for something called Impact Level 5, or IL5, which is a DoD certification standard for handling sensitive, unclassified data.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a component of their solution. That contract is still running, and that news — that relationship — became blown into a ‘massive partnership’ with Palantir&amp;nbsp;… We have a small commercial relationship. The systems are not connected. There’s no way Palantir has access to your data. No one in the government can see your data. No one at Palantir can see your data. Totally overblown,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hale added that the privacy and security of user data are important to the company and its customers. He also pointed out that Oura’s terms of service state that it will oppose any efforts designed to use user data for surveillance or prosecution purposes. He even noted that when users authorize Oura to examine their data (e.g., for tech support purposes), the person who reviews it has a limited role in the company and can only see specifically what was authorized.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t look at people’s data&amp;nbsp;… you can’t do that,” he said. (Technically, they can — the data isn’t end-to-end encrypted. Data is encrypted in transit between the Oura App and the Oura Cloud using TLS 1.2.) &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The CEO also briefly addressed Oura’s future, observing that the market was shifting — particularly in Asia and India — to smaller, cheaper wrist-borne wearables. Ring wearables, meanwhile, doubled in size.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re growing north of 100%,” Hale noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company sees its potential as becoming a “preventionist” health device, one that alerts users to issues before they become problems that make them sick. This is aided by the fact that Oura rings are designed to give users insights about how their health metrics are evolving. The company also leverages machine intelligence and offers a dedicated health adviser.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Oura does see itself working more with the government, just not in the way that influencers described. Hale said the company partnered with Medicare Advantage to provide rings to eligible patients, for example.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hale also hinted at the possibility of other wearables. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’d be really cool if there was one ring to rule them all, but we know practically that’s not true,” he said. [W]hether it’s metabolic [monitoring], maybe it’s blood pressure, maybe it’s activity, maybe it’s other things — maybe it’s other kinds of metrics that are going to be brought together. So I believe very much that we’ll see a cloud of wearables. And the choice of those wearables will be relevant to the clinical use you’re trying to put it to.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/oura-ceo-hale.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Oura CEO Tom Hale is trying to set the record straight about the smart ring maker’s partnership with the Department of Defense (DoD) and data miner Palantir, which is used by defense, intelligence, and law enforcement agencies in the United States and elsewhere. At the Fortune Brainstorm Tech conference on Monday, Hale’s interview started off with a bang with his outright denial that the company was sharing user data with the government.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There was a lot of misinformation about this,” he said, referring to the numerous influencer-driven reports that led to a viral backlash against the health tracker. Oura’s rings collect information about users’ heart rates, sleep, body temperature, movement, menstrual cycles, and more.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Hale had already gone online to address the misleading reports and subsequent PR backlash, assuring users in his first-ever TikTok video that the company didn’t sell their data to third parties “without your explicit consent.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, he explained that the DoD program Oura is involved in requires the company to run its enterprise solution in a separate, secure environment and that the government does not have access to users’ Oura health data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hale reiterated these points on Monday, saying, “For the record, we will never share your data with anyone unless you direct us to do it. We will never sell your data to anyone ever.” He said the reports spreading online that Oura partnered with the U.S. government to share user data were “simply not true,” and he’s thankful the outrage had begun to calm down. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, he attempted to clear up confusion over the company’s relationship with Palantir, saying that calling it a “partnership” was “a bit of a strong sell.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, Hale explained that Oura had acquired a company last year that had a SaaS (software-as-a-service) relationship with Palantir — meaning a business contract rather than a data-sharing agreement. That relationship was for something called Impact Level 5, or IL5, which is a DoD certification standard for handling sensitive, unclassified data.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a component of their solution. That contract is still running, and that news — that relationship — became blown into a ‘massive partnership’ with Palantir&amp;nbsp;… We have a small commercial relationship. The systems are not connected. There’s no way Palantir has access to your data. No one in the government can see your data. No one at Palantir can see your data. Totally overblown,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hale added that the privacy and security of user data are important to the company and its customers. He also pointed out that Oura’s terms of service state that it will oppose any efforts designed to use user data for surveillance or prosecution purposes. He even noted that when users authorize Oura to examine their data (e.g., for tech support purposes), the person who reviews it has a limited role in the company and can only see specifically what was authorized.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t look at people’s data&amp;nbsp;… you can’t do that,” he said. (Technically, they can — the data isn’t end-to-end encrypted. Data is encrypted in transit between the Oura App and the Oura Cloud using TLS 1.2.) &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The CEO also briefly addressed Oura’s future, observing that the market was shifting — particularly in Asia and India — to smaller, cheaper wrist-borne wearables. Ring wearables, meanwhile, doubled in size.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re growing north of 100%,” Hale noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company sees its potential as becoming a “preventionist” health device, one that alerts users to issues before they become problems that make them sick. This is aided by the fact that Oura rings are designed to give users insights about how their health metrics are evolving. The company also leverages machine intelligence and offers a dedicated health adviser.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Oura does see itself working more with the government, just not in the way that influencers described. Hale said the company partnered with Medicare Advantage to provide rings to eligible patients, for example.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hale also hinted at the possibility of other wearables. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’d be really cool if there was one ring to rule them all, but we know practically that’s not true,” he said. [W]hether it’s metabolic [monitoring], maybe it’s blood pressure, maybe it’s activity, maybe it’s other things — maybe it’s other kinds of metrics that are going to be brought together. So I believe very much that we’ll see a cloud of wearables. And the choice of those wearables will be relevant to the clinical use you’re trying to put it to.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/09/smart-ring-maker-ouras-ceo-addresses-recent-backlash-says-future-is-a-cloud-of-wearables/</guid><pubDate>Tue, 09 Sep 2025 15:37:58 +0000</pubDate></item><item><title>‘Safety First, Always,’ NVIDIA VP of Automotive Says, Unveiling the Future of AI-Defined Vehicles at IAA Mobility (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/iaa-mobility-ai-defined-vehicles/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At this week’s IAA Mobility conference in Munich, NVIDIA Vice President of Automotive Ali Kani outlined how cloud-to-car AI platforms are bringing new levels of safety, intelligence and trust to the road.&lt;/p&gt;
&lt;p&gt;NVIDIA and its partners didn’t just show off cars at the conference — they showed off what cars are becoming: AI-defined machines, built as much in the data center as they are in the factory.&lt;/p&gt;
&lt;p&gt;Kani framed this shift during his IAA keynote today: vehicles are moving from being dependent on horsepower to compute power, from mechanical systems to software stacks.&lt;/p&gt;
&lt;p&gt;In Germany and around the world, automotive engineering is now infused with silicon acceleration, as automakers and suppliers adopt NVIDIA’s cloud-to-car platform to drive safety, intelligence and efficiency into tomorrow’s vehicles.&lt;/p&gt;
&lt;p&gt;NVIDIA is the only company that offers an end-to-end compute stack for autonomous driving. Its three AI compute platforms critical for autonomy are:&lt;/p&gt;

&lt;p&gt;Together, these platforms form a feedback loop for learning, testing and deployment that tightens the cycle of innovation while keeping safety front and center.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;It’s All About Safety: NVIDIA Halos Sets the Standard&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Safety is a core theme at IAA. NVIDIA Halos is a full-stack, comprehensive safety system that unifies vehicle architecture, AI models, chips, software, tools and services to ensure the safe development of autonomous vehicles, from cloud to car.&lt;/p&gt;
&lt;p&gt;NVIDIA Halos brings together safety-assessed systems-on-a-chip, the safety-certified NVIDIA DriveOS operating system and the DRIVE AGX Hyperion architecture into a unified platform for autonomous driving. This platform is backed by the NVIDIA Halos Certified Program and its AI Systems Inspection Lab, which deliver rigorous validation to ensure real-time AI operates with end-to-end reliability.&lt;/p&gt;
&lt;p&gt;With AI-driven workflows and high-fidelity sensor simulations built with NVIDIA Omniverse and Cosmos, automakers can train, test and safely validate vehicle performance — even under conditions that are hard to experiment with in the real world, such as rare or hazardous traffic situations and edge-case events, and in complex environments.&lt;/p&gt;
&lt;p&gt;Simulation tools are increasingly critical for advancing safe, scalable autonomous vehicle development.&lt;/p&gt;
&lt;p&gt;The popular autonomous driving simulator CARLA now integrates the NVIDIA Cosmos Transfer world foundation model, along with NVIDIA Omniverse NuRec reconstruction libraries, to bring diverse, high-fidelity simulations directly into autonomous vehicle testing pipelines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Capgemini&lt;/strong&gt; and &lt;strong&gt;TCS&lt;/strong&gt; are already tapping into this integration to expand their simulation capabilities and push the boundaries of software-defined vehicle development.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Expanding the Ecosystem: Automotive Leaders Embrace Cloud-to-Car AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Automotive leaders are embracing NVIDIA’s cloud-to-car AI platform to transform their next-generation vehicles.&lt;b&gt;&amp;nbsp;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lucid&lt;/strong&gt; headlined the IAA showcase with its all-electric Lucid Gravity SUV, which is accelerated by the NVIDIA DRIVE AGX platform, uses the NVIDIA Blackwell architecture and operates on NVIDIA DriveOS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mercedes-Benz&lt;/strong&gt; introduced its all-new GLC with EQ-technology and announced expansions to its CLA family with the first fully electric shooting brake — all built on NVIDIA AI, DRIVE AV software and accelerated compute.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-84663 size-large" height="1120" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/mercedes-benz-iaa-2025-1680x1120.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lotus&lt;/strong&gt; is featuring the all-electric Eletre SUV, the hyper-GT Emeya and the Theory 1 concept — all accelerated by NVIDIA DRIVE AGX to deliver high-performance, AI-driven functions for intelligent and safer mobility.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-84666 size-full" height="619" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/lotus-iaa-2025.jpg" width="1100" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ZYT&lt;/strong&gt; is showcasing its autonomous vehicle software platforms built on NVIDIA DRIVE AGX, highlighting how this advanced technology accelerates safer and smarter mobility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Volvo Cars&lt;/strong&gt; highlighted its ES90 Single Motor Extended Range Ultra and EX90 Twin Motor Performance Ultra models, equipped with enhanced safety and driver-assistance capabilities and powered by NVIDIA DRIVE AGX and DriveOS for improved AI performance and safety.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-84669 size-full" height="900" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/volvo-cars-iaa-2025.jpg" width="1350" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;XPENG &lt;/b&gt;&lt;span&gt;showcased how NVIDIA DRIVE AGX underpins its G6, G9 and X9 models, delivering XPILOT smart driving assistance, advanced autonomy and intelligent cockpit features.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Global Tech Leaders Accelerate Software-Defined Vehicles With NVIDIA&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Beyond automakers, technology leaders across the globe are building on NVIDIA AI to accelerate the development of software-defined vehicles.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MediaTek&lt;/strong&gt; is working closely with NVIDIA to bring GPU-powered intelligence into its Dimensity Auto Cockpit solutions, enabling advanced in-car experiences through premium graphics and intelligent assistants.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ThunderSoft&lt;/strong&gt; introduced its new AI Box built on DRIVE AGX, designed to run large-scale AI models for intelligent cockpits. The AI Box is complete with personalized copilots, safety monitoring and immersive cabin experiences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cerence&lt;/strong&gt; is presenting its xUI AI assistant at IAA, built on CaLLM models and running on NVIDIA DRIVE AGX with DriveOS. With NVIDIA NeMo Guardrails, it ensures safe, context-aware, brand-specific voice interactions across both edge and cloud.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ZF Group&lt;/strong&gt; is showcasing its ProAI supercomputer accelerated by NVIDIA DRIVE AGX. The supercomputer unifies advanced driver-assistance systems, automated driving or chassis control into a scalable architecture to unlock capabilities, from entry-level deployments to full autonomy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RoboSense&lt;/strong&gt; is integrating its high-performance automotive-grade digital lidar with the DRIVE AGX platform, enhancing system performance, while Desay SV is showcasing its NVIDIA DRIVE Thor-based domain controller, a next-generation smart mobility solution shaped by advanced AI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Magna&lt;/strong&gt; is showcasing its future-ready, centralized advanced driver-assistance system platform designed for flexibility and scalability. This advanced system integrates a comprehensive suite of sensors, accelerated by NVIDIA DRIVE AGX Thor.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Watch &lt;/i&gt;&lt;i&gt;Kani’s IAA keynote&lt;/i&gt;&lt;i&gt; to see how NVIDIA is accelerating the future of autonomous driving with a cloud-to-car platform. &lt;/i&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;i&gt;Learn more about NVIDIA’s work in &lt;/i&gt;&lt;i&gt;autonomous vehicles&lt;/i&gt; and the &lt;i&gt;NVIDIA automotive partner ecosystem&lt;/i&gt;&lt;i&gt;. &lt;/i&gt;&lt;i&gt;Follow NVIDIA DRIVE on&lt;/i&gt;&lt;i&gt; LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At this week’s IAA Mobility conference in Munich, NVIDIA Vice President of Automotive Ali Kani outlined how cloud-to-car AI platforms are bringing new levels of safety, intelligence and trust to the road.&lt;/p&gt;
&lt;p&gt;NVIDIA and its partners didn’t just show off cars at the conference — they showed off what cars are becoming: AI-defined machines, built as much in the data center as they are in the factory.&lt;/p&gt;
&lt;p&gt;Kani framed this shift during his IAA keynote today: vehicles are moving from being dependent on horsepower to compute power, from mechanical systems to software stacks.&lt;/p&gt;
&lt;p&gt;In Germany and around the world, automotive engineering is now infused with silicon acceleration, as automakers and suppliers adopt NVIDIA’s cloud-to-car platform to drive safety, intelligence and efficiency into tomorrow’s vehicles.&lt;/p&gt;
&lt;p&gt;NVIDIA is the only company that offers an end-to-end compute stack for autonomous driving. Its three AI compute platforms critical for autonomy are:&lt;/p&gt;

&lt;p&gt;Together, these platforms form a feedback loop for learning, testing and deployment that tightens the cycle of innovation while keeping safety front and center.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;It’s All About Safety: NVIDIA Halos Sets the Standard&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Safety is a core theme at IAA. NVIDIA Halos is a full-stack, comprehensive safety system that unifies vehicle architecture, AI models, chips, software, tools and services to ensure the safe development of autonomous vehicles, from cloud to car.&lt;/p&gt;
&lt;p&gt;NVIDIA Halos brings together safety-assessed systems-on-a-chip, the safety-certified NVIDIA DriveOS operating system and the DRIVE AGX Hyperion architecture into a unified platform for autonomous driving. This platform is backed by the NVIDIA Halos Certified Program and its AI Systems Inspection Lab, which deliver rigorous validation to ensure real-time AI operates with end-to-end reliability.&lt;/p&gt;
&lt;p&gt;With AI-driven workflows and high-fidelity sensor simulations built with NVIDIA Omniverse and Cosmos, automakers can train, test and safely validate vehicle performance — even under conditions that are hard to experiment with in the real world, such as rare or hazardous traffic situations and edge-case events, and in complex environments.&lt;/p&gt;
&lt;p&gt;Simulation tools are increasingly critical for advancing safe, scalable autonomous vehicle development.&lt;/p&gt;
&lt;p&gt;The popular autonomous driving simulator CARLA now integrates the NVIDIA Cosmos Transfer world foundation model, along with NVIDIA Omniverse NuRec reconstruction libraries, to bring diverse, high-fidelity simulations directly into autonomous vehicle testing pipelines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Capgemini&lt;/strong&gt; and &lt;strong&gt;TCS&lt;/strong&gt; are already tapping into this integration to expand their simulation capabilities and push the boundaries of software-defined vehicle development.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Expanding the Ecosystem: Automotive Leaders Embrace Cloud-to-Car AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Automotive leaders are embracing NVIDIA’s cloud-to-car AI platform to transform their next-generation vehicles.&lt;b&gt;&amp;nbsp;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lucid&lt;/strong&gt; headlined the IAA showcase with its all-electric Lucid Gravity SUV, which is accelerated by the NVIDIA DRIVE AGX platform, uses the NVIDIA Blackwell architecture and operates on NVIDIA DriveOS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mercedes-Benz&lt;/strong&gt; introduced its all-new GLC with EQ-technology and announced expansions to its CLA family with the first fully electric shooting brake — all built on NVIDIA AI, DRIVE AV software and accelerated compute.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-84663 size-large" height="1120" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/mercedes-benz-iaa-2025-1680x1120.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lotus&lt;/strong&gt; is featuring the all-electric Eletre SUV, the hyper-GT Emeya and the Theory 1 concept — all accelerated by NVIDIA DRIVE AGX to deliver high-performance, AI-driven functions for intelligent and safer mobility.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-84666 size-full" height="619" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/lotus-iaa-2025.jpg" width="1100" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ZYT&lt;/strong&gt; is showcasing its autonomous vehicle software platforms built on NVIDIA DRIVE AGX, highlighting how this advanced technology accelerates safer and smarter mobility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Volvo Cars&lt;/strong&gt; highlighted its ES90 Single Motor Extended Range Ultra and EX90 Twin Motor Performance Ultra models, equipped with enhanced safety and driver-assistance capabilities and powered by NVIDIA DRIVE AGX and DriveOS for improved AI performance and safety.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-84669 size-full" height="900" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/volvo-cars-iaa-2025.jpg" width="1350" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;XPENG &lt;/b&gt;&lt;span&gt;showcased how NVIDIA DRIVE AGX underpins its G6, G9 and X9 models, delivering XPILOT smart driving assistance, advanced autonomy and intelligent cockpit features.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Global Tech Leaders Accelerate Software-Defined Vehicles With NVIDIA&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Beyond automakers, technology leaders across the globe are building on NVIDIA AI to accelerate the development of software-defined vehicles.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MediaTek&lt;/strong&gt; is working closely with NVIDIA to bring GPU-powered intelligence into its Dimensity Auto Cockpit solutions, enabling advanced in-car experiences through premium graphics and intelligent assistants.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ThunderSoft&lt;/strong&gt; introduced its new AI Box built on DRIVE AGX, designed to run large-scale AI models for intelligent cockpits. The AI Box is complete with personalized copilots, safety monitoring and immersive cabin experiences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cerence&lt;/strong&gt; is presenting its xUI AI assistant at IAA, built on CaLLM models and running on NVIDIA DRIVE AGX with DriveOS. With NVIDIA NeMo Guardrails, it ensures safe, context-aware, brand-specific voice interactions across both edge and cloud.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ZF Group&lt;/strong&gt; is showcasing its ProAI supercomputer accelerated by NVIDIA DRIVE AGX. The supercomputer unifies advanced driver-assistance systems, automated driving or chassis control into a scalable architecture to unlock capabilities, from entry-level deployments to full autonomy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RoboSense&lt;/strong&gt; is integrating its high-performance automotive-grade digital lidar with the DRIVE AGX platform, enhancing system performance, while Desay SV is showcasing its NVIDIA DRIVE Thor-based domain controller, a next-generation smart mobility solution shaped by advanced AI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Magna&lt;/strong&gt; is showcasing its future-ready, centralized advanced driver-assistance system platform designed for flexibility and scalability. This advanced system integrates a comprehensive suite of sensors, accelerated by NVIDIA DRIVE AGX Thor.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Watch &lt;/i&gt;&lt;i&gt;Kani’s IAA keynote&lt;/i&gt;&lt;i&gt; to see how NVIDIA is accelerating the future of autonomous driving with a cloud-to-car platform. &lt;/i&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;i&gt;Learn more about NVIDIA’s work in &lt;/i&gt;&lt;i&gt;autonomous vehicles&lt;/i&gt; and the &lt;i&gt;NVIDIA automotive partner ecosystem&lt;/i&gt;&lt;i&gt;. &lt;/i&gt;&lt;i&gt;Follow NVIDIA DRIVE on&lt;/i&gt;&lt;i&gt; LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/iaa-mobility-ai-defined-vehicles/</guid><pubDate>Tue, 09 Sep 2025 16:00:22 +0000</pubDate></item><item><title>Judge: Anthropic’s $1.5B settlement is being shoved “down the throat of authors” (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/09/judge-anthropics-1-5b-settlement-is-being-shoved-down-the-throat-of-authors/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Feeling “misled,” judge refuses to rubber-stamp Anthropic's proposed settlement.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2206295220-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2206295220-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          SOPA Images / Contributor | LightRocket

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;At a hearing Monday, US District Judge William Alsup blasted a proposed $1.5 billion settlement over Anthropic's rampant piracy of books to train AI.&lt;/p&gt;
&lt;p&gt;The proposed settlement comes in a case where Anthropic could have owed more than $1 trillion in damages after Alsup certified a class that included up to 7 million claimants whose works were illegally downloaded by the AI company.&lt;/p&gt;
&lt;p&gt;Instead, critics fear Anthropic will get off cheaply, striking a deal with authors suing that covers less than 500,000 works and paying a small fraction of its total valuation (currently $183 billion) to get away with the massive theft. Defector noted that the settlement doesn't even require Anthropic to admit wrongdoing, while the company continues raising billions based on models trained on authors' works. Most recently, Anthropic raised $13 billion in a funding round, making back about 10 times the proposed settlement amount after announcing the deal.&lt;/p&gt;
&lt;p&gt;Alsup expressed grave concerns that lawyers rushed the deal, which he said now risks being shoved "down the throat of authors," Bloomberg Law reported.&lt;/p&gt;
&lt;p&gt;In an order, Alsup clarified why he thought the proposed settlement was a chaotic mess. The judge said he was "disappointed that counsel have left important questions to be answered in the future," seeking approval for the settlement despite the Works List, the Class List, the Claim Form, and the process for notification, allocation, and dispute resolution all remaining unresolved.&lt;/p&gt;
&lt;p&gt;Denying preliminary approval of the settlement, Alsup suggested that the agreement is "nowhere close to complete," forcing Anthropic and authors' lawyers to "recalibrate" the largest publicly reported copyright class-action settlement ever inked, Bloomberg reported.&lt;/p&gt;
&lt;p&gt;Of particular concern, the settlement failed to outline how disbursements would be managed for works with multiple claimants, Alsup noted. Until all these details are ironed out, Alsup intends to withhold approval, the order said.&lt;/p&gt;
&lt;p&gt;One big change the judge wants to see is the addition of instructions requiring "anyone with copyright ownership" to opt in, with the consequence that the work won't be covered if even one rights holder opts out, Bloomberg reported. There should also be instruction that any disputes over ownership or submitted claims should be settled in state court, Alsup said.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;To Alsup, the settlement likely risks setting up a future where courts are bogged down over disputes linked to the class action for years. That's perhaps a bigger concern if many authors and publishers miss out on filing claims or receiving payments, since the judge noted that class members frequently "get the shaft" in class actions where attorneys stop caring after monetary relief is granted, Bloomberg reported. Further, Alsup is worried that an improper notification scheme could leave Anthropic in a vulnerable position, facing future claimants "coming out of the woodwork later," Bloomberg reported, despite doling out more than $1 billion.&lt;/p&gt;
&lt;p&gt;"When they pay that kind of money, they’re going to get the relief in the form of a clean bill of health going forward," Alsup said at the hearing, suggesting that the settlement must get Anthropic completely off the hook for future legal claims over the AI training piracy. Warning class counsel that&amp;nbsp;he felt "misled," the judge asked for more information about the claims process, noting, "I have an uneasy feeling about hangers-on with all this money on the table."&lt;/p&gt;
&lt;p&gt;Following the hearing, the judge set a schedule to ensure that lists of covered works and class members would be finalized by September 15, followed by the claims process finalized by September 25. That schedule would position the court to potentially preliminarily approve the settlement by October 10, Alsup suggested.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Why the deal only covers about 500,000 works&lt;/h2&gt;
&lt;p&gt;As of this writing, the list of covered works spans about 465,000, Alsup said.&lt;/p&gt;
&lt;p&gt;That's a far cry from the 7 million works that he initially certified as covered in the class. A breakdown from the Authors Guild—which consulted on the case and is part of a Working Group helping to allocate claims of $3,000 per work to authors and publishers—explained that "after accounting for the many duplicates," foreign editions, unregistered works, and books missing class criteria, "only approximately 500,000 titles meet the definition required to be part of the class."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But duplicate downloads and other missing criteria don't explain why the payout per work seems so small, and that's a problem for authors who want higher payouts since this settlement could become a template in other cases where AI companies are accused of pirating works for AI training.&lt;/p&gt;
&lt;p&gt;According to the Authors Guild, "the Copyright Act gives courts discretion to award statutory damages of at least $750 and no more than $150,000 per infringed work when the infringement is willful, as is the case here."&lt;/p&gt;
&lt;p&gt;However, "when there are a large number of works at issue," it's rare that courts award maximum damages, the group said. Hoping to avoid a dragged-out legal battle that "could tie up the case for years," authors suing Anthropic settled on "a strong payout without the risks of trial," the Authors Guild said. Going that route, they supposedly "avoided years of delay through appeals" and "achieved a certain, immediate result that sends a powerful signal to the industry that piracy will cost you a lot," the Authors Guild suggested. The settlement will also likely serve to push more AI companies to avoid piracy and actually pay to license content for training, the group said.&lt;/p&gt;
&lt;p&gt;The Authors Guild confirmed that once the list is finalized, likely by October 10, a searchable database will be created for authors to confirm if their works are covered. Until then, authors can submit contact information through a website set up to manage the settlement process. That will ensure that authors are notified when the claims process begins, which, if the settlement is ultimately approved, will likely happen this fall, the Authors Guild said.&lt;/p&gt;
&lt;p&gt;"If your book is included in the class list, you will receive a formal notice by mail or email from the settlement administrator," the group said. "The notice will explain the terms of the settlement, your rights, and next steps. The Authors Guild will also share information to help authors understand the process."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Feeling “misled,” judge refuses to rubber-stamp Anthropic's proposed settlement.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2206295220-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2206295220-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          SOPA Images / Contributor | LightRocket

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;At a hearing Monday, US District Judge William Alsup blasted a proposed $1.5 billion settlement over Anthropic's rampant piracy of books to train AI.&lt;/p&gt;
&lt;p&gt;The proposed settlement comes in a case where Anthropic could have owed more than $1 trillion in damages after Alsup certified a class that included up to 7 million claimants whose works were illegally downloaded by the AI company.&lt;/p&gt;
&lt;p&gt;Instead, critics fear Anthropic will get off cheaply, striking a deal with authors suing that covers less than 500,000 works and paying a small fraction of its total valuation (currently $183 billion) to get away with the massive theft. Defector noted that the settlement doesn't even require Anthropic to admit wrongdoing, while the company continues raising billions based on models trained on authors' works. Most recently, Anthropic raised $13 billion in a funding round, making back about 10 times the proposed settlement amount after announcing the deal.&lt;/p&gt;
&lt;p&gt;Alsup expressed grave concerns that lawyers rushed the deal, which he said now risks being shoved "down the throat of authors," Bloomberg Law reported.&lt;/p&gt;
&lt;p&gt;In an order, Alsup clarified why he thought the proposed settlement was a chaotic mess. The judge said he was "disappointed that counsel have left important questions to be answered in the future," seeking approval for the settlement despite the Works List, the Class List, the Claim Form, and the process for notification, allocation, and dispute resolution all remaining unresolved.&lt;/p&gt;
&lt;p&gt;Denying preliminary approval of the settlement, Alsup suggested that the agreement is "nowhere close to complete," forcing Anthropic and authors' lawyers to "recalibrate" the largest publicly reported copyright class-action settlement ever inked, Bloomberg reported.&lt;/p&gt;
&lt;p&gt;Of particular concern, the settlement failed to outline how disbursements would be managed for works with multiple claimants, Alsup noted. Until all these details are ironed out, Alsup intends to withhold approval, the order said.&lt;/p&gt;
&lt;p&gt;One big change the judge wants to see is the addition of instructions requiring "anyone with copyright ownership" to opt in, with the consequence that the work won't be covered if even one rights holder opts out, Bloomberg reported. There should also be instruction that any disputes over ownership or submitted claims should be settled in state court, Alsup said.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;To Alsup, the settlement likely risks setting up a future where courts are bogged down over disputes linked to the class action for years. That's perhaps a bigger concern if many authors and publishers miss out on filing claims or receiving payments, since the judge noted that class members frequently "get the shaft" in class actions where attorneys stop caring after monetary relief is granted, Bloomberg reported. Further, Alsup is worried that an improper notification scheme could leave Anthropic in a vulnerable position, facing future claimants "coming out of the woodwork later," Bloomberg reported, despite doling out more than $1 billion.&lt;/p&gt;
&lt;p&gt;"When they pay that kind of money, they’re going to get the relief in the form of a clean bill of health going forward," Alsup said at the hearing, suggesting that the settlement must get Anthropic completely off the hook for future legal claims over the AI training piracy. Warning class counsel that&amp;nbsp;he felt "misled," the judge asked for more information about the claims process, noting, "I have an uneasy feeling about hangers-on with all this money on the table."&lt;/p&gt;
&lt;p&gt;Following the hearing, the judge set a schedule to ensure that lists of covered works and class members would be finalized by September 15, followed by the claims process finalized by September 25. That schedule would position the court to potentially preliminarily approve the settlement by October 10, Alsup suggested.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Why the deal only covers about 500,000 works&lt;/h2&gt;
&lt;p&gt;As of this writing, the list of covered works spans about 465,000, Alsup said.&lt;/p&gt;
&lt;p&gt;That's a far cry from the 7 million works that he initially certified as covered in the class. A breakdown from the Authors Guild—which consulted on the case and is part of a Working Group helping to allocate claims of $3,000 per work to authors and publishers—explained that "after accounting for the many duplicates," foreign editions, unregistered works, and books missing class criteria, "only approximately 500,000 titles meet the definition required to be part of the class."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But duplicate downloads and other missing criteria don't explain why the payout per work seems so small, and that's a problem for authors who want higher payouts since this settlement could become a template in other cases where AI companies are accused of pirating works for AI training.&lt;/p&gt;
&lt;p&gt;According to the Authors Guild, "the Copyright Act gives courts discretion to award statutory damages of at least $750 and no more than $150,000 per infringed work when the infringement is willful, as is the case here."&lt;/p&gt;
&lt;p&gt;However, "when there are a large number of works at issue," it's rare that courts award maximum damages, the group said. Hoping to avoid a dragged-out legal battle that "could tie up the case for years," authors suing Anthropic settled on "a strong payout without the risks of trial," the Authors Guild said. Going that route, they supposedly "avoided years of delay through appeals" and "achieved a certain, immediate result that sends a powerful signal to the industry that piracy will cost you a lot," the Authors Guild suggested. The settlement will also likely serve to push more AI companies to avoid piracy and actually pay to license content for training, the group said.&lt;/p&gt;
&lt;p&gt;The Authors Guild confirmed that once the list is finalized, likely by October 10, a searchable database will be created for authors to confirm if their works are covered. Until then, authors can submit contact information through a website set up to manage the settlement process. That will ensure that authors are notified when the claims process begins, which, if the settlement is ultimately approved, will likely happen this fall, the Authors Guild said.&lt;/p&gt;
&lt;p&gt;"If your book is included in the class list, you will receive a formal notice by mail or email from the settlement administrator," the group said. "The notice will explain the terms of the settlement, your rights, and next steps. The Authors Guild will also share information to help authors understand the process."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/09/judge-anthropics-1-5b-settlement-is-being-shoved-down-the-throat-of-authors/</guid><pubDate>Tue, 09 Sep 2025 16:21:02 +0000</pubDate></item><item><title>Nvidia unveils new GPU designed for long-context inference (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/09/nvidia-unveils-new-gpu-designed-for-long-context-inference/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205210966.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At the AI Infrastructure Summit on Tuesday, Nvidia announced a new GPU called the Rubin CPX, designed for context windows larger than 1 million tokens.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Part of the chip giant’s forthcoming Rubin series, the CPX is optimized for processing large sequences of context and is meant to be used as part of a broader “disaggregated inference” infrastructure approach. For users, the result will be better performance on long-context tasks like video generation or software development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nvidia’s relentless development cycle has resulted in enormous profits for the company, which brought in $41.1 billion in data center sales in its most recent quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Rubin CPX is slated to be available at the end of 2026.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205210966.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At the AI Infrastructure Summit on Tuesday, Nvidia announced a new GPU called the Rubin CPX, designed for context windows larger than 1 million tokens.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Part of the chip giant’s forthcoming Rubin series, the CPX is optimized for processing large sequences of context and is meant to be used as part of a broader “disaggregated inference” infrastructure approach. For users, the result will be better performance on long-context tasks like video generation or software development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nvidia’s relentless development cycle has resulted in enormous profits for the company, which brought in $41.1 billion in data center sales in its most recent quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Rubin CPX is slated to be available at the end of 2026.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/09/nvidia-unveils-new-gpu-designed-for-long-context-inference/</guid><pubDate>Tue, 09 Sep 2025 16:35:47 +0000</pubDate></item><item><title>Accelerating scientific discovery with AI-powered empirical software (The latest research from Google)</title><link>https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;How it works&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;The input to our system is a scorable task, which includes a problem description, a scoring metric, and data suitable for training, validation, and evaluation. A user can also provide context, such as ideas from external literature, or directives for methodologies to prioritize.&lt;/p&gt;&lt;p&gt;The system then generates research ideas, including programmatic reproduction, optimization, and recombination of known methods, leading to novel and highly performant approaches. Ideas are implemented as executable code and the system uses a tree search strategy with an upper confidence bound (inspired by AlphaZero) to create a tree of software candidates and decide which candidates warrant further exploration. It then uses an LLM to rewrite the code to attempt to improve its quality score, and can exhaustively and tirelessly carry out solution searches at an unprecedented scale, identifying high-quality solutions quickly, reducing exploration time from months to hours or days. Its outputs, as coded solutions, are verifiable, interpretable and reproducible.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;How it works&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;The input to our system is a scorable task, which includes a problem description, a scoring metric, and data suitable for training, validation, and evaluation. A user can also provide context, such as ideas from external literature, or directives for methodologies to prioritize.&lt;/p&gt;&lt;p&gt;The system then generates research ideas, including programmatic reproduction, optimization, and recombination of known methods, leading to novel and highly performant approaches. Ideas are implemented as executable code and the system uses a tree search strategy with an upper confidence bound (inspired by AlphaZero) to create a tree of software candidates and decide which candidates warrant further exploration. It then uses an LLM to rewrite the code to attempt to improve its quality score, and can exhaustively and tirelessly carry out solution searches at an unprecedented scale, identifying high-quality solutions quickly, reducing exploration time from months to hours or days. Its outputs, as coded solutions, are verifiable, interpretable and reproducible.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/</guid><pubDate>Tue, 09 Sep 2025 17:08:01 +0000</pubDate></item><item><title>[NEW] Watch Apple’s AirPod Pro 3 live translation feature in action (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/video/watch-apples-airpod-pro-3-live-translation-feature-in-action/</link><description>&lt;img alt="Event Logo" class="rightrail-promo__logo" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_Disrupt-Color.png" width="99" /&gt;
				&lt;div class="rightrail-promo__date-location"&gt;
							&lt;p&gt;October 27-29, 2025&lt;/p&gt;
										&lt;p&gt;San Francisco&lt;/p&gt;
					&lt;/div&gt;
					&lt;p class="rightrail-promo__description"&gt;&lt;em&gt;&lt;strong&gt;Founders:&lt;/strong&gt;&lt;/em&gt; land your investor and sharpen your pitch. &lt;em&gt;&lt;strong&gt;Investors:&lt;/strong&gt;&lt;/em&gt; discover your next breakout startup. &lt;em&gt;&lt;strong&gt;Innovators:&lt;/strong&gt;&lt;/em&gt; claim a front-row seat to the future. Join 10,000+ tech leaders at the epicenter of innovation. Register now and save up to $668.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Regular Bird rates end September 26&lt;/strong&gt;&lt;/p&gt;</description><content:encoded>&lt;img alt="Event Logo" class="rightrail-promo__logo" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_Disrupt-Color.png" width="99" /&gt;
				&lt;div class="rightrail-promo__date-location"&gt;
							&lt;p&gt;October 27-29, 2025&lt;/p&gt;
										&lt;p&gt;San Francisco&lt;/p&gt;
					&lt;/div&gt;
					&lt;p class="rightrail-promo__description"&gt;&lt;em&gt;&lt;strong&gt;Founders:&lt;/strong&gt;&lt;/em&gt; land your investor and sharpen your pitch. &lt;em&gt;&lt;strong&gt;Investors:&lt;/strong&gt;&lt;/em&gt; discover your next breakout startup. &lt;em&gt;&lt;strong&gt;Innovators:&lt;/strong&gt;&lt;/em&gt; claim a front-row seat to the future. Join 10,000+ tech leaders at the epicenter of innovation. Register now and save up to $668.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Regular Bird rates end September 26&lt;/strong&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/video/watch-apples-airpod-pro-3-live-translation-feature-in-action/</guid><pubDate>Tue, 09 Sep 2025 17:45:00 +0000</pubDate></item><item><title>[NEW] Apple’s new iPhone 17 devices don’t have an AI-powered Siri yet. It doesn’t matter. (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/09/apples-new-iphone-17-devices-dont-have-an-ai-powered-siri-yet-it-doesnt-matter/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/iphone-17-4-up.jpg?resize=1200,662" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At yet another splashy event, Apple on Tuesday introduced its latest lineup of iPhones: the iPhone 17, 17 Pro and 17 Pro Max, and a new slimmer version dubbed the iPhone Air. The “Air” branding is meant to bring to mind other lightweight — and sometimes less expensive — Apple products like the MacBook Air and iPad Air. But it also recalls a time when smartphone makers were chasing an ever-thinner phone. In the AI era, however, it’s not necessarily the device’s size that matters; it’s what the software it runs can do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On this front, Apple has lagged its competitors.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At its iPhone 17 event, the company only referenced AI technology a few times: to rehash some updates announced in June at WWDC, like Visual Intelligence and its on-device models, and in some aspects of its camera upgrades, like the iPhone 17’s front camera, which it calls Center Stage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most compelling use of AI wasn’t even introduced as a phone upgrade; it was the AI-powered Live Translation feature coming to Apple’s AirPods 3.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There was no mention of Siri at all, AI-powered or otherwise. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Much has been made about how Apple’s miscalculation on AI could negatively affect its industry standing and future success.&amp;nbsp;Meanwhile, Google last month rolled out its latest release of an AI-powered Android phone with its Pixel 10, as iPhone owners still await an AI Siri, which has been delayed until 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, Apple has only released what could be considered baseline AI features for its devices, like AI writing tools, summarization, generative AI images (which some complain are not very good), live translation, visual search, and Genmoji, among others. Yet a digital assistant that understands a wide range of questions — without deferring to ChatGPT — or one that can provide further context from your iPhone apps remains overdue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Recently, it was reported that Apple is looking to third parties to help it catch up in the AI race. An AI-enhanced Siri could be running some other technology — like Google Gemini — under the hood.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At first glance, this delay, combined with the decision to rely on a third party — or even possibly a sizable acquisition — seems like it could spell bad news for Apple. However, Apple’s decision to outsource some of the phone’s AI technology could actually become a selling point for consumers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Today’s iPhone owners often swap out Apple’s technology for Google’s by opting for Gmail, Google Drive and Docs, Google Maps, and Chrome over Apple’s own apps like Mail, its iWork suite, Apple Maps, and Safari, for example. When people search the web, they turn to Google’s Search app, not Apple’s built-in Spotlight search, despite its many integrations over the years to offer basic facts and answers, leveraging sources like Wikipedia.&amp;nbsp;Why, then, shouldn’t they be able to use Google’s AI technology, too?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If Apple does proceed with a third-party deal to integrate AI into its devices, it may work out to be an even bigger win for iPhone owners. It would mean that high-performing AI technology would be integrated into the device more natively. It would feel more seamless, more a part of the iPhone experience itself than simply running an AI app. And Apple could get there without having to invest as heavily in the infrastructure required to compete in the AI race, which is good for the company’s (already healthy) bottom line.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, given the speed with which AI technology has been evolving, this design would leave room for Apple to swap out models or expand support to include others, as AI companies edge themselves ahead of others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The result for consumers would be the best of both worlds: the aesthetics and hardware quality (thinness and all!) of the iPhone, with Google’s technology (or Anthropic’s or OpenAI’s) powering some of the key AI components.&amp;nbsp;That could also be beneficial to Apple’s overall brand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It also means that the look and feel of updated iPhones and their hardware advances will continue to drive sales and upgrades, allowing Apple to do what it does best: focus on build quality, camera improvements, privacy-preserving tech, intentional software design changes like Liquid Glass — and yes, super-thin phones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple could continue to market itself as a best-in-class hardware maker first, not an AI device maker; customers could still long for the latest iPhones, as always, without having to sacrifice the latest technology advances when they make the choice to buy a phone from Apple.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, this scenario only plays out if and when Apple opts to launch a version of Siri that runs a third-party’s AI technology to enhance its own (or if it buys an AI company). But if Apple decides to only rely on its Apple Intelligence offerings without getting them up to speed quickly, the outcome could be much different.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/iphone-17-4-up.jpg?resize=1200,662" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At yet another splashy event, Apple on Tuesday introduced its latest lineup of iPhones: the iPhone 17, 17 Pro and 17 Pro Max, and a new slimmer version dubbed the iPhone Air. The “Air” branding is meant to bring to mind other lightweight — and sometimes less expensive — Apple products like the MacBook Air and iPad Air. But it also recalls a time when smartphone makers were chasing an ever-thinner phone. In the AI era, however, it’s not necessarily the device’s size that matters; it’s what the software it runs can do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On this front, Apple has lagged its competitors.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At its iPhone 17 event, the company only referenced AI technology a few times: to rehash some updates announced in June at WWDC, like Visual Intelligence and its on-device models, and in some aspects of its camera upgrades, like the iPhone 17’s front camera, which it calls Center Stage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most compelling use of AI wasn’t even introduced as a phone upgrade; it was the AI-powered Live Translation feature coming to Apple’s AirPods 3.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There was no mention of Siri at all, AI-powered or otherwise. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Much has been made about how Apple’s miscalculation on AI could negatively affect its industry standing and future success.&amp;nbsp;Meanwhile, Google last month rolled out its latest release of an AI-powered Android phone with its Pixel 10, as iPhone owners still await an AI Siri, which has been delayed until 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, Apple has only released what could be considered baseline AI features for its devices, like AI writing tools, summarization, generative AI images (which some complain are not very good), live translation, visual search, and Genmoji, among others. Yet a digital assistant that understands a wide range of questions — without deferring to ChatGPT — or one that can provide further context from your iPhone apps remains overdue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Recently, it was reported that Apple is looking to third parties to help it catch up in the AI race. An AI-enhanced Siri could be running some other technology — like Google Gemini — under the hood.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At first glance, this delay, combined with the decision to rely on a third party — or even possibly a sizable acquisition — seems like it could spell bad news for Apple. However, Apple’s decision to outsource some of the phone’s AI technology could actually become a selling point for consumers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Today’s iPhone owners often swap out Apple’s technology for Google’s by opting for Gmail, Google Drive and Docs, Google Maps, and Chrome over Apple’s own apps like Mail, its iWork suite, Apple Maps, and Safari, for example. When people search the web, they turn to Google’s Search app, not Apple’s built-in Spotlight search, despite its many integrations over the years to offer basic facts and answers, leveraging sources like Wikipedia.&amp;nbsp;Why, then, shouldn’t they be able to use Google’s AI technology, too?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If Apple does proceed with a third-party deal to integrate AI into its devices, it may work out to be an even bigger win for iPhone owners. It would mean that high-performing AI technology would be integrated into the device more natively. It would feel more seamless, more a part of the iPhone experience itself than simply running an AI app. And Apple could get there without having to invest as heavily in the infrastructure required to compete in the AI race, which is good for the company’s (already healthy) bottom line.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, given the speed with which AI technology has been evolving, this design would leave room for Apple to swap out models or expand support to include others, as AI companies edge themselves ahead of others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The result for consumers would be the best of both worlds: the aesthetics and hardware quality (thinness and all!) of the iPhone, with Google’s technology (or Anthropic’s or OpenAI’s) powering some of the key AI components.&amp;nbsp;That could also be beneficial to Apple’s overall brand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It also means that the look and feel of updated iPhones and their hardware advances will continue to drive sales and upgrades, allowing Apple to do what it does best: focus on build quality, camera improvements, privacy-preserving tech, intentional software design changes like Liquid Glass — and yes, super-thin phones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple could continue to market itself as a best-in-class hardware maker first, not an AI device maker; customers could still long for the latest iPhones, as always, without having to sacrifice the latest technology advances when they make the choice to buy a phone from Apple.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, this scenario only plays out if and when Apple opts to launch a version of Siri that runs a third-party’s AI technology to enhance its own (or if it buys an AI company). But if Apple decides to only rely on its Apple Intelligence offerings without getting them up to speed quickly, the outcome could be much different.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/09/apples-new-iphone-17-devices-dont-have-an-ai-powered-siri-yet-it-doesnt-matter/</guid><pubDate>Tue, 09 Sep 2025 18:50:09 +0000</pubDate></item><item><title>[NEW] Microsoft to lessen reliance on OpenAI by buying AI from rival Anthropic (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/09/microsoft-to-lessen-reliance-on-openai-by-buying-ai-from-rival-anthropic/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/09/53202070940_ea57312b1a_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft will pay to use Anthropic’s AI in Office 365 apps, The Information reports, citing two sources. The move means that Anthropic’s tech will help power new features in Word, Excel, Outlook, and PowerPoint alongside OpenAI’s, marking the end of Microsoft’s previous reliance solely on the ChatGPT maker for its productivity suite.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft’s move to diversify its AI partnerships comes amid a growing rift with OpenAI, which has pursued its own infrastructure projects as well as a potential LinkedIn competitor.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft’s Anthropic deal also comes as the company negotiates a new deal with OpenAI to secure access to its AI models after a pending for-profit restructuring. But The Information says the move isn’t a negotiating tactic. Leaders at Microsoft believe Anthropic’s latest models — Claude Sonnet 4, specifically — perform better than OpenAI’s in certain functions, like creating aesthetically pleasing PowerPoint presentations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This isn’t the first time Microsoft has branched out, though. While OpenAI is the default model, Microsoft offers other models like xAI’s Grok and Anthropic’s Claude through GitHub Copilot. Microsoft is also trying to set itself up for self-reliance. The company recently introduced its first two in-house models: MAI-Voice-1 and MAI-1-preview.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, OpenAI is similarly seeking to step out from under Microsoft’s influence. Last week, OpenAI launched a jobs platform to take on Microsoft’s LinkedIn, and The Financial Times reported that OpenAI is set to begin mass production on its first AI chips in partnership with Broadcom in 2026. That means it will be able to potentially run training and inference on hardware it controls, rather than being dependent on Microsoft’s Azure setup.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we’ve said, OpenAI will continue to be our partner on frontier models and we remain committed to our long-term partnership,” Microsoft spokesperson Michael Collins told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Anthropic for comment. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated with comment from Microsoft and additional context. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/09/53202070940_ea57312b1a_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft will pay to use Anthropic’s AI in Office 365 apps, The Information reports, citing two sources. The move means that Anthropic’s tech will help power new features in Word, Excel, Outlook, and PowerPoint alongside OpenAI’s, marking the end of Microsoft’s previous reliance solely on the ChatGPT maker for its productivity suite.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft’s move to diversify its AI partnerships comes amid a growing rift with OpenAI, which has pursued its own infrastructure projects as well as a potential LinkedIn competitor.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft’s Anthropic deal also comes as the company negotiates a new deal with OpenAI to secure access to its AI models after a pending for-profit restructuring. But The Information says the move isn’t a negotiating tactic. Leaders at Microsoft believe Anthropic’s latest models — Claude Sonnet 4, specifically — perform better than OpenAI’s in certain functions, like creating aesthetically pleasing PowerPoint presentations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This isn’t the first time Microsoft has branched out, though. While OpenAI is the default model, Microsoft offers other models like xAI’s Grok and Anthropic’s Claude through GitHub Copilot. Microsoft is also trying to set itself up for self-reliance. The company recently introduced its first two in-house models: MAI-Voice-1 and MAI-1-preview.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, OpenAI is similarly seeking to step out from under Microsoft’s influence. Last week, OpenAI launched a jobs platform to take on Microsoft’s LinkedIn, and The Financial Times reported that OpenAI is set to begin mass production on its first AI chips in partnership with Broadcom in 2026. That means it will be able to potentially run training and inference on hardware it controls, rather than being dependent on Microsoft’s Azure setup.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we’ve said, OpenAI will continue to be our partner on frontier models and we remain committed to our long-term partnership,” Microsoft spokesperson Michael Collins told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Anthropic for comment. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated with comment from Microsoft and additional context. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/09/microsoft-to-lessen-reliance-on-openai-by-buying-ai-from-rival-anthropic/</guid><pubDate>Tue, 09 Sep 2025 20:11:30 +0000</pubDate></item><item><title>[NEW] Claude’s new AI file creation feature ships with deep security risks built in (AI – Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/09/anthropics-new-claude-feature-can-leak-data-users-told-to-monitor-chats-closely/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Expert calls security advice "unfairly outsourcing the problem to Anthropic's users."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Closeup shot of the leaking water pipe with red valve" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/pipe_leak_1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Closeup shot of the leaking water pipe with red valve" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/pipe_leak_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Wirestock via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, Anthropic launched a new file creation feature for its Claude AI assistant that enables users to generate Excel spreadsheets, PowerPoint presentations, and other documents directly within conversations on the web interface and in the Claude desktop app. While the feature may be handy for Claude users, the company's support documentation also warns that it "may put your data at risk" and details how the AI assistant can be manipulated to transmit user data to external servers.&lt;/p&gt;
&lt;p&gt;The feature, awkwardly named "Upgraded file creation and analysis," is basically Anthropic's version of ChatGPT's Code Interpreter and an upgraded version of Anthropic's "analysis" tool. It's currently available as a preview for Max, Team, and Enterprise plan users, with Pro users scheduled to receive access "in the coming weeks," according to the announcement.&lt;/p&gt;
&lt;p&gt;The security issue comes from the fact that the new feature gives Claude access to a sandbox computing environment, which enables it to download packages and run code to create files. "This feature gives Claude Internet access to create and analyze files, which may put your data at risk," Anthropic writes in its blog announcement. "Monitor chats closely when using this feature."&lt;/p&gt;
&lt;p&gt;According to Anthropic's documentation, "a bad actor" manipulating this feature could potentially "inconspicuously add instructions via external files or websites" that manipulate Claude into "reading sensitive data from a claude.ai connected knowledge source" and "using the sandbox environment to make an external network request to leak the data."&lt;/p&gt;
&lt;p&gt;This describes a prompt injection attack, where hidden instructions embedded in seemingly innocent content can manipulate the AI model's behavior—a vulnerability that security researchers first documented in 2022. These attacks represent a pernicious, unsolved security flaw of AI language models, since both data and instructions in how to process it are fed through as part of the "context window" to the model in the same format, making it difficult for the AI to distinguish between legitimate instructions and malicious commands hidden in user-provided content.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Claude file creation demo video by Anthropic.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;The company states in its security documentation that it discovered the vulnerabilities of the new feature through "red-teaming and security testing" before release. Anthropic's recommended mitigation for users is to "monitor Claude while using the feature and stop it if you see it using or accessing data unexpectedly," although this places the burden of security entirely on the user in what is marketed as an automated, hands-off system.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Independent AI researcher Simon Willison, reviewing the feature today on his blog, noted that Anthropic's advice to "monitor Claude while using the feature" amounts to "unfairly outsourcing the problem to Anthropic's users."&lt;/p&gt;
&lt;h2&gt;Anthropic’s mitigations&lt;/h2&gt;
&lt;p&gt;Anthropic is not completely ignoring the problem, however. The company has implemented several security measures for the file creation feature. For Pro and Max users, Anthropic disabled public sharing of conversations that use the file creation feature. For Enterprise users, the company implemented sandbox isolation so that environments are never shared between users. The company also limited task duration and container runtime "to avoid loops of malicious activity."&lt;/p&gt;
&lt;p&gt;For Team and Enterprise administrators, Anthropic also provides an allowlist of domains Claude can access, including api.anthropic.com, github.com, registry.npmjs.org, and pypi.org. The documentation states that "Claude can only be tricked into leaking data it has access to in a conversation via an individual user's prompt, project or activated connections."&lt;/p&gt;
&lt;p&gt;Anthropic's documentation states the company has "a continuous process for ongoing security testing and red-teaming of this feature." The company encourages organizations to "evaluate these protections against their specific security requirements when deciding whether to enable this feature."&lt;/p&gt;
&lt;h2&gt;Prompt injections galore&lt;/h2&gt;
&lt;p&gt;Even with Anthropic's security measures, Willison says he'll be cautious. "I plan to be cautious using this feature with any data that I very much don’t want to be leaked to a third party, if there’s even the slightest chance that a malicious instruction might sneak its way in," he wrote on his blog.&lt;/p&gt;
&lt;p&gt;We covered a similar potential prompt injection vulnerability with Anthropic's Claude for Chrome, which launched as a research preview last month. For enterprise customers considering Claude for sensitive business documents, Anthropic's decision to ship with documented vulnerabilities suggests competitive pressure may be overriding security considerations in the AI arms race.&lt;/p&gt;
&lt;p&gt;That kind of "ship first, secure it later" philosophy has caused frustrations among some AI experts like Willison, who has extensively documented prompt injection vulnerabilities (and coined the term). He recently described the current state of AI security as "horrifying" on his blog, noting that these prompt injection vulnerabilities remain widespread "almost three years after we first started talking about them."&lt;/p&gt;
&lt;p&gt;In a prescient warning from September 2022, Willison wrote that "there may be systems that should not be built at all until we have a robust solution." His recent assessment in the present? "It looks like we built them anyway!"&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Expert calls security advice "unfairly outsourcing the problem to Anthropic's users."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Closeup shot of the leaking water pipe with red valve" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/pipe_leak_1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Closeup shot of the leaking water pipe with red valve" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/pipe_leak_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Wirestock via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, Anthropic launched a new file creation feature for its Claude AI assistant that enables users to generate Excel spreadsheets, PowerPoint presentations, and other documents directly within conversations on the web interface and in the Claude desktop app. While the feature may be handy for Claude users, the company's support documentation also warns that it "may put your data at risk" and details how the AI assistant can be manipulated to transmit user data to external servers.&lt;/p&gt;
&lt;p&gt;The feature, awkwardly named "Upgraded file creation and analysis," is basically Anthropic's version of ChatGPT's Code Interpreter and an upgraded version of Anthropic's "analysis" tool. It's currently available as a preview for Max, Team, and Enterprise plan users, with Pro users scheduled to receive access "in the coming weeks," according to the announcement.&lt;/p&gt;
&lt;p&gt;The security issue comes from the fact that the new feature gives Claude access to a sandbox computing environment, which enables it to download packages and run code to create files. "This feature gives Claude Internet access to create and analyze files, which may put your data at risk," Anthropic writes in its blog announcement. "Monitor chats closely when using this feature."&lt;/p&gt;
&lt;p&gt;According to Anthropic's documentation, "a bad actor" manipulating this feature could potentially "inconspicuously add instructions via external files or websites" that manipulate Claude into "reading sensitive data from a claude.ai connected knowledge source" and "using the sandbox environment to make an external network request to leak the data."&lt;/p&gt;
&lt;p&gt;This describes a prompt injection attack, where hidden instructions embedded in seemingly innocent content can manipulate the AI model's behavior—a vulnerability that security researchers first documented in 2022. These attacks represent a pernicious, unsolved security flaw of AI language models, since both data and instructions in how to process it are fed through as part of the "context window" to the model in the same format, making it difficult for the AI to distinguish between legitimate instructions and malicious commands hidden in user-provided content.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Claude file creation demo video by Anthropic.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;The company states in its security documentation that it discovered the vulnerabilities of the new feature through "red-teaming and security testing" before release. Anthropic's recommended mitigation for users is to "monitor Claude while using the feature and stop it if you see it using or accessing data unexpectedly," although this places the burden of security entirely on the user in what is marketed as an automated, hands-off system.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Independent AI researcher Simon Willison, reviewing the feature today on his blog, noted that Anthropic's advice to "monitor Claude while using the feature" amounts to "unfairly outsourcing the problem to Anthropic's users."&lt;/p&gt;
&lt;h2&gt;Anthropic’s mitigations&lt;/h2&gt;
&lt;p&gt;Anthropic is not completely ignoring the problem, however. The company has implemented several security measures for the file creation feature. For Pro and Max users, Anthropic disabled public sharing of conversations that use the file creation feature. For Enterprise users, the company implemented sandbox isolation so that environments are never shared between users. The company also limited task duration and container runtime "to avoid loops of malicious activity."&lt;/p&gt;
&lt;p&gt;For Team and Enterprise administrators, Anthropic also provides an allowlist of domains Claude can access, including api.anthropic.com, github.com, registry.npmjs.org, and pypi.org. The documentation states that "Claude can only be tricked into leaking data it has access to in a conversation via an individual user's prompt, project or activated connections."&lt;/p&gt;
&lt;p&gt;Anthropic's documentation states the company has "a continuous process for ongoing security testing and red-teaming of this feature." The company encourages organizations to "evaluate these protections against their specific security requirements when deciding whether to enable this feature."&lt;/p&gt;
&lt;h2&gt;Prompt injections galore&lt;/h2&gt;
&lt;p&gt;Even with Anthropic's security measures, Willison says he'll be cautious. "I plan to be cautious using this feature with any data that I very much don’t want to be leaked to a third party, if there’s even the slightest chance that a malicious instruction might sneak its way in," he wrote on his blog.&lt;/p&gt;
&lt;p&gt;We covered a similar potential prompt injection vulnerability with Anthropic's Claude for Chrome, which launched as a research preview last month. For enterprise customers considering Claude for sensitive business documents, Anthropic's decision to ship with documented vulnerabilities suggests competitive pressure may be overriding security considerations in the AI arms race.&lt;/p&gt;
&lt;p&gt;That kind of "ship first, secure it later" philosophy has caused frustrations among some AI experts like Willison, who has extensively documented prompt injection vulnerabilities (and coined the term). He recently described the current state of AI security as "horrifying" on his blog, noting that these prompt injection vulnerabilities remain widespread "almost three years after we first started talking about them."&lt;/p&gt;
&lt;p&gt;In a prescient warning from September 2022, Willison wrote that "there may be systems that should not be built at all until we have a robust solution." His recent assessment in the present? "It looks like we built them anyway!"&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/09/anthropics-new-claude-feature-can-leak-data-users-told-to-monitor-chats-closely/</guid><pubDate>Tue, 09 Sep 2025 20:55:34 +0000</pubDate></item><item><title>[NEW] Reddit bug caused lesbian subreddit to be labeled as a place for “straight” women (AI – Ars Technica)</title><link>https://arstechnica.com/gadgets/2025/09/reddit-bug-caused-lesbian-subreddit-to-be-labeled-as-a-place-for-straight-women/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Users feared Reddit used generative AI to rewrite user-created content.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1364392195-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1364392195-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Getty

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Reddit has addressed a problem where the description of subreddits originally written by moderators appeared differently—and, often, inaccurately—on Reddit’s Android app. For weeks, the problem was largely suspected to be caused by Reddit clumsily using generative AI to create new subreddit descriptions. However, Reddit says that the changes were the fault of a bug tied to its AI-powered translation technology.&lt;/p&gt;
&lt;p&gt;Reports of subreddits suddenly having inaccurate summaries when viewed on Reddit’s Android app started surfacing on Reddit a couple of weeks ago. On August 29, a moderator&amp;nbsp;reported on the r/ModSupport subreddit for moderators that the r/ThronesAndDominions subreddit's description changed from “The wayward adventures of Dylan Carlson and the band Earth” to “The crazy adventures of Dylan Carlson and the band Earth.” The problem got more attention when r/actuallesbians’ Android app description described the community as “a place for straight and transgender lesbians …” instead of “a place for cis and trans lesbians …”&lt;/p&gt;
&lt;p&gt;Other complaints followed, including from r/autisticparents, a subreddit for parents with autism whose description was changed to say that it is a group for “parents of autistic children.” A moderator who asked to be referred to by their Reddit username, Paige_Railstone, described the confusion that the changes brought to their subreddit:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Many of our subreddit’s users have seen their communities get overrun time and time again, so it is completely understandable that there would be a great deal of panic and concern when it became apparent that the description had been changed.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2&gt;Reddit blames a translation glitch&lt;/h2&gt;
&lt;p&gt;Today, a Reddit admin, going by redtaboo on the site, apologized and offered an explanation for the inaccurate subreddit descriptions, saying that "a bug with our translation service took to translating from [E]nglish to [E]nglish."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Explaining further to Ars, Reddit spokesperson Tim Rathschmidt said:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;There was a small bug in a test we ran that mistakenly caused the English-to-English translation(s) you saw. That bug has been resolved. Unsurprisingly, English-to-English translations are not part of our strategy, as they aren't necessary. English-to-English translations were not a desired or expected outcome of the test.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Reddit pulled the test it was running,&amp;nbsp;but its machine learning-powered translations are still functioning, Rathschmidt said. The company plans to fix the bug and run its unspecified "test" again.&lt;/p&gt;
&lt;p&gt;Reddit’s explanation differs from user theories floating around beforehand, which were mainly that Reddit was rewriting user-created summaries with generative AI, possibly to boost SEO. Some may still be perturbed by the problem persisting for weeks without explanation and the apparent lack of manual checks for the translation service. However, Redditors can now take comfort in knowing that Reddit is not currently using generative AI to alter user-generated content without notice.&lt;/p&gt;
&lt;p&gt;Paige_Railstone, however, maintains frustration and wants to tell Reddit admins, “STOP. Hand off.” The translation bug, they noted, led to people posting on a subreddit for parents with autism that their child might be autistic, “and how terrible that would be for them,” Paige_Railstone recalled.&lt;/p&gt;
&lt;p&gt;"These are the kind of unintentionally insulting posts that drive autistics into leaving a community, and it increases the workload of us moderators,” they said.&lt;/p&gt;
&lt;p&gt;Paige_Railstone also sees the incident as a reason for moderators to be more cautious.&lt;/p&gt;
&lt;p&gt;“This never used to be a concern, but this translation service was rolled out without any notification that I’m aware of, and no option to disable it within the mods' control. That has the potential to cause problems, as we’ve seen over the past two weeks,” they said.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Disclosure: Advance Publications, which owns Ars Technica parent Condé Nast, is the largest shareholder in Reddit.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Users feared Reddit used generative AI to rewrite user-created content.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1364392195-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1364392195-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Getty

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Reddit has addressed a problem where the description of subreddits originally written by moderators appeared differently—and, often, inaccurately—on Reddit’s Android app. For weeks, the problem was largely suspected to be caused by Reddit clumsily using generative AI to create new subreddit descriptions. However, Reddit says that the changes were the fault of a bug tied to its AI-powered translation technology.&lt;/p&gt;
&lt;p&gt;Reports of subreddits suddenly having inaccurate summaries when viewed on Reddit’s Android app started surfacing on Reddit a couple of weeks ago. On August 29, a moderator&amp;nbsp;reported on the r/ModSupport subreddit for moderators that the r/ThronesAndDominions subreddit's description changed from “The wayward adventures of Dylan Carlson and the band Earth” to “The crazy adventures of Dylan Carlson and the band Earth.” The problem got more attention when r/actuallesbians’ Android app description described the community as “a place for straight and transgender lesbians …” instead of “a place for cis and trans lesbians …”&lt;/p&gt;
&lt;p&gt;Other complaints followed, including from r/autisticparents, a subreddit for parents with autism whose description was changed to say that it is a group for “parents of autistic children.” A moderator who asked to be referred to by their Reddit username, Paige_Railstone, described the confusion that the changes brought to their subreddit:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Many of our subreddit’s users have seen their communities get overrun time and time again, so it is completely understandable that there would be a great deal of panic and concern when it became apparent that the description had been changed.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2&gt;Reddit blames a translation glitch&lt;/h2&gt;
&lt;p&gt;Today, a Reddit admin, going by redtaboo on the site, apologized and offered an explanation for the inaccurate subreddit descriptions, saying that "a bug with our translation service took to translating from [E]nglish to [E]nglish."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Explaining further to Ars, Reddit spokesperson Tim Rathschmidt said:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;There was a small bug in a test we ran that mistakenly caused the English-to-English translation(s) you saw. That bug has been resolved. Unsurprisingly, English-to-English translations are not part of our strategy, as they aren't necessary. English-to-English translations were not a desired or expected outcome of the test.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Reddit pulled the test it was running,&amp;nbsp;but its machine learning-powered translations are still functioning, Rathschmidt said. The company plans to fix the bug and run its unspecified "test" again.&lt;/p&gt;
&lt;p&gt;Reddit’s explanation differs from user theories floating around beforehand, which were mainly that Reddit was rewriting user-created summaries with generative AI, possibly to boost SEO. Some may still be perturbed by the problem persisting for weeks without explanation and the apparent lack of manual checks for the translation service. However, Redditors can now take comfort in knowing that Reddit is not currently using generative AI to alter user-generated content without notice.&lt;/p&gt;
&lt;p&gt;Paige_Railstone, however, maintains frustration and wants to tell Reddit admins, “STOP. Hand off.” The translation bug, they noted, led to people posting on a subreddit for parents with autism that their child might be autistic, “and how terrible that would be for them,” Paige_Railstone recalled.&lt;/p&gt;
&lt;p&gt;"These are the kind of unintentionally insulting posts that drive autistics into leaving a community, and it increases the workload of us moderators,” they said.&lt;/p&gt;
&lt;p&gt;Paige_Railstone also sees the incident as a reason for moderators to be more cautious.&lt;/p&gt;
&lt;p&gt;“This never used to be a concern, but this translation service was rolled out without any notification that I’m aware of, and no option to disable it within the mods' control. That has the potential to cause problems, as we’ve seen over the past two weeks,” they said.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Disclosure: Advance Publications, which owns Ars Technica parent Condé Nast, is the largest shareholder in Reddit.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/gadgets/2025/09/reddit-bug-caused-lesbian-subreddit-to-be-labeled-as-a-place-for-straight-women/</guid><pubDate>Tue, 09 Sep 2025 22:32:52 +0000</pubDate></item><item><title>[NEW] From mixers to pitch-offs — your brand event belongs at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/09/from-mixers-to-pitch-offs-your-brand-event-belongs-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last year’s &lt;strong&gt;TechCrunch Disrupt&lt;/strong&gt; Side Events drew hundreds of founders, investors, and operators after hours — from intimate roundtables to lively happy hours to full-on pitch competitions. Each event unlocked new opportunities for the hosts: investor deal flow, talent connections, and brand exposure with the startup community.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year, you can do the same. As a &lt;strong&gt;Side Event&lt;/strong&gt; host during “Disrupt Week” (October 25–31), you’ll tap into an audience of 10,000+ attendees, plus the broader Bay Area tech community.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-host-a-side-event"&gt;Why host a Side Event?&lt;/h2&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Visibility:&lt;/strong&gt; Your brand featured in Disrupt 2025 Side Event listings on the event site, event app, and TechCrunch.com.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Connections:&lt;/strong&gt; Meet startup leaders and investors in your own environment.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; From panels to parties, it’s your format, your brand.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Applications are free — and spots are limited. &lt;strong&gt;Submit your Side Event&lt;/strong&gt; before applications close this Friday, September 12.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last year’s &lt;strong&gt;TechCrunch Disrupt&lt;/strong&gt; Side Events drew hundreds of founders, investors, and operators after hours — from intimate roundtables to lively happy hours to full-on pitch competitions. Each event unlocked new opportunities for the hosts: investor deal flow, talent connections, and brand exposure with the startup community.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year, you can do the same. As a &lt;strong&gt;Side Event&lt;/strong&gt; host during “Disrupt Week” (October 25–31), you’ll tap into an audience of 10,000+ attendees, plus the broader Bay Area tech community.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-host-a-side-event"&gt;Why host a Side Event?&lt;/h2&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Visibility:&lt;/strong&gt; Your brand featured in Disrupt 2025 Side Event listings on the event site, event app, and TechCrunch.com.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Connections:&lt;/strong&gt; Meet startup leaders and investors in your own environment.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; From panels to parties, it’s your format, your brand.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Applications are free — and spots are limited. &lt;strong&gt;Submit your Side Event&lt;/strong&gt; before applications close this Friday, September 12.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/09/from-mixers-to-pitch-offs-your-brand-event-belongs-at-techcrunch-disrupt-2025/</guid><pubDate>Tue, 09 Sep 2025 22:40:47 +0000</pubDate></item><item><title>[NEW] Sources: AI training startup Mercor eyes $10B+ valuation on $450 million run rate (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/09/sources-ai-training-startup-mercor-eyes-10b-valuation-on-450-million-run-rate/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/IMG_9747_3d5c52.jpeg?resize=1200,960" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Mercor, a startup that connects companies like OpenAI and Meta with domain experts needed to train and refine their foundational AI models, is in discussions with investors for a Series C round, according to a marketing document viewed by TechCrunch and two sources familiar with the deal talks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Felicis, a returning investor, is considering doubling down on the company for the Series C, according to two sources. Felicis declined to comment.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company is currently targeting a valuation of $10 billion or more, one person said. That’s up from an $8 billion target valuation that the company discussed a couple of months ago, one person said. However, terms of the final deal could still change.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has told potential investors that it already has multiple offers. VCs have been reaching out to Mercor preemptively with offers valuing the company at as much as $10 billion, the Information previously reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch also understands that the company has brought on at least two new investors to raise funds for the potential deal through special purpose vehicles (SPVs).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s previous round was announced in February – a $100 million Series B at a $2 billion valuation led by Felicis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2022, Mercor is approaching $450 million in annualized run-rate revenue, one person said. The company told TechCrunch in February that its annual revenue (calculated by multiplying the latest month by 12) had reached $75 million at that time. In March, Mercor CEO Brendan Foody posted on X that ARR was $100 million.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company has told investors it is on track to hit the $500 million ARR milestone faster than Anysphere,&lt;strong&gt; &lt;/strong&gt;the startup that makes AI coding assistant Cursor, according to one source familiar with the situation. Anysphere famously hit $500 million in ARR about a year after its product launched. Unlike Anysphere, which is still burning cash, Mercor generated $6 million in profit in the first half of the year, Forbes reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mercor earns revenue by providing companies with specialized domain experts to&amp;nbsp; perform AI model training — such as scientists, doctors, and lawyers — and charging an hourly finder’s fee and matching rate for their work.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company claims to supply data labeling contractors to five top AI labs, including Amazon, Google, Meta, Microsoft, and OpenAI, as well as to Tesla and Nvidia. According to sources, an outsized portion of its revenue is coming from a subset of those brands, including OpenAI.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To further diversify its business model, Mercor has been telling investors that it is adding more software infrastructure for reinforcement learning –&amp;nbsp; a training method where a model or agent’s decisions are verified or disputed, enabling it to incorporate feedback and improve over time. The company also intends to eventually build an AI-powered recruiting marketplace.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Mercor faces competition from companies like Surge AI, which is reportedly in talks to raise funding at a $25 billion valuation, as well as from Turing Labs and other data labeling firms like Scale AI that are also expanding into RL services. Some believe that OpenAI’s recently launched hiring platform could lead the AI giant to create its own human-expert-powered RL training service.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When reached for comment, Foody told TechCrunch, “We haven’t been trying to raise at all,” and, “We turn down offers every month.” He also said the company’s ARR is higher than $450 million. However, he clarified that the company’s revenue includes the total amount that customers pay Mercor for services before its contractors receive their portion. He added this is a common accounting practice recommended by audit firms and used by competitors Surge AI and Scale AI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup was co-founded in 2023 by Thiel Fellows and Harvard dropouts Brendan Foody (CEO), Adarsh Hiremath (CTO), and Surya Midha (COO). All three co-founders are still in their early twenties. To take the company to the next level, Mercor recently appointed Sundeep Jain, a former chief product officer at Uber with decades of experience, as its first president, Forbes reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mercor was recently sued by competitor Scale AI for misappropriation of trade secrets. Scale AI alleges that one of its former employees who later joined Mercor “stole more than 100 confidential documents concerning Scale’s customer strategies and other proprietary information,” according to a copy of the lawsuit TechCrunch previously reviewed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Maxwell Zeff contributed reporting&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/IMG_9747_3d5c52.jpeg?resize=1200,960" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Mercor, a startup that connects companies like OpenAI and Meta with domain experts needed to train and refine their foundational AI models, is in discussions with investors for a Series C round, according to a marketing document viewed by TechCrunch and two sources familiar with the deal talks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Felicis, a returning investor, is considering doubling down on the company for the Series C, according to two sources. Felicis declined to comment.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company is currently targeting a valuation of $10 billion or more, one person said. That’s up from an $8 billion target valuation that the company discussed a couple of months ago, one person said. However, terms of the final deal could still change.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has told potential investors that it already has multiple offers. VCs have been reaching out to Mercor preemptively with offers valuing the company at as much as $10 billion, the Information previously reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch also understands that the company has brought on at least two new investors to raise funds for the potential deal through special purpose vehicles (SPVs).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s previous round was announced in February – a $100 million Series B at a $2 billion valuation led by Felicis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2022, Mercor is approaching $450 million in annualized run-rate revenue, one person said. The company told TechCrunch in February that its annual revenue (calculated by multiplying the latest month by 12) had reached $75 million at that time. In March, Mercor CEO Brendan Foody posted on X that ARR was $100 million.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company has told investors it is on track to hit the $500 million ARR milestone faster than Anysphere,&lt;strong&gt; &lt;/strong&gt;the startup that makes AI coding assistant Cursor, according to one source familiar with the situation. Anysphere famously hit $500 million in ARR about a year after its product launched. Unlike Anysphere, which is still burning cash, Mercor generated $6 million in profit in the first half of the year, Forbes reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mercor earns revenue by providing companies with specialized domain experts to&amp;nbsp; perform AI model training — such as scientists, doctors, and lawyers — and charging an hourly finder’s fee and matching rate for their work.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company claims to supply data labeling contractors to five top AI labs, including Amazon, Google, Meta, Microsoft, and OpenAI, as well as to Tesla and Nvidia. According to sources, an outsized portion of its revenue is coming from a subset of those brands, including OpenAI.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To further diversify its business model, Mercor has been telling investors that it is adding more software infrastructure for reinforcement learning –&amp;nbsp; a training method where a model or agent’s decisions are verified or disputed, enabling it to incorporate feedback and improve over time. The company also intends to eventually build an AI-powered recruiting marketplace.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Mercor faces competition from companies like Surge AI, which is reportedly in talks to raise funding at a $25 billion valuation, as well as from Turing Labs and other data labeling firms like Scale AI that are also expanding into RL services. Some believe that OpenAI’s recently launched hiring platform could lead the AI giant to create its own human-expert-powered RL training service.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When reached for comment, Foody told TechCrunch, “We haven’t been trying to raise at all,” and, “We turn down offers every month.” He also said the company’s ARR is higher than $450 million. However, he clarified that the company’s revenue includes the total amount that customers pay Mercor for services before its contractors receive their portion. He added this is a common accounting practice recommended by audit firms and used by competitors Surge AI and Scale AI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup was co-founded in 2023 by Thiel Fellows and Harvard dropouts Brendan Foody (CEO), Adarsh Hiremath (CTO), and Surya Midha (COO). All three co-founders are still in their early twenties. To take the company to the next level, Mercor recently appointed Sundeep Jain, a former chief product officer at Uber with decades of experience, as its first president, Forbes reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mercor was recently sued by competitor Scale AI for misappropriation of trade secrets. Scale AI alleges that one of its former employees who later joined Mercor “stole more than 100 confidential documents concerning Scale’s customer strategies and other proprietary information,” according to a copy of the lawsuit TechCrunch previously reviewed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Maxwell Zeff contributed reporting&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/09/sources-ai-training-startup-mercor-eyes-10b-valuation-on-450-million-run-rate/</guid><pubDate>Wed, 10 Sep 2025 01:36:51 +0000</pubDate></item></channel></rss>