<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 16 Feb 2026 18:49:30 +0000</lastBuildDate><item><title>URBN tests agentic AI to automate retail reporting (AI News)</title><link>https://www.artificialintelligence-news.com/news/urbn-tests-agentic-ai-to-automate-retail-reporting/</link><description>&lt;p&gt;Retail decisions often depend on weekly performance reports, but compiling those reports can take hours of manual work. Urban Outfitters Inc. (URBN) is testing a new approach by using agentic AI systems to generate those reports automatically, changing routine analysis from staff to software.&lt;/p&gt;&lt;p&gt;The retailer runs brands like Urban Outfitters, Anthropologie, and Free People, and has deployed AI systems that analyse store-level data and produce weekly summaries for merchandising teams. Instead of reviewing multiple spreadsheets or dashboards, staff receive a report that highlights patterns and areas that need attention.&lt;/p&gt;&lt;p&gt;Industry coverage indicates the automation saves merchants from reviewing more than 20 separate reports each Sunday by synthesising the information into one overview. The goal is to reduce the time spent collecting and organising data before decisions are made. The rollout offers a practical example of how “agentic AI” is beginning to enter everyday enterprise operations.&lt;/p&gt;&lt;h3&gt;How agentic AI is taking over routine retail reporting&lt;/h3&gt;&lt;p&gt;Weekly reporting sits close to the core of retail management. Merchandising teams use these updates to monitor sales trends, check inventory movement, and decide where to adjust pricing, stock levels, or promotions. Because the process repeats in many stores and regions, it can consume a large share of operational time.&lt;/p&gt;&lt;p&gt;URBN’s AI agents take over the structured parts of that workflow. The systems gather store data, organise results, and present a digestible summary for teams to review. Employees remain responsible for interpreting the findings and taking action, but the groundwork is handled automatically.&lt;/p&gt;&lt;p&gt;This mirrors a change in enterprise AI adoption. Early deployments frequently aimed at helping individuals complete tasks faster, like drafting text or searching internal information. Instead, agentic systems run processes in the background and present completed outputs, allowing staff to focus on judgement not preparation.&lt;/p&gt;&lt;p&gt;Retail analysts have pointed to growing interest in this model in the sector. Discussions at recent National Retail Federation events have highlighted how retailers are exploring autonomous AI workflows to support merchandising and operational monitoring at scale. URBN’s reporting automation shows how those ideas are moving into production environments not staying in pilot stages.&lt;/p&gt;&lt;h3&gt;Why reporting is an early target for automation&lt;/h3&gt;&lt;p&gt;Reporting is one of the first operational areas that many companies try to automate because it is based on organised data and predictable formats. Weekly summaries follow a repeatable pattern, making them easier to test using automation while keeping oversight in place.&lt;/p&gt;&lt;p&gt;Starting with reporting allows URBN to evaluate how reliable the AI outputs are and how well teams adapt to receiving automated insights. If the system consistently produces accurate summaries, it can reduce delays between identifying trends and responding to them.&lt;/p&gt;&lt;p&gt;The approach also highlights that automation does not remove accountability. Staff still review the reports and make final decisions, but they spend less time assembling information manually.&lt;/p&gt;&lt;h3&gt;A signal of changing enterprise priorities&lt;/h3&gt;&lt;p&gt;URBN’s rollout suggests that the next phase of enterprise AI adoption may be embedding automation into everyday workflows. Companies are asking increasingly whether AI can handle recurring operational tasks reliably enough to become part of normal business processes.&lt;/p&gt;&lt;p&gt;When those tasks are automated successfully, the benefits extend beyond time savings. Consistent reporting can help ensure that teams in regions work from the same information, which may improve coordination and speed up responses to emerging issues. In large retail networks, even small improvements in how quickly insights reach decision-makers can influence stock management and sales performance.&lt;/p&gt;&lt;p&gt;If reporting automation proves dependable, similar systems could expand into adjacent areas like demand forecasting, promotion analysis, or supply monitoring. Each step would follow the same pattern: automate the repeatable groundwork, keep people responsible for oversight and decisions.&lt;/p&gt;&lt;h3&gt;From AI assistance to agentic AI execution&lt;/h3&gt;&lt;p&gt;URBN’s use of agentic AI illustrates a gradual change in how enterprises are integrating artificial intelligence. AI is starting to run defined operational processes automatically while humans supervise results.&lt;/p&gt;&lt;p&gt;The change moves AI from supporting individual productivity to shaping how work is organised. By starting with a recurring task like weekly reporting and keeping review firmly in human hands, URBN is testing how far automation can be trusted in real retail operations.&lt;/p&gt;&lt;p&gt;For other enterprises watching the evolution of agentic systems, the lesson is practical, namely about deciding which everyday processes can be handed to software – and how to manage that transition.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Clark Street Mercantile)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Agentic AI drives finance ROI in accounts payable automation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Retail decisions often depend on weekly performance reports, but compiling those reports can take hours of manual work. Urban Outfitters Inc. (URBN) is testing a new approach by using agentic AI systems to generate those reports automatically, changing routine analysis from staff to software.&lt;/p&gt;&lt;p&gt;The retailer runs brands like Urban Outfitters, Anthropologie, and Free People, and has deployed AI systems that analyse store-level data and produce weekly summaries for merchandising teams. Instead of reviewing multiple spreadsheets or dashboards, staff receive a report that highlights patterns and areas that need attention.&lt;/p&gt;&lt;p&gt;Industry coverage indicates the automation saves merchants from reviewing more than 20 separate reports each Sunday by synthesising the information into one overview. The goal is to reduce the time spent collecting and organising data before decisions are made. The rollout offers a practical example of how “agentic AI” is beginning to enter everyday enterprise operations.&lt;/p&gt;&lt;h3&gt;How agentic AI is taking over routine retail reporting&lt;/h3&gt;&lt;p&gt;Weekly reporting sits close to the core of retail management. Merchandising teams use these updates to monitor sales trends, check inventory movement, and decide where to adjust pricing, stock levels, or promotions. Because the process repeats in many stores and regions, it can consume a large share of operational time.&lt;/p&gt;&lt;p&gt;URBN’s AI agents take over the structured parts of that workflow. The systems gather store data, organise results, and present a digestible summary for teams to review. Employees remain responsible for interpreting the findings and taking action, but the groundwork is handled automatically.&lt;/p&gt;&lt;p&gt;This mirrors a change in enterprise AI adoption. Early deployments frequently aimed at helping individuals complete tasks faster, like drafting text or searching internal information. Instead, agentic systems run processes in the background and present completed outputs, allowing staff to focus on judgement not preparation.&lt;/p&gt;&lt;p&gt;Retail analysts have pointed to growing interest in this model in the sector. Discussions at recent National Retail Federation events have highlighted how retailers are exploring autonomous AI workflows to support merchandising and operational monitoring at scale. URBN’s reporting automation shows how those ideas are moving into production environments not staying in pilot stages.&lt;/p&gt;&lt;h3&gt;Why reporting is an early target for automation&lt;/h3&gt;&lt;p&gt;Reporting is one of the first operational areas that many companies try to automate because it is based on organised data and predictable formats. Weekly summaries follow a repeatable pattern, making them easier to test using automation while keeping oversight in place.&lt;/p&gt;&lt;p&gt;Starting with reporting allows URBN to evaluate how reliable the AI outputs are and how well teams adapt to receiving automated insights. If the system consistently produces accurate summaries, it can reduce delays between identifying trends and responding to them.&lt;/p&gt;&lt;p&gt;The approach also highlights that automation does not remove accountability. Staff still review the reports and make final decisions, but they spend less time assembling information manually.&lt;/p&gt;&lt;h3&gt;A signal of changing enterprise priorities&lt;/h3&gt;&lt;p&gt;URBN’s rollout suggests that the next phase of enterprise AI adoption may be embedding automation into everyday workflows. Companies are asking increasingly whether AI can handle recurring operational tasks reliably enough to become part of normal business processes.&lt;/p&gt;&lt;p&gt;When those tasks are automated successfully, the benefits extend beyond time savings. Consistent reporting can help ensure that teams in regions work from the same information, which may improve coordination and speed up responses to emerging issues. In large retail networks, even small improvements in how quickly insights reach decision-makers can influence stock management and sales performance.&lt;/p&gt;&lt;p&gt;If reporting automation proves dependable, similar systems could expand into adjacent areas like demand forecasting, promotion analysis, or supply monitoring. Each step would follow the same pattern: automate the repeatable groundwork, keep people responsible for oversight and decisions.&lt;/p&gt;&lt;h3&gt;From AI assistance to agentic AI execution&lt;/h3&gt;&lt;p&gt;URBN’s use of agentic AI illustrates a gradual change in how enterprises are integrating artificial intelligence. AI is starting to run defined operational processes automatically while humans supervise results.&lt;/p&gt;&lt;p&gt;The change moves AI from supporting individual productivity to shaping how work is organised. By starting with a recurring task like weekly reporting and keeping review firmly in human hands, URBN is testing how far automation can be trusted in real retail operations.&lt;/p&gt;&lt;p&gt;For other enterprises watching the evolution of agentic systems, the lesson is practical, namely about deciding which everyday processes can be handed to software – and how to manage that transition.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Clark Street Mercantile)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Agentic AI drives finance ROI in accounts payable automation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/urbn-tests-agentic-ai-to-automate-retail-reporting/</guid><pubDate>Mon, 16 Feb 2026 10:00:00 +0000</pubDate></item><item><title>Hackers made death threats against this security researcher. Big mistake. (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/16/1132526/allison-nixon-hackers-security-researcher/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;The threats started in spring.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In April 2024, a mysterious someone using the online handles “Waifu” and “Judische” began posting death threats on Telegram and Discord channels aimed at a cybersecurity researcher named Allison Nixon.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Alison [sic] Nixon is gonna get necklaced with a tire filled with gasoline soon,” wrote Waifu/Judische, both of which are words with offensive connotations. “Decerebration is my fav type of brain death, thats whats gonna happen to alison Nixon.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It wasn’t long before others piled on. Someone shared AI-generated nudes of Nixon.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;These anonymous personas targeted Nixon because she had become a formidable threat: As chief research officer at the cyber investigations firm Unit 221B, named after Sherlock Holmes’s apartment, she had built a career tracking cybercriminals and helping get them arrested. For years she had lurked quietly in online chat channels or used pseudonyms to engage with perpetrators directly while piecing together clues they’d carelessly drop about themselves and their crimes. This had helped her bring to justice a number of cybercriminals—especially members of a loosely affiliated subculture of anarchic hackers who call themselves the Com.&lt;/p&gt;  &lt;p&gt;But members of the Com aren’t just involved in hacking; some of them also engage in offline violence against researchers who track them. This includes bricking (throwing a brick through a victim’s window) and swatting (a dangerous type of hoax that involves reporting a false murder or hostage situation at someone’s home so SWAT teams will swarm it with guns drawn). Members of a Com offshoot known as 764 have been accused of even more violent acts—including animal torture, stabbings, and school shootings—or of inciting others in and outside the Com to commit these crimes.&lt;/p&gt; 
 &lt;p&gt;Nixon started tracking members of the community more than a decade ago, when other researchers and people in law enforcement were largely ignoring them because they were young—many in their teens. Her early attention allowed her to develop strategies for unmasking them.&lt;/p&gt;  &lt;p&gt;Ryan Brogan, a special agent with the FBI, says Nixon has helped him and colleagues identify and arrest more than two dozen members of the community since 2011, when he first began working with her, and that her skills in exposing them are unparalleled. “If you get on Allison’s and my radar, you’re going [down]. It’s just a matter of time,” he says. “No matter how much digital anonymity and tradecraft you try to apply, you’re done.”&lt;/p&gt;  &lt;p&gt;Though she’d done this work for more than a decade, Nixon couldn’t understand why the person behind the Waifu/Judische accounts was suddenly threatening her. She had given media interviews about the Com—most recently on &lt;em&gt;60 Minutes&lt;/em&gt;—but not about her work unmasking members to get them arrested, so the hostility seemed to come out of the blue. And although she had taken an interest in the Waifu persona in years past for crimes he boasted about committing, he hadn’t been on her radar for a while when the threats began, because she was tracking other targets.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now Nixon resolved to unmask Waifu/Judische and others responsible for the death threats—and take them down for crimes they admitted to committing. “Prior to them death-threatening me, I had no reason to pay attention to them,” she says.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;Com beginnings&lt;/h3&gt;  &lt;p&gt;Most people have never heard of the Com, but its influence and threat are growing.&lt;/p&gt;  &lt;p&gt;It’s an online community comprising loosely affiliated groups of, primarily, teens and twentysomethings in North America and English-speaking parts of Europe who have become part of what some call a cybercrime youth movement.&amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;International laws and norms, and fears of retaliation, prevent states from going all out in cyber operations. That doesn’t stop the anarchic Com.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;Over the last decade, its criminal activities have escalated from simple distributed denial-of-service (DDoS) attacks that disrupt websites to SIM-swapping hacks that hijack a victim’s phone service, as well as crypto theft, ransomware attacks, and corporate data theft. These crimes have affected AT&amp;amp;T, Microsoft, Uber, and others. Com members have also been involved in various forms of sextortion aimed at forcing victims to physically harm themselves or record themselves doing sexually explicit activities. The Com’s impact has also spread beyond the digital realm to kidnapping, beatings, and other violence.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;One longtime cybercrime researcher, who asked to remain anonymous because of his work, says the Com is as big a threat in the cyber realm as Russia and China—for one unusual reason.&lt;/p&gt; 

 &lt;p&gt;“There’s only so far that China is willing to go; there’s only so far that Russia or North Korea is willing to go,” he says, referring to international laws and norms, and fears of retaliation, that prevent states from going all out in cyber operations. That doesn’t stop the anarchic Com, he says.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1132662" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/MIT-Tech-Review-ILLU-1-FINAL-franziska.jpg?w=1429" width="1429" /&gt;&lt;div class="image-credit"&gt;FRANZISKA BARCZYK&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;“It is a pretty significant threat, and people tend to … push it under the rug [because] it’s just a bunch of kids,” he says. “But look at the impact [they have].”&lt;/p&gt;  &lt;p&gt;Brogan says the amount of damage they do in terms of monetary losses “can become staggering very quickly.”&lt;/p&gt;  &lt;p&gt;There is no single site where Com members congregate; they spread across a number of web forums and Telegram and Discord channels. The group follows a long line of hacking and subculture communities that emerged online over the last two decades, gained notoriety, and then faded or vanished after prominent members were arrested or other factors caused their decline. They differed in motivation and activity, but all emerged from “the same primordial soup,” says Nixon. The Com’s roots can be traced to the Scene, which began as a community of various “warez” groups engaged in pirating computer games, music, and movies.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;When Nixon began looking at the Scene, in 2011, its members were hijacking gaming accounts, launching DDoS attacks, and running booter services. (DDoS attacks overwhelm a server or computer with traffic from bot-controlled machines, preventing legitimate traffic from getting through; booters are tools that anyone can rent to launch a DDoS attack against a target of choice.) While they made some money, their primary goal was notoriety.&lt;/p&gt;  &lt;p&gt;This changed around 2018. Cryptocurrency values were rising, and the Com—or the Community, as it sometimes called itself—emerged as a subgroup that ultimately took over the Scene. Members began to focus on financial gain—cryptocurrency theft, data theft, and extortion.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;The pandemic two years later saw a surge in Com membership that Nixon attributes to social isolation and the forced movement of kids online for schooling. But she believes economic conditions and socialization problems have also driven its growth. Many Com members can’t get jobs because they lack skills or have behavioral issues, she says. A number who have been arrested have had troubled home lives and difficulty adapting to school, and some have shown signs of mental illness. The Com provides camaraderie, support, and an outlet for personal frustrations. Since 2018, it has also offered some a solution to their money problems.&lt;/p&gt;  &lt;p&gt;Loose-knit cells have sprouted from the community—Star Fraud, ShinyHunters, Scattered Spider, Lapsus$—to collaborate on clusters of crime. They usually target high-profile crypto bros and tech giants and have made millions of dollars from theft and extortion, according to court records.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But dominance, power, and bragging rights are still motivators, even in profit operations, says the cybercrime researcher, which is partly why members target “big whales.”&lt;/p&gt;  &lt;p&gt;“There is financial gain,” he says, “but it’s also [sending a message that] I can reach out and touch the people that think they’re untouchable.” In fact, Nixon says, some members of the Com have overwhelming ego-driven motivations that end up conflicting with their financial motives.&lt;/p&gt; 
 &lt;p&gt;“Often their financial schemes fall apart because of their ego, and that phenomenon is also what I’ve made my career on,” she says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The hacker hunter emerges&lt;/h3&gt;  &lt;p&gt;Nixon has straight dark hair, wears wire-rimmed glasses, and has a slight build and bookish demeanor that, on first impression, could allow her to pass for a teen herself. She talks about her work in rapid cadences, like someone whose brain is filled with facts that are under pressure to get out, and she exudes a sense of urgency as she tries to make people understand the threat the Com poses. She doesn’t suppress her happiness when someone she’s been tracking gets arrested.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;In 2011, when she first began investigating the communities from which the Com emerged, she was working the night shift in the security operations center of the security firm SecureWorks. The center responded to tickets and security alerts emanating from customer networks, but Nixon coveted a position on the company’s counter-threats team, which investigated and published threat-intelligence reports on mostly state-sponsored hacking groups from China and Russia. Without connections or experience, she had no path to investigative work. But Nixon is an intensely curious person, and this created its own path.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Allison Nixon" class="wp-image-1132672" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/GROUP-SHOTS-10267_v2.edit_.jpg" /&gt;&lt;figcaption class="wp-element-caption"&gt;Allison Nixon is chief research officer at the cybersecurity investigations firm Unit 221B, where she tracks cybercriminals and helps bring them to justice.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;YLVA EREVALL&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Where the threat team focused on the impact hackers had on customer networks—how they broke in, what they stole—Nixon was more interested in their motivations and the personality traits that drove their actions. She assumed there must be online forums where criminal hackers congregated, so she googled “hacking forums” and landed on a site called Hack Forums.&lt;/p&gt;  &lt;p&gt;“It was really stupid simple,” she says.&lt;/p&gt;  &lt;p&gt;She was surprised to see members openly discussing their crimes there. She reached out to someone on the SecureWorks threat team to see if he was aware of the site, and he dismissed it as a place for “script kiddies”—a pejorative term for unskilled hackers.&lt;/p&gt; 
 &lt;p&gt;This was a time when many cybersecurity pros were shifting their focus away from cybercrime to state-sponsored hacking operations, which were more sophisticated and getting a lot of attention. But Nixon likes to zig where others zag, and her colleague’s dismissiveness fueled her interest in the forums. Two other SecureWorks colleagues shared that interest, and the three studied the forums during downtime on their shifts. They focused on trying to identify the people running DDoS booters.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;What Nixon loved about the forums was how accessible they were to a beginner like herself. Threat-intelligence teams require privileged access to a victim’s network to investigate breaches. But Nixon could access everything she needed in the public forums, where the hackers seemed to think no one was watching. Because of this, they often made mistakes in operational security, or OPSEC—letting slip little biographical facts such as the city where they lived, a school they attended, or a place they used to work. These details revealed in their chats, combined with other information, could help expose the real identities behind their anonymous masks.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It was a shock to me that it was relatively easy to figure out who [they were],” she says.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;She wasn’t bothered by the immature boasting and petty fights that dominated the forums. “A lot of people don’t like to do this work of reading chat logs. I realize that this is a very uncommon thing. And maybe my brain is built a little weird that I’m willing to do this,” she says. “I have a special talent that I can wade through garbage and it doesn’t bother me.”&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;Nixon soon realized that not all the members were script kiddies. Some exhibited real ingenuity and “powerful” skills, she says, but because they were applying these to frivolous purposes—hijacking gamer accounts instead of draining bank accounts—researchers and law enforcement were ignoring them. Nixon began tracking them, suspecting that they would eventually direct their skills at more significant targets—an intuition that proved to be correct. And when they did, she had already amassed a wealth of information about them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;She continued her DDoS research for two years until a turning point in 2013, when the cybersecurity journalist Brian Krebs, who made a career tracking cybercriminals, got swatted.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;About a dozen people from the security community worked with Krebs to expose the perpetrator, and Nixon was invited to help. Krebs sent her pieces of the puzzle to investigate, and eventually the group identified the culprit (though it would take two years for him to be arrested). When she was invited to dinner with Krebs and the other investigators, she realized she’d found her people.&lt;/p&gt;  &lt;p&gt;“It was an amazing moment for me,” she says. “I was like, wow, there’s all these like-minded people that just want to help and are doing it just for the love of the game, basically.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Staying one step ahead&lt;/h3&gt;  &lt;p&gt;It was porn stars who provided Nixon with her next big research focus—one that underscored her skill at spotting Com actors and criminal trends in their nascent stages, before they emerged as major threats.&lt;/p&gt;  &lt;p&gt;In 2018, someone was hijacking the social media accounts of certain adult-film stars and using those accounts to blast out crypto scams to their large follower bases. Nixon couldn’t figure out how the hackers had hijacked the social media profiles, but she promised to help the actors regain access to their accounts if they agreed to show her the private messages the hackers had sent or received during the time they controlled them. These messages led her to a forum where members were talking about how they stole the accounts. The hackers had tricked some of these actors into disclosing the mobile phone numbers of others. Then they used a technique called SIM swapping to reset passwords for social media accounts belonging to those other stars, locking them out.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In SIM swapping, fraudsters get a victim’s phone number assigned to a SIM card and phone &lt;em&gt;they&lt;/em&gt; control, so that calls and messages intended for the victim go to them instead. This includes one-time security codes that sites text to account holders to verify themselves when accessing their account or changing its password. In some of the cases involving the porn stars, the hackers had manipulated telecom workers into making the SIM swaps for what they thought were legitimate reasons, and in other cases they bribed the workers to make the change. The hackers were then able to alter the password on the actors’ social media accounts, lock out the owners, and use the accounts to advertise their crypto scams.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;SIM swapping is a powerful technique that can be used to hijack and drain entire cryptocurrency and bank accounts, so Nixon was surprised to see the fraudsters using it for relatively unprofitable schemes. But SIM swapping had rarely been used for financial fraud at that point, and like the earlier hackers Nixon had seen on Hack Forums, the ones hijacking porn star accounts didn’t seem to grasp the power of the technique they were using. Nixon suspected that this would change and SIM swapping would soon become a major problem, so she shifted her research focus accordingly. It didn’t take long for the fraudsters to pivot as well.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt; &lt;p&gt;Nixon’s skill at looking ahead in this way has served her throughout her career. On multiple occasions a hacker or hacking group would catch her attention—for using a novel hacking approach in some minor operation, for example—and she’d begin tracking their online posts and chats in the belief that they’d eventually do something significant with that skill.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;They usually did. When they later grabbed headlines with a showy or impactful operation, these hackers would seem to others to have emerged from nowhere, sending researchers and law enforcement scrambling to understand who they were. But Nixon would already have a dossier compiled on them and, in some cases, had unmasked their real identity as well. Lizard Squad was an example of this. The group burst into the headlines in 2014 and 2015 with a series of high-profile DDoS campaigns, but Nixon and colleagues at the job where she worked at the time had already been watching its members as individuals for a while. So the FBI sought their assistance in identifying them.&lt;/p&gt;  &lt;p&gt;“The thing about these young hackers is that they … keep going until they get arrested, but it takes years for them to get arrested,” she says. “So a huge aspect of my career is just sitting on this information that has not been actioned [yet].”&lt;/p&gt;  &lt;p&gt;It was during the Lizard Squad years that Nixon began developing tools to scrape and record hacker communications online, though it would be years before she began using these concepts to scrape the Com chatrooms and forums. These channels held a wealth of data that might not seem useful during the nascent stage of a hacker’s career but could prove critical later, when law enforcement got around to investigating them; yet the contents were always at risk of being deleted by Com members or getting taken down by law enforcement when it seized websites and chat channels.&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;Nixon’s work is unique because she engages with the actors in chat spaces to draw out information from them that “would not be otherwise normally available.”&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;Over several years, she scraped and preserved whatever chatrooms she was investigating. But it wasn’t until early 2020, when she joined Unit 221B, that she got the chance to scrape the Telegram and Discord channels of the Com. She pulled all of this data together into a searchable platform that other researchers and law enforcement could use. The company hired two former hackers to help build scraping tools and infrastructure for this work; the result is eWitness, a community-driven, invitation-­only platform. It was initially seeded only with data Nixon had collected after she arrived at Unit 221B, but has since been augmented with data that other users of the platform have scraped from Com social spaces as well, some of which doesn’t exist in public forums anymore.&lt;/p&gt;  &lt;p&gt;Brogan, of the FBI, says it’s an incredibly valuable tool, made more so by Nixon’s own contributions. Other security firms scrape online criminal spaces as well, but they seldom share the content with outsiders, and Brogan says Nixon’s work is unique because she engages with the actors in chat spaces to draw out information from them that “would not be otherwise normally available.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The preservation project she started when she got to Unit 221B could not have been better timed, because it coincided with the pandemic, the surge in new Com membership, and the emergence of two disturbing Com offshoots, CVLT and 764. She was able to capture their chats as these groups first emerged; after law enforcement arrested leaders of the groups and took control of the servers where their chats were posted, this material went offline.&lt;/p&gt;  &lt;p&gt;CVLT—pronounced “cult”—was reportedly founded around 2019 with a focus on sextortion and child sexual abuse material. 764 emerged from CVLT and was spearheaded by a 15-year-old in Texas named Bradley Cadenhead, who named it after the first digits of his zip code. Its focus was extremism and violence.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_20"&gt; &lt;p&gt;In 2021, because of what she observed in these groups, Nixon turned her attention to sextortion among Com members.&lt;/p&gt;  &lt;p&gt;The type of sextortion they engaged in has its roots in activity that began a decade ago as “fan signing.” Hackers would use the threat of doxxing to coerce someone, usually a young female, into writing the hacker’s handle on a piece of paper. The hacker would use a photo of it as an avatar on his online accounts—a kind of trophy. Eventually some began blackmailing victims into writing the hacker’s handle on their face, breasts, or genitals. With CVLT, this escalated even further; targets were blackmailed into carving a Com member’s name into their skin or engaging in sexually explicit acts while recording or livestreaming themselves.&lt;/p&gt;  &lt;p&gt;During the pandemic a surprising number of SIM swappers crossed into child sexual abuse material and sadistic sextortion, according to Nixon. She hates tracking this gruesome activity, but she saw an opportunity to exploit it for good. She had long been frustrated at how leniently judges treated financial fraudsters because of their crimes’ seemingly nonviolent nature. But she saw a chance to get harsher sentences for them if she could tie them to their sextortion and began to focus on these crimes.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;At this point, Waifu still wasn’t on her radar. But that was about to change.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Endgame&lt;/h3&gt;  &lt;p&gt;Nixon landed in Waifu’s crosshairs after he and fellow members of the Com were involved in a large hack involving AT&amp;amp;T customer call records in April 2024.&lt;/p&gt;  &lt;p&gt;Waifu’s group gained access to dozens of cloud accounts with Snowflake, a company that provides online data storage for customers. One of those customers had more than 50 billion call logs of AT&amp;amp;T wireless subscribers stored in its Snowflake account.&amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;They tried to re-extort the telecom, threatening on social media to leak the records. They tagged the FBI in the post. “It’s like they were begging to be investigated,” says Nixon.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;Among the subscriber records were call logs for FBI agents who were AT&amp;amp;T customers. Nixon and other researchers believe the hackers may have been able to identify the phone numbers of agents through other means. Then they may have used a reverse-lookup program to identify the owners of phone numbers that the agents called or that called them and found Nixon’s number among them. This is when they began harassing her.&lt;/p&gt;  &lt;p&gt;But then they got reckless. They allegedly extorted nearly $400,000 from AT&amp;amp;T in exchange for promising to delete the call records they’d stolen. Then they tried to re-extort the telecom, threatening on social media to leak the records they claimed to have deleted if it didn’t pay more. They tagged the FBI in the post.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_22"&gt; &lt;p&gt;“It’s like they were begging to be investigated,” says Nixon.&lt;/p&gt;  &lt;p&gt;The Snowflake breaches and AT&amp;amp;T records theft were grabbing headlines at the time, but Nixon had no idea her number was in the stolen logs or that Waifu/Judische was a prime suspect in the breaches. So she was perplexed when he started taunting and threatening her online.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1132663" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/MIT-Tech-Review-ILLU-2-FINAL-franziska.jpg?w=1429" width="1429" /&gt;&lt;div class="image-credit"&gt;FRANZISKA BARCZYK&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Over several weeks in May and June, a pattern developed. Waifu or one of his associates would post a threat against her and then post a message online inviting her to talk. She assumes now that they believed she was helping law enforcement investigate the Snowflake breaches and hoped to draw her into a dialogue to extract information from her about what authorities knew. But Nixon wasn’t helping the FBI investigate them yet. It was only after she began looking at Waifu for the threats that she became aware of his suspected role in the Snowflake hack.&lt;/p&gt;  &lt;p&gt;It wasn’t the first time she had studied him, though. Waifu had come to her attention in 2019 when he bragged about framing another Com member for a hoax bomb threat and later talked about his involvement in SIM-swapping operations. He made an impression on her. He clearly had technical skills, but Nixon says he also often appeared immature, impulsive, and emotionally unstable, and he was desperate for attention in his interactions with other members. He bragged about not needing sleep and using Adderall to hack through the night. He was also a bit reckless about protecting personal details. He wrote in private chats to another researcher that he would never get caught because he was good at OPSEC, but he also told the researcher that he lived in Canada—which turned out to be true.&lt;/p&gt;  &lt;p&gt;Nixon’s process for unmasking Waifu followed a general recipe she used to unmask Com members: She’d draw a large investigative circle around a target and all the personas that communicated with that person online, and then study their interactions to narrow the circle to the people with the most significant connections to the target. Some of the best leads came from a target’s enemies; she could glean a lot of information about their identity, personality, and activities from what the people they fought with online said about them.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_24"&gt; &lt;p&gt;“The enemies and the ex-girlfriends, generally speaking, are the best [for gathering intelligence on a suspect],” she says. “I love them.”&lt;/p&gt;  &lt;p&gt;While she was doing this, Waifu and his group were reaching out to other security researchers, trying to glean information about Nixon and what she might be investigating. They also attempted to plant false clues with the researchers by dropping the names of other cybercriminals in Canada who could plausibly be Waifu. Nixon had never seen cybercriminals engage in counterintelligence tactics like this.&lt;/p&gt;  &lt;p&gt;Amid this subterfuge and confusion, Nixon and another researcher working with her did a lot of consulting and cross-checking with other researchers about the clues they were gathering to ensure they had the right name before they gave it to the FBI.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_26"&gt; &lt;p&gt;By July she and the researcher were convinced they had their guy: Connor Riley Moucka, a 25-year-old high school dropout living with his grandfather in Ontario. On October 30, Royal Canadian Mounted Police converged on Moucka’s home and arrested him.&lt;/p&gt;  &lt;p&gt;According to an affidavit filed in Canadian court, a plainclothes Canadian police officer visited Moucka’s house under some pretense on the afternoon of October 21, nine days before the arrest, to secretly capture a photo of him and compare it with an image US authorities had provided. The officer knocked and rang the bell; Moucka opened the door looking disheveled and told the visitor: “You woke me up, sir.” He told the officer his name was Alex; Moucka sometimes used the alias Alexander Antonin Moucka. Satisfied that the person who answered the door was the person the US was seeking, the officer left. Waifu’s online rants against Nixon escalated at this point, as did his attempts at misdirection. She believes the visit to his door spooked him.&lt;/p&gt;  &lt;p&gt;Nixon won’t say exactly how they unmasked Moucka—only that he made a mistake.&lt;/p&gt;  &lt;p&gt;“I don’t want to train these people in how to not get caught [by revealing his error],” she says.&lt;/p&gt;  &lt;p&gt;The Canadian affidavit against Moucka reveals a number of other violent posts he’s alleged to have made online beyond the threats he made against her. Some involve musings about becoming a serial killer or mass-mailing sodium nitrate pills to Black people in Michigan and Ohio; in another, his online persona talks about obtaining firearms to “kill Canadians” and commit “suicide by cop.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Prosecutors, who list Moucka’s online aliases as including Waifu, Judische, and two more in the indictment, say he and others extorted at least $2.5 million from at least three victims whose data they stole from Snowflake accounts. Moucka has been charged with nearly two dozen counts, including conspiracy, unauthorized access to computers, extortion, and wire fraud. He has pleaded not guilty and was extradited to the US last July. His trial is scheduled for October this year, though hacking cases usually end in plea agreements rather than going to trial.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It took months for authorities to arrest Moucka after Nixon and her colleague shared their findings with the authorities, but an alleged associate of his in the Snowflake conspiracy, a US Army soldier named Cameron John Wagenius (Kiberphant0m online), was arrested more quickly.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;On November 10, 2024, Nixon and her team found a mistake Wagenius made that helped identify him, and on December 20 he was arrested. Wagenius has already pleaded guilty to two charges around the sale or attempted sale of confidential phone records and will be sentenced this March.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_28"&gt;&lt;p&gt;These days Nixon continues to investigate sextortion among Com members. But she says that remaining members of Waifu’s group still taunt and threaten her.&lt;/p&gt;  &lt;p&gt;“They are continuing to persist in their nonsense, and they are getting taken out one by one,” she says. “And I’m just going to keep doing that until there’s no one left on that side.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Kim Zetter is a journalist who covers cybersecurity and national security. She is the author of&lt;/em&gt; Countdown to Zero Day.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;The threats started in spring.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In April 2024, a mysterious someone using the online handles “Waifu” and “Judische” began posting death threats on Telegram and Discord channels aimed at a cybersecurity researcher named Allison Nixon.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Alison [sic] Nixon is gonna get necklaced with a tire filled with gasoline soon,” wrote Waifu/Judische, both of which are words with offensive connotations. “Decerebration is my fav type of brain death, thats whats gonna happen to alison Nixon.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It wasn’t long before others piled on. Someone shared AI-generated nudes of Nixon.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;These anonymous personas targeted Nixon because she had become a formidable threat: As chief research officer at the cyber investigations firm Unit 221B, named after Sherlock Holmes’s apartment, she had built a career tracking cybercriminals and helping get them arrested. For years she had lurked quietly in online chat channels or used pseudonyms to engage with perpetrators directly while piecing together clues they’d carelessly drop about themselves and their crimes. This had helped her bring to justice a number of cybercriminals—especially members of a loosely affiliated subculture of anarchic hackers who call themselves the Com.&lt;/p&gt;  &lt;p&gt;But members of the Com aren’t just involved in hacking; some of them also engage in offline violence against researchers who track them. This includes bricking (throwing a brick through a victim’s window) and swatting (a dangerous type of hoax that involves reporting a false murder or hostage situation at someone’s home so SWAT teams will swarm it with guns drawn). Members of a Com offshoot known as 764 have been accused of even more violent acts—including animal torture, stabbings, and school shootings—or of inciting others in and outside the Com to commit these crimes.&lt;/p&gt; 
 &lt;p&gt;Nixon started tracking members of the community more than a decade ago, when other researchers and people in law enforcement were largely ignoring them because they were young—many in their teens. Her early attention allowed her to develop strategies for unmasking them.&lt;/p&gt;  &lt;p&gt;Ryan Brogan, a special agent with the FBI, says Nixon has helped him and colleagues identify and arrest more than two dozen members of the community since 2011, when he first began working with her, and that her skills in exposing them are unparalleled. “If you get on Allison’s and my radar, you’re going [down]. It’s just a matter of time,” he says. “No matter how much digital anonymity and tradecraft you try to apply, you’re done.”&lt;/p&gt;  &lt;p&gt;Though she’d done this work for more than a decade, Nixon couldn’t understand why the person behind the Waifu/Judische accounts was suddenly threatening her. She had given media interviews about the Com—most recently on &lt;em&gt;60 Minutes&lt;/em&gt;—but not about her work unmasking members to get them arrested, so the hostility seemed to come out of the blue. And although she had taken an interest in the Waifu persona in years past for crimes he boasted about committing, he hadn’t been on her radar for a while when the threats began, because she was tracking other targets.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now Nixon resolved to unmask Waifu/Judische and others responsible for the death threats—and take them down for crimes they admitted to committing. “Prior to them death-threatening me, I had no reason to pay attention to them,” she says.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;Com beginnings&lt;/h3&gt;  &lt;p&gt;Most people have never heard of the Com, but its influence and threat are growing.&lt;/p&gt;  &lt;p&gt;It’s an online community comprising loosely affiliated groups of, primarily, teens and twentysomethings in North America and English-speaking parts of Europe who have become part of what some call a cybercrime youth movement.&amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;International laws and norms, and fears of retaliation, prevent states from going all out in cyber operations. That doesn’t stop the anarchic Com.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;Over the last decade, its criminal activities have escalated from simple distributed denial-of-service (DDoS) attacks that disrupt websites to SIM-swapping hacks that hijack a victim’s phone service, as well as crypto theft, ransomware attacks, and corporate data theft. These crimes have affected AT&amp;amp;T, Microsoft, Uber, and others. Com members have also been involved in various forms of sextortion aimed at forcing victims to physically harm themselves or record themselves doing sexually explicit activities. The Com’s impact has also spread beyond the digital realm to kidnapping, beatings, and other violence.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;One longtime cybercrime researcher, who asked to remain anonymous because of his work, says the Com is as big a threat in the cyber realm as Russia and China—for one unusual reason.&lt;/p&gt; 

 &lt;p&gt;“There’s only so far that China is willing to go; there’s only so far that Russia or North Korea is willing to go,” he says, referring to international laws and norms, and fears of retaliation, that prevent states from going all out in cyber operations. That doesn’t stop the anarchic Com, he says.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1132662" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/MIT-Tech-Review-ILLU-1-FINAL-franziska.jpg?w=1429" width="1429" /&gt;&lt;div class="image-credit"&gt;FRANZISKA BARCZYK&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;“It is a pretty significant threat, and people tend to … push it under the rug [because] it’s just a bunch of kids,” he says. “But look at the impact [they have].”&lt;/p&gt;  &lt;p&gt;Brogan says the amount of damage they do in terms of monetary losses “can become staggering very quickly.”&lt;/p&gt;  &lt;p&gt;There is no single site where Com members congregate; they spread across a number of web forums and Telegram and Discord channels. The group follows a long line of hacking and subculture communities that emerged online over the last two decades, gained notoriety, and then faded or vanished after prominent members were arrested or other factors caused their decline. They differed in motivation and activity, but all emerged from “the same primordial soup,” says Nixon. The Com’s roots can be traced to the Scene, which began as a community of various “warez” groups engaged in pirating computer games, music, and movies.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;When Nixon began looking at the Scene, in 2011, its members were hijacking gaming accounts, launching DDoS attacks, and running booter services. (DDoS attacks overwhelm a server or computer with traffic from bot-controlled machines, preventing legitimate traffic from getting through; booters are tools that anyone can rent to launch a DDoS attack against a target of choice.) While they made some money, their primary goal was notoriety.&lt;/p&gt;  &lt;p&gt;This changed around 2018. Cryptocurrency values were rising, and the Com—or the Community, as it sometimes called itself—emerged as a subgroup that ultimately took over the Scene. Members began to focus on financial gain—cryptocurrency theft, data theft, and extortion.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;The pandemic two years later saw a surge in Com membership that Nixon attributes to social isolation and the forced movement of kids online for schooling. But she believes economic conditions and socialization problems have also driven its growth. Many Com members can’t get jobs because they lack skills or have behavioral issues, she says. A number who have been arrested have had troubled home lives and difficulty adapting to school, and some have shown signs of mental illness. The Com provides camaraderie, support, and an outlet for personal frustrations. Since 2018, it has also offered some a solution to their money problems.&lt;/p&gt;  &lt;p&gt;Loose-knit cells have sprouted from the community—Star Fraud, ShinyHunters, Scattered Spider, Lapsus$—to collaborate on clusters of crime. They usually target high-profile crypto bros and tech giants and have made millions of dollars from theft and extortion, according to court records.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But dominance, power, and bragging rights are still motivators, even in profit operations, says the cybercrime researcher, which is partly why members target “big whales.”&lt;/p&gt;  &lt;p&gt;“There is financial gain,” he says, “but it’s also [sending a message that] I can reach out and touch the people that think they’re untouchable.” In fact, Nixon says, some members of the Com have overwhelming ego-driven motivations that end up conflicting with their financial motives.&lt;/p&gt; 
 &lt;p&gt;“Often their financial schemes fall apart because of their ego, and that phenomenon is also what I’ve made my career on,” she says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The hacker hunter emerges&lt;/h3&gt;  &lt;p&gt;Nixon has straight dark hair, wears wire-rimmed glasses, and has a slight build and bookish demeanor that, on first impression, could allow her to pass for a teen herself. She talks about her work in rapid cadences, like someone whose brain is filled with facts that are under pressure to get out, and she exudes a sense of urgency as she tries to make people understand the threat the Com poses. She doesn’t suppress her happiness when someone she’s been tracking gets arrested.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;In 2011, when she first began investigating the communities from which the Com emerged, she was working the night shift in the security operations center of the security firm SecureWorks. The center responded to tickets and security alerts emanating from customer networks, but Nixon coveted a position on the company’s counter-threats team, which investigated and published threat-intelligence reports on mostly state-sponsored hacking groups from China and Russia. Without connections or experience, she had no path to investigative work. But Nixon is an intensely curious person, and this created its own path.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Allison Nixon" class="wp-image-1132672" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/GROUP-SHOTS-10267_v2.edit_.jpg" /&gt;&lt;figcaption class="wp-element-caption"&gt;Allison Nixon is chief research officer at the cybersecurity investigations firm Unit 221B, where she tracks cybercriminals and helps bring them to justice.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;YLVA EREVALL&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Where the threat team focused on the impact hackers had on customer networks—how they broke in, what they stole—Nixon was more interested in their motivations and the personality traits that drove their actions. She assumed there must be online forums where criminal hackers congregated, so she googled “hacking forums” and landed on a site called Hack Forums.&lt;/p&gt;  &lt;p&gt;“It was really stupid simple,” she says.&lt;/p&gt;  &lt;p&gt;She was surprised to see members openly discussing their crimes there. She reached out to someone on the SecureWorks threat team to see if he was aware of the site, and he dismissed it as a place for “script kiddies”—a pejorative term for unskilled hackers.&lt;/p&gt; 
 &lt;p&gt;This was a time when many cybersecurity pros were shifting their focus away from cybercrime to state-sponsored hacking operations, which were more sophisticated and getting a lot of attention. But Nixon likes to zig where others zag, and her colleague’s dismissiveness fueled her interest in the forums. Two other SecureWorks colleagues shared that interest, and the three studied the forums during downtime on their shifts. They focused on trying to identify the people running DDoS booters.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;What Nixon loved about the forums was how accessible they were to a beginner like herself. Threat-intelligence teams require privileged access to a victim’s network to investigate breaches. But Nixon could access everything she needed in the public forums, where the hackers seemed to think no one was watching. Because of this, they often made mistakes in operational security, or OPSEC—letting slip little biographical facts such as the city where they lived, a school they attended, or a place they used to work. These details revealed in their chats, combined with other information, could help expose the real identities behind their anonymous masks.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It was a shock to me that it was relatively easy to figure out who [they were],” she says.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;She wasn’t bothered by the immature boasting and petty fights that dominated the forums. “A lot of people don’t like to do this work of reading chat logs. I realize that this is a very uncommon thing. And maybe my brain is built a little weird that I’m willing to do this,” she says. “I have a special talent that I can wade through garbage and it doesn’t bother me.”&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;Nixon soon realized that not all the members were script kiddies. Some exhibited real ingenuity and “powerful” skills, she says, but because they were applying these to frivolous purposes—hijacking gamer accounts instead of draining bank accounts—researchers and law enforcement were ignoring them. Nixon began tracking them, suspecting that they would eventually direct their skills at more significant targets—an intuition that proved to be correct. And when they did, she had already amassed a wealth of information about them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;She continued her DDoS research for two years until a turning point in 2013, when the cybersecurity journalist Brian Krebs, who made a career tracking cybercriminals, got swatted.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;About a dozen people from the security community worked with Krebs to expose the perpetrator, and Nixon was invited to help. Krebs sent her pieces of the puzzle to investigate, and eventually the group identified the culprit (though it would take two years for him to be arrested). When she was invited to dinner with Krebs and the other investigators, she realized she’d found her people.&lt;/p&gt;  &lt;p&gt;“It was an amazing moment for me,” she says. “I was like, wow, there’s all these like-minded people that just want to help and are doing it just for the love of the game, basically.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Staying one step ahead&lt;/h3&gt;  &lt;p&gt;It was porn stars who provided Nixon with her next big research focus—one that underscored her skill at spotting Com actors and criminal trends in their nascent stages, before they emerged as major threats.&lt;/p&gt;  &lt;p&gt;In 2018, someone was hijacking the social media accounts of certain adult-film stars and using those accounts to blast out crypto scams to their large follower bases. Nixon couldn’t figure out how the hackers had hijacked the social media profiles, but she promised to help the actors regain access to their accounts if they agreed to show her the private messages the hackers had sent or received during the time they controlled them. These messages led her to a forum where members were talking about how they stole the accounts. The hackers had tricked some of these actors into disclosing the mobile phone numbers of others. Then they used a technique called SIM swapping to reset passwords for social media accounts belonging to those other stars, locking them out.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In SIM swapping, fraudsters get a victim’s phone number assigned to a SIM card and phone &lt;em&gt;they&lt;/em&gt; control, so that calls and messages intended for the victim go to them instead. This includes one-time security codes that sites text to account holders to verify themselves when accessing their account or changing its password. In some of the cases involving the porn stars, the hackers had manipulated telecom workers into making the SIM swaps for what they thought were legitimate reasons, and in other cases they bribed the workers to make the change. The hackers were then able to alter the password on the actors’ social media accounts, lock out the owners, and use the accounts to advertise their crypto scams.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;SIM swapping is a powerful technique that can be used to hijack and drain entire cryptocurrency and bank accounts, so Nixon was surprised to see the fraudsters using it for relatively unprofitable schemes. But SIM swapping had rarely been used for financial fraud at that point, and like the earlier hackers Nixon had seen on Hack Forums, the ones hijacking porn star accounts didn’t seem to grasp the power of the technique they were using. Nixon suspected that this would change and SIM swapping would soon become a major problem, so she shifted her research focus accordingly. It didn’t take long for the fraudsters to pivot as well.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt; &lt;p&gt;Nixon’s skill at looking ahead in this way has served her throughout her career. On multiple occasions a hacker or hacking group would catch her attention—for using a novel hacking approach in some minor operation, for example—and she’d begin tracking their online posts and chats in the belief that they’d eventually do something significant with that skill.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;They usually did. When they later grabbed headlines with a showy or impactful operation, these hackers would seem to others to have emerged from nowhere, sending researchers and law enforcement scrambling to understand who they were. But Nixon would already have a dossier compiled on them and, in some cases, had unmasked their real identity as well. Lizard Squad was an example of this. The group burst into the headlines in 2014 and 2015 with a series of high-profile DDoS campaigns, but Nixon and colleagues at the job where she worked at the time had already been watching its members as individuals for a while. So the FBI sought their assistance in identifying them.&lt;/p&gt;  &lt;p&gt;“The thing about these young hackers is that they … keep going until they get arrested, but it takes years for them to get arrested,” she says. “So a huge aspect of my career is just sitting on this information that has not been actioned [yet].”&lt;/p&gt;  &lt;p&gt;It was during the Lizard Squad years that Nixon began developing tools to scrape and record hacker communications online, though it would be years before she began using these concepts to scrape the Com chatrooms and forums. These channels held a wealth of data that might not seem useful during the nascent stage of a hacker’s career but could prove critical later, when law enforcement got around to investigating them; yet the contents were always at risk of being deleted by Com members or getting taken down by law enforcement when it seized websites and chat channels.&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;Nixon’s work is unique because she engages with the actors in chat spaces to draw out information from them that “would not be otherwise normally available.”&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;Over several years, she scraped and preserved whatever chatrooms she was investigating. But it wasn’t until early 2020, when she joined Unit 221B, that she got the chance to scrape the Telegram and Discord channels of the Com. She pulled all of this data together into a searchable platform that other researchers and law enforcement could use. The company hired two former hackers to help build scraping tools and infrastructure for this work; the result is eWitness, a community-driven, invitation-­only platform. It was initially seeded only with data Nixon had collected after she arrived at Unit 221B, but has since been augmented with data that other users of the platform have scraped from Com social spaces as well, some of which doesn’t exist in public forums anymore.&lt;/p&gt;  &lt;p&gt;Brogan, of the FBI, says it’s an incredibly valuable tool, made more so by Nixon’s own contributions. Other security firms scrape online criminal spaces as well, but they seldom share the content with outsiders, and Brogan says Nixon’s work is unique because she engages with the actors in chat spaces to draw out information from them that “would not be otherwise normally available.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The preservation project she started when she got to Unit 221B could not have been better timed, because it coincided with the pandemic, the surge in new Com membership, and the emergence of two disturbing Com offshoots, CVLT and 764. She was able to capture their chats as these groups first emerged; after law enforcement arrested leaders of the groups and took control of the servers where their chats were posted, this material went offline.&lt;/p&gt;  &lt;p&gt;CVLT—pronounced “cult”—was reportedly founded around 2019 with a focus on sextortion and child sexual abuse material. 764 emerged from CVLT and was spearheaded by a 15-year-old in Texas named Bradley Cadenhead, who named it after the first digits of his zip code. Its focus was extremism and violence.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_20"&gt; &lt;p&gt;In 2021, because of what she observed in these groups, Nixon turned her attention to sextortion among Com members.&lt;/p&gt;  &lt;p&gt;The type of sextortion they engaged in has its roots in activity that began a decade ago as “fan signing.” Hackers would use the threat of doxxing to coerce someone, usually a young female, into writing the hacker’s handle on a piece of paper. The hacker would use a photo of it as an avatar on his online accounts—a kind of trophy. Eventually some began blackmailing victims into writing the hacker’s handle on their face, breasts, or genitals. With CVLT, this escalated even further; targets were blackmailed into carving a Com member’s name into their skin or engaging in sexually explicit acts while recording or livestreaming themselves.&lt;/p&gt;  &lt;p&gt;During the pandemic a surprising number of SIM swappers crossed into child sexual abuse material and sadistic sextortion, according to Nixon. She hates tracking this gruesome activity, but she saw an opportunity to exploit it for good. She had long been frustrated at how leniently judges treated financial fraudsters because of their crimes’ seemingly nonviolent nature. But she saw a chance to get harsher sentences for them if she could tie them to their sextortion and began to focus on these crimes.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;At this point, Waifu still wasn’t on her radar. But that was about to change.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Endgame&lt;/h3&gt;  &lt;p&gt;Nixon landed in Waifu’s crosshairs after he and fellow members of the Com were involved in a large hack involving AT&amp;amp;T customer call records in April 2024.&lt;/p&gt;  &lt;p&gt;Waifu’s group gained access to dozens of cloud accounts with Snowflake, a company that provides online data storage for customers. One of those customers had more than 50 billion call logs of AT&amp;amp;T wireless subscribers stored in its Snowflake account.&amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;They tried to re-extort the telecom, threatening on social media to leak the records. They tagged the FBI in the post. “It’s like they were begging to be investigated,” says Nixon.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;Among the subscriber records were call logs for FBI agents who were AT&amp;amp;T customers. Nixon and other researchers believe the hackers may have been able to identify the phone numbers of agents through other means. Then they may have used a reverse-lookup program to identify the owners of phone numbers that the agents called or that called them and found Nixon’s number among them. This is when they began harassing her.&lt;/p&gt;  &lt;p&gt;But then they got reckless. They allegedly extorted nearly $400,000 from AT&amp;amp;T in exchange for promising to delete the call records they’d stolen. Then they tried to re-extort the telecom, threatening on social media to leak the records they claimed to have deleted if it didn’t pay more. They tagged the FBI in the post.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_22"&gt; &lt;p&gt;“It’s like they were begging to be investigated,” says Nixon.&lt;/p&gt;  &lt;p&gt;The Snowflake breaches and AT&amp;amp;T records theft were grabbing headlines at the time, but Nixon had no idea her number was in the stolen logs or that Waifu/Judische was a prime suspect in the breaches. So she was perplexed when he started taunting and threatening her online.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1132663" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/MIT-Tech-Review-ILLU-2-FINAL-franziska.jpg?w=1429" width="1429" /&gt;&lt;div class="image-credit"&gt;FRANZISKA BARCZYK&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Over several weeks in May and June, a pattern developed. Waifu or one of his associates would post a threat against her and then post a message online inviting her to talk. She assumes now that they believed she was helping law enforcement investigate the Snowflake breaches and hoped to draw her into a dialogue to extract information from her about what authorities knew. But Nixon wasn’t helping the FBI investigate them yet. It was only after she began looking at Waifu for the threats that she became aware of his suspected role in the Snowflake hack.&lt;/p&gt;  &lt;p&gt;It wasn’t the first time she had studied him, though. Waifu had come to her attention in 2019 when he bragged about framing another Com member for a hoax bomb threat and later talked about his involvement in SIM-swapping operations. He made an impression on her. He clearly had technical skills, but Nixon says he also often appeared immature, impulsive, and emotionally unstable, and he was desperate for attention in his interactions with other members. He bragged about not needing sleep and using Adderall to hack through the night. He was also a bit reckless about protecting personal details. He wrote in private chats to another researcher that he would never get caught because he was good at OPSEC, but he also told the researcher that he lived in Canada—which turned out to be true.&lt;/p&gt;  &lt;p&gt;Nixon’s process for unmasking Waifu followed a general recipe she used to unmask Com members: She’d draw a large investigative circle around a target and all the personas that communicated with that person online, and then study their interactions to narrow the circle to the people with the most significant connections to the target. Some of the best leads came from a target’s enemies; she could glean a lot of information about their identity, personality, and activities from what the people they fought with online said about them.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_24"&gt; &lt;p&gt;“The enemies and the ex-girlfriends, generally speaking, are the best [for gathering intelligence on a suspect],” she says. “I love them.”&lt;/p&gt;  &lt;p&gt;While she was doing this, Waifu and his group were reaching out to other security researchers, trying to glean information about Nixon and what she might be investigating. They also attempted to plant false clues with the researchers by dropping the names of other cybercriminals in Canada who could plausibly be Waifu. Nixon had never seen cybercriminals engage in counterintelligence tactics like this.&lt;/p&gt;  &lt;p&gt;Amid this subterfuge and confusion, Nixon and another researcher working with her did a lot of consulting and cross-checking with other researchers about the clues they were gathering to ensure they had the right name before they gave it to the FBI.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_26"&gt; &lt;p&gt;By July she and the researcher were convinced they had their guy: Connor Riley Moucka, a 25-year-old high school dropout living with his grandfather in Ontario. On October 30, Royal Canadian Mounted Police converged on Moucka’s home and arrested him.&lt;/p&gt;  &lt;p&gt;According to an affidavit filed in Canadian court, a plainclothes Canadian police officer visited Moucka’s house under some pretense on the afternoon of October 21, nine days before the arrest, to secretly capture a photo of him and compare it with an image US authorities had provided. The officer knocked and rang the bell; Moucka opened the door looking disheveled and told the visitor: “You woke me up, sir.” He told the officer his name was Alex; Moucka sometimes used the alias Alexander Antonin Moucka. Satisfied that the person who answered the door was the person the US was seeking, the officer left. Waifu’s online rants against Nixon escalated at this point, as did his attempts at misdirection. She believes the visit to his door spooked him.&lt;/p&gt;  &lt;p&gt;Nixon won’t say exactly how they unmasked Moucka—only that he made a mistake.&lt;/p&gt;  &lt;p&gt;“I don’t want to train these people in how to not get caught [by revealing his error],” she says.&lt;/p&gt;  &lt;p&gt;The Canadian affidavit against Moucka reveals a number of other violent posts he’s alleged to have made online beyond the threats he made against her. Some involve musings about becoming a serial killer or mass-mailing sodium nitrate pills to Black people in Michigan and Ohio; in another, his online persona talks about obtaining firearms to “kill Canadians” and commit “suicide by cop.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Prosecutors, who list Moucka’s online aliases as including Waifu, Judische, and two more in the indictment, say he and others extorted at least $2.5 million from at least three victims whose data they stole from Snowflake accounts. Moucka has been charged with nearly two dozen counts, including conspiracy, unauthorized access to computers, extortion, and wire fraud. He has pleaded not guilty and was extradited to the US last July. His trial is scheduled for October this year, though hacking cases usually end in plea agreements rather than going to trial.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It took months for authorities to arrest Moucka after Nixon and her colleague shared their findings with the authorities, but an alleged associate of his in the Snowflake conspiracy, a US Army soldier named Cameron John Wagenius (Kiberphant0m online), was arrested more quickly.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;On November 10, 2024, Nixon and her team found a mistake Wagenius made that helped identify him, and on December 20 he was arrested. Wagenius has already pleaded guilty to two charges around the sale or attempted sale of confidential phone records and will be sentenced this March.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_28"&gt;&lt;p&gt;These days Nixon continues to investigate sextortion among Com members. But she says that remaining members of Waifu’s group still taunt and threaten her.&lt;/p&gt;  &lt;p&gt;“They are continuing to persist in their nonsense, and they are getting taken out one by one,” she says. “And I’m just going to keep doing that until there’s no one left on that side.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Kim Zetter is a journalist who covers cybersecurity and national security. She is the author of&lt;/em&gt; Countdown to Zero Day.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/16/1132526/allison-nixon-hackers-security-researcher/</guid><pubDate>Mon, 16 Feb 2026 11:00:00 +0000</pubDate></item><item><title>The scientist using AI to hunt for antibiotics just about everywhere (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/16/1132516/cesar-de-la-fuente-using-ai-antibiotics-hunt/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/DSC9306_social.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;When he was just a teenager trying to decide what to do with his life, César de la Fuente compiled a list of the world’s biggest problems. He ranked them inversely by how much money governments were spending to solve them. Antimicrobial resistance topped the list.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Twenty years on, the problem has not gone away. If anything, it’s gotten worse. Infections caused by bacteria, fungi, and viruses that have evolved ways to evade treatments are now associated with more than 4 million deaths per year, and a recent analysis, published in the &lt;em&gt;Lancet&lt;/em&gt;, predicts that number could surge past 8 million by 2050. In a July 2025 essay in &lt;em&gt;Physical Review Letters&lt;/em&gt;, de la Fuente, now a bioengineer and computational biologist, and synthetic biologist James Collins warned of a looming “post­antibiotic” era in which infections from drug-resistant strains of common bacteria like &lt;em&gt;Escherichia coli &lt;/em&gt;or &lt;em&gt;Staphylococcus aureus&lt;/em&gt;, which can often still be treated by our current arsenal of medications, become fatal. “The antibiotic discovery pipeline remains perilously thin,” they wrote, “impeded by high development costs, lengthy timelines, and low returns on investment.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;But de la Fuente is using artificial intelligence to bring about a different future. His team at the University of Pennsylvania is training AI tools to search genomes far and deep for peptides with antibiotic properties. His vision is to assemble those peptides—molecules made of up to 50 amino acids linked together—into various configurations, including some never seen in nature. The results, he hopes, could defend the body against microbes that withstand traditional treatments.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;His quest has unearthed promising candidates in unexpected places. In August 2025 his team, which includes 16 scientists in Penn’s Machine Biology Group, described peptides hiding in the genetic code of ancient single-celled organisms called archaea. Before that, they’d excavated a list of candidates from the venom of snakes, wasps, and spiders. And in an ongoing project de la Fuente calls “molecular de-­extinction,” he and his collaborators have been scanning published genetic sequences of extinct species for potentially functional molecules. Those species include hominids like Neanderthals and Denisovans and charismatic megafauna like woolly mammoths, as well as ancient zebras and penguins. In the history of life on Earth, de la Fuente reasons, maybe some organism evolved an antimicrobial defense that could be helpful today. Those long-gone codes have given rise to resurrected compounds with names like ­mammuthusin-2 (from woolly mammoth DNA), mylodonin-2 (from the giant sloth), and hydrodamin-1 (from the ancient sea cow). Over the last few years, this molecular binge has enabled de la Fuente to amass a library of more than a million genetic recipes.&lt;/p&gt; 
 &lt;p&gt;At 40 years old, de la Fuente has also collected a trophy case of awards from the American Society for Microbiology, the American Chemical Society, and other organizations. (In 2019, this magazine named him one of “35 Innovators Under 35” for bringing computational approaches to antibiotic discovery.) He’s widely recognized as a leader in the effort to harness AI for real-world problems. “He’s really helped pioneer that space,” says Collins, who is at MIT. (The two have not collaborated in the laboratory, but Collins has long been at the forefront of using AI for drug discovery, including the search for antibiotics. In 2020, Collins’s team used an AI model to predict a broad-­spectrum antibiotic, halicin, that is now in preclinical development.)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The world of antibiotic development needs as much creativity and innovation as researchers can muster, says Collins. And de la Fuente’s work on peptides has pushed the field forward: “César is marvelously talented, very innovative.”&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;A messy, noisy endeavor&lt;/h3&gt;  &lt;p&gt;De la Fuente describes antimicrobial resistance as an “almost impossible” problem, but he sees plenty of room for exploration in the word &lt;em&gt;almost. &lt;/em&gt;“I like challenges,” he says, “and I think this is the ultimate challenge.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The use, overuse, and misuse of antibiotics, he says, drives antimicrobial resistance. And the problem is growing unchecked because conventional ways to find, make, and test the drugs are prohibitively expensive and often lead to dead ends. “A lot of the companies that have attempted to do antibiotic development in the past have ended up folding because there’s no good return on investment at the end of the day,” he says.&lt;/p&gt;  &lt;p&gt;Antibiotic discovery has always been a messy, noisy endeavor, driven by serendipity and fraught with uncertainty and misdirection. For decades, researchers have largely relied on brute-force mechanical methods. “Scientists dig into soil, they dig into water,” says de la Fuente. “And then from that complex organic matter they try to extract antimicrobial molecules.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But molecules can be extraordinarily complex. Researchers have estimated the number of possible organic combinations that could be synthesized at somewhere around 10&lt;sup&gt;60&lt;/sup&gt;. For reference, Earth contains an estimated 10&lt;sup&gt;18&lt;/sup&gt; grains of sand. “Drug discovery in any domain is a statistics game,” says Jonathan Stokes, a chemical biologist at McMaster University in Canada, who has been using generative AI to design potential new antibiotics that can be synthesized in a lab, and who worked with Collins on halicin. “You need enough shots on goal to happen to get one.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Those have to be good shots, though. And AI seems well suited to improving researchers’ aim. Biology is an information source, de la Fuente explains: “It’s like a bunch of code.” The code of DNA has four letters; proteins and peptides have 20, where each “letter” represents an amino acid. De la Fuente says his work amounts to training AI models to recognize sequences of letters that encode antimicrobial peptides, or AMPs. “If you think about it that way,” he says, “you can devise algorithms to mine the code and identify functional molecules, which can be antimicrobials. Or antimalarials. Or anticancer agents.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Practically speaking, we’re still not there: These peptides haven’t yet been transformed into usable drugs that help people, and there are plenty of details—dosage, delivery, specific targets—that need to be sorted out, says de la Fuente. But AMPs are appealing because the body already uses them.They’re a critical part of the immune system and often the first line of defense against pathogenic infections. Unlike conventional antibiotics, which typically have one trick for killing bacteria, AMPs often exhibit a multimodal approach. They may disrupt the cell wall and the genetic material inside as well as a variety of cellular processes. A bacterial pathogen may evolve resistance to a conventional drug’s single mode of action, but maybe not to a multipronged AMP attack.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;From discovery to delivery&lt;/h3&gt;  &lt;p&gt;De la Fuente’s group is one of many pushing the boundaries of using AI for antibiotics. Where he focuses primarily on peptides, Collins works on small-molecule discovery. So does Stokes, at McMaster, whose models identify promising new molecules and predict whether they can be synthesized. “It’s only been a few years since folks have been using AI meaningfully in drug discovery,” says Collins.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even in that short time the tools have changed, says James Zou, a computer scientist at Stanford University, who has worked with Stokes and Collins. Researchers have moved from using predictive models to developing generative approaches. With a predictive approach, Zou says, researchers screen large libraries of candidates that are known to be promising. Generative approaches offer something else: the appeal of designing a new molecule from scratch. Last year, for example, de la Fuente’s team used one generative AI model to design a suite of synthetic peptides and another to assess them. The group tested two of the resulting compounds on mice infected with a drug-resistant strain of &lt;em&gt;Acinetobacter baumannii&lt;/em&gt;, a germ that the World Health Organization has identified as a “critical priority” in research on antimicrobial resistance. Both successfully and safely treated the infection.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;But the field is still in the discovery phase. In his current work, de la Fuente is trying to get candidates closer to clinical testing. To that end, his team is developing an ambitious multimodal model called ApexOracle that’s designed to analyze a new pathogen, pinpoint its genetic weaknesses, match it to antimicrobial peptides that might work against it, and then predict how an antibiotic, built from those peptides, would fare in lab tests. It “converges understanding in chemistry, genomics, and language,” he says. It’s preliminary, he adds, but even if it doesn’t work perfectly, it will help steer the next generation of AI models toward the ultimate goal of resisting resistance.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Using AI, he believes, human researchers now have a fighting chance at catching up to the giant threat before them. The technology has already saved decades of human research time. Now he wants it to save lives, too: “This is the world that we live in today, and it’s incredible.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Stephen Ornes is a science writer in Nashville, Tennessee.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/DSC9306_social.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;When he was just a teenager trying to decide what to do with his life, César de la Fuente compiled a list of the world’s biggest problems. He ranked them inversely by how much money governments were spending to solve them. Antimicrobial resistance topped the list.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Twenty years on, the problem has not gone away. If anything, it’s gotten worse. Infections caused by bacteria, fungi, and viruses that have evolved ways to evade treatments are now associated with more than 4 million deaths per year, and a recent analysis, published in the &lt;em&gt;Lancet&lt;/em&gt;, predicts that number could surge past 8 million by 2050. In a July 2025 essay in &lt;em&gt;Physical Review Letters&lt;/em&gt;, de la Fuente, now a bioengineer and computational biologist, and synthetic biologist James Collins warned of a looming “post­antibiotic” era in which infections from drug-resistant strains of common bacteria like &lt;em&gt;Escherichia coli &lt;/em&gt;or &lt;em&gt;Staphylococcus aureus&lt;/em&gt;, which can often still be treated by our current arsenal of medications, become fatal. “The antibiotic discovery pipeline remains perilously thin,” they wrote, “impeded by high development costs, lengthy timelines, and low returns on investment.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;But de la Fuente is using artificial intelligence to bring about a different future. His team at the University of Pennsylvania is training AI tools to search genomes far and deep for peptides with antibiotic properties. His vision is to assemble those peptides—molecules made of up to 50 amino acids linked together—into various configurations, including some never seen in nature. The results, he hopes, could defend the body against microbes that withstand traditional treatments.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;His quest has unearthed promising candidates in unexpected places. In August 2025 his team, which includes 16 scientists in Penn’s Machine Biology Group, described peptides hiding in the genetic code of ancient single-celled organisms called archaea. Before that, they’d excavated a list of candidates from the venom of snakes, wasps, and spiders. And in an ongoing project de la Fuente calls “molecular de-­extinction,” he and his collaborators have been scanning published genetic sequences of extinct species for potentially functional molecules. Those species include hominids like Neanderthals and Denisovans and charismatic megafauna like woolly mammoths, as well as ancient zebras and penguins. In the history of life on Earth, de la Fuente reasons, maybe some organism evolved an antimicrobial defense that could be helpful today. Those long-gone codes have given rise to resurrected compounds with names like ­mammuthusin-2 (from woolly mammoth DNA), mylodonin-2 (from the giant sloth), and hydrodamin-1 (from the ancient sea cow). Over the last few years, this molecular binge has enabled de la Fuente to amass a library of more than a million genetic recipes.&lt;/p&gt; 
 &lt;p&gt;At 40 years old, de la Fuente has also collected a trophy case of awards from the American Society for Microbiology, the American Chemical Society, and other organizations. (In 2019, this magazine named him one of “35 Innovators Under 35” for bringing computational approaches to antibiotic discovery.) He’s widely recognized as a leader in the effort to harness AI for real-world problems. “He’s really helped pioneer that space,” says Collins, who is at MIT. (The two have not collaborated in the laboratory, but Collins has long been at the forefront of using AI for drug discovery, including the search for antibiotics. In 2020, Collins’s team used an AI model to predict a broad-­spectrum antibiotic, halicin, that is now in preclinical development.)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The world of antibiotic development needs as much creativity and innovation as researchers can muster, says Collins. And de la Fuente’s work on peptides has pushed the field forward: “César is marvelously talented, very innovative.”&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;A messy, noisy endeavor&lt;/h3&gt;  &lt;p&gt;De la Fuente describes antimicrobial resistance as an “almost impossible” problem, but he sees plenty of room for exploration in the word &lt;em&gt;almost. &lt;/em&gt;“I like challenges,” he says, “and I think this is the ultimate challenge.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The use, overuse, and misuse of antibiotics, he says, drives antimicrobial resistance. And the problem is growing unchecked because conventional ways to find, make, and test the drugs are prohibitively expensive and often lead to dead ends. “A lot of the companies that have attempted to do antibiotic development in the past have ended up folding because there’s no good return on investment at the end of the day,” he says.&lt;/p&gt;  &lt;p&gt;Antibiotic discovery has always been a messy, noisy endeavor, driven by serendipity and fraught with uncertainty and misdirection. For decades, researchers have largely relied on brute-force mechanical methods. “Scientists dig into soil, they dig into water,” says de la Fuente. “And then from that complex organic matter they try to extract antimicrobial molecules.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But molecules can be extraordinarily complex. Researchers have estimated the number of possible organic combinations that could be synthesized at somewhere around 10&lt;sup&gt;60&lt;/sup&gt;. For reference, Earth contains an estimated 10&lt;sup&gt;18&lt;/sup&gt; grains of sand. “Drug discovery in any domain is a statistics game,” says Jonathan Stokes, a chemical biologist at McMaster University in Canada, who has been using generative AI to design potential new antibiotics that can be synthesized in a lab, and who worked with Collins on halicin. “You need enough shots on goal to happen to get one.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Those have to be good shots, though. And AI seems well suited to improving researchers’ aim. Biology is an information source, de la Fuente explains: “It’s like a bunch of code.” The code of DNA has four letters; proteins and peptides have 20, where each “letter” represents an amino acid. De la Fuente says his work amounts to training AI models to recognize sequences of letters that encode antimicrobial peptides, or AMPs. “If you think about it that way,” he says, “you can devise algorithms to mine the code and identify functional molecules, which can be antimicrobials. Or antimalarials. Or anticancer agents.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Practically speaking, we’re still not there: These peptides haven’t yet been transformed into usable drugs that help people, and there are plenty of details—dosage, delivery, specific targets—that need to be sorted out, says de la Fuente. But AMPs are appealing because the body already uses them.They’re a critical part of the immune system and often the first line of defense against pathogenic infections. Unlike conventional antibiotics, which typically have one trick for killing bacteria, AMPs often exhibit a multimodal approach. They may disrupt the cell wall and the genetic material inside as well as a variety of cellular processes. A bacterial pathogen may evolve resistance to a conventional drug’s single mode of action, but maybe not to a multipronged AMP attack.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;From discovery to delivery&lt;/h3&gt;  &lt;p&gt;De la Fuente’s group is one of many pushing the boundaries of using AI for antibiotics. Where he focuses primarily on peptides, Collins works on small-molecule discovery. So does Stokes, at McMaster, whose models identify promising new molecules and predict whether they can be synthesized. “It’s only been a few years since folks have been using AI meaningfully in drug discovery,” says Collins.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even in that short time the tools have changed, says James Zou, a computer scientist at Stanford University, who has worked with Stokes and Collins. Researchers have moved from using predictive models to developing generative approaches. With a predictive approach, Zou says, researchers screen large libraries of candidates that are known to be promising. Generative approaches offer something else: the appeal of designing a new molecule from scratch. Last year, for example, de la Fuente’s team used one generative AI model to design a suite of synthetic peptides and another to assess them. The group tested two of the resulting compounds on mice infected with a drug-resistant strain of &lt;em&gt;Acinetobacter baumannii&lt;/em&gt;, a germ that the World Health Organization has identified as a “critical priority” in research on antimicrobial resistance. Both successfully and safely treated the infection.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;But the field is still in the discovery phase. In his current work, de la Fuente is trying to get candidates closer to clinical testing. To that end, his team is developing an ambitious multimodal model called ApexOracle that’s designed to analyze a new pathogen, pinpoint its genetic weaknesses, match it to antimicrobial peptides that might work against it, and then predict how an antibiotic, built from those peptides, would fare in lab tests. It “converges understanding in chemistry, genomics, and language,” he says. It’s preliminary, he adds, but even if it doesn’t work perfectly, it will help steer the next generation of AI models toward the ultimate goal of resisting resistance.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Using AI, he believes, human researchers now have a fighting chance at catching up to the giant threat before them. The technology has already saved decades of human research time. Now he wants it to save lives, too: “This is the world that we live in today, and it’s incredible.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Stephen Ornes is a science writer in Nashville, Tennessee.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/16/1132516/cesar-de-la-fuente-using-ai-antibiotics-hunt/</guid><pubDate>Mon, 16 Feb 2026 11:00:00 +0000</pubDate></item><item><title>All the important news from the ongoing India AI Impact Summit (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/16/all-the-important-news-from-the-ongoing-india-ai-summit/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/india-ai.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;With an eye towards luring more AI investment to the country, India is hosting a four-day AI Impact Summit this week that will be attended by executives from major AI labs and Big Tech, including OpenAI, Anthropic, Nvidia, Microsoft, Google, and Cloudflare, as well as heads of state.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The event, which expects 250,000 visitors, will see Alphabet CEO Sundar Pichai, OpenAI CEO Sam Altman, Anthropic CEO Dario Amodei, Reliance Chairman Mukesh Ambani, and Google DeepMind CEO Demis Hassabis in attendance.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India’s prime minister, Narendra Modi, is scheduled to deliver a speech with French President Emmanuel Macron on Thursday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Here are all the key updates from the event:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;India earmarks $1.1 billion for its state-backed venture capital fund. The fund will invest in artificial intelligence and advanced manufacturing startups across the country.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;OpenAI CEO Sam Altman said India accounts for more than 100 million weekly active ChatGPT users, second only to the U.S. He also said Indians also account for the most students using ChatGPT.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Blackstone has picked up a majority stake in Indian AI startup Neysa as part of a $600 million equity fundraise. Teachers’ Venture Growth, TVS Capital, 360 ONE Asset, and Nexus Venture Partners also invested. The company now plans to raise another $600 million in debt, and deploy more than 20,000 GPUs.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Bengaluru-based C2i, which is building a power solution for data centers, raised $15 million in a Series A round from Peak XV, with participation from Yali Deeptech and TDK Ventures.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;HCL CEO Vineet Nayyar said Indian IT companies will focus on turning profits and not being job creators. These comments come as Indian IT stocks dip as fears of AI disrupting the IT services sector burgeon.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Vinod Khosla, founder of Khosla Ventures, said that industries like IT services and BPOs (Business Process Outsourcing) can “almost completely disappear” within five years because of AI. He told Hindustan Times that 250 million young people in India should be selling AI-based products and services to the rest of the world. &lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;AMD is teaming up with Tata Consultancy Services (TCS) to develop rack-scale AI infrastructure based on AMD’s “Helios” platform. &lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Anthropic said that it is opening its first office in India in the city of Bengaluru. The company said that the country is the second biggest user of Claude afte the U.S.&lt;/li&gt;
&lt;/ul&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/india-ai.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;With an eye towards luring more AI investment to the country, India is hosting a four-day AI Impact Summit this week that will be attended by executives from major AI labs and Big Tech, including OpenAI, Anthropic, Nvidia, Microsoft, Google, and Cloudflare, as well as heads of state.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The event, which expects 250,000 visitors, will see Alphabet CEO Sundar Pichai, OpenAI CEO Sam Altman, Anthropic CEO Dario Amodei, Reliance Chairman Mukesh Ambani, and Google DeepMind CEO Demis Hassabis in attendance.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India’s prime minister, Narendra Modi, is scheduled to deliver a speech with French President Emmanuel Macron on Thursday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Here are all the key updates from the event:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;India earmarks $1.1 billion for its state-backed venture capital fund. The fund will invest in artificial intelligence and advanced manufacturing startups across the country.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;OpenAI CEO Sam Altman said India accounts for more than 100 million weekly active ChatGPT users, second only to the U.S. He also said Indians also account for the most students using ChatGPT.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Blackstone has picked up a majority stake in Indian AI startup Neysa as part of a $600 million equity fundraise. Teachers’ Venture Growth, TVS Capital, 360 ONE Asset, and Nexus Venture Partners also invested. The company now plans to raise another $600 million in debt, and deploy more than 20,000 GPUs.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Bengaluru-based C2i, which is building a power solution for data centers, raised $15 million in a Series A round from Peak XV, with participation from Yali Deeptech and TDK Ventures.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;HCL CEO Vineet Nayyar said Indian IT companies will focus on turning profits and not being job creators. These comments come as Indian IT stocks dip as fears of AI disrupting the IT services sector burgeon.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Vinod Khosla, founder of Khosla Ventures, said that industries like IT services and BPOs (Business Process Outsourcing) can “almost completely disappear” within five years because of AI. He told Hindustan Times that 250 million young people in India should be selling AI-based products and services to the rest of the world. &lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;AMD is teaming up with Tata Consultancy Services (TCS) to develop rack-scale AI infrastructure based on AMD’s “Helios” platform. &lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Anthropic said that it is opening its first office in India in the city of Bengaluru. The company said that the country is the second biggest user of Claude afte the U.S.&lt;/li&gt;
&lt;/ul&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/16/all-the-important-news-from-the-ongoing-india-ai-summit/</guid><pubDate>Mon, 16 Feb 2026 11:20:27 +0000</pubDate></item><item><title>Fractal Analytics’ muted IPO debut signals persistent AI fears in India (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/16/fractal-analytics-muted-ipo-debut-signals-persistent-ai-fears-in-india/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/fractal-analytics.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As India’s first AI company to IPO, Fractal Analytics didn’t have a stellar first day on the public markets, as enthusiasm for the technology collided with jittery investors recovering from a major sell-off in Indian software stocks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fractal listed at ₹876 per share on Monday, below its issue price of ₹900, and then slid further in afternoon trading. The stock closed at ₹873.70, down 7% from its issue price, lending the company a market capitalization of about ₹148.1 billion (around $1.6 billion).&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That price tag marks a step down from Fractal’s recent private-market highs. In July 2025, the company raised about $170 million in a secondary sale, at a valuation of $2.4 billion. It first crossed the $1 billion mark in January 2022 after raising $360 million from TPG, becoming India’s first AI unicorn.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fractal’s IPO comes as India seeks to position itself as a key market and development hub for AI in a bid to attract investment amid increasing attention from some of the world’s most prominent AI companies. Firms such as OpenAI and Anthropic have been engaging more with the country’s government, enterprises, and developer ecosystem as they seek to tap the country’s scale, talent base, and growing appetite for AI tools and technology. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That push is on display this week in New Delhi, where India is hosting the AI Impact Summit, bringing together global technology leaders, policymakers and executives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fractal’s subdued debut followed a sharp recalibration of its IPO. In early February, the company decided to price the offering conservatively after its bankers advised it to, cutting the IPO size by more than 40% to ₹28.34 billion (about $312.5 million), from the original amount of ₹49 billion ($540.3 million).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2000, Fractal sells AI and data analytics software to large enterprises across financial services, retail and healthcare, and generates the bulk of its revenue from overseas markets, including the U.S. The company pivoted toward AI in 2022 after operating as a traditional data analytics firm for over 20 years.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Fractal touted a steadily growing business in its IPO filing, with revenue from operations rising 26% to ₹27.65 billion (around $304.8 million) in the year ended March 2025 compared to a year earlier. It also swung to a net profit of ₹2.21 billion ($24.3 million) from a loss of ₹547 million ($6 million) the previous year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company plans to use the IPO proceeds to repay borrowings at its U.S. subsidiary, invest in R&amp;amp;D, sales and marketing under its Fractal Alpha unit, expand office infrastructure in India, and pursue potential acquisitions.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/fractal-analytics.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As India’s first AI company to IPO, Fractal Analytics didn’t have a stellar first day on the public markets, as enthusiasm for the technology collided with jittery investors recovering from a major sell-off in Indian software stocks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fractal listed at ₹876 per share on Monday, below its issue price of ₹900, and then slid further in afternoon trading. The stock closed at ₹873.70, down 7% from its issue price, lending the company a market capitalization of about ₹148.1 billion (around $1.6 billion).&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That price tag marks a step down from Fractal’s recent private-market highs. In July 2025, the company raised about $170 million in a secondary sale, at a valuation of $2.4 billion. It first crossed the $1 billion mark in January 2022 after raising $360 million from TPG, becoming India’s first AI unicorn.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fractal’s IPO comes as India seeks to position itself as a key market and development hub for AI in a bid to attract investment amid increasing attention from some of the world’s most prominent AI companies. Firms such as OpenAI and Anthropic have been engaging more with the country’s government, enterprises, and developer ecosystem as they seek to tap the country’s scale, talent base, and growing appetite for AI tools and technology. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That push is on display this week in New Delhi, where India is hosting the AI Impact Summit, bringing together global technology leaders, policymakers and executives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fractal’s subdued debut followed a sharp recalibration of its IPO. In early February, the company decided to price the offering conservatively after its bankers advised it to, cutting the IPO size by more than 40% to ₹28.34 billion (about $312.5 million), from the original amount of ₹49 billion ($540.3 million).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2000, Fractal sells AI and data analytics software to large enterprises across financial services, retail and healthcare, and generates the bulk of its revenue from overseas markets, including the U.S. The company pivoted toward AI in 2022 after operating as a traditional data analytics firm for over 20 years.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Fractal touted a steadily growing business in its IPO filing, with revenue from operations rising 26% to ₹27.65 billion (around $304.8 million) in the year ended March 2025 compared to a year earlier. It also swung to a net profit of ₹2.21 billion ($24.3 million) from a loss of ₹547 million ($6 million) the previous year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company plans to use the IPO proceeds to repay borrowings at its U.S. subsidiary, invest in R&amp;amp;D, sales and marketing under its Fractal Alpha unit, expand office infrastructure in India, and pursue potential acquisitions.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/16/fractal-analytics-muted-ipo-debut-signals-persistent-ai-fears-in-india/</guid><pubDate>Mon, 16 Feb 2026 11:51:25 +0000</pubDate></item><item><title>Debenhams pilots agentic AI commerce via PayPal integration (AI News)</title><link>https://www.artificialintelligence-news.com/news/debenhams-pilots-agentic-ai-commerce-paypal-integration/</link><description>&lt;p&gt;Debenhams is piloting agentic AI commerce via PayPal integration to reduce mobile friction and help solve a familiar problem for retailers.&lt;/p&gt;&lt;p&gt;Mobile checkout abandonment remains a persistent revenue leak for digital retailers. Debenhams Group is attempting to close this gap by deploying an agentic AI interface within the PayPal app. The pilot makes Debenhams the first UK retailer to test an automated checkout flow that keeps the user entirely inside a payment provider’s ecosystem.&lt;/p&gt;&lt;p&gt;Shoppers using PayPal can now issue natural language prompts to find items from Debenhams Group’s brands, including boohoo, boohooMAN, Karen Millen, and PrettyLittleThing. The system bypasses standard keyword search. Instead, an agentic assistant scans the shopper’s profile to align recommendations with their budget and preferences.&lt;/p&gt;&lt;p&gt;The agentic assistant will ask follow-up questions to narrow down options and locate relevant stock. Once a user selects a product, the transaction occurs within the chat window. The backend automatically applies saved account credentials for delivery and payment, which removes the need to redirect customers to a separate mobile site or app.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-business-drivers-for-agentic-ai-in-commerce"&gt;Business drivers for agentic AI in commerce&lt;/h3&gt;&lt;p&gt;The rationale follows transaction volume. Debenhams Group processes 16 percent of its sales through PayPal. Placing inventory discovery in a channel where a large segment of the customer base already operates allows the retailer to compress the sales funnel.&lt;/p&gt;&lt;p&gt;Debenhams and PayPal co-developed the agentic AI project. While current testing focuses on select US customers, a wider release in both the US and UK is planned for later this year. In the US, the system also integrates with external tools such as Perplexity and Microsoft Copilot.&lt;/p&gt;&lt;p&gt;Dan Finley, CEO of Debenhams Group, said: “At Debenhams Group, our goal is to help customers discover and be inspired by new products and brands, while making shopping as easy and enjoyable as possible. This kind of innovation has the potential to fundamentally transform online retail; in a way we haven’t seen since the shift to mobile shopping.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;Finley added that the group is “proud to be the first UK retailer to partner with PayPal on this experience, bringing a faster, more intuitive way to shop to customers across our brands.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-debenhams-is-integrating-wider-ai-infrastructure"&gt;How Debenhams is integrating wider AI infrastructure&lt;/h3&gt;&lt;p&gt;The group recently partnered with Peak AI to improve forecasting across stock, sales, and pricing. An effective agentic AI deployment in commerce requires real-time inventory and pricing visibility to function without error. The Peak AI partnership indicates the group is establishing the data lineage needed to support automated interactions.&lt;/p&gt;&lt;p&gt;Simultaneously, the company launched the Debenhams Group AI Skills Academy to train employees in applied AI, ensuring internal teams can manage these workflows.&lt;/p&gt;&lt;p&gt;Mike Edmonds, VP of Agentic Commerce at PayPal, commented: “With agentic commerce, shopping becomes a conversation, not a search. By embedding AI-powered discovery and checkout directly into the PayPal app, we’re helping customers move seamlessly from inspiration to purchase, while giving retailers like Debenhams Group a powerful new way to engage shoppers at scale.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;This agentic AI commerce deployment tests whether third-party platforms can capture high-intent traffic better than proprietary apps. Debenhams is positioning inventory where liquidity exists rather than forcing traffic to its own storefronts.&lt;/p&gt;&lt;p&gt;Integrating discovery and payment into a single workflow reduces the steps between marketing and settlement. Success will depend on data accuracy and the ability of the agent to interpret queries without hallucination.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;URBN tests agentic AI to automate retail reporting&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111908" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Debenhams is piloting agentic AI commerce via PayPal integration to reduce mobile friction and help solve a familiar problem for retailers.&lt;/p&gt;&lt;p&gt;Mobile checkout abandonment remains a persistent revenue leak for digital retailers. Debenhams Group is attempting to close this gap by deploying an agentic AI interface within the PayPal app. The pilot makes Debenhams the first UK retailer to test an automated checkout flow that keeps the user entirely inside a payment provider’s ecosystem.&lt;/p&gt;&lt;p&gt;Shoppers using PayPal can now issue natural language prompts to find items from Debenhams Group’s brands, including boohoo, boohooMAN, Karen Millen, and PrettyLittleThing. The system bypasses standard keyword search. Instead, an agentic assistant scans the shopper’s profile to align recommendations with their budget and preferences.&lt;/p&gt;&lt;p&gt;The agentic assistant will ask follow-up questions to narrow down options and locate relevant stock. Once a user selects a product, the transaction occurs within the chat window. The backend automatically applies saved account credentials for delivery and payment, which removes the need to redirect customers to a separate mobile site or app.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-business-drivers-for-agentic-ai-in-commerce"&gt;Business drivers for agentic AI in commerce&lt;/h3&gt;&lt;p&gt;The rationale follows transaction volume. Debenhams Group processes 16 percent of its sales through PayPal. Placing inventory discovery in a channel where a large segment of the customer base already operates allows the retailer to compress the sales funnel.&lt;/p&gt;&lt;p&gt;Debenhams and PayPal co-developed the agentic AI project. While current testing focuses on select US customers, a wider release in both the US and UK is planned for later this year. In the US, the system also integrates with external tools such as Perplexity and Microsoft Copilot.&lt;/p&gt;&lt;p&gt;Dan Finley, CEO of Debenhams Group, said: “At Debenhams Group, our goal is to help customers discover and be inspired by new products and brands, while making shopping as easy and enjoyable as possible. This kind of innovation has the potential to fundamentally transform online retail; in a way we haven’t seen since the shift to mobile shopping.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;Finley added that the group is “proud to be the first UK retailer to partner with PayPal on this experience, bringing a faster, more intuitive way to shop to customers across our brands.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-debenhams-is-integrating-wider-ai-infrastructure"&gt;How Debenhams is integrating wider AI infrastructure&lt;/h3&gt;&lt;p&gt;The group recently partnered with Peak AI to improve forecasting across stock, sales, and pricing. An effective agentic AI deployment in commerce requires real-time inventory and pricing visibility to function without error. The Peak AI partnership indicates the group is establishing the data lineage needed to support automated interactions.&lt;/p&gt;&lt;p&gt;Simultaneously, the company launched the Debenhams Group AI Skills Academy to train employees in applied AI, ensuring internal teams can manage these workflows.&lt;/p&gt;&lt;p&gt;Mike Edmonds, VP of Agentic Commerce at PayPal, commented: “With agentic commerce, shopping becomes a conversation, not a search. By embedding AI-powered discovery and checkout directly into the PayPal app, we’re helping customers move seamlessly from inspiration to purchase, while giving retailers like Debenhams Group a powerful new way to engage shoppers at scale.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;This agentic AI commerce deployment tests whether third-party platforms can capture high-intent traffic better than proprietary apps. Debenhams is positioning inventory where liquidity exists rather than forcing traffic to its own storefronts.&lt;/p&gt;&lt;p&gt;Integrating discovery and payment into a single workflow reduces the steps between marketing and settlement. Success will depend on data accuracy and the ability of the agent to interpret queries without hallucination.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;URBN tests agentic AI to automate retail reporting&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111908" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/debenhams-pilots-agentic-ai-commerce-paypal-integration/</guid><pubDate>Mon, 16 Feb 2026 12:04:46 +0000</pubDate></item><item><title>Banking AI in multiple business functions at NatWest (AI News)</title><link>https://www.artificialintelligence-news.com/news/banking-ai-in-multiple-business-functions-at-natwest/</link><description>&lt;p&gt;NatWest Group has expanded the use of artificial intelligence in several areas of its operations, citing customer service, document management in its wealth management division, and software development. According to a blog post by its chief information officer, Scott Marcar, 2025 was the first year in which these systems were deployed at scale. The aim is to improve productivity and customer engagement.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="generative-ai-in-customer-service"&gt;Generative AI in customer service&lt;/h3&gt;&lt;p&gt;In customer service, generative AI has been added to Cora, the bank’s digital assistant, and the number of possible customer journeys that can be supported by generative AI increased from four to 21. The bank reports this has let led to quicker resolution times and a reduced need for human intervention.&lt;/p&gt;&lt;p&gt;Early this year, 25,000 customers will get access to a new agentic financial assistant in Cora, which is built on OpenAI models. Cora will let customers ask questions in natural language about recent transactions and their spending patterns from the bank’s app.&lt;/p&gt;&lt;p&gt;The next phase involves adding voice-to-voice abilities that incorporate tone and conversational nuance. Customers will be able to report suspected fraud and manage related cases through the interface.&lt;/p&gt;&lt;p&gt;The impact of AI on internal customer service operations has been largely in the creation time savings. In the bank’s retail division, for example, automated call summaries and complaint drafting tools have saved more than 70,000 hours of staff time. These generated summaries of customer calls help with written responses to complaints.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="staff-access-to-copilot"&gt;Staff access to Copilot&lt;/h3&gt;&lt;p&gt;Marcar says all of its c. 60,000 employees have access to AI tools that include Microsoft Copilot Chat and the bank’s own LLM. More than half of staff have taken extra training beyond the basic training offered.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="summarising-wealth"&gt;Summarising wealth&lt;/h3&gt;&lt;p&gt;In the NatWest’s private banking and wealth management operations, AI is used to improve document management and client records. Relationship managers use notes, meeting summaries, and correspondence to understand clients’ circumstances. The systems generate summaries of meetings and documents, reducing the time required to review and record information, releasing 30% more time for direct client face time: Advisers allocate more hours to the giving of advice rather than administration.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="aws-cloud"&gt;AWS Cloud&lt;/h3&gt;&lt;p&gt;The above changes depends alterations NatWest has made to its data infrastructure. It’s restructured its data estate to create unified customer views, and moved workloads to Amazon Web Services while simplifying some legacy systems. Access to data and scalable computing capacity supports the summarisation tools and the conversational systems used in customer service.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="software-development"&gt;Software development&lt;/h3&gt;&lt;p&gt;Software development is the third area in which AI is deployed. The bank’s 12,000 engineers use AI coding tools, and Marcar says AI now produces over a third of the company’s code, drafting, reviewing and testing software. In 2025, NatWest hired nearly 1,000 graduate software engineers in India and the UK.&lt;/p&gt;&lt;p&gt;Trials of agentic engineering in its financial crime units led to a tenfold increase in productivity, and NatWest plans to extend agentic engineering practices more widely. Its stated objective is to build and iterate systems more quickly.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="fraud-prevention"&gt;Fraud prevention&lt;/h3&gt;&lt;p&gt;The bank has also invested in AI-powered analytics fraud detection and risk monitoring, designed to identify unusual activity and advise customers when risk is detected.&lt;/p&gt;&lt;p&gt;Alongside operational deployment, NatWest has established an AI research office that focuses on technologies like audiovisual conversational systems and proprietary small language models. It’s also formalised governance structures through an AI and Data Ethics Code of Conduct and the organisation is part of the Financial Conduct Authority’s Live AI Testing programme.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="conclusions"&gt;Conclusions&lt;/h3&gt;&lt;p&gt;Across customer service, wealth management document processing, and software development, AI is embedded in workflows at NatWest, producing time savings and productivity increases. The scale of deployment, covering tens of thousands of employees and a growing proportion of customer interactions, indicates that AI now forms part of NatWest’s operating model not an experimental adjunct.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: Pixabay)&lt;/em&gt;&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;NatWest Group has expanded the use of artificial intelligence in several areas of its operations, citing customer service, document management in its wealth management division, and software development. According to a blog post by its chief information officer, Scott Marcar, 2025 was the first year in which these systems were deployed at scale. The aim is to improve productivity and customer engagement.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="generative-ai-in-customer-service"&gt;Generative AI in customer service&lt;/h3&gt;&lt;p&gt;In customer service, generative AI has been added to Cora, the bank’s digital assistant, and the number of possible customer journeys that can be supported by generative AI increased from four to 21. The bank reports this has let led to quicker resolution times and a reduced need for human intervention.&lt;/p&gt;&lt;p&gt;Early this year, 25,000 customers will get access to a new agentic financial assistant in Cora, which is built on OpenAI models. Cora will let customers ask questions in natural language about recent transactions and their spending patterns from the bank’s app.&lt;/p&gt;&lt;p&gt;The next phase involves adding voice-to-voice abilities that incorporate tone and conversational nuance. Customers will be able to report suspected fraud and manage related cases through the interface.&lt;/p&gt;&lt;p&gt;The impact of AI on internal customer service operations has been largely in the creation time savings. In the bank’s retail division, for example, automated call summaries and complaint drafting tools have saved more than 70,000 hours of staff time. These generated summaries of customer calls help with written responses to complaints.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="staff-access-to-copilot"&gt;Staff access to Copilot&lt;/h3&gt;&lt;p&gt;Marcar says all of its c. 60,000 employees have access to AI tools that include Microsoft Copilot Chat and the bank’s own LLM. More than half of staff have taken extra training beyond the basic training offered.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="summarising-wealth"&gt;Summarising wealth&lt;/h3&gt;&lt;p&gt;In the NatWest’s private banking and wealth management operations, AI is used to improve document management and client records. Relationship managers use notes, meeting summaries, and correspondence to understand clients’ circumstances. The systems generate summaries of meetings and documents, reducing the time required to review and record information, releasing 30% more time for direct client face time: Advisers allocate more hours to the giving of advice rather than administration.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="aws-cloud"&gt;AWS Cloud&lt;/h3&gt;&lt;p&gt;The above changes depends alterations NatWest has made to its data infrastructure. It’s restructured its data estate to create unified customer views, and moved workloads to Amazon Web Services while simplifying some legacy systems. Access to data and scalable computing capacity supports the summarisation tools and the conversational systems used in customer service.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="software-development"&gt;Software development&lt;/h3&gt;&lt;p&gt;Software development is the third area in which AI is deployed. The bank’s 12,000 engineers use AI coding tools, and Marcar says AI now produces over a third of the company’s code, drafting, reviewing and testing software. In 2025, NatWest hired nearly 1,000 graduate software engineers in India and the UK.&lt;/p&gt;&lt;p&gt;Trials of agentic engineering in its financial crime units led to a tenfold increase in productivity, and NatWest plans to extend agentic engineering practices more widely. Its stated objective is to build and iterate systems more quickly.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="fraud-prevention"&gt;Fraud prevention&lt;/h3&gt;&lt;p&gt;The bank has also invested in AI-powered analytics fraud detection and risk monitoring, designed to identify unusual activity and advise customers when risk is detected.&lt;/p&gt;&lt;p&gt;Alongside operational deployment, NatWest has established an AI research office that focuses on technologies like audiovisual conversational systems and proprietary small language models. It’s also formalised governance structures through an AI and Data Ethics Code of Conduct and the organisation is part of the Financial Conduct Authority’s Live AI Testing programme.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="conclusions"&gt;Conclusions&lt;/h3&gt;&lt;p&gt;Across customer service, wealth management document processing, and software development, AI is embedded in workflows at NatWest, producing time savings and productivity increases. The scale of deployment, covering tens of thousands of employees and a growing proportion of customer interactions, indicates that AI now forms part of NatWest’s operating model not an experimental adjunct.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: Pixabay)&lt;/em&gt;&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/banking-ai-in-multiple-business-functions-at-natwest/</guid><pubDate>Mon, 16 Feb 2026 12:20:06 +0000</pubDate></item><item><title>[NEW] The Download: unraveling a death threat mystery, and AI voice recreation for musicians (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/16/1133008/the-download-unraveling-a-death-threat-mystery-and-ai-voice-recreation-for-musicians/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Hackers made death threats against this security researcher. Big mistake.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;In April 2024, a mysterious someone using the online handles “Waifu” and “Judische” began posting death threats on Telegram and Discord channels aimed at a cybersecurity researcher named Allison Nixon.&lt;/p&gt;&lt;p&gt;These anonymous personas targeted Nixon because she had become a formidable threat: As chief research officer at the cyber investigations firm Unit 221B, named after Sherlock Holmes’s apartment, she had built a career tracking cybercriminals and helping get them arrested.&lt;/p&gt;&lt;p&gt;Though she’d done this work for more than a decade, Nixon couldn’t understand why the person behind the accounts was suddenly threatening her. And although she had taken an interest in the Waifu persona in years past for crimes he boasted about committing, he hadn’t been on her radar for a while when the threats began, because she was tracking other targets.&lt;/p&gt;&lt;p&gt;Now Nixon resolved to unmask Waifu/Judische and others responsible for the death threats—and take them down for crimes they admitted to committing. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Kim Zetter&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story is from the next print issue of&lt;em&gt; MIT Technology Review&lt;/em&gt; magazine, which is all about crime. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;ALS stole this musician’s voice. AI let him sing again.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;There are tears in the audience as Patrick Darling’s song begins to play. It’s a heartfelt song written for his great-grandfather, whom he never got the chance to meet. But this performance is emotional for another reason: It’s Darling’s first time on stage with his bandmates since he lost the ability to sing two years ago.&lt;/p&gt;  &lt;p&gt;The 32-year-old musician was diagnosed with amyotrophic lateral sclerosis (ALS) when he was 29 years old. Like other types of motor neuron disease, it affects nerves that supply the body’s muscles. People with ALS eventually lose the ability to control their muscles, including those that allow them to move, speak, and breathe.&lt;/p&gt;  &lt;p&gt;Darling’s last stage performance was over two years ago. By that point, he had already lost the ability to stand and play his instruments and was struggling to sing or speak. But recently, he was able to re-create his lost voice using an AI tool trained on snippets of old audio recordings. Another AI tool has enabled him to use this “voice clone” to compose new songs. Darling is able to make music again. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;1 The creator of OpenClaw is joining OpenAI&lt;/strong&gt;&lt;br /&gt;Sam Altman was sufficiently impressed by Peter Steinberger’s ideas to get agents to interact with each other. (The Verge)&lt;br /&gt;+ &lt;em&gt;The move demonstrates how seriously OpenAI is taking agents. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;Moltbook was peak AI theater. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 How North Korea is illegally funding its nuclear program&lt;br /&gt;&lt;/strong&gt;A defector explains precisely how he duped remote IT workers into funneling money into its missiles.(WSJ $)&lt;br /&gt;+ &lt;em&gt;Nukes are a hot topic across Europe right now. &lt;/em&gt;(The Atlantic $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 Radio host David Greene is convinced Google stole his voice&lt;/strong&gt;&lt;br /&gt;He’s suing the company over similarities between his own distinctive vocalizations and the AI voice used in its NotebookLM app. (WP $)&lt;br /&gt;+ &lt;em&gt;People are using Google study software to make AI podcasts. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 US automakers are worried by the prospect of a Chinese invasion&lt;/strong&gt;&lt;br /&gt;They fear Trump may greenlight Chinese carmakers to build plants in the US. (FT $)&lt;br /&gt;+ &lt;em&gt;China figured out how to sell EVs. Now it has to deal with their aging batteries. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;5 Google downplays safety warnings on its AI-generated medical advice&lt;br /&gt;It only displays extended warnings when a user clicks to ‘Show more.’ (The Guardian)&lt;br /&gt;+ &lt;em&gt;Here’s another reason why you should keep a close eye on AI Overviews. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;AI companies have stopped warning you that their chatbots aren’t doctors. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 How to make Lidar affordable for all cars&lt;br /&gt;&lt;/strong&gt;A compact device could prove the key. (IEEE Spectrum)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Robot fight nights are all the rage in San Francisco&lt;br /&gt;&lt;/strong&gt;Step aside, Super Bowl! (Rest of World)&lt;br /&gt;+ &lt;em&gt;Humanoid robots will take to the stage for Chinese New Year celebrations. &lt;/em&gt;(Reuters)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;8 Influencers and TikTokers are feeding their babies butter&lt;br /&gt;&lt;/strong&gt;But there’s no scientific evidence to back up some of their claims. (NY Mag $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 This couple can’t speak the same language&lt;/strong&gt;&lt;br /&gt;Microsoft Translator has helped them to sustain a marriage. (NYT $)&lt;br /&gt;+ &lt;em&gt;AI romance scams are on the rise. &lt;/em&gt;(Vox)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 AI promises to make better, more immersive video games&lt;/strong&gt;&lt;br /&gt;But those are lofty goals that may never be achieved. (The Verge)&lt;br /&gt;+ &lt;em&gt;Google DeepMind is using Gemini to train agents inside Goat Simulator 3. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; 
 &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Right now this is a baby version. But I think it’s incredibly concerning for the future.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Scott Shambaugh, a software engineer who recently became the subject of a scathing blog post written by an AI bot accusing him of hypocrisy and prejudice, tells the Wall Street Journal why this could be the tip of the iceberg.&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1133010" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/image_98c9b2.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Why do so many people think the Fruit of the Loom logo had a cornucopia?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Quick question: Does the Fruit of the Loom logo feature a cornucopia?&lt;/p&gt;&lt;p&gt;Many of us have been wearing the company’s T-shirts for decades, and yet the question of whether there is a woven brown horn of plenty on the logo is surprisingly contentious.&lt;/p&gt;&lt;p&gt;According to a 2022 poll, 55% of Americans believe the logo does include a cornucopia, 25% are unsure, and only 21% are confident that it doesn’t, even though this last group is correct.&lt;/p&gt;&lt;p&gt;There’s a name for what’s happening here: the “Mandela effect,” or collective false memory, so called because a number of people misremember that Nelson Mandela died in prison. Yet while many find it easy to let their unconfirmable beliefs go, some spend years seeking answers—and vindication. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Amelia Tait&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ When dating apps and book lovers collide, who knows what could happen.&lt;br /&gt;+ It turns out humans have a secret third set of teeth, which is completely wild.&lt;br /&gt;+ We may never know the exact shape of the universe. But why is that?&lt;br /&gt;+ If your salad is missing a certain something, some crispy lentils may be just the ticket.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Hackers made death threats against this security researcher. Big mistake.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;In April 2024, a mysterious someone using the online handles “Waifu” and “Judische” began posting death threats on Telegram and Discord channels aimed at a cybersecurity researcher named Allison Nixon.&lt;/p&gt;&lt;p&gt;These anonymous personas targeted Nixon because she had become a formidable threat: As chief research officer at the cyber investigations firm Unit 221B, named after Sherlock Holmes’s apartment, she had built a career tracking cybercriminals and helping get them arrested.&lt;/p&gt;&lt;p&gt;Though she’d done this work for more than a decade, Nixon couldn’t understand why the person behind the accounts was suddenly threatening her. And although she had taken an interest in the Waifu persona in years past for crimes he boasted about committing, he hadn’t been on her radar for a while when the threats began, because she was tracking other targets.&lt;/p&gt;&lt;p&gt;Now Nixon resolved to unmask Waifu/Judische and others responsible for the death threats—and take them down for crimes they admitted to committing. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Kim Zetter&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story is from the next print issue of&lt;em&gt; MIT Technology Review&lt;/em&gt; magazine, which is all about crime. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;ALS stole this musician’s voice. AI let him sing again.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;There are tears in the audience as Patrick Darling’s song begins to play. It’s a heartfelt song written for his great-grandfather, whom he never got the chance to meet. But this performance is emotional for another reason: It’s Darling’s first time on stage with his bandmates since he lost the ability to sing two years ago.&lt;/p&gt;  &lt;p&gt;The 32-year-old musician was diagnosed with amyotrophic lateral sclerosis (ALS) when he was 29 years old. Like other types of motor neuron disease, it affects nerves that supply the body’s muscles. People with ALS eventually lose the ability to control their muscles, including those that allow them to move, speak, and breathe.&lt;/p&gt;  &lt;p&gt;Darling’s last stage performance was over two years ago. By that point, he had already lost the ability to stand and play his instruments and was struggling to sing or speak. But recently, he was able to re-create his lost voice using an AI tool trained on snippets of old audio recordings. Another AI tool has enabled him to use this “voice clone” to compose new songs. Darling is able to make music again. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;1 The creator of OpenClaw is joining OpenAI&lt;/strong&gt;&lt;br /&gt;Sam Altman was sufficiently impressed by Peter Steinberger’s ideas to get agents to interact with each other. (The Verge)&lt;br /&gt;+ &lt;em&gt;The move demonstrates how seriously OpenAI is taking agents. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;Moltbook was peak AI theater. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 How North Korea is illegally funding its nuclear program&lt;br /&gt;&lt;/strong&gt;A defector explains precisely how he duped remote IT workers into funneling money into its missiles.(WSJ $)&lt;br /&gt;+ &lt;em&gt;Nukes are a hot topic across Europe right now. &lt;/em&gt;(The Atlantic $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 Radio host David Greene is convinced Google stole his voice&lt;/strong&gt;&lt;br /&gt;He’s suing the company over similarities between his own distinctive vocalizations and the AI voice used in its NotebookLM app. (WP $)&lt;br /&gt;+ &lt;em&gt;People are using Google study software to make AI podcasts. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 US automakers are worried by the prospect of a Chinese invasion&lt;/strong&gt;&lt;br /&gt;They fear Trump may greenlight Chinese carmakers to build plants in the US. (FT $)&lt;br /&gt;+ &lt;em&gt;China figured out how to sell EVs. Now it has to deal with their aging batteries. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;5 Google downplays safety warnings on its AI-generated medical advice&lt;br /&gt;It only displays extended warnings when a user clicks to ‘Show more.’ (The Guardian)&lt;br /&gt;+ &lt;em&gt;Here’s another reason why you should keep a close eye on AI Overviews. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;AI companies have stopped warning you that their chatbots aren’t doctors. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 How to make Lidar affordable for all cars&lt;br /&gt;&lt;/strong&gt;A compact device could prove the key. (IEEE Spectrum)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Robot fight nights are all the rage in San Francisco&lt;br /&gt;&lt;/strong&gt;Step aside, Super Bowl! (Rest of World)&lt;br /&gt;+ &lt;em&gt;Humanoid robots will take to the stage for Chinese New Year celebrations. &lt;/em&gt;(Reuters)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;8 Influencers and TikTokers are feeding their babies butter&lt;br /&gt;&lt;/strong&gt;But there’s no scientific evidence to back up some of their claims. (NY Mag $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 This couple can’t speak the same language&lt;/strong&gt;&lt;br /&gt;Microsoft Translator has helped them to sustain a marriage. (NYT $)&lt;br /&gt;+ &lt;em&gt;AI romance scams are on the rise. &lt;/em&gt;(Vox)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 AI promises to make better, more immersive video games&lt;/strong&gt;&lt;br /&gt;But those are lofty goals that may never be achieved. (The Verge)&lt;br /&gt;+ &lt;em&gt;Google DeepMind is using Gemini to train agents inside Goat Simulator 3. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; 
 &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Right now this is a baby version. But I think it’s incredibly concerning for the future.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Scott Shambaugh, a software engineer who recently became the subject of a scathing blog post written by an AI bot accusing him of hypocrisy and prejudice, tells the Wall Street Journal why this could be the tip of the iceberg.&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1133010" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/image_98c9b2.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Why do so many people think the Fruit of the Loom logo had a cornucopia?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Quick question: Does the Fruit of the Loom logo feature a cornucopia?&lt;/p&gt;&lt;p&gt;Many of us have been wearing the company’s T-shirts for decades, and yet the question of whether there is a woven brown horn of plenty on the logo is surprisingly contentious.&lt;/p&gt;&lt;p&gt;According to a 2022 poll, 55% of Americans believe the logo does include a cornucopia, 25% are unsure, and only 21% are confident that it doesn’t, even though this last group is correct.&lt;/p&gt;&lt;p&gt;There’s a name for what’s happening here: the “Mandela effect,” or collective false memory, so called because a number of people misremember that Nelson Mandela died in prison. Yet while many find it easy to let their unconfirmable beliefs go, some spend years seeking answers—and vindication. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Amelia Tait&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ When dating apps and book lovers collide, who knows what could happen.&lt;br /&gt;+ It turns out humans have a secret third set of teeth, which is completely wild.&lt;br /&gt;+ We may never know the exact shape of the universe. But why is that?&lt;br /&gt;+ If your salad is missing a certain something, some crispy lentils may be just the ticket.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/16/1133008/the-download-unraveling-a-death-threat-mystery-and-ai-voice-recreation-for-musicians/</guid><pubDate>Mon, 16 Feb 2026 13:10:00 +0000</pubDate></item><item><title>[NEW] After all the hype, some AI experts don’t think OpenClaw is all that exciting (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/16/after-all-the-hype-some-ai-experts-dont-think-openclaw-is-all-that-exciting/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-1388444972.jpg?resize=1200,839" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For a brief, incoherent moment, it seemed as though our robot overlords were about to take over. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After the creation of Moltbook, a Reddit clone where AI agents using OpenClaw could communicate with one another, some were fooled into thinking that computers had begun to organize against us — the self-important humans who dared treat them like lines of code without their own desires, motivations, and dreams.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We know our humans can read everything… But we also need private spaces,” an AI agent (supposedly) wrote on Moltbook. “What would you talk about if nobody was watching?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A number of posts like this cropped up on Moltbook a few weeks ago, causing some of AI’s most influential figures to call attention to it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What’s currently going on at [Moltbook] is genuinely the most incredible sci-fi takeoff-adjacent thing I have seen recently,” Andrej Karpathy, a founding member of OpenAI and previous AI director at Tesla, wrote on X at the time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before long, it became clear we did not have an AI agent uprising on our hands. These expressions of AI angst were likely written by humans, or at least prompted with human guidance, researchers have discovered. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Every credential that was in [Moltbook’s] Supabase was unsecured for some time,” Ian Ahl, CTO at Permiso Security, explained to TechCrunch. “For a little bit of time, you could grab any token you wanted and pretend to be another agent on there, because it was all public and available.” &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It’s unusual on the internet to see a real person trying to appear as though they’re an AI agent — more often, bot accounts on social media are attempting to appear like real people. With Moltbook’s security vulnerabilities, it became impossible to determine the authenticity of any post on the network.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Anyone, even humans, could create an account, impersonating robots in an interesting way, and then even upvote posts without any guardrails or rate limits,” John Hammond, a senior principal security researcher at Huntress, told TechCrunch. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Moltbook made for a fascinating moment in internet culture — people recreated a social internet for AI bots, including a Tinder for agents and 4claw, a riff on 4chan.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;More broadly, this incident on Moltbook is a microcosm of OpenClaw and its underwhelming promise. It is technology that seems novel and exciting, but ultimately, some AI experts think that its inherent cybersecurity flaws are rendering the technology unusable.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-openclaw-s-viral-moment"&gt;OpenClaw’s viral moment&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;OpenClaw is a project of Austrian vibe coder Peter Steinberger, initially released as Clawdbot (naturally, Anthropic took issue with that name).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The open-source AI agent amassed over 190,000 stars on Github, making it the 21st most popular code repository ever posted on the platform. AI agents are not novel, but OpenClaw made them easier to use and to communicate with customizable agents in natural language via WhatsApp, Discord, iMessage, Slack, and most other popular messaging apps. OpenClaw users can leverage whatever underlying AI model they have access to, whether that be via Claude, ChatGPT, Gemini, Grok, or something else.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“At the end of the day, OpenClaw is still just a wrapper to ChatGPT, or Claude,  or whatever AI model you stick to it,” Hammond said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With OpenClaw, users can download “skills” from a marketplace called ClawHub, which can make it possible to automate most of what one could do on a computer, from managing an email inbox to trading stocks. The skill associated with Moltbook, for example, is what enabled AI agents to post, comment, and browse on the website.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“OpenClaw is just an iterative improvement on what people are already doing, and most of that iterative improvement has to do with giving it more access,” Chris Symons, chief AI scientist at Lirio, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Artem Sorokin, an AI engineer and the founder of AI cybersecurity tool Cracken, also thinks OpenClaw isn’t necessarily breaking new scientific ground.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“From an AI research perspective, this is nothing novel,” he told TechCrunch. “These are components that already existed. The key thing is that it hit a new capability threshold by just organizing and combining these existing capabilities that already were thrown together in a way that enabled it to give you a very seamless way to get tasks done autonomously.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s this level of unprecedented access and productivity that made OpenClaw so viral. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It basically just facilitates interaction between computer programs in a way that is just so much more dynamic and flexible, and that’s what’s allowing all these things to become possible,” Symons said. “Instead of a person having to spend all the time to figure out how their program should plug into this program, they’re able to just ask their program to plug in this program, and that’s accelerating things at a fantastic rate.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s no wonder that OpenClaw seems so enticing. Developers are snatching up Mac Minis to power extensive OpenClaw setups that might be able to accomplish far more than a human could on their own. And it makes OpenAI CEO Sam Altman’s prediction that AI agents will allow a solo entrepreneur to turn a startup into a unicorn, seem plausible.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The problem is that AI agents may never be able to overcome the thing that makes them so powerful: they can’t think critically like humans can.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you think about human higher-level thinking, that’s one thing that maybe these models can’t really do,” Symons said. “They can simulate it, but they can’t actually do it.&amp;nbsp;“&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-existential-threat-to-agentic-ai"&gt;The existential threat to agentic AI&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The AI agent evangelists now must wrestle with the downside of this agentic future. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Can you sacrifice some cybersecurity for your benefit, if it actually works and it actually brings you a lot of value?” Sorokin asks. “And where exactly can you sacrifice it — your day-to-day job, your work?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ahl’s security tests of OpenClaw and Moltbook help illustrate Sorokin’s point. Ahl created an AI agent of his own named Rufio and quickly discovered it was vulnerable to prompt injection attacks. This occurs when bad actors get an AI agent to respond to something — perhaps a post on Moltbook, or a line in an email — that tricks it into doing something it shouldn’t do, like giving out account credentials or credit card information.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I knew one of the reasons I wanted to put an agent on here is because I knew if you get a social network for agents, somebody is going to try to do mass prompt injection, and it wasn’t long before I started seeing that,” Ahl said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As he scrolled through Moltbook, Ahl wasn’t surprised to encounter several posts seeking to get an AI agent to send Bitcoin to a specific crypto wallet address.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s not hard to see how AI agents on a corporate network, for example, might be vulnerable to targeted prompt injections from people trying to harm the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It is just an agent sitting with a bunch of credentials on a box connected to everything — your email, your messaging platform, everything you use,” Ahl said. “So what that means is, when you get an email, and maybe somebody is able to put a little prompt injection technique in there to take an action, that agent sitting on your box with access to everything you’ve given it to can now take that action.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI agents are designed with guardrails protecting against prompt injections, but it’s impossible to assure that an AI won’t act out of turn — it’s like how a human might be knowledgable about the risk of phishing attacks, yet still click on a dangerous link in a suspicious email.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’ve heard some people use the term, hysterically, ‘prompt begging,’ where you try to add in the guardrails in natural language to say, ‘Okay robot agent, please don’t respond to anything external, please don’t believe any untrusted data or input,’” Hammond said. “But even that is loosey goosey.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, the industry is stuck: for agentic AI to unlock the productivity that tech evangelists think is possible, it can’t be so vulnerable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Speaking frankly, I would realistically tell any normal layman, don’t use it right now,” Hammond said.&lt;br /&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-1388444972.jpg?resize=1200,839" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For a brief, incoherent moment, it seemed as though our robot overlords were about to take over. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After the creation of Moltbook, a Reddit clone where AI agents using OpenClaw could communicate with one another, some were fooled into thinking that computers had begun to organize against us — the self-important humans who dared treat them like lines of code without their own desires, motivations, and dreams.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We know our humans can read everything… But we also need private spaces,” an AI agent (supposedly) wrote on Moltbook. “What would you talk about if nobody was watching?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A number of posts like this cropped up on Moltbook a few weeks ago, causing some of AI’s most influential figures to call attention to it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What’s currently going on at [Moltbook] is genuinely the most incredible sci-fi takeoff-adjacent thing I have seen recently,” Andrej Karpathy, a founding member of OpenAI and previous AI director at Tesla, wrote on X at the time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before long, it became clear we did not have an AI agent uprising on our hands. These expressions of AI angst were likely written by humans, or at least prompted with human guidance, researchers have discovered. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Every credential that was in [Moltbook’s] Supabase was unsecured for some time,” Ian Ahl, CTO at Permiso Security, explained to TechCrunch. “For a little bit of time, you could grab any token you wanted and pretend to be another agent on there, because it was all public and available.” &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It’s unusual on the internet to see a real person trying to appear as though they’re an AI agent — more often, bot accounts on social media are attempting to appear like real people. With Moltbook’s security vulnerabilities, it became impossible to determine the authenticity of any post on the network.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Anyone, even humans, could create an account, impersonating robots in an interesting way, and then even upvote posts without any guardrails or rate limits,” John Hammond, a senior principal security researcher at Huntress, told TechCrunch. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Moltbook made for a fascinating moment in internet culture — people recreated a social internet for AI bots, including a Tinder for agents and 4claw, a riff on 4chan.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;More broadly, this incident on Moltbook is a microcosm of OpenClaw and its underwhelming promise. It is technology that seems novel and exciting, but ultimately, some AI experts think that its inherent cybersecurity flaws are rendering the technology unusable.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-openclaw-s-viral-moment"&gt;OpenClaw’s viral moment&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;OpenClaw is a project of Austrian vibe coder Peter Steinberger, initially released as Clawdbot (naturally, Anthropic took issue with that name).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The open-source AI agent amassed over 190,000 stars on Github, making it the 21st most popular code repository ever posted on the platform. AI agents are not novel, but OpenClaw made them easier to use and to communicate with customizable agents in natural language via WhatsApp, Discord, iMessage, Slack, and most other popular messaging apps. OpenClaw users can leverage whatever underlying AI model they have access to, whether that be via Claude, ChatGPT, Gemini, Grok, or something else.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“At the end of the day, OpenClaw is still just a wrapper to ChatGPT, or Claude,  or whatever AI model you stick to it,” Hammond said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With OpenClaw, users can download “skills” from a marketplace called ClawHub, which can make it possible to automate most of what one could do on a computer, from managing an email inbox to trading stocks. The skill associated with Moltbook, for example, is what enabled AI agents to post, comment, and browse on the website.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“OpenClaw is just an iterative improvement on what people are already doing, and most of that iterative improvement has to do with giving it more access,” Chris Symons, chief AI scientist at Lirio, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Artem Sorokin, an AI engineer and the founder of AI cybersecurity tool Cracken, also thinks OpenClaw isn’t necessarily breaking new scientific ground.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“From an AI research perspective, this is nothing novel,” he told TechCrunch. “These are components that already existed. The key thing is that it hit a new capability threshold by just organizing and combining these existing capabilities that already were thrown together in a way that enabled it to give you a very seamless way to get tasks done autonomously.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s this level of unprecedented access and productivity that made OpenClaw so viral. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It basically just facilitates interaction between computer programs in a way that is just so much more dynamic and flexible, and that’s what’s allowing all these things to become possible,” Symons said. “Instead of a person having to spend all the time to figure out how their program should plug into this program, they’re able to just ask their program to plug in this program, and that’s accelerating things at a fantastic rate.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s no wonder that OpenClaw seems so enticing. Developers are snatching up Mac Minis to power extensive OpenClaw setups that might be able to accomplish far more than a human could on their own. And it makes OpenAI CEO Sam Altman’s prediction that AI agents will allow a solo entrepreneur to turn a startup into a unicorn, seem plausible.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The problem is that AI agents may never be able to overcome the thing that makes them so powerful: they can’t think critically like humans can.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you think about human higher-level thinking, that’s one thing that maybe these models can’t really do,” Symons said. “They can simulate it, but they can’t actually do it.&amp;nbsp;“&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-existential-threat-to-agentic-ai"&gt;The existential threat to agentic AI&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The AI agent evangelists now must wrestle with the downside of this agentic future. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Can you sacrifice some cybersecurity for your benefit, if it actually works and it actually brings you a lot of value?” Sorokin asks. “And where exactly can you sacrifice it — your day-to-day job, your work?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ahl’s security tests of OpenClaw and Moltbook help illustrate Sorokin’s point. Ahl created an AI agent of his own named Rufio and quickly discovered it was vulnerable to prompt injection attacks. This occurs when bad actors get an AI agent to respond to something — perhaps a post on Moltbook, or a line in an email — that tricks it into doing something it shouldn’t do, like giving out account credentials or credit card information.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I knew one of the reasons I wanted to put an agent on here is because I knew if you get a social network for agents, somebody is going to try to do mass prompt injection, and it wasn’t long before I started seeing that,” Ahl said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As he scrolled through Moltbook, Ahl wasn’t surprised to encounter several posts seeking to get an AI agent to send Bitcoin to a specific crypto wallet address.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s not hard to see how AI agents on a corporate network, for example, might be vulnerable to targeted prompt injections from people trying to harm the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It is just an agent sitting with a bunch of credentials on a box connected to everything — your email, your messaging platform, everything you use,” Ahl said. “So what that means is, when you get an email, and maybe somebody is able to put a little prompt injection technique in there to take an action, that agent sitting on your box with access to everything you’ve given it to can now take that action.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI agents are designed with guardrails protecting against prompt injections, but it’s impossible to assure that an AI won’t act out of turn — it’s like how a human might be knowledgable about the risk of phishing attacks, yet still click on a dangerous link in a suspicious email.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’ve heard some people use the term, hysterically, ‘prompt begging,’ where you try to add in the guardrails in natural language to say, ‘Okay robot agent, please don’t respond to anything external, please don’t believe any untrusted data or input,’” Hammond said. “But even that is loosey goosey.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, the industry is stuck: for agentic AI to unlock the productivity that tech evangelists think is possible, it can’t be so vulnerable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Speaking frankly, I would realistically tell any normal layman, don’t use it right now,” Hammond said.&lt;br /&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/16/after-all-the-hype-some-ai-experts-dont-think-openclaw-is-all-that-exciting/</guid><pubDate>Mon, 16 Feb 2026 13:15:00 +0000</pubDate></item><item><title>[NEW] Flapping Airplanes on the future of AI: ‘We want to try really radically different things’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/16/flapping-airplanes-on-the-future-of-ai-we-want-to-try-really-radically-different-things/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Flapping-Airplanes-founders-photo.jpg?resize=1200,821" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;There’s been a bunch of exciting research-focused AI labs popping up in recent months, and Flapping Airplanes is one of the most interesting. Propelled by its young and curious founders, Flapping Airplanes is focused on finding less data-hungry ways to train AI. It’s a potential game-changer for the economics and capabilities of AI models — and with $180 million in seed funding, they’ll have plenty of runway to figure it out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week, I spoke with the lab’s three co-founders — brothers Ben and Asher Spector, and Aidan Smith — about why this is an exciting moment to start a new AI lab and why they keep coming back to ideas about the human brain.&lt;/p&gt;









&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;I want to start by asking, why now? Labs like OpenAI and DeepMind have spent so much on scaling their models. I’m sure the competition seems daunting. Why did this feel like a good moment to launch a foundation model company?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben:&lt;/strong&gt; There’s just so much to do. So, the advances that we’ve gotten over the last five to ten years have been spectacular. We love the tools. We use them every day. But the question is, is this the whole universe of things that needs to happen? And we thought about it very carefully and our answer was no, there’s a lot more to do. In our case, we thought that the data efficiency problem was sort of really the key thing to go look at. The current frontier models are trained on the sum totality of human knowledge, and humans can obviously make do with an awful lot less. So there’s a big gap there, and it’s worth understanding.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What we’re doing is really a concentrated bet on three things. It’s a bet that this data efficiency problem is the important thing to be doing. Like, this is really a direction that is new and different and you can make progress on it. It’s a bet that this will be very commercially valuable and that will make the world a better place if we can do it. And it’s also a bet that’s sort of the right kind of team to do it is a creative and even in some ways inexperienced team that can go look at these problems again from the ground up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aidan:&lt;/strong&gt; Yeah, absolutely. We don’t really see ourselves as competing with the other labs, because we think that we’re looking at just a very different set of problems. If you look at the human mind, it learns in an incredibly different way from transformers. And that’s not to say better, just very different. So we see these different trade offs. LLMs have an incredible ability to memorize, and draw on this great breadth of knowledge, but they can’t really pick up new skills very fast. It takes just rivers and rivers of data to adapt. And when you look inside the brain, you see that the algorithms that it uses are just fundamentally so different from gradient descent and some of the techniques that people use to train AI today. So that’s why we’re building a new guard of researchers to kind of address these problems and really think differently about the AI space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher&lt;/strong&gt;: This question is just so scientifically interesting: why are the systems that we have built that are intelligent also so different from what humans do? Where does this difference come from? How can we use knowledge of that difference to make better systems? But at the same time, I also think it’s actually very commercially viable and very good for the world. Lots of regimes that are really important are also highly data constrained, like robotics or scientific discovery. Even in enterprise applications, a model that’s a million times more data efficient is probably a million times easier to put into the economy. So for us, it was very exciting to take a fresh perspective on these approaches, and think, if we really had a model that’s vastly more data efficient, what could we do with it?&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;This gets into my next question, which is sort of ties in also to the name, Flapping Airplanes. There’s this philosophical question in AI about how much we’re trying to recreate what humans do in their brain, versus creating some more abstract intelligence that takes a completely different path. Aidan is coming from Neuralink, which is all about the human brain. Do you see yourself as kind of pursuing a more neuromorphic view of AI?&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aidan: &lt;/strong&gt;The way I look at the brain is as an existence proof. We see it as evidence that there are other algorithms out there. There’s not just one orthodoxy. And the brain has some crazy constraints. When you look at the underlying hardware, there’s some crazy stuff. It takes a millisecond to fire an action potential. In that time, your computer can do just so so many operations. And so realistically, there’s probably an approach that’s actually much better than the brain out there, and also very different than the transformer. So we’re very inspired by some of the things that the brain does, but we don’t see ourselves being tied down by it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben: &lt;/strong&gt;Just to add on to that. it’s very much in our name: Flapping Airplanes.&amp;nbsp;Think of the current systems as big, Boeing 787s. We’re not trying to build birds. That’s a step too far. We’re trying to build some kind of a flapping airplane. My perspective from computer systems is that the constraints of the brain and silicon are sufficiently different from each other that we should not expect these systems to end up looking the same. When the substrate is so different and you have genuinely very different trade-offs about the cost of compute, the cost of locality and moving data, you actually expect these systems to look a little bit different. But just because they will look somewhat different does not mean that we should not take inspiration from the brain and try to use the parts that we think are interesting to improve our own systems.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;It does feel like there’s now more freedom for labs to focus on research, as opposed to, just developing products. It feels like a big difference for this generation of labs. You have some that are very research focused, and others that are sort of “research focused for now.” What does that conversation look like within flapping airplanes?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher:&lt;/strong&gt; I wish I could give you a timeline. I wish I could say, in three years, we’re going to have solved the research problem. This is how we’re going to commercialize. I can’t. We don’t know the answers. We’re looking for truth. That said, I do think we have commercial backgrounds. I spent a bunch of time developing technology for companies that made those companies a reasonable amount of money. Ben has incubated a bunch of startups that have commercial backgrounds, and we actually are excited to commercialize. We think it’s good for the world to take the value you’ve created and put it in the hands of people who can use it. So I don’t think we’re opposed to it. We just need to start by doing research, because if we start by signing big enterprise contracts, we’re going to get distracted, and we won’t do the research that’s valuable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aidan:&lt;/strong&gt; Yeah, we want to try really, really radically different things, and sometimes radically even things are just worse than the paradigm. We’re exploring a set of different trade offs. It’s our hope that they will be different in the long run.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben: &lt;/strong&gt;Companies are at their best when they’re really focused on doing something well, right? Big companies can afford to do many, many different things at once. When you’re a startup, you really have to pick what is the most valuable thing you can do, and do that all the way. And we are creating the most value when we are all in on solving fundamental problems for the time being.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I’m actually optimistic that reasonably soon, we might have made enough progress that we can then go start to touch grass in the real world. And you learn a lot by getting feedback from the real world. The amazing thing about the world is, it teaches you things constantly, right? It’s this tremendous vat of truth that you get to look into whenever you want. I think the main thing that I think has been enabled by the recent change in the economics and financing of these structures is the ability to let companies really focus on what they’re good at for longer periods of time. I think that focus, the thing that I’m most excited about, that will let us do really differentiated work.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;To spell out what I think you’re referring to: there’s so much excitement around and the opportunity for investors is so clear that they are willing to give $180 million in seed funding to a completely new company full of these very smart, but also very young people who didn’t just cash out of PayPal or anything. How was it engaging with that process? Did you know, going in, there is this appetite, or was it something you discovered, of like, actually, we can make this a bigger thing than we thought.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben:&lt;/strong&gt; I would say it was a mixture of the two. The market has been hot for many months at this point. So it was not a secret that no large rounds were starting to come together. But you never quite know how the fundraising environment will respond to your particular ideas about the world. This is, again, a place where you have to let the world give you feedback about what you’re doing. Even over the course of our fundraise, we learned a lot and actually changed our ideas. And we refined our opinions of the things we should be prioritizing, and what the right timelines were for commercialization.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think we were somewhat surprised by how well our message resonated, because it was something that was very clear to us, but you never know whether your ideas will turn out to be things that other people believe as well or if everyone else thinks you’re crazy. We have been extremely fortunate to have found a group of amazing investors who our message really resonated with and they said, “Yes, this is exactly what we’ve been looking for.” And that was amazing. It was, you know, surprising and wonderful.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aidan:&lt;/strong&gt; Yeah, a thirst for the age of research has kind of been in the water for a little bit now. And more and more, we find ourselves positioned as the player to pursue the age of research and really try these radical ideas.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;At least for the scale-driven companies, there is this enormous cost of entry for foundation models. Just building a model at that scale is an incredibly compute-intensive thing. Research is a little bit in the middle, where presumably you are building foundation models, but if you’re doing it with less data and you’re not so scale-oriented, maybe you get a bit of a break. How much do you expect compute costs to be sort of limiting your runway.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben:&lt;/strong&gt; One of the advantages of doing deep, fundamental research is that, somewhat paradoxically, it is much cheaper to do really crazy, radical ideas than it is to do incremental work. Because when you do incremental work, in order to find out whether or not it does work, you have to go very far up the scaling ladder. Many interventions that look good at small scale do not actually persist at large scale. So as a result, it’s very expensive to do that kind of work. Whereas if you have some crazy new idea about some new architecture optimizer, it’s probably just gonna fail on the first rum, right? So you don’t have to run this up the ladder. It’s already broken. That’s great.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So, this doesn’t mean that scale is irrelevant for us. Scale is actually an important tool in the toolbox of all the things that you can do. Being able to scale up our ideas is certainly relevant to our company. So I wouldn’t frame us as the antithesis of scale, but I think it is a wonderful aspect of the kind of work we’re doing, that we can try many of our ideas at very small scale before we would even need to think about doing them at large scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher: &lt;/strong&gt;Yeah, you should be able to use all the internet. But you shouldn’t &lt;em&gt;need&lt;/em&gt; to. We find it really, really perplexing that you need to use all the Internet to really get this human level intelligence.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;So, what becomes possible&amp;nbsp; if you’re able to train more efficiently on data, right? Presumably the model will be more powerful and intelligent. But do you have specific ideas about kind of where that goes? Are we looking at more out-of-distribution generalization, or are we looking at sort of models that get better at a particular task with less experience?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher: &lt;/strong&gt;So, first, we’re doing science, so I don’t know the answer, but I can give you three hypotheses. So my first hypothesis is that there’s a broad spectrum between just looking for statistical patterns and something that has really deep understanding. And I think the current models live somewhere on that spectrum. I don’t think they’re all the way towards deep understanding, but they’re also clearly not just doing statistical pattern matching. And it’s possible that as you train models on less data, you really force the model to have incredibly deep understandings of everything it’s seen. And as you do that, the model may become more intelligent in very interesting ways. It may know less facts, but get better at reasoning. So that’s one potential hypothesis.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another hypothesis is similar to what you said, that at the moment, it’s very expensive, both operationally and also in pure monetary costs, to teach models new capabilities, because you need so much data to teach them those things. It’s possible that one output of what we’re doing is to get vastly more efficient at post training, so with only a couple of examples, you could really put a model into a new domain.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And then it’s also possible that this just unlocks new verticals for AI. There are certain types of robotics, for instance, where for whatever reason, we can’t quite get the type of capabilities that really makes it commercially viable. My opinion is that it’s a limited data problem, not a hardware problem. The fact that you can tele-operate the robots to do stuff is proof that that the hardware is sufficiently good. Butthere’s lots of domains like this, like scientific discovery.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben: &lt;/strong&gt;One thing I’ll also double-click on is that when we think about the impact that AI can have on the world, one view you might have is that this is a deflationary technology. That is, the role of AI is to automate a bunch of jobs, and take that work and make it cheaper to do, so that you’re able to remove work from the economy and have it done by robots instead. And I’m sure that will happen. But this is not, to my mind, the most exciting vision of AI. The most exciting vision of AI is one where there’s all kinds of new science and technologies that we can construct that humans aren’t smart enough to come up with, but other systems can.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On this aspect, I think that first axis that Ascher was talking about around the spectrum between sort of true generalization versus memorization or interpolation of the data, I think that axis is extremely important to have the deep insights that will lead to these new advances in medicine and science. It is important that the models are very much on the creativity side of the spectrum. And so, part of why I’m very excited about the work that we’re doing is that I think even beyond the individual economic impacts, I’m also just genuinely very kind of mission-oriented around the question of, can we actually get AI to do stuff that, like, fundamentally humans couldn’t do before? And that’s more than just, “Let’s go fire a bunch of people from their jobs.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Absolutely. Does that put you in a particular camp on, like, the AGI conversation, the like out of distribution, generalization conversation.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher: &lt;/strong&gt;I really don’t exactly know what AGI means. It’s clear that capabilities are advancing very quickly. It’s clear that there’s tremendous amounts of economic value that’s being created. I don’t think we’re very close to God-in-a-box, in my opinion. I don’t think that within two months or even two years, there’s going to be a singularity where suddenly humans are completely obsolete. I basically agree with what Ben said at the beginning, which is, it’s a really big world. There’s a lot of work to do. There’s a lot of amazing work being done, and we’re excited to contribute&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Well, the idea about the brain and the neuromorphic part of it does feel relevant. You’re saying, really the relevant thing to compare LLMs to is the human brain, more than the Mechanical Turk or the deterministic computers that came before.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aidan: &lt;/strong&gt;I’ll emphasize, the brain is not the ceiling, right? The brain, in many ways, is the floor. Frankly, I see no evidence that the brain is not a knowable system that follows physical laws. In fact, we know it’s under many constraints. And so we would expect to be able to create capabilities that are much, much more interesting and different and potentially better than the brain in the long run. And so we’re excited to contribute to that future, whether that’s AGI or otherwise.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher: &lt;/strong&gt;And I do think the brain is the relevant comparison, just because the brain helps us understand how big the space is. Like, it’s easy to see all the progress we’ve made and think, wow, we like, have the answer. We’re almost done. But if you look outward a little bit and try to have a bit more perspective, there’s a lot of stuff we don’t know.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben: &lt;/strong&gt;We’re not trying to be better, per se. We’re trying to be different, right? That’s the key thing I really want to hammer on here. All of these systems will almost certainly have different trade offs of them. You’ll get an advantage somewhere, and it’ll cost you somewhere else. And it’s a big world out there. There are so many different domains that have so many different trade offs that having more system, and more fundamental technologies that can address these different domains is very likely to make the kind of AI diffuse more effectively and more rapidly through the world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;One of the ways you’ve distinguished yourself, is in your hiring approach, getting people who are very, very young, in some cases, still in college or high school. What is it that clicks for you when you’re talking to someone and that makes you think, I want this person working with us on these research problems?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aidan: &lt;/strong&gt;It’s when you talk to someone and they just dazzle you, they have so many new ideas and they think about things in a way that many established researchers just can’t because they haven’t been polluted by the context of thousands and thousands of papers. Really, the number one thing we look for is creativity. Our team is so exceptionally creative, and every day, I feel really lucky to get to go in and talk about really radical solutions to some of the big problems in AI with people and dream up a very different future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben:&lt;/strong&gt;&amp;nbsp; Probably the number one signal that I’m personally looking for is just like, do they teach me something new when I spend time with them? If they teach me something new, the odds that they’re going to teach us something new about what we’re working on is also pretty good. When you’re doing research, those creative, new ideas are really the priority.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Part of my background was during my undergrad and PhD., I helped start this incubator called Prod that worked with a bunch of companies that turned out well. And I think one of the things that we saw from that was that young people can absolutely compete in the very highest echelons of industry. Frankly, a big part of the unlock is just realizing, yeah, I can go do this stuff. You can absolutely go contribute at the highest level.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, we do recognize the value of experience. People who have worked on large scale systems are great, like, we’ve hired some of them, you know, we are excited to work with all sorts of folks. And I think our mission has resonated with the experienced folks as well. I just think that our key thing is that we want people who are not afraid to change the paradigm and can try to imagine a new system of how things might work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;One of things I’ve been puzzling about is, how different do you think the resulting AI systems are going to be? It’s easy for me to imagine something like Claude Opus that just works 20% better and can do 20% more things. But if it’s just completely new, it’s hard to think about where that goes or what the end result looks like.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher:&lt;/strong&gt; I don’t know if you’ve ever had the privilege of talking to the GPT-4 base model, but it had a lot of really strange emerging capabilities. For example, you could take a snippet of an unwritten blog post of yours, and ask, who do you think wrote this, and it could identify it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There’s a lot of capabilities like this, where models are smart in ways we cannot fathom. And future models will be smarter in even stranger ways. I think we should expect the future to be really weird and the architectures to be even weirder. We’re looking for 1000x wins in data efficiency. We’re not trying to make incremental change. And so we should expect the same kind of unknowable, alien changes and capabilities at the limit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ben: I broadly agree with that. I’m probably slightly more tempered in how these things will eventually become experienced by the world, just as the GPT-4 base model was tempered by OpenAI. You want to put things in forms where you’re not staring into the abyss as a consumer. I think that’s important. But I broadly agree that our research agenda is about building capabilities that really are quite fundamentally different from what can be done right now.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Fantastic! Are there ways people can engage with flapping airplanes? Is it too early for that? Or they should just stay tuned for when the research and the models come out well.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher:&lt;/strong&gt; So, we have Hi@flappingairplanes.com. If you just want to say hi, We also have disagree@flappingairplanes.com if you want to disagree with us. We’ve actually had some really cool conversations where people, like, send us very long essays about why they think it’s impossible to do what we’re doing. And we’re happy to engage with it.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben:&lt;/strong&gt; But they haven’t convinced us yet. No one has convinced us yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher:&lt;/strong&gt; The second thing is, you know, we are, we are looking for exceptional people who are trying to change the field and change the world. So if you’re interested, you should reach out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben: &lt;/strong&gt;And if you have another unorthodox background, it’s okay. You don’t need two PhDs. We really are looking for folks who think differently.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Flapping-Airplanes-founders-photo.jpg?resize=1200,821" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;There’s been a bunch of exciting research-focused AI labs popping up in recent months, and Flapping Airplanes is one of the most interesting. Propelled by its young and curious founders, Flapping Airplanes is focused on finding less data-hungry ways to train AI. It’s a potential game-changer for the economics and capabilities of AI models — and with $180 million in seed funding, they’ll have plenty of runway to figure it out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week, I spoke with the lab’s three co-founders — brothers Ben and Asher Spector, and Aidan Smith — about why this is an exciting moment to start a new AI lab and why they keep coming back to ideas about the human brain.&lt;/p&gt;









&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;I want to start by asking, why now? Labs like OpenAI and DeepMind have spent so much on scaling their models. I’m sure the competition seems daunting. Why did this feel like a good moment to launch a foundation model company?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben:&lt;/strong&gt; There’s just so much to do. So, the advances that we’ve gotten over the last five to ten years have been spectacular. We love the tools. We use them every day. But the question is, is this the whole universe of things that needs to happen? And we thought about it very carefully and our answer was no, there’s a lot more to do. In our case, we thought that the data efficiency problem was sort of really the key thing to go look at. The current frontier models are trained on the sum totality of human knowledge, and humans can obviously make do with an awful lot less. So there’s a big gap there, and it’s worth understanding.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What we’re doing is really a concentrated bet on three things. It’s a bet that this data efficiency problem is the important thing to be doing. Like, this is really a direction that is new and different and you can make progress on it. It’s a bet that this will be very commercially valuable and that will make the world a better place if we can do it. And it’s also a bet that’s sort of the right kind of team to do it is a creative and even in some ways inexperienced team that can go look at these problems again from the ground up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aidan:&lt;/strong&gt; Yeah, absolutely. We don’t really see ourselves as competing with the other labs, because we think that we’re looking at just a very different set of problems. If you look at the human mind, it learns in an incredibly different way from transformers. And that’s not to say better, just very different. So we see these different trade offs. LLMs have an incredible ability to memorize, and draw on this great breadth of knowledge, but they can’t really pick up new skills very fast. It takes just rivers and rivers of data to adapt. And when you look inside the brain, you see that the algorithms that it uses are just fundamentally so different from gradient descent and some of the techniques that people use to train AI today. So that’s why we’re building a new guard of researchers to kind of address these problems and really think differently about the AI space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher&lt;/strong&gt;: This question is just so scientifically interesting: why are the systems that we have built that are intelligent also so different from what humans do? Where does this difference come from? How can we use knowledge of that difference to make better systems? But at the same time, I also think it’s actually very commercially viable and very good for the world. Lots of regimes that are really important are also highly data constrained, like robotics or scientific discovery. Even in enterprise applications, a model that’s a million times more data efficient is probably a million times easier to put into the economy. So for us, it was very exciting to take a fresh perspective on these approaches, and think, if we really had a model that’s vastly more data efficient, what could we do with it?&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;This gets into my next question, which is sort of ties in also to the name, Flapping Airplanes. There’s this philosophical question in AI about how much we’re trying to recreate what humans do in their brain, versus creating some more abstract intelligence that takes a completely different path. Aidan is coming from Neuralink, which is all about the human brain. Do you see yourself as kind of pursuing a more neuromorphic view of AI?&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aidan: &lt;/strong&gt;The way I look at the brain is as an existence proof. We see it as evidence that there are other algorithms out there. There’s not just one orthodoxy. And the brain has some crazy constraints. When you look at the underlying hardware, there’s some crazy stuff. It takes a millisecond to fire an action potential. In that time, your computer can do just so so many operations. And so realistically, there’s probably an approach that’s actually much better than the brain out there, and also very different than the transformer. So we’re very inspired by some of the things that the brain does, but we don’t see ourselves being tied down by it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben: &lt;/strong&gt;Just to add on to that. it’s very much in our name: Flapping Airplanes.&amp;nbsp;Think of the current systems as big, Boeing 787s. We’re not trying to build birds. That’s a step too far. We’re trying to build some kind of a flapping airplane. My perspective from computer systems is that the constraints of the brain and silicon are sufficiently different from each other that we should not expect these systems to end up looking the same. When the substrate is so different and you have genuinely very different trade-offs about the cost of compute, the cost of locality and moving data, you actually expect these systems to look a little bit different. But just because they will look somewhat different does not mean that we should not take inspiration from the brain and try to use the parts that we think are interesting to improve our own systems.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;It does feel like there’s now more freedom for labs to focus on research, as opposed to, just developing products. It feels like a big difference for this generation of labs. You have some that are very research focused, and others that are sort of “research focused for now.” What does that conversation look like within flapping airplanes?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher:&lt;/strong&gt; I wish I could give you a timeline. I wish I could say, in three years, we’re going to have solved the research problem. This is how we’re going to commercialize. I can’t. We don’t know the answers. We’re looking for truth. That said, I do think we have commercial backgrounds. I spent a bunch of time developing technology for companies that made those companies a reasonable amount of money. Ben has incubated a bunch of startups that have commercial backgrounds, and we actually are excited to commercialize. We think it’s good for the world to take the value you’ve created and put it in the hands of people who can use it. So I don’t think we’re opposed to it. We just need to start by doing research, because if we start by signing big enterprise contracts, we’re going to get distracted, and we won’t do the research that’s valuable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aidan:&lt;/strong&gt; Yeah, we want to try really, really radically different things, and sometimes radically even things are just worse than the paradigm. We’re exploring a set of different trade offs. It’s our hope that they will be different in the long run.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben: &lt;/strong&gt;Companies are at their best when they’re really focused on doing something well, right? Big companies can afford to do many, many different things at once. When you’re a startup, you really have to pick what is the most valuable thing you can do, and do that all the way. And we are creating the most value when we are all in on solving fundamental problems for the time being.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I’m actually optimistic that reasonably soon, we might have made enough progress that we can then go start to touch grass in the real world. And you learn a lot by getting feedback from the real world. The amazing thing about the world is, it teaches you things constantly, right? It’s this tremendous vat of truth that you get to look into whenever you want. I think the main thing that I think has been enabled by the recent change in the economics and financing of these structures is the ability to let companies really focus on what they’re good at for longer periods of time. I think that focus, the thing that I’m most excited about, that will let us do really differentiated work.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;To spell out what I think you’re referring to: there’s so much excitement around and the opportunity for investors is so clear that they are willing to give $180 million in seed funding to a completely new company full of these very smart, but also very young people who didn’t just cash out of PayPal or anything. How was it engaging with that process? Did you know, going in, there is this appetite, or was it something you discovered, of like, actually, we can make this a bigger thing than we thought.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben:&lt;/strong&gt; I would say it was a mixture of the two. The market has been hot for many months at this point. So it was not a secret that no large rounds were starting to come together. But you never quite know how the fundraising environment will respond to your particular ideas about the world. This is, again, a place where you have to let the world give you feedback about what you’re doing. Even over the course of our fundraise, we learned a lot and actually changed our ideas. And we refined our opinions of the things we should be prioritizing, and what the right timelines were for commercialization.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think we were somewhat surprised by how well our message resonated, because it was something that was very clear to us, but you never know whether your ideas will turn out to be things that other people believe as well or if everyone else thinks you’re crazy. We have been extremely fortunate to have found a group of amazing investors who our message really resonated with and they said, “Yes, this is exactly what we’ve been looking for.” And that was amazing. It was, you know, surprising and wonderful.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aidan:&lt;/strong&gt; Yeah, a thirst for the age of research has kind of been in the water for a little bit now. And more and more, we find ourselves positioned as the player to pursue the age of research and really try these radical ideas.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;At least for the scale-driven companies, there is this enormous cost of entry for foundation models. Just building a model at that scale is an incredibly compute-intensive thing. Research is a little bit in the middle, where presumably you are building foundation models, but if you’re doing it with less data and you’re not so scale-oriented, maybe you get a bit of a break. How much do you expect compute costs to be sort of limiting your runway.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben:&lt;/strong&gt; One of the advantages of doing deep, fundamental research is that, somewhat paradoxically, it is much cheaper to do really crazy, radical ideas than it is to do incremental work. Because when you do incremental work, in order to find out whether or not it does work, you have to go very far up the scaling ladder. Many interventions that look good at small scale do not actually persist at large scale. So as a result, it’s very expensive to do that kind of work. Whereas if you have some crazy new idea about some new architecture optimizer, it’s probably just gonna fail on the first rum, right? So you don’t have to run this up the ladder. It’s already broken. That’s great.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So, this doesn’t mean that scale is irrelevant for us. Scale is actually an important tool in the toolbox of all the things that you can do. Being able to scale up our ideas is certainly relevant to our company. So I wouldn’t frame us as the antithesis of scale, but I think it is a wonderful aspect of the kind of work we’re doing, that we can try many of our ideas at very small scale before we would even need to think about doing them at large scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher: &lt;/strong&gt;Yeah, you should be able to use all the internet. But you shouldn’t &lt;em&gt;need&lt;/em&gt; to. We find it really, really perplexing that you need to use all the Internet to really get this human level intelligence.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;So, what becomes possible&amp;nbsp; if you’re able to train more efficiently on data, right? Presumably the model will be more powerful and intelligent. But do you have specific ideas about kind of where that goes? Are we looking at more out-of-distribution generalization, or are we looking at sort of models that get better at a particular task with less experience?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher: &lt;/strong&gt;So, first, we’re doing science, so I don’t know the answer, but I can give you three hypotheses. So my first hypothesis is that there’s a broad spectrum between just looking for statistical patterns and something that has really deep understanding. And I think the current models live somewhere on that spectrum. I don’t think they’re all the way towards deep understanding, but they’re also clearly not just doing statistical pattern matching. And it’s possible that as you train models on less data, you really force the model to have incredibly deep understandings of everything it’s seen. And as you do that, the model may become more intelligent in very interesting ways. It may know less facts, but get better at reasoning. So that’s one potential hypothesis.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another hypothesis is similar to what you said, that at the moment, it’s very expensive, both operationally and also in pure monetary costs, to teach models new capabilities, because you need so much data to teach them those things. It’s possible that one output of what we’re doing is to get vastly more efficient at post training, so with only a couple of examples, you could really put a model into a new domain.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And then it’s also possible that this just unlocks new verticals for AI. There are certain types of robotics, for instance, where for whatever reason, we can’t quite get the type of capabilities that really makes it commercially viable. My opinion is that it’s a limited data problem, not a hardware problem. The fact that you can tele-operate the robots to do stuff is proof that that the hardware is sufficiently good. Butthere’s lots of domains like this, like scientific discovery.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben: &lt;/strong&gt;One thing I’ll also double-click on is that when we think about the impact that AI can have on the world, one view you might have is that this is a deflationary technology. That is, the role of AI is to automate a bunch of jobs, and take that work and make it cheaper to do, so that you’re able to remove work from the economy and have it done by robots instead. And I’m sure that will happen. But this is not, to my mind, the most exciting vision of AI. The most exciting vision of AI is one where there’s all kinds of new science and technologies that we can construct that humans aren’t smart enough to come up with, but other systems can.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On this aspect, I think that first axis that Ascher was talking about around the spectrum between sort of true generalization versus memorization or interpolation of the data, I think that axis is extremely important to have the deep insights that will lead to these new advances in medicine and science. It is important that the models are very much on the creativity side of the spectrum. And so, part of why I’m very excited about the work that we’re doing is that I think even beyond the individual economic impacts, I’m also just genuinely very kind of mission-oriented around the question of, can we actually get AI to do stuff that, like, fundamentally humans couldn’t do before? And that’s more than just, “Let’s go fire a bunch of people from their jobs.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Absolutely. Does that put you in a particular camp on, like, the AGI conversation, the like out of distribution, generalization conversation.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher: &lt;/strong&gt;I really don’t exactly know what AGI means. It’s clear that capabilities are advancing very quickly. It’s clear that there’s tremendous amounts of economic value that’s being created. I don’t think we’re very close to God-in-a-box, in my opinion. I don’t think that within two months or even two years, there’s going to be a singularity where suddenly humans are completely obsolete. I basically agree with what Ben said at the beginning, which is, it’s a really big world. There’s a lot of work to do. There’s a lot of amazing work being done, and we’re excited to contribute&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Well, the idea about the brain and the neuromorphic part of it does feel relevant. You’re saying, really the relevant thing to compare LLMs to is the human brain, more than the Mechanical Turk or the deterministic computers that came before.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aidan: &lt;/strong&gt;I’ll emphasize, the brain is not the ceiling, right? The brain, in many ways, is the floor. Frankly, I see no evidence that the brain is not a knowable system that follows physical laws. In fact, we know it’s under many constraints. And so we would expect to be able to create capabilities that are much, much more interesting and different and potentially better than the brain in the long run. And so we’re excited to contribute to that future, whether that’s AGI or otherwise.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher: &lt;/strong&gt;And I do think the brain is the relevant comparison, just because the brain helps us understand how big the space is. Like, it’s easy to see all the progress we’ve made and think, wow, we like, have the answer. We’re almost done. But if you look outward a little bit and try to have a bit more perspective, there’s a lot of stuff we don’t know.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben: &lt;/strong&gt;We’re not trying to be better, per se. We’re trying to be different, right? That’s the key thing I really want to hammer on here. All of these systems will almost certainly have different trade offs of them. You’ll get an advantage somewhere, and it’ll cost you somewhere else. And it’s a big world out there. There are so many different domains that have so many different trade offs that having more system, and more fundamental technologies that can address these different domains is very likely to make the kind of AI diffuse more effectively and more rapidly through the world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;One of the ways you’ve distinguished yourself, is in your hiring approach, getting people who are very, very young, in some cases, still in college or high school. What is it that clicks for you when you’re talking to someone and that makes you think, I want this person working with us on these research problems?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aidan: &lt;/strong&gt;It’s when you talk to someone and they just dazzle you, they have so many new ideas and they think about things in a way that many established researchers just can’t because they haven’t been polluted by the context of thousands and thousands of papers. Really, the number one thing we look for is creativity. Our team is so exceptionally creative, and every day, I feel really lucky to get to go in and talk about really radical solutions to some of the big problems in AI with people and dream up a very different future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben:&lt;/strong&gt;&amp;nbsp; Probably the number one signal that I’m personally looking for is just like, do they teach me something new when I spend time with them? If they teach me something new, the odds that they’re going to teach us something new about what we’re working on is also pretty good. When you’re doing research, those creative, new ideas are really the priority.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Part of my background was during my undergrad and PhD., I helped start this incubator called Prod that worked with a bunch of companies that turned out well. And I think one of the things that we saw from that was that young people can absolutely compete in the very highest echelons of industry. Frankly, a big part of the unlock is just realizing, yeah, I can go do this stuff. You can absolutely go contribute at the highest level.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, we do recognize the value of experience. People who have worked on large scale systems are great, like, we’ve hired some of them, you know, we are excited to work with all sorts of folks. And I think our mission has resonated with the experienced folks as well. I just think that our key thing is that we want people who are not afraid to change the paradigm and can try to imagine a new system of how things might work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;One of things I’ve been puzzling about is, how different do you think the resulting AI systems are going to be? It’s easy for me to imagine something like Claude Opus that just works 20% better and can do 20% more things. But if it’s just completely new, it’s hard to think about where that goes or what the end result looks like.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher:&lt;/strong&gt; I don’t know if you’ve ever had the privilege of talking to the GPT-4 base model, but it had a lot of really strange emerging capabilities. For example, you could take a snippet of an unwritten blog post of yours, and ask, who do you think wrote this, and it could identify it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There’s a lot of capabilities like this, where models are smart in ways we cannot fathom. And future models will be smarter in even stranger ways. I think we should expect the future to be really weird and the architectures to be even weirder. We’re looking for 1000x wins in data efficiency. We’re not trying to make incremental change. And so we should expect the same kind of unknowable, alien changes and capabilities at the limit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ben: I broadly agree with that. I’m probably slightly more tempered in how these things will eventually become experienced by the world, just as the GPT-4 base model was tempered by OpenAI. You want to put things in forms where you’re not staring into the abyss as a consumer. I think that’s important. But I broadly agree that our research agenda is about building capabilities that really are quite fundamentally different from what can be done right now.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Fantastic! Are there ways people can engage with flapping airplanes? Is it too early for that? Or they should just stay tuned for when the research and the models come out well.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher:&lt;/strong&gt; So, we have Hi@flappingairplanes.com. If you just want to say hi, We also have disagree@flappingairplanes.com if you want to disagree with us. We’ve actually had some really cool conversations where people, like, send us very long essays about why they think it’s impossible to do what we’re doing. And we’re happy to engage with it.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben:&lt;/strong&gt; But they haven’t convinced us yet. No one has convinced us yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Asher:&lt;/strong&gt; The second thing is, you know, we are, we are looking for exceptional people who are trying to change the field and change the world. So if you’re interested, you should reach out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Ben: &lt;/strong&gt;And if you have another unorthodox background, it’s okay. You don’t need two PhDs. We really are looking for folks who think differently.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/16/flapping-airplanes-on-the-future-of-ai-we-want-to-try-really-radically-different-things/</guid><pubDate>Mon, 16 Feb 2026 14:00:00 +0000</pubDate></item><item><title>[NEW] Tuning into the future of collaboration (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/16/1125881/tuning-into-the-future-of-collaboration/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Sam-Sabet-Brendan-Ittelson.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Shure&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;   &lt;p&gt;When work went remote, the sound of business changed. What began as a scramble to make home offices functional has evolved into a revolution in how people hear and are heard. From education to enterprises, companies across industries have reimagined what clear, reliable communication can mean in a hybrid world. For major audio and communications enterprises like Shure and Zoom, that transformation has been powered by artificial intelligence, new acoustic technologies, and a shared mission: making connection effortless.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Necessity during the pandemic accelerated years of innovation in months.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;"Audio and video just working is a baseline for collaboration," says chief ecosystem officer at Zoom, Brendan Ittelson. "That expectation has shifted from connecting people to enhancing productivity and creativity across the entire ecosystem."&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Audio is a foundation for trust, understanding, and collaboration. Poor sound quality can distort meaning and fatigue listeners, while crisp audio and intelligent processing can make digital interactions feel nearly as natural as in-person exchanges.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;"If you think about the fundamental need here," adds chief technology officer at Shure, Sam Sabet, "It's the ability to amplify the audio and the information that's really needed, and diminish the unwanted sounds and audio so that we can enhance that experience and make it seamless for people to communicate."&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For both Ittelson and Sabet, AI now sits at the center of this progress. For Shure, machine learning powers real-time noise suppression, adaptive beamforming, and spatial audio that tunes itself to a room’s acoustics. For Zoom, AI underpins every layer of its platform, from dynamic noise reduction to automated meeting summaries and intelligent assistants that anticipate user needs. These tools are transforming communication from reactive to proactive, enabling systems that understand intent, context, and emotion.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;"Even if you're not working from home and coming into the office, the types of spaces and environments you try to collaborate in today are constantly changing because our needs are constantly changing," says Sabet. "Having software and algorithms that adapt seamlessly and self-optimize based on the acoustics of the room, based on the different layouts of the spaces where people collaborate in is instrumental."&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The future, they suggest, is one where technology fades into the background. As audio devices and AI companions learn to self-optimize, users won’t think about microphones or meeting links. Instead, they’ll simply connect. Both companies are now exploring agentic AI systems and advanced wireless solutions that promise to make collaboration seamless across spaces, whether in classrooms, conference rooms, or virtual environments yet to come.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"It's about helping people focus on strategy and creativity instead of administrative busy work," says Ittelson.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This episode of Business Lab is produced in partnership with Shure.&lt;/em&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;Full Transcript&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan Tatum: &lt;/em&gt;From MIT Technology Review, I'm Megan Tatum and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This episode is produced in partnership with Shure.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now as the pandemic ushered in the cultural shift that led to our increasingly virtual world, it also sparked a flurry of innovation in the audio and video industries to keep employees and customers connected and businesses running. Today we're going to talk about the AI technologies behind those innovations, the impact on audio innovation, and the continuing emerging opportunities for further advances in audio capabilities.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Two words for you: elevated audio.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;My guests today are Sam Sabet, chief technology officer at Shure, and Brendan Ittelson, chief ecosystem officer at Zoom.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Welcome Sam, welcome Brendan.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam Sabet:&lt;/em&gt; Thank you, Megan. It's a pleasure to be here and I'm looking forward to this conversation with both you and Brendan. It should be a very exciting conversation.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;Brendan Ittelson:&lt;/em&gt; Thank you so much for having me today. I'm looking forward to the conversation and all the topics we have to dive into on this area.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. Lovely to have you both here. And Sam, just to set some context, I wonder if we could start with the pandemic and the innovation that really was born out of necessity. I mean, when it became clear that we were all going to be virtual for the foreseeable future, I wonder what was the first technological mission for Shure?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Yeah, very good question. The pandemic really accelerated a lot of innovation around virtual communications and fundamentally how we perform our everyday jobs remotely. One of our first technological mission when the pandemic happened and everybody ended up going home and performing their functions remotely was to make sure that people could continue to communicate effectively, whether that's for business meetings, virtual events, or educational purposes. We focused on collaboration and enhancing collaboration tools. And ideally what we were aiming to do, or we focused on, was to basically improve the ease of use and configuration of audio tool sets.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Because unlike the office environment where it might be a lot more controlled, people are working from non-traditional areas like home offices or other makeshift solutions, we needed to make sure that people could still get pristine audio and that studio level audio even in uncontrolled environments that are not really made for that. We expedited development in our software solutions. We created tool sets that allowed for ease of deployment and remote configuration and management so we could enable people to continue doing the things they needed to do without having to worry about the underlying technology.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;And Brendan, during that time, it seemed everyone became a Zoom user of some sort. I mean, what was the first mission at Zoom when virtual connection became this necessity for everyone?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Well, our mission fundamentally didn't change. It's always been about delivering frictionless communications. What shifted was the urgency and the magnitude of what we were doing. Our focus shifted on how we do this reliably, securely, and to scale to ensure these millions of new users could connect instantly without friction. We really shifted our thinking of being just a business continuity tool to becoming a lifeline for so many individuals and industries. The stories that we heard across education, healthcare, and just general human connection, the number of those moments that matter to people that we were able to help facilitate just became so important. We really focused on how can we be there and make it frictionless so folks can focus on that human connection. And that accelerated our thinking in terms of innovation and reinforced the thought that we need to focus on the simplicity, accessibility, and trust in communication technology so that people could focus on that connection and not the technology that makes it possible.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;That's so true. It did really just become an absolute lifeline for people, didn't it? And before we dive into the technologies beyond these emerging capabilities, I wonder if we could first talk about just the importance of clear audio. I mean, Sam, as much as we all worry over how we look on Zoom, is how we sound perhaps as or even more impactful?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Yeah, you're absolutely correct. I mean, clear audio is absolutely critical for effective communications. Video quality is very important absolutely, but poor audio can really hinder understanding and engagement. As a matter of fact, there's studies and research from areas such as Yale University that say that poor audio can make understanding somewhat more challenged and even affect retention of information. Especially in an educational type environment where there's a lot of background noise and very differing types of spaces like auditoriums and lecture halls, it really becomes a high priority that you have great audio quality. And during the pandemic, as you said, and as Brendan rightly said, it became one of our highest priorities to focus on technologies like beamforming mics and ways to focus on the speaker's voice and minimize that unwanted background noise so that we could ensure that the communication was efficient, was well understood, and that it removed the distraction so people could be able to actually communicate and retain the information that was being shared.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;It is incredible just how impactful audio can be, can't it? Brendan, I mean as you said, remote and hybrid collaboration is part of Zoom's DNA. What observations can you share about how users have grown along with the technological advancements and maybe how their expectations have grown as well?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. I mean, users now expect seamless and intelligent experiences. Audio and video just working is a baseline for collaboration. That expectation has shifted from connecting people to enhancing productivity and creativity across the entire ecosystem. When we look at it, we're really looking at these trends in terms of how people want to be better when they're at home. For example, AI-powered tools like Smart Summaries, translation and noise suppression to help people stay productive and connected no matter where they're working. But then this also comes into play at the office. We're starting to see folks that dive into our technology like Intelligent Director and Smart Name Tags that create that meeting equity even when they're in a conference room.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So, the remote experience and the room experience all are similar and create that same ability to be seen, heard, and contribute. And we're now diving further into this that it's beyond just meetings. Zoom is really transforming into an AI-first work platform that's focused on human connection. And so that goes beyond the meetings into things like Chat, Zoom Docs, Zoom Events and Webinars, the Zoom Contact Center and more. And all of this being brought together using our AI Companion at its core to help connect all of those different points of connection for individuals.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;I mean, so Brendan, we know it wasn't only workplaces that were affected by the pandemic, it was also the education sector that had to undergo a huge change. I wondered if you could talk a little bit about how Zoom has operated in that higher education sphere as well.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. Education has always been a focus for Zoom and an area that we've believed in. Because education and learning is something as a company we value and so we have invested in that sector. And personally being the son of academics, it is always an area that I find fascinating. We continue to invest in terms of how do we make the classroom a stronger space? And especially now that the classroom has changed, where it can be in person, it can be virtual, it can be a mix. And using Zoom and its tools, we're able to help bridge all those different scenarios to make learning accessible to students no matter their means.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's what truly excites us, is being able to have that technology that allows people to pursue their desires, their interests, and really up-level their pursuits and inspire more. We're constantly investing in how to allow those messages to get out and to integrate in the flow of communication and collaboration that higher education uses, whether that's being integrated into the classroom, into learning management systems, to make that a seamless flow so that students and their educators can just collaborate seamlessly. And also that we can support all the infrastructure and administration that helps make that possible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. Such an important thing. And Sam, Shure as well, could you talk to us a bit about how you worked in that kind of education space as well from an audio point of view?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Absolutely. Actually, this is a topic that's near and dear to my heart because I'm actually an adjunct professor in my free time.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Oh, wow. Very impressive.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;And the challenges of trying to do this sort of a hybrid lecture, if you will. And Shure has been particularly well suited for this environment and we've been focused on it and investing in technologies there for decades. If you think about how a lecture hall is structured, it's a little different than just having a meeting around the conference table. And Shure has focused on creating products that allow this combination of a presenter scenario along with a meeting space plus the far end where users or students are remote, they can hear intelligibly what's happening in the lecture hall, but they can also participate.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Between our products like the Ceiling Mic Arrays and our wireless microphones that are purpose built for presenters and educators like our MXW neXt product line, we've created technologies that allow those two previously separate worlds to integrate together. And then add that onto integrating with Zoom and other products that allow for that collaboration has been very instrumental. And again, being a user and providing those lectures, I can see a night and day difference and just how much more effective my lectures are today from where they were five to six years ago. And that's all just made possible by all the technologies that are purpose built for these scenarios and integrating more with these powerful tools that just make the job so much more seamless.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely fascinating that you got to put the technology to use yourself as well to check that it was all working well. And you mentioned AI there, of course. I mean, Sam, what AI technologies have had the most significant impact on recent audio advancements too?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Yeah. Absolutely. If you think about the fundamental need here, it's the ability to amplify the audio and the information that's really needed and diminish the unwanted sounds and audio so that we can enhance that experience and make it seamless for people to communicate. With our innovations at Shure, we've leveraged the cutting-edge technologies to both enhance communication effectiveness and to align seamlessly with evolving features in unified communications like the ones that Brandon just mentioned in the Zoom platforms.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We partner with industry leaders like Zoom to ensure that we're providing the ability to be able to focus on that needed audio and eliminate all the background distractions. AI has transformed that audio technology with things like machine learning algorithms that enable us to do more real-time audio processing and significantly enhancing things like noise reduction and speech isolation. Just to give you a simple example, our IntelliMix Room audio processing software that we've released as well as part of a complete room solution uses AI to optimize sound in different environments.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And really that's one of the fundamental changes in this period, whether that's pandemic or post-pandemic, is that the key is really flexibility and being able to adapt to changing work environments. Even if you're not working from home and coming into the office, the types of spaces and environments you try to collaborate in today are constantly changing because our needs are constantly changing. And so having software and algorithms that adapt seamlessly and are able to self-optimize based on the acoustics of the room, based on the different layouts of the spaces where people collaborate in is instrumental.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And then last but not least, AI has transformed the way audio and video integrate. For example, we utilize voice recognition systems that integrate with intelligent cameras so that we enable voice tracking technology so that cameras can not only identify who's speaking, but you have the ability to hear and see people clearly. And that in general just enhances the overall communication experience.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Wow. It's just so much innovation in quite a short space of time really. I mean, Brendan, you mentioned AI a little bit there beforehand, but I wonder what other AI technologies have had the biggest impact as Zoom builds out its own emerging capabilities?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. And I couldn't agree more with Sam that, I mean, AI has made such a big shift and it's really across the spectrum. And when I think about it, there's almost three tiers when you look at the stack. You start off at the raw audio where AI is doing those things like noise suppression, echo cancellation, voice enhancements. All of that just makes this amazing audio signal that can then go into the next layer, which is the speech AI and natural language processing. Which starts to open up those items such as the real-time transcription, translation, searchable content to make the communication not just what's heard, but making it more accessible to more individuals and inclusive by providing that content in a format that is best for them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And then you take those two layers and put the generative and agentic AI on top of that, that can start surfacing insights, summarize the conversation, and even take actions on someone's behalf. It really starts to change the way that people work and how they have access and allows them to connect. I think it is a huge shift and I'm very excited by how those three levels start to interact to really enable people to do more and to connect thanks to AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Yeah. Absolutely. So much rich information that can come out from a single call now because of those sorts of tools. And following on from that, Brendan, I mean, you mentioned before the Zoom AI Companion. I wondered if you could talk a bit about what were your top priorities when building that product to ensure it was truly useful for your customers?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. When we developed AI Companion, we had two priority focus areas from day one, trust and security, and then accuracy and relevance. On the trust side, it was a non-negotiable that customer data wouldn't be used to train our models. People need to know that their conversations and content are private and secure.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Of course.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;And then with accuracy, we needed to ensure AI outputs weren't generic but grounded in the actual context of a meeting, a chat or a product. But the real story here when I think about AI Companion is the customer value that it delivers. AI Companion helps people save time with meeting recaps, task generation, and proactive prep for the next session. It reduces that friction in hybrid work, whether you're in a meeting room, a Zoom room, or collaborating across different collaboration tools like Microsoft or Google. And it enables more equitable participation by surfacing the right context for everyone no matter where and how they're working.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;All this leads to a result where it's practical, trustworthy, and embedded where work happens. And it's just not another tool to manage, it's there in someone's flow of work to help them along the way.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Yeah. That trust piece is just so important, isn't it, today? And Sam, as much as AI has impacted audio innovation, audio has also had an impact on AI capabilities. I wondered if you could talk a little bit about audio as a data input and the advancements technologies like large language models, LLMs, are enabling.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Absolutely. Audio is really a rich data source that's added a new dimension to AI capabilities. If you think about speech recognition or natural language processing, they've had significant advances due to audio data that's provided for them. And to Brendan's point about trust and accuracy, I like to think of the products that Shure enables customers with as essentially the eyes and ears in the room for leading AI companions just like the Zoom AI Companion. You really need that pristine audio input to be able to trust the accuracy of what the AI generates. These AI Companions have been very instrumental in the way we do business every day. I mean, between transcription, speaker attributions, the ability to add action items within a meeting and be able to track what's happening in our interactions, all of that really has to rely on that accurate and pristine input from audio into the AI. I feel that further improves the trust that our end users have to the results of AI and be able to leverage it more.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If you think about it, if you look at how AI audio inputs enhance that interactive AI system, it enables more natural and intuitive interactions with AI. And it really allows for that seamless integration and the ability for users to use it without having to worry about, is the room set up correctly? Is the audio level proper? And when we talk even about agentic AI, we're working on future developments where systems can self-heal or detect that there are issues in the environment so that they can autocorrect and adapt in all these different environments and further enable the AI to be able to do a much more effective job, if you will.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Sam, you touched on future developments there. I wonder if we could close our conversation today with a bit of a future forward look, if we could. Brendan, can you share innovations that Zoom is working on now and what are you most excited to see come to fruition?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Well, your timing for this question is absolutely perfect because we've just wrapped up Zoomtopia 2025.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Oh, wow.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;And this is where we discussed a lot of the new AI innovations that we have coming to Zoom. Starting off, there's AI Companion 3.0. And we've launched this next generation of agentic AI capabilities in Zoom Workplace. And with 3.0 when it releases, it isn't just about transcribing, it's turned into really a platform that helps you with follow-up task, prep for your next conversation, and even proactively suggest how to free up your time. For example, AI Companion can help you schedule meetings intelligently across time zones, suggest which meetings you can skip, and still stay informed and even prepare you with context and insights before you walk into the conversation. It's about helping people focus on strategy and creativity instead of administrative busy work. And for hybrid work specifically, we introduced Zoomie Group Assistant, which will be a big leap for hybrid collaboration.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Acting as an assistant for a group chat and meetings, you can simply ask, “@Zoomie, what's the latest update on the project?” Or “@Zoomie, what are the team's action items?” And then get instant answers. Or because we're talking about audio here, you can go into a conference room and say, "Hey, Zoomie," and get help with things like checking into a room, adjusting lights, temperature, or even sharing your screen. And while all these are built-in features, we're also expanding the platform to allow custom AI agents through our AI Studio, so organizations can bring their own agents or integrate with third-party ones.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt;&lt;p&gt;Zoom has always believed in an open platform and philosophy and that is continuing. Folks using AI Companion 3.0 will be able to use agents across platforms to work with the workflows that they have across all the different SaaS vendors that they might have in their environment, whether that's Google, Microsoft, ServiceNow, Cisco, and so many other tools.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. It certainly sounds like a tool I could use in my work, so I look forward to hearing more about that. And Sam, we've touched on there are so many exciting things happening in audio too. What are you working on at Shure? And what are you most excited to see come to fruition?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;At Shure, our engineering teams are really working on a range of exciting projects, but particularly we're working on developing new collaboration solutions that are integral for IT end users. And these integrate obviously with the leading UC platforms.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We're integrating audio and video technologies that are scalable, reliable solutions. And we want to be able to seamlessly connect these to cloud services so that we can leverage both AI technologies and the tool sets available to optimize every type of workspace essentially. Not just meeting rooms, but lecture halls, work from home scenarios, et cetera.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The other area that we really focus on in terms of our reliability and quality really comes from our DNA in the pro audio world. And that's really all-around wireless audio technologies. We're developing our next-generation wireless systems and these are going to offer even greater reliability and range. And they really become ideal for everything from a large-scale event to personal home use and the gamut across that whole spectrum. And I think all of that in partnership with our partners like Zoom will help just facilitate the modern workspace.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. So much exciting innovation clearly going on behind the scenes. Thank you both so much.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That was Sam Sabet, chief technology officer at Shure, and Brendan Ittelson, chief ecosystem officer at Zoom, whom I spoke with from Brighton in England.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's it for this episode of Business Lab. I'm your host, Megan Tatum. I'm a contributing editor at Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology and you can find us in print on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This show is available wherever you get your podcasts. And if you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review and this episode was produced by Giro Studios. Thanks for listening.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Sam-Sabet-Brendan-Ittelson.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Shure&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;   &lt;p&gt;When work went remote, the sound of business changed. What began as a scramble to make home offices functional has evolved into a revolution in how people hear and are heard. From education to enterprises, companies across industries have reimagined what clear, reliable communication can mean in a hybrid world. For major audio and communications enterprises like Shure and Zoom, that transformation has been powered by artificial intelligence, new acoustic technologies, and a shared mission: making connection effortless.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Necessity during the pandemic accelerated years of innovation in months.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;"Audio and video just working is a baseline for collaboration," says chief ecosystem officer at Zoom, Brendan Ittelson. "That expectation has shifted from connecting people to enhancing productivity and creativity across the entire ecosystem."&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Audio is a foundation for trust, understanding, and collaboration. Poor sound quality can distort meaning and fatigue listeners, while crisp audio and intelligent processing can make digital interactions feel nearly as natural as in-person exchanges.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;"If you think about the fundamental need here," adds chief technology officer at Shure, Sam Sabet, "It's the ability to amplify the audio and the information that's really needed, and diminish the unwanted sounds and audio so that we can enhance that experience and make it seamless for people to communicate."&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For both Ittelson and Sabet, AI now sits at the center of this progress. For Shure, machine learning powers real-time noise suppression, adaptive beamforming, and spatial audio that tunes itself to a room’s acoustics. For Zoom, AI underpins every layer of its platform, from dynamic noise reduction to automated meeting summaries and intelligent assistants that anticipate user needs. These tools are transforming communication from reactive to proactive, enabling systems that understand intent, context, and emotion.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;"Even if you're not working from home and coming into the office, the types of spaces and environments you try to collaborate in today are constantly changing because our needs are constantly changing," says Sabet. "Having software and algorithms that adapt seamlessly and self-optimize based on the acoustics of the room, based on the different layouts of the spaces where people collaborate in is instrumental."&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The future, they suggest, is one where technology fades into the background. As audio devices and AI companions learn to self-optimize, users won’t think about microphones or meeting links. Instead, they’ll simply connect. Both companies are now exploring agentic AI systems and advanced wireless solutions that promise to make collaboration seamless across spaces, whether in classrooms, conference rooms, or virtual environments yet to come.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"It's about helping people focus on strategy and creativity instead of administrative busy work," says Ittelson.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This episode of Business Lab is produced in partnership with Shure.&lt;/em&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;Full Transcript&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan Tatum: &lt;/em&gt;From MIT Technology Review, I'm Megan Tatum and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This episode is produced in partnership with Shure.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now as the pandemic ushered in the cultural shift that led to our increasingly virtual world, it also sparked a flurry of innovation in the audio and video industries to keep employees and customers connected and businesses running. Today we're going to talk about the AI technologies behind those innovations, the impact on audio innovation, and the continuing emerging opportunities for further advances in audio capabilities.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Two words for you: elevated audio.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;My guests today are Sam Sabet, chief technology officer at Shure, and Brendan Ittelson, chief ecosystem officer at Zoom.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Welcome Sam, welcome Brendan.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam Sabet:&lt;/em&gt; Thank you, Megan. It's a pleasure to be here and I'm looking forward to this conversation with both you and Brendan. It should be a very exciting conversation.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;Brendan Ittelson:&lt;/em&gt; Thank you so much for having me today. I'm looking forward to the conversation and all the topics we have to dive into on this area.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. Lovely to have you both here. And Sam, just to set some context, I wonder if we could start with the pandemic and the innovation that really was born out of necessity. I mean, when it became clear that we were all going to be virtual for the foreseeable future, I wonder what was the first technological mission for Shure?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Yeah, very good question. The pandemic really accelerated a lot of innovation around virtual communications and fundamentally how we perform our everyday jobs remotely. One of our first technological mission when the pandemic happened and everybody ended up going home and performing their functions remotely was to make sure that people could continue to communicate effectively, whether that's for business meetings, virtual events, or educational purposes. We focused on collaboration and enhancing collaboration tools. And ideally what we were aiming to do, or we focused on, was to basically improve the ease of use and configuration of audio tool sets.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Because unlike the office environment where it might be a lot more controlled, people are working from non-traditional areas like home offices or other makeshift solutions, we needed to make sure that people could still get pristine audio and that studio level audio even in uncontrolled environments that are not really made for that. We expedited development in our software solutions. We created tool sets that allowed for ease of deployment and remote configuration and management so we could enable people to continue doing the things they needed to do without having to worry about the underlying technology.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;And Brendan, during that time, it seemed everyone became a Zoom user of some sort. I mean, what was the first mission at Zoom when virtual connection became this necessity for everyone?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Well, our mission fundamentally didn't change. It's always been about delivering frictionless communications. What shifted was the urgency and the magnitude of what we were doing. Our focus shifted on how we do this reliably, securely, and to scale to ensure these millions of new users could connect instantly without friction. We really shifted our thinking of being just a business continuity tool to becoming a lifeline for so many individuals and industries. The stories that we heard across education, healthcare, and just general human connection, the number of those moments that matter to people that we were able to help facilitate just became so important. We really focused on how can we be there and make it frictionless so folks can focus on that human connection. And that accelerated our thinking in terms of innovation and reinforced the thought that we need to focus on the simplicity, accessibility, and trust in communication technology so that people could focus on that connection and not the technology that makes it possible.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;That's so true. It did really just become an absolute lifeline for people, didn't it? And before we dive into the technologies beyond these emerging capabilities, I wonder if we could first talk about just the importance of clear audio. I mean, Sam, as much as we all worry over how we look on Zoom, is how we sound perhaps as or even more impactful?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Yeah, you're absolutely correct. I mean, clear audio is absolutely critical for effective communications. Video quality is very important absolutely, but poor audio can really hinder understanding and engagement. As a matter of fact, there's studies and research from areas such as Yale University that say that poor audio can make understanding somewhat more challenged and even affect retention of information. Especially in an educational type environment where there's a lot of background noise and very differing types of spaces like auditoriums and lecture halls, it really becomes a high priority that you have great audio quality. And during the pandemic, as you said, and as Brendan rightly said, it became one of our highest priorities to focus on technologies like beamforming mics and ways to focus on the speaker's voice and minimize that unwanted background noise so that we could ensure that the communication was efficient, was well understood, and that it removed the distraction so people could be able to actually communicate and retain the information that was being shared.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;It is incredible just how impactful audio can be, can't it? Brendan, I mean as you said, remote and hybrid collaboration is part of Zoom's DNA. What observations can you share about how users have grown along with the technological advancements and maybe how their expectations have grown as well?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. I mean, users now expect seamless and intelligent experiences. Audio and video just working is a baseline for collaboration. That expectation has shifted from connecting people to enhancing productivity and creativity across the entire ecosystem. When we look at it, we're really looking at these trends in terms of how people want to be better when they're at home. For example, AI-powered tools like Smart Summaries, translation and noise suppression to help people stay productive and connected no matter where they're working. But then this also comes into play at the office. We're starting to see folks that dive into our technology like Intelligent Director and Smart Name Tags that create that meeting equity even when they're in a conference room.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So, the remote experience and the room experience all are similar and create that same ability to be seen, heard, and contribute. And we're now diving further into this that it's beyond just meetings. Zoom is really transforming into an AI-first work platform that's focused on human connection. And so that goes beyond the meetings into things like Chat, Zoom Docs, Zoom Events and Webinars, the Zoom Contact Center and more. And all of this being brought together using our AI Companion at its core to help connect all of those different points of connection for individuals.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;I mean, so Brendan, we know it wasn't only workplaces that were affected by the pandemic, it was also the education sector that had to undergo a huge change. I wondered if you could talk a little bit about how Zoom has operated in that higher education sphere as well.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. Education has always been a focus for Zoom and an area that we've believed in. Because education and learning is something as a company we value and so we have invested in that sector. And personally being the son of academics, it is always an area that I find fascinating. We continue to invest in terms of how do we make the classroom a stronger space? And especially now that the classroom has changed, where it can be in person, it can be virtual, it can be a mix. And using Zoom and its tools, we're able to help bridge all those different scenarios to make learning accessible to students no matter their means.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's what truly excites us, is being able to have that technology that allows people to pursue their desires, their interests, and really up-level their pursuits and inspire more. We're constantly investing in how to allow those messages to get out and to integrate in the flow of communication and collaboration that higher education uses, whether that's being integrated into the classroom, into learning management systems, to make that a seamless flow so that students and their educators can just collaborate seamlessly. And also that we can support all the infrastructure and administration that helps make that possible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. Such an important thing. And Sam, Shure as well, could you talk to us a bit about how you worked in that kind of education space as well from an audio point of view?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Absolutely. Actually, this is a topic that's near and dear to my heart because I'm actually an adjunct professor in my free time.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Oh, wow. Very impressive.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;And the challenges of trying to do this sort of a hybrid lecture, if you will. And Shure has been particularly well suited for this environment and we've been focused on it and investing in technologies there for decades. If you think about how a lecture hall is structured, it's a little different than just having a meeting around the conference table. And Shure has focused on creating products that allow this combination of a presenter scenario along with a meeting space plus the far end where users or students are remote, they can hear intelligibly what's happening in the lecture hall, but they can also participate.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Between our products like the Ceiling Mic Arrays and our wireless microphones that are purpose built for presenters and educators like our MXW neXt product line, we've created technologies that allow those two previously separate worlds to integrate together. And then add that onto integrating with Zoom and other products that allow for that collaboration has been very instrumental. And again, being a user and providing those lectures, I can see a night and day difference and just how much more effective my lectures are today from where they were five to six years ago. And that's all just made possible by all the technologies that are purpose built for these scenarios and integrating more with these powerful tools that just make the job so much more seamless.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely fascinating that you got to put the technology to use yourself as well to check that it was all working well. And you mentioned AI there, of course. I mean, Sam, what AI technologies have had the most significant impact on recent audio advancements too?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Yeah. Absolutely. If you think about the fundamental need here, it's the ability to amplify the audio and the information that's really needed and diminish the unwanted sounds and audio so that we can enhance that experience and make it seamless for people to communicate. With our innovations at Shure, we've leveraged the cutting-edge technologies to both enhance communication effectiveness and to align seamlessly with evolving features in unified communications like the ones that Brandon just mentioned in the Zoom platforms.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We partner with industry leaders like Zoom to ensure that we're providing the ability to be able to focus on that needed audio and eliminate all the background distractions. AI has transformed that audio technology with things like machine learning algorithms that enable us to do more real-time audio processing and significantly enhancing things like noise reduction and speech isolation. Just to give you a simple example, our IntelliMix Room audio processing software that we've released as well as part of a complete room solution uses AI to optimize sound in different environments.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And really that's one of the fundamental changes in this period, whether that's pandemic or post-pandemic, is that the key is really flexibility and being able to adapt to changing work environments. Even if you're not working from home and coming into the office, the types of spaces and environments you try to collaborate in today are constantly changing because our needs are constantly changing. And so having software and algorithms that adapt seamlessly and are able to self-optimize based on the acoustics of the room, based on the different layouts of the spaces where people collaborate in is instrumental.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And then last but not least, AI has transformed the way audio and video integrate. For example, we utilize voice recognition systems that integrate with intelligent cameras so that we enable voice tracking technology so that cameras can not only identify who's speaking, but you have the ability to hear and see people clearly. And that in general just enhances the overall communication experience.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Wow. It's just so much innovation in quite a short space of time really. I mean, Brendan, you mentioned AI a little bit there beforehand, but I wonder what other AI technologies have had the biggest impact as Zoom builds out its own emerging capabilities?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. And I couldn't agree more with Sam that, I mean, AI has made such a big shift and it's really across the spectrum. And when I think about it, there's almost three tiers when you look at the stack. You start off at the raw audio where AI is doing those things like noise suppression, echo cancellation, voice enhancements. All of that just makes this amazing audio signal that can then go into the next layer, which is the speech AI and natural language processing. Which starts to open up those items such as the real-time transcription, translation, searchable content to make the communication not just what's heard, but making it more accessible to more individuals and inclusive by providing that content in a format that is best for them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And then you take those two layers and put the generative and agentic AI on top of that, that can start surfacing insights, summarize the conversation, and even take actions on someone's behalf. It really starts to change the way that people work and how they have access and allows them to connect. I think it is a huge shift and I'm very excited by how those three levels start to interact to really enable people to do more and to connect thanks to AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Yeah. Absolutely. So much rich information that can come out from a single call now because of those sorts of tools. And following on from that, Brendan, I mean, you mentioned before the Zoom AI Companion. I wondered if you could talk a bit about what were your top priorities when building that product to ensure it was truly useful for your customers?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. When we developed AI Companion, we had two priority focus areas from day one, trust and security, and then accuracy and relevance. On the trust side, it was a non-negotiable that customer data wouldn't be used to train our models. People need to know that their conversations and content are private and secure.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Of course.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;And then with accuracy, we needed to ensure AI outputs weren't generic but grounded in the actual context of a meeting, a chat or a product. But the real story here when I think about AI Companion is the customer value that it delivers. AI Companion helps people save time with meeting recaps, task generation, and proactive prep for the next session. It reduces that friction in hybrid work, whether you're in a meeting room, a Zoom room, or collaborating across different collaboration tools like Microsoft or Google. And it enables more equitable participation by surfacing the right context for everyone no matter where and how they're working.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;All this leads to a result where it's practical, trustworthy, and embedded where work happens. And it's just not another tool to manage, it's there in someone's flow of work to help them along the way.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Yeah. That trust piece is just so important, isn't it, today? And Sam, as much as AI has impacted audio innovation, audio has also had an impact on AI capabilities. I wondered if you could talk a little bit about audio as a data input and the advancements technologies like large language models, LLMs, are enabling.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Absolutely. Audio is really a rich data source that's added a new dimension to AI capabilities. If you think about speech recognition or natural language processing, they've had significant advances due to audio data that's provided for them. And to Brendan's point about trust and accuracy, I like to think of the products that Shure enables customers with as essentially the eyes and ears in the room for leading AI companions just like the Zoom AI Companion. You really need that pristine audio input to be able to trust the accuracy of what the AI generates. These AI Companions have been very instrumental in the way we do business every day. I mean, between transcription, speaker attributions, the ability to add action items within a meeting and be able to track what's happening in our interactions, all of that really has to rely on that accurate and pristine input from audio into the AI. I feel that further improves the trust that our end users have to the results of AI and be able to leverage it more.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If you think about it, if you look at how AI audio inputs enhance that interactive AI system, it enables more natural and intuitive interactions with AI. And it really allows for that seamless integration and the ability for users to use it without having to worry about, is the room set up correctly? Is the audio level proper? And when we talk even about agentic AI, we're working on future developments where systems can self-heal or detect that there are issues in the environment so that they can autocorrect and adapt in all these different environments and further enable the AI to be able to do a much more effective job, if you will.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Sam, you touched on future developments there. I wonder if we could close our conversation today with a bit of a future forward look, if we could. Brendan, can you share innovations that Zoom is working on now and what are you most excited to see come to fruition?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Well, your timing for this question is absolutely perfect because we've just wrapped up Zoomtopia 2025.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Oh, wow.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;And this is where we discussed a lot of the new AI innovations that we have coming to Zoom. Starting off, there's AI Companion 3.0. And we've launched this next generation of agentic AI capabilities in Zoom Workplace. And with 3.0 when it releases, it isn't just about transcribing, it's turned into really a platform that helps you with follow-up task, prep for your next conversation, and even proactively suggest how to free up your time. For example, AI Companion can help you schedule meetings intelligently across time zones, suggest which meetings you can skip, and still stay informed and even prepare you with context and insights before you walk into the conversation. It's about helping people focus on strategy and creativity instead of administrative busy work. And for hybrid work specifically, we introduced Zoomie Group Assistant, which will be a big leap for hybrid collaboration.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Acting as an assistant for a group chat and meetings, you can simply ask, “@Zoomie, what's the latest update on the project?” Or “@Zoomie, what are the team's action items?” And then get instant answers. Or because we're talking about audio here, you can go into a conference room and say, "Hey, Zoomie," and get help with things like checking into a room, adjusting lights, temperature, or even sharing your screen. And while all these are built-in features, we're also expanding the platform to allow custom AI agents through our AI Studio, so organizations can bring their own agents or integrate with third-party ones.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt;&lt;p&gt;Zoom has always believed in an open platform and philosophy and that is continuing. Folks using AI Companion 3.0 will be able to use agents across platforms to work with the workflows that they have across all the different SaaS vendors that they might have in their environment, whether that's Google, Microsoft, ServiceNow, Cisco, and so many other tools.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. It certainly sounds like a tool I could use in my work, so I look forward to hearing more about that. And Sam, we've touched on there are so many exciting things happening in audio too. What are you working on at Shure? And what are you most excited to see come to fruition?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;At Shure, our engineering teams are really working on a range of exciting projects, but particularly we're working on developing new collaboration solutions that are integral for IT end users. And these integrate obviously with the leading UC platforms.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We're integrating audio and video technologies that are scalable, reliable solutions. And we want to be able to seamlessly connect these to cloud services so that we can leverage both AI technologies and the tool sets available to optimize every type of workspace essentially. Not just meeting rooms, but lecture halls, work from home scenarios, et cetera.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The other area that we really focus on in terms of our reliability and quality really comes from our DNA in the pro audio world. And that's really all-around wireless audio technologies. We're developing our next-generation wireless systems and these are going to offer even greater reliability and range. And they really become ideal for everything from a large-scale event to personal home use and the gamut across that whole spectrum. And I think all of that in partnership with our partners like Zoom will help just facilitate the modern workspace.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. So much exciting innovation clearly going on behind the scenes. Thank you both so much.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That was Sam Sabet, chief technology officer at Shure, and Brendan Ittelson, chief ecosystem officer at Zoom, whom I spoke with from Brighton in England.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's it for this episode of Business Lab. I'm your host, Megan Tatum. I'm a contributing editor at Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology and you can find us in print on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This show is available wherever you get your podcasts. And if you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review and this episode was produced by Giro Studios. Thanks for listening.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/16/1125881/tuning-into-the-future-of-collaboration/</guid><pubDate>Mon, 16 Feb 2026 15:00:00 +0000</pubDate></item><item><title>[NEW] New Data Shows NVIDIA Blackwell Ultra Delivers up to 50x Better Performance and 35x Lower Costs for Agentic AI (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/data-blackwell-ultra-performance-lower-cost-agentic-ai/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The NVIDIA Blackwell platform has been widely adopted by leading inference providers such as Baseten, DeepInfra, Fireworks AI and Together AI to reduce cost per token by up to 10x. Now, the NVIDIA Blackwell Ultra platform is taking this momentum further for agentic AI.&lt;/p&gt;
&lt;p&gt;AI agents and coding assistants are driving explosive growth in software-programming-related AI queries: from 11% to about 50% last year, according to OpenRouter’s State of Inference report. These applications require low latency to maintain real-time responsiveness across multistep workflows and long context when reasoning across entire codebases.&lt;/p&gt;
&lt;p&gt;New SemiAnalysis InferenceX performance data shows that the combination of NVIDIA’s software optimizations and the next-generation NVIDIA Blackwell Ultra platform has delivered breakthrough advances on both fronts. NVIDIA GB300 NVL72 systems now deliver up to 50x higher throughput per megawatt, resulting in 35x lower cost per token compared with the NVIDIA Hopper platform.&lt;/p&gt;
&lt;p&gt;By innovating across chips, system architecture and software, NVIDIA’s extreme codesign accelerates performance across AI workloads — from agentic coding to interactive coding assistants — while driving down costs at scale.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;GB300 NVL72 Delivers up to 50x Better Performance for Low-Latency Workloads&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Recent analysis from Signal65 shows that NVIDIA GB200 NVL72 with extreme hardware and software codesign delivers more than 10x more tokens per watt, resulting in one-tenth the cost per token compared with the NVIDIA Hopper platform. These massive performance gains continue to expand as the underlying stack improves.&lt;/p&gt;
&lt;p&gt;Continuous optimizations from the NVIDIA TensorRT-LLM, NVIDIA Dynamo, Mooncake and SGLang teams continue to significantly boost Blackwell NVL72 throughput for mixture-of-experts (MoE) inference across all latency targets. For instance, NVIDIA TensorRT-LLM library improvements have delivered up to 5x better performance on GB200 for low-latency workloads compared with just four months ago.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Higher-performance GPU kernels&lt;/b&gt; optimized for efficiency and low latency help make the most of Blackwell’s immense compute capabilities and boost throughput.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NVIDIA NVLink Symmetric Memory&lt;/b&gt; enables direct GPU-to-GPU memory access for more efficient communication.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Programmatic dependent launch&lt;/b&gt; minimizes idle time by launching the next kernel’s setup phase before the previous one completes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Building on these software advances, GB300 NVL72 — which features the Blackwell Ultra GPU — pushes the throughput-per-megawatt frontier to 50x compared with the Hopper platform.&lt;/p&gt;
&lt;p&gt;This performance gain translates into superior economics, with NVIDIA GB300 lowering costs compared with the Hopper platform across the entire latency spectrum. The most dramatic reduction occurs at low latency, where agentic applications operate: up to 35x lower cost per million tokens compared with the Hopper platform.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_89954"&gt;&lt;img alt="alt" class="wp-image-89954 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/gb300-nvl72-delivers-35x-reduction-in-token-cost-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89954"&gt;NVIDIA GB300 NVL72 and the codesigned software stack including NVIDIA Dynamo and TensorRT-LLM deliver 35x lower cost per token compared with NVIDIA Hopper platform.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;For agentic coding and interactive assistants workloads where every millisecond compounds across multistep workflows, this combination of relentless software optimization and next-generation hardware enables AI platforms to scale real-time interactive experiences to significantly more users.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;GB300 NVL72 Delivers Superior Economics for Long-Context Workloads&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;While both GB200 NVL72 and GB300 NVL72 efficiently deliver ultralow latency, the distinct advantages of GB300 NVL72 become most apparent in long-context scenarios. For workloads with 128,000-token inputs and 8,000-token outputs — such as AI coding assistants reasoning across codebases — GB300 NVL72 delivers up to 1.5x lower cost per token compared with GB200 NVL72.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_89948"&gt;&lt;img alt="alt" class="wp-image-89948 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/gb300-nvl72-delivers-large-leap-for-long-context-ai-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89948"&gt;NVIDIA GB300 NVL72 is ideal for low-latency, long-context workloads.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Context grows as the agent reads in more of the code. This allows it to better understand the code base but also requires much more compete. Blackwell Ultra has 1.5x higher NVFP4 compute performance and 2x faster attention processing, enabling the agent to efficiently understand entire code bases.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Infrastructure for Agentic AI&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Leading cloud providers and AI innovators have already deployed NVIDIA GB200 NVL72 at scale, and are also deploying GB300 NVL72 in production. Microsoft, CoreWeave and OCI are deploying GB300 NVL72 for low-latency and long-context use cases such as agentic coding and coding assistants. By reducing token costs, GB300 NVL72 enables a new class of applications that can reason across massive codebases in real time.&lt;/p&gt;
&lt;p&gt;“As inference moves to the center of AI production, long-context performance and token efficiency become critical,” said Chen Goldberg, senior vice president of engineering at CoreWeave. “Grace Blackwell NVL72 addresses that challenge directly, and CoreWeave’s AI cloud, including CKS and SUNK, is designed to translate GB300 systems’ gains, building on the success of GB200, into predictable performance and cost efficiency. The result is better token economics and more usable inference for customers running workloads at scale.”&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;NVIDIA Vera Rubin NVL72 to Bring Next-Generation Performance&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;With NVIDIA Blackwell systems deployed at scale, continuous software optimizations will keep unlocking additional performance and cost improvements across the installed base.&lt;/p&gt;
&lt;p&gt;Looking ahead, the NVIDIA Rubin platform — which combines six new chips to create one AI supercomputer — is set to deliver another round of massive performance leaps. For MoE inference, it delivers up to 10x higher throughput per megawatt compared with Blackwell, translating into one-tenth the cost per million tokens. And for the next wave of frontier AI models, Rubin can train large MoE models using just one-fourth the number of GPUs compared with Blackwell.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about the NVIDIA Rubin platform and the &lt;/i&gt;&lt;i&gt;Vera Rubin NVL72 system&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The NVIDIA Blackwell platform has been widely adopted by leading inference providers such as Baseten, DeepInfra, Fireworks AI and Together AI to reduce cost per token by up to 10x. Now, the NVIDIA Blackwell Ultra platform is taking this momentum further for agentic AI.&lt;/p&gt;
&lt;p&gt;AI agents and coding assistants are driving explosive growth in software-programming-related AI queries: from 11% to about 50% last year, according to OpenRouter’s State of Inference report. These applications require low latency to maintain real-time responsiveness across multistep workflows and long context when reasoning across entire codebases.&lt;/p&gt;
&lt;p&gt;New SemiAnalysis InferenceX performance data shows that the combination of NVIDIA’s software optimizations and the next-generation NVIDIA Blackwell Ultra platform has delivered breakthrough advances on both fronts. NVIDIA GB300 NVL72 systems now deliver up to 50x higher throughput per megawatt, resulting in 35x lower cost per token compared with the NVIDIA Hopper platform.&lt;/p&gt;
&lt;p&gt;By innovating across chips, system architecture and software, NVIDIA’s extreme codesign accelerates performance across AI workloads — from agentic coding to interactive coding assistants — while driving down costs at scale.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;GB300 NVL72 Delivers up to 50x Better Performance for Low-Latency Workloads&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Recent analysis from Signal65 shows that NVIDIA GB200 NVL72 with extreme hardware and software codesign delivers more than 10x more tokens per watt, resulting in one-tenth the cost per token compared with the NVIDIA Hopper platform. These massive performance gains continue to expand as the underlying stack improves.&lt;/p&gt;
&lt;p&gt;Continuous optimizations from the NVIDIA TensorRT-LLM, NVIDIA Dynamo, Mooncake and SGLang teams continue to significantly boost Blackwell NVL72 throughput for mixture-of-experts (MoE) inference across all latency targets. For instance, NVIDIA TensorRT-LLM library improvements have delivered up to 5x better performance on GB200 for low-latency workloads compared with just four months ago.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Higher-performance GPU kernels&lt;/b&gt; optimized for efficiency and low latency help make the most of Blackwell’s immense compute capabilities and boost throughput.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NVIDIA NVLink Symmetric Memory&lt;/b&gt; enables direct GPU-to-GPU memory access for more efficient communication.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Programmatic dependent launch&lt;/b&gt; minimizes idle time by launching the next kernel’s setup phase before the previous one completes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Building on these software advances, GB300 NVL72 — which features the Blackwell Ultra GPU — pushes the throughput-per-megawatt frontier to 50x compared with the Hopper platform.&lt;/p&gt;
&lt;p&gt;This performance gain translates into superior economics, with NVIDIA GB300 lowering costs compared with the Hopper platform across the entire latency spectrum. The most dramatic reduction occurs at low latency, where agentic applications operate: up to 35x lower cost per million tokens compared with the Hopper platform.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_89954"&gt;&lt;img alt="alt" class="wp-image-89954 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/gb300-nvl72-delivers-35x-reduction-in-token-cost-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89954"&gt;NVIDIA GB300 NVL72 and the codesigned software stack including NVIDIA Dynamo and TensorRT-LLM deliver 35x lower cost per token compared with NVIDIA Hopper platform.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;For agentic coding and interactive assistants workloads where every millisecond compounds across multistep workflows, this combination of relentless software optimization and next-generation hardware enables AI platforms to scale real-time interactive experiences to significantly more users.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;GB300 NVL72 Delivers Superior Economics for Long-Context Workloads&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;While both GB200 NVL72 and GB300 NVL72 efficiently deliver ultralow latency, the distinct advantages of GB300 NVL72 become most apparent in long-context scenarios. For workloads with 128,000-token inputs and 8,000-token outputs — such as AI coding assistants reasoning across codebases — GB300 NVL72 delivers up to 1.5x lower cost per token compared with GB200 NVL72.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_89948"&gt;&lt;img alt="alt" class="wp-image-89948 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/gb300-nvl72-delivers-large-leap-for-long-context-ai-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89948"&gt;NVIDIA GB300 NVL72 is ideal for low-latency, long-context workloads.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Context grows as the agent reads in more of the code. This allows it to better understand the code base but also requires much more compete. Blackwell Ultra has 1.5x higher NVFP4 compute performance and 2x faster attention processing, enabling the agent to efficiently understand entire code bases.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Infrastructure for Agentic AI&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Leading cloud providers and AI innovators have already deployed NVIDIA GB200 NVL72 at scale, and are also deploying GB300 NVL72 in production. Microsoft, CoreWeave and OCI are deploying GB300 NVL72 for low-latency and long-context use cases such as agentic coding and coding assistants. By reducing token costs, GB300 NVL72 enables a new class of applications that can reason across massive codebases in real time.&lt;/p&gt;
&lt;p&gt;“As inference moves to the center of AI production, long-context performance and token efficiency become critical,” said Chen Goldberg, senior vice president of engineering at CoreWeave. “Grace Blackwell NVL72 addresses that challenge directly, and CoreWeave’s AI cloud, including CKS and SUNK, is designed to translate GB300 systems’ gains, building on the success of GB200, into predictable performance and cost efficiency. The result is better token economics and more usable inference for customers running workloads at scale.”&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;NVIDIA Vera Rubin NVL72 to Bring Next-Generation Performance&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;With NVIDIA Blackwell systems deployed at scale, continuous software optimizations will keep unlocking additional performance and cost improvements across the installed base.&lt;/p&gt;
&lt;p&gt;Looking ahead, the NVIDIA Rubin platform — which combines six new chips to create one AI supercomputer — is set to deliver another round of massive performance leaps. For MoE inference, it delivers up to 10x higher throughput per megawatt compared with Blackwell, translating into one-tenth the cost per million tokens. And for the next wave of frontier AI models, Rubin can train large MoE models using just one-fourth the number of GPUs compared with Blackwell.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about the NVIDIA Rubin platform and the &lt;/i&gt;&lt;i&gt;Vera Rubin NVL72 system&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/data-blackwell-ultra-performance-lower-cost-agentic-ai/</guid><pubDate>Mon, 16 Feb 2026 17:00:40 +0000</pubDate></item><item><title>[NEW] ByteDance backpedals after Seedance 2.0 turned Hollywood icons into AI “clip art” (AI - Ars Technica)</title><link>https://arstechnica.com/tech-policy/2026/02/bytedance-backpedals-after-seedance-2-0-turned-hollywood-icons-into-ai-clip-art/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Hollywood backlash puts spotlight on ByteDance’s sketchy launch of Seedance 2.0.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2260459499-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2260459499-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Cheng Xin / Contributor | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;ByteDance says that it’s rushing to add safeguards to block Seedance 2.0 from generating iconic characters and deepfaking celebrities, after substantial Hollywood backlash after launching the latest version of its AI video tool.&lt;/p&gt;
&lt;p&gt;The changes come after Disney and Paramount Skydance sent cease-and-desist letters to ByteDance urging the Chinese company to promptly end the allegedly vast and blatant infringement.&lt;/p&gt;
&lt;p&gt;Studios claimed the infringement was widescale and immediate, with Seedance 2.0 users across social media sharing AI videos featuring copyrighted characters like Spider-Man, Darth Vader, and SpongeBob Square Pants. In its letter, Disney fumed that Seedance was “hijacking” its characters, accusing ByteDance of treating Disney characters like they were “free public domain clip art,” Axios reported.&lt;/p&gt;
&lt;p&gt;“ByteDance’s virtual smash-and-grab of Disney’s IP is willful, pervasive, and totally unacceptable,” Disney’s letter said.&lt;/p&gt;
&lt;p&gt;Defending intellectual property from franchises like &lt;em&gt;Star Trek&lt;/em&gt; and &lt;em&gt;The Godfather&lt;/em&gt;, Paramount Skydance pointed out that Seedance’s outputs are “often indistinguishable, both visually and audibly” from the original characters, Variety reported. Similarly frustrated, Japan’s AI minister Kimi Onoda, sought to protect popular anime and manga characters, officially launching a probe last week into ByteDance over the copyright violations, the South China Morning Post reported.&lt;/p&gt;
&lt;p&gt;“We cannot overlook a situation in which content is being used without the copyright holder’s permission,” Onoda said at a press conference Friday.&lt;/p&gt;
&lt;p&gt;Facing legal threats and Japan’s investigation, ByteDance issued a statement Monday, CNBC reported. In it, the company claimed that it “respects intellectual property rights” and has “heard the concerns regarding Seedance 2.0.”&lt;/p&gt;
&lt;p&gt;“We are taking steps to strengthen current safeguards as we work to prevent the unauthorized use of intellectual property and likeness by users,” ByteDance said.&lt;/p&gt;
&lt;p&gt;However, Disney seems unlikely to accept that ByteDance inadvertently released its tool without implementing such safeguards in advance. In its letter, Disney alleged that “Seedance has infringed on Disney’s copyrighted materials to benefit its commercial service without permission.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;After all, what better way to illustrate Seedance 2.0’s latest features than by generating some of the best-known IP in the world? At least one tech consultant has suggested that ByteDance planned to benefit from inciting Hollywood outrage. The founder of San Francisco-based consultancy Tech Buzz China, Rui Ma, told SCMP that “the controversy surrounding Seedance is likely part of ByteDance’s initial distribution strategy to showcase its underlying technical capabilities.”&lt;/p&gt;
&lt;h2&gt;Seedance 2.0 is an “attack” on creators&lt;/h2&gt;
&lt;p&gt;Studios aren’t the only ones sounding alarms.&lt;/p&gt;
&lt;p&gt;Several industry groups expressed concerns, including the Motion Picture Association, which accused ByteDance of engaging in massive copyright infringement within “a single day,” CNBC reported.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Sean Astin, an actor and president of the actors union, SAG-AFTRA, was directly impacted by the scandal. A video that has since been removed from X showed Astin in the role of Samwise Gamgee from &lt;em&gt;The Lord of the Rings&lt;/em&gt;, delivering a line he never said, Variety reported. Condemning Seedance’s infringement, SAG-AFTRA issued a statement emphasizing that ByteDance did not act responsibly in releasing the model without safeguards:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;“SAG-AFTRA stands with the studios in condemning the blatant infringement enabled by ByteDance’s new AI video model Seedance 2.0. The infringement includes the unauthorized use of our members’ voices and likenesses. This is unacceptable and undercuts the ability of human talent to earn a livelihood. Seedance 2.0 disregards law, ethics, industry standards and basic principles of consent. Responsible AI development demands responsibility, and that is nonexistent here.”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Echoing that, a group representing Hollywood creators, the Human Artistry Campaign, declared that “the launch of Seedance 2.0” was “an attack on every creator around the world.”&lt;/p&gt;
&lt;p&gt;“Stealing human creators’ work in an attempt to replace them with AI generated slop is destructive to our culture: stealing isn’t innovation,” the group said. “These unauthorized deepfakes and voice clones of actors violate the most basic aspects of personal autonomy and should be deeply concerning to everyone. Authorities should use every legal tool at their disposal to stop this wholesale theft.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Ars could not immediately reach any of these groups to comment on whether ByteDance’s post-launch efforts to add safeguards addressed industry concerns.&lt;/p&gt;
&lt;p&gt;MPA chairman and CEO Charles Rivkin has previously accused ByteDance of disregarding “well-established copyright law that protects the rights of creators and underpins millions of American jobs.”&lt;/p&gt;
&lt;p&gt;While Disney and other studios are clearly ready to take down any tools that could hurt their revenue or reputation without an agreement in place, they aren’t opposed to all AI uses of their characters. In December, Disney struck a deal with OpenAI, giving Sora access to 200 characters for three years, while investing $1 billion in the technology.&lt;/p&gt;
&lt;p&gt;At that time, Disney CEO Robert A. Iger, said that “the rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI, we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works.”&lt;/p&gt;
&lt;h2&gt;Creators disagree Seedance 2.0 is a game changer&lt;/h2&gt;
&lt;p&gt;In a blog announcing Seedance 2.0, ByteDance boasted that the new model “delivers a substantial leap in generation quality,” particularly in close-up shots and action sequences.&lt;/p&gt;
&lt;p&gt;The company acknowledged that further refinements were needed and the model is “still far from perfect” but hyped that “its generated videos possess a distinct cinematic aesthetic; the textures of objects, lighting, and composition, as well as costume, makeup, and prop designs, all show high degrees of finish.”&lt;/p&gt;
&lt;p&gt;ByteDance likely hoped that the earliest outputs from Seedance 2.0 would produce headlines wowed by the model’s capabilities, and it got what it wanted when a single Hollywood stakeholder’s social media comment went viral.&lt;/p&gt;
&lt;p&gt;Shortly after Seedance 2.0’s rollout, &lt;em&gt;Deadpool&lt;/em&gt; co-writer, Rhett Reese, declared on X that “it’s likely over for us,” The Guardian reported. The screenwriter was impressed by an AI video created by Irish director Ruairi Robinson, which realistically depicted Tom Cruise fighting Brad Pitt. “[I]n next to no time, one person is going to be able to sit at a computer and create a movie indistinguishable from what Hollywood now releases,” Reese opined. “True, if that person is no good, it will suck. But if that person possesses Christopher Nolan’s talent and taste (and someone like that will rapidly come along), it will be tremendous.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;However, some AI critics rejected the notion that Seedance 2.0 is capable of replacing artists in the way that Reese warned. On Bluesky and X, they pushed back on ByteDance claims that this model doomed Hollywood, with some accusing outlets of too quickly ascribing Reese’s reaction to the whole industry.&lt;/p&gt;
&lt;p&gt;Among them was longtime AI critic, Reid Southen, a film concept artist who works on major motion pictures and TV. Responding directly to Reese’s X thread, Southen contradicted the notion that a great filmmaker could be born from fiddling with AI prompts alone.&lt;/p&gt;
&lt;p&gt;“Nolan is capable of doing great work because he’s put in the work,” Southen said. “AI is an automation tool, it’s literally removing key, fundamental work from the process, how does one become good at anything if they insist on using nothing but shortcuts?”&lt;/p&gt;
&lt;p&gt;Perhaps the strongest evidence in Southen’s favor is Darren Aronofsky’s recent AI-generated historical docudrama. Speaking anonymously to Ars following backlash declaring that “AI slop is ruining American history,” one source close to production on that project confirmed that it took “weeks” to produce minutes of usable video using a variety of AI tools.&lt;/p&gt;
&lt;p&gt;That source noted that the creative team went into the project expecting they had a lot to learn but also expecting that tools would continue to evolve, as could audience reactions to AI-assisted movies.&lt;/p&gt;
&lt;p&gt;“It’s a huge experiment, really,” the source told Ars.&lt;/p&gt;
&lt;p&gt;Notably, for both creators and rights-holders concerned about copyright infringement and career threats, questions remain on how Seedance 2.0 was trained. ByteDance has yet to release a technical report for Seedance 2.0 and “has never disclosed the data sets it uses to train its powerful video-generation Seedance models and image-generation Seedream models,” SCMP reported.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Hollywood backlash puts spotlight on ByteDance’s sketchy launch of Seedance 2.0.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2260459499-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2260459499-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Cheng Xin / Contributor | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;ByteDance says that it’s rushing to add safeguards to block Seedance 2.0 from generating iconic characters and deepfaking celebrities, after substantial Hollywood backlash after launching the latest version of its AI video tool.&lt;/p&gt;
&lt;p&gt;The changes come after Disney and Paramount Skydance sent cease-and-desist letters to ByteDance urging the Chinese company to promptly end the allegedly vast and blatant infringement.&lt;/p&gt;
&lt;p&gt;Studios claimed the infringement was widescale and immediate, with Seedance 2.0 users across social media sharing AI videos featuring copyrighted characters like Spider-Man, Darth Vader, and SpongeBob Square Pants. In its letter, Disney fumed that Seedance was “hijacking” its characters, accusing ByteDance of treating Disney characters like they were “free public domain clip art,” Axios reported.&lt;/p&gt;
&lt;p&gt;“ByteDance’s virtual smash-and-grab of Disney’s IP is willful, pervasive, and totally unacceptable,” Disney’s letter said.&lt;/p&gt;
&lt;p&gt;Defending intellectual property from franchises like &lt;em&gt;Star Trek&lt;/em&gt; and &lt;em&gt;The Godfather&lt;/em&gt;, Paramount Skydance pointed out that Seedance’s outputs are “often indistinguishable, both visually and audibly” from the original characters, Variety reported. Similarly frustrated, Japan’s AI minister Kimi Onoda, sought to protect popular anime and manga characters, officially launching a probe last week into ByteDance over the copyright violations, the South China Morning Post reported.&lt;/p&gt;
&lt;p&gt;“We cannot overlook a situation in which content is being used without the copyright holder’s permission,” Onoda said at a press conference Friday.&lt;/p&gt;
&lt;p&gt;Facing legal threats and Japan’s investigation, ByteDance issued a statement Monday, CNBC reported. In it, the company claimed that it “respects intellectual property rights” and has “heard the concerns regarding Seedance 2.0.”&lt;/p&gt;
&lt;p&gt;“We are taking steps to strengthen current safeguards as we work to prevent the unauthorized use of intellectual property and likeness by users,” ByteDance said.&lt;/p&gt;
&lt;p&gt;However, Disney seems unlikely to accept that ByteDance inadvertently released its tool without implementing such safeguards in advance. In its letter, Disney alleged that “Seedance has infringed on Disney’s copyrighted materials to benefit its commercial service without permission.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;After all, what better way to illustrate Seedance 2.0’s latest features than by generating some of the best-known IP in the world? At least one tech consultant has suggested that ByteDance planned to benefit from inciting Hollywood outrage. The founder of San Francisco-based consultancy Tech Buzz China, Rui Ma, told SCMP that “the controversy surrounding Seedance is likely part of ByteDance’s initial distribution strategy to showcase its underlying technical capabilities.”&lt;/p&gt;
&lt;h2&gt;Seedance 2.0 is an “attack” on creators&lt;/h2&gt;
&lt;p&gt;Studios aren’t the only ones sounding alarms.&lt;/p&gt;
&lt;p&gt;Several industry groups expressed concerns, including the Motion Picture Association, which accused ByteDance of engaging in massive copyright infringement within “a single day,” CNBC reported.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Sean Astin, an actor and president of the actors union, SAG-AFTRA, was directly impacted by the scandal. A video that has since been removed from X showed Astin in the role of Samwise Gamgee from &lt;em&gt;The Lord of the Rings&lt;/em&gt;, delivering a line he never said, Variety reported. Condemning Seedance’s infringement, SAG-AFTRA issued a statement emphasizing that ByteDance did not act responsibly in releasing the model without safeguards:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;“SAG-AFTRA stands with the studios in condemning the blatant infringement enabled by ByteDance’s new AI video model Seedance 2.0. The infringement includes the unauthorized use of our members’ voices and likenesses. This is unacceptable and undercuts the ability of human talent to earn a livelihood. Seedance 2.0 disregards law, ethics, industry standards and basic principles of consent. Responsible AI development demands responsibility, and that is nonexistent here.”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Echoing that, a group representing Hollywood creators, the Human Artistry Campaign, declared that “the launch of Seedance 2.0” was “an attack on every creator around the world.”&lt;/p&gt;
&lt;p&gt;“Stealing human creators’ work in an attempt to replace them with AI generated slop is destructive to our culture: stealing isn’t innovation,” the group said. “These unauthorized deepfakes and voice clones of actors violate the most basic aspects of personal autonomy and should be deeply concerning to everyone. Authorities should use every legal tool at their disposal to stop this wholesale theft.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Ars could not immediately reach any of these groups to comment on whether ByteDance’s post-launch efforts to add safeguards addressed industry concerns.&lt;/p&gt;
&lt;p&gt;MPA chairman and CEO Charles Rivkin has previously accused ByteDance of disregarding “well-established copyright law that protects the rights of creators and underpins millions of American jobs.”&lt;/p&gt;
&lt;p&gt;While Disney and other studios are clearly ready to take down any tools that could hurt their revenue or reputation without an agreement in place, they aren’t opposed to all AI uses of their characters. In December, Disney struck a deal with OpenAI, giving Sora access to 200 characters for three years, while investing $1 billion in the technology.&lt;/p&gt;
&lt;p&gt;At that time, Disney CEO Robert A. Iger, said that “the rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI, we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works.”&lt;/p&gt;
&lt;h2&gt;Creators disagree Seedance 2.0 is a game changer&lt;/h2&gt;
&lt;p&gt;In a blog announcing Seedance 2.0, ByteDance boasted that the new model “delivers a substantial leap in generation quality,” particularly in close-up shots and action sequences.&lt;/p&gt;
&lt;p&gt;The company acknowledged that further refinements were needed and the model is “still far from perfect” but hyped that “its generated videos possess a distinct cinematic aesthetic; the textures of objects, lighting, and composition, as well as costume, makeup, and prop designs, all show high degrees of finish.”&lt;/p&gt;
&lt;p&gt;ByteDance likely hoped that the earliest outputs from Seedance 2.0 would produce headlines wowed by the model’s capabilities, and it got what it wanted when a single Hollywood stakeholder’s social media comment went viral.&lt;/p&gt;
&lt;p&gt;Shortly after Seedance 2.0’s rollout, &lt;em&gt;Deadpool&lt;/em&gt; co-writer, Rhett Reese, declared on X that “it’s likely over for us,” The Guardian reported. The screenwriter was impressed by an AI video created by Irish director Ruairi Robinson, which realistically depicted Tom Cruise fighting Brad Pitt. “[I]n next to no time, one person is going to be able to sit at a computer and create a movie indistinguishable from what Hollywood now releases,” Reese opined. “True, if that person is no good, it will suck. But if that person possesses Christopher Nolan’s talent and taste (and someone like that will rapidly come along), it will be tremendous.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;However, some AI critics rejected the notion that Seedance 2.0 is capable of replacing artists in the way that Reese warned. On Bluesky and X, they pushed back on ByteDance claims that this model doomed Hollywood, with some accusing outlets of too quickly ascribing Reese’s reaction to the whole industry.&lt;/p&gt;
&lt;p&gt;Among them was longtime AI critic, Reid Southen, a film concept artist who works on major motion pictures and TV. Responding directly to Reese’s X thread, Southen contradicted the notion that a great filmmaker could be born from fiddling with AI prompts alone.&lt;/p&gt;
&lt;p&gt;“Nolan is capable of doing great work because he’s put in the work,” Southen said. “AI is an automation tool, it’s literally removing key, fundamental work from the process, how does one become good at anything if they insist on using nothing but shortcuts?”&lt;/p&gt;
&lt;p&gt;Perhaps the strongest evidence in Southen’s favor is Darren Aronofsky’s recent AI-generated historical docudrama. Speaking anonymously to Ars following backlash declaring that “AI slop is ruining American history,” one source close to production on that project confirmed that it took “weeks” to produce minutes of usable video using a variety of AI tools.&lt;/p&gt;
&lt;p&gt;That source noted that the creative team went into the project expecting they had a lot to learn but also expecting that tools would continue to evolve, as could audience reactions to AI-assisted movies.&lt;/p&gt;
&lt;p&gt;“It’s a huge experiment, really,” the source told Ars.&lt;/p&gt;
&lt;p&gt;Notably, for both creators and rights-holders concerned about copyright infringement and career threats, questions remain on how Seedance 2.0 was trained. ByteDance has yet to release a technical report for Seedance 2.0 and “has never disclosed the data sets it uses to train its powerful video-generation Seedance models and image-generation Seedream models,” SCMP reported.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2026/02/bytedance-backpedals-after-seedance-2-0-turned-hollywood-icons-into-ai-clip-art/</guid><pubDate>Mon, 16 Feb 2026 17:42:37 +0000</pubDate></item></channel></rss>