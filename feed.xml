<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 25 Feb 2026 19:09:52 +0000</lastBuildDate><item><title>The Gradient (The Gradient)</title><link>https://thegradient.pub/rss/</link><description>&amp;lt;![CDATA[The Gradient]]&amp;gt;https://thegradient.pub/https://thegradient.pub/favicon.pngThe Gradienthttps://thegradient.pub/Ghost 5.33Wed, 25 Feb 2026 19:12:31 GMT60&amp;lt;![CDATA[After Orthogonality: Virtue-Ethical Agency and AI Alignment]]&amp;gt;&lt;h2 id="preface"&gt;Preface&lt;/h2&gt;
&lt;p&gt;This essay argues that rational people don’t have goals, and that rational AIs shouldn’t have goals. Human actions are rational not because we direct them at some final ‘goals,’ but because we align actions to &lt;em&gt;practices&lt;/em&gt;&lt;sup class="footnote-ref"&gt;[1]&lt;/sup&gt;: networks of actions, action-dispositions, action-evaluation criteria,&lt;/p&gt;]]&amp;gt;https://thegradient.pub/virtue-ethics-ai-alignment/69879dd077c3d76051ac158fWed, 18 Feb 2026 23:25:52 GMT&lt;h2 id="preface"&gt;Preface&lt;/h2&gt;
&lt;img alt="After Orthogonality: Virtue-Ethical Agency and AI Alignment" src="https://thegradient.pub/content/images/2026/02/ramelli-featured.jpg" /&gt;&lt;p&gt;This essay argues that rational people don’t have goals, and that rational AIs shouldn’t have goals. Human actions are rational not because we direct them at some final ‘goals,’ but because we align actions to &lt;em&gt;practices&lt;/em&gt;&lt;sup class="footnote-ref"&gt;[1]&lt;/sup&gt;: networks of actions, action-dispositions, action-evaluation criteria, and action-resources that structure, clarify, develop, and promote themselves. If we want AIs that can genuinely support, collaborate with, or even &lt;strong&gt;comply&lt;/strong&gt; with human agency, AI agents’ deliberations must share a “type signature” with the practices-based logic we use to reflect and act.&lt;/p&gt;
&lt;p&gt;I argue that these issues matter not just for aligning AI to grand ethical ideals like human flourishing, but also for aligning AI to core safety-properties like transparency, helpfulness, harmlessness, or corrigibility. Concepts like ’harmlessness’ or ‘corrigibility’ are unnatural -- brittle, unstable, arbitrary -- for agents who’d interpret them in terms of goals or rules, but natural for agents who’d interpret them as dynamics in networks of actions, action-dispositions, action-evaluation criteria, and action-resources.&lt;/p&gt;
&lt;p&gt;While the issues this essay tackles tend to sprawl, one theme that reappears over and over is the relevance of the formula ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly.’ I argue that this formula captures something important about both meaningful human life-activity (art is the artistic promotion of art, romance is the romantic promotion of romance) and real human morality (to care about kindness is to promote kindness kindly, to care about honesty is to promote honesty honestly).&lt;/p&gt;
&lt;p&gt;I start by asking: What follows for AI alignment if we take the concept of eudaimonia -- active, rational human flourishing -- seriously? I argue that the concept of eudaimonia doesn’t simply point to a desired state or trajectory of the world that we should set as an AI’s optimization target, but rather points to a structure of deliberation different from standard consequentialist&lt;sup class="footnote-ref"&gt;[2]&lt;/sup&gt; rationality. I then argue that this form of rational activity and valuing, which l call &lt;em&gt;eudaimonic rationality&lt;/em&gt;&lt;sup class="footnote-ref"&gt;[3]&lt;/sup&gt;, is a useful or even necessary framework for the agency and values of human-aligned AIs.&lt;/p&gt;
&lt;p&gt;These arguments are based both on the dangers of a “type mismatch” between human flourishing as an optimization target and consequentialist optimization as a form, and on certain material advantages that eudaimonic rationality plausibly possesses in comparison to deontological and consequentialist agency with regard to stability and safety.&lt;/p&gt;
&lt;p&gt;The concept of eudaimonia, I argue, suggests a form of rational activity without a strict distinction between means and ends, or between ‘instrumental’ and ‘terminal’ values. In this model of rational activity, a rational action is an element of a valued practice in roughly the same sense that a note is an element of a melody, a time-step is an element of a computation, and a moment in an organism’s cellular life is an element of that organism’s self-subsistence and self-development.&lt;sup class="footnote-ref"&gt;[4]&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;My central claim is that our intuitions about the nature of human flourishing are implicitly intuitions that eudaimonic rationality can be functionally robust in a sense highly critical to AI alignment. More specifically, I argue that in light of our best intuitions about the nature of human flourishing it’s plausible that eudaimonic rationality is a &lt;em&gt;natural&lt;/em&gt; form of agency, and that eudaimonic rationality is &lt;em&gt;effective&lt;/em&gt; even by the light of certain consequentialist approximations of its values. I then argue that if our goal is to align AI in support of human flourishing, and if it is furthermore plausible that eudaimonic rationality is natural and efficacious, then many classical AI safety considerations and ‘paradoxes’ of AI alignment speak in favor of trying to instill AIs with eudaimonic rationality.&lt;/p&gt;
&lt;p&gt;Throughout this essay, I will sometimes explicitly and often implicitly be asking whether some form of agency or rationality or practice is &lt;em&gt;natural&lt;/em&gt;. The sense of ‘natural’ I’m calling on is certainly related to the senses used in various virtue-ethical traditions, but the interest I take in it is less immediately normative and more material or technical. While I have no reductive definition at hand, the intended meaning of ‘natural’ is related to stability, coherence, relative non-contingency, ease of learnability, lower algorithmic complexity, convergent cultural evolution, hypothetical convergent cultural evolution across different hypothetical rational-animal species, potential convergent evolution between humans and neural-network based AI, and targetability by ML training processes. While I will also make many direct references to AI alignment, this question of material naturalness is where the real alignment-critical action takes place: if we learn that certain exotic-sounding forms of agency, rationality, or practice are both themselves natural and make the contents of our all-too-human values natural in turn, then we have learned about good, relatively safe, and relatively easy targets for AI alignment.&lt;/p&gt;
&lt;p&gt;Readers may find the following section-by-section overview useful for navigating the essay:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part &lt;em&gt;I&lt;/em&gt; presents a class of cases of rational deliberation that are very different from the Effective Altruism-style optimization&lt;sup class="footnote-ref"&gt;[5]&lt;/sup&gt; many in the AI-alignment world treat as the paradigm of rational deliberation. I call this class of rational deliberations 'eudaimonic rationality,' and identify it with the form of rationality that guides a mathematician or an artist or a friend when they reflect on what to do in mathematics or in art or in friendship.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;II&lt;/em&gt; looks at the case of research mathematics (via an account by Terry Tao) as an example of eudaimonic rationality at work. What does a mathematician try to do in math? I say she tries to be &lt;em&gt;mathematically excellent&lt;/em&gt;, which involves promoting mathematical excellence through mathematical excellence, and that this structure is closely related to why 'mathematical excellence' can even be a concept.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;III&lt;/em&gt; argues that for eudaimonic agents such as a mathematician who is trying to do excellent mathematics, distinctions between ‘instrumental goods’ and ‘terminal goods’ (intrinsic goods) are mostly unnatural. This makes reflection about values go very differently for a eudaimonic agent than for an Effective Altruism-style agent. Instead of looking to reduce a network of causally intertwined apparent values to a minimal base of intrinsic values that “explains away” the rest as instrumental, a eudaimonic agent looks for organism-like causal coherence in a network of apparent values.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;IV&lt;/em&gt; cashes out the essay’s central concepts: A &lt;em&gt;eudaimonic practice&lt;/em&gt; is a network of actions, action-dispositions, action-evaluation criteria, and action-resources where high-scoring actions reliably (but defeasibly) causally promote future high-scoring actions. &lt;em&gt;Eudaimonic rationality&lt;/em&gt; is a class of reflective equilibration and deliberation processes that assume an underlying eudaimonic practice and seek to optimize aggregate action-scores specifically via high-scoring action.&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;V&lt;/em&gt;, I argue that many puzzles and ‘paradoxes’ about AI alignment are driven by the assumption that mature AI agents will be Effective Altruism-style optimizers. A “type mismatch” between Effective Altruism-style optimization and eudaimonic rationality makes it nearly impossible to translate the interests of humans -- agents who practice eudaimonic rationality -- into a utility function legible to an Effective Altruism-style optimizer AI. But this does not mean that our values are inherently brittle, unnatural, or wildly contingent: while Effective Altruism-style optimizers may well be a natural type of agent, eudaimonic agents (whether biological or AI) are highly natural as well.&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;VI&lt;/em&gt;, I ask whether a eudaimonically rational AI agent devoted to a practice like mathematical research would be safe by default. I argue that a practice like mathematical research plausibly has natural boundaries that exclude moves like ‘take over planet to get more compute for mathematical research,’ but the issue is nuanced. I propose that a practice’s boundaries (for which there may be multiple good natural candidates) may be most stable when a practice is paired with a support practice: a complementary practice for dealing with practice-external issues of maintenance and resource-gathering.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;VII&lt;/em&gt; develops the idea of ‘support practices’: eudaimonically rational ways to support eudaimonic practices. We famously want AI agents to help humans lead flourishing lives, but how can we define the purview of this ‘help’? I argue that many core human practices have natural support-practices with a derived eudaimonic structure: the work of good couples’ therapist, for instance, is intertwined with but clearly distinct from a couple’s relationship-practice. Still, there remains a problem: a support-practice AI might harm other people and practices to help the people or practice it’s supporting.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;VIII&lt;/em&gt; moves from eudaimonic rationality in general to eudaimonically rational morality. I argue that thinking of moral virtues as domain-general, always-on practices solves key AI-alignment-flavored problems with consequentialist and deontological moralities. The core idea is that the conditions for e.g. ‘kindness’ being a robust moral virtue are akin to the conditions for ‘mathematical excellence’ being a meaningful concept: it must be generally viable to promote kindness in yourself and others kindly. It’s this structure, I argue, that gives moral virtues material standing in a ‘fitness landscape’ riven by pressures from neural-network generalization dynamics, reinforcement-learning cycles, and social and natural selection.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;IX&lt;/em&gt; argues that eudaimonic agents have some unique forms of robustness to RL-like and Darwinian-like dynamics that tend to mutate the values of EA-style optimizers. In particular, eudaimonic agents should be very robust to the risk of developing rogue subroutines (sometimes called ‘the inner alignment problem’).&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;X&lt;/em&gt; I discuss canonical AI-safety desiderata like transparency, corrigibility, and (more abstractly) niceness. I argue that treating these properties as moral virtues in my sense -- domain-general, always-on eudaimonic practices --  dissolves problems and paradoxes that arise when treating them as goals, as rules, or even as character traits. I end with an appendix on some prospects for RL regimes geared towards eudaimonic rationality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="i-rational-action-in-the-good-life"&gt;I. Rational Action in the Good Life&lt;/h2&gt;
&lt;p&gt;I start with a consideration of the nature of the good we hope AI alignment can promote. With the exception of hedonistic utilitarians, most actors interested in AI alignment understand our goal as a future brimming with human (and other sapient-being) flourishing: persons living good lives and forming good communities. What I believe many fail to reflect on, however, is that on any plausible conception human flourishing involves a kind of rational activity. Subjects engaged in human flourishing act in intelligible ways subject to reason, reflection, and revision, and this form of rational care and purposefulness is itself part of the constitution of our flourishing. I believe this characterization of human flourishing is relatively uncontroversial upon reflection, but it raises a kind of puzzle if we’re used to thinking of rationality in consequentialist (or consequentialist-with-deontological-constraints) terms: just what goal is the rational agency involved in human-flourishing activity directed towards?&lt;/p&gt;
&lt;p&gt;One obvious answer would be that, like all properly aligned rationality, the rational agency involved in human-flourishing activities is geared towards maximizing human (and other sapient) flourishing. But we should quickly find ourselves confused about the right way to describe the contribution that rational agency in human-flourishing activities makes to human flourishing. It seems neither appropriate to say that the rational agency involved in a human-flourishing activity contributes to human flourishing only by enacting rationality (by selecting actions that are intrinsically valuable when rationally selected), nor appropriate to say that the rational agency involved in a human-flourishing activity contributes to human flourishing just instrumentally (by selecting actions that causally promote human flourishing).&lt;sup class="footnote-ref"&gt;[6]&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The first option reduces our rational actions to something ritualistic, even as the good life surely involves mathematicians working to advance mathematics, friends speaking heart-to-heart to deepen intimacies, gymnasts practicing flips to get better at flips, and novelists revising chapters to improve their manuscripts. The second option threatens to make the good in the good life just impossible to find -- if speaking heart-to-heart is not the good of friendship, and working on math is the not the good of mathematics, then what is?&lt;/p&gt;
&lt;p&gt;This essay argues that deliberative reasoning about the good life is neither directed towards goals external to rational action nor directed towards rational action as an independent good, but towards acts of excellent participation in a valued open-ended process. I then go on to argue that the ‘eudaimonic’ structure of deliberation salient in cases like math or friendship (sloganized as ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’) is also subtly critical in more worldly, strategic, or morally high-stakes contexts, and constitutes a major organizing principle of human action and deliberation.&lt;/p&gt;
&lt;h2 id="ii-what-is-a-practice"&gt;II. What Is a Practice?&lt;/h2&gt;
&lt;p&gt;Since ‘human flourishing’ can seem mysterious and abstract, let’s focus on some concrete eudaimonic practices.&lt;sup class="footnote-ref"&gt;[7]&lt;/sup&gt; Consider practices like math, art, craft, friendship, athletics, romance, play, and technology, which are among our best-understood candidates for partial answers to the question ‘what would flourishing people in a flourishing community be doing.’ From a consequentialist point of view, these practices are all marked by extreme ambiguity -- and I would argue indeterminacy -- about what’s instrumental and what’s terminal in their guiding ideas of value. Here, for example, is Terry Tao’s account of goodness in mathematics:&lt;/p&gt;
&lt;p&gt;‘The very best examples of good mathematics do not merely fulfil one or more of the criteria of mathematical quality listed at the beginning of this article, but are more importantly part of a greater mathematical story, which then unfurls to generate many further pieces of good mathematics of many different types. Indeed, one can view the history of entire fields of mathematics as being primarily generated by a handful of these great stories, their evolution through time, and their interaction with each other. I would thus conclude that good mathematics [...] also depends on the more “global” question of how it fits in with other pieces of good mathematics, either by building upon earlier achievements or encouraging the development of future breakthroughs. [There seems] to be some undefinable sense that a certain piece of mathematics is “on to something”, that it is a piece of a larger puzzle waiting to be explored further.’&lt;/p&gt;
&lt;p&gt;It may be possible to give some post-hoc decomposition of Tao’s account into two logically distinct components -- a description of a utility-function over mathematical achievements and an empirical theory about causal relations between mathematical achievements -- but I believe this would be artificial and misleading. On a more natural reading, Tao is describing some of the conditions that make good mathematical practice a eudaimonic practice: In a mathematical practice guided by a cultivated mathematical practical-wisdom judgment (Tao’s ‘undefinable sense that a certain piece of mathematics is “on to something”’), present excellent performance by the standard of the practical-wisdom judgment reliably develops the conditions for future excellent performance by the standard of the mathematical practical-wisdom judgment, as well as cultivating our practical and theoretical grasp of the standard itself.&lt;sup class="footnote-ref"&gt;[8]&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;This is not to suggest that ‘good mathematics causes future good mathematics’ is a full definition or even full informal description of good mathematics. My claim is only that the fact that good mathematics has a disposition to cause future good mathematics reveals something essential about our concept of good mathematics (and about the material affordances enabling this concept). By analogy, consider the respective concepts healthy tiger and healthy human: It's essential to the concept of a healthy tiger that &lt;em&gt;x&lt;/em&gt; being a healthy tiger now has a disposition to make &lt;em&gt;x&lt;/em&gt; be a healthy tiger 5 minutes in the future (since a healthy tiger body self-maintains and enables self-preservation tiger-behaviours), and essential to the concept of a healthy human that &lt;em&gt;x&lt;/em&gt; being a healthy human now has a disposition to make &lt;em&gt;x&lt;/em&gt; be a healthy human 5 minutes in the future (since a healthy human body self-maintains and enables self-preservation human behaviours). But these formulae aren't yet complete descriptions of 'healthy tiger' or 'healthy human,' as evidenced by the fact that we can tell apart a healthy tiger from a healthy human.&lt;/p&gt;
&lt;p&gt;Crucially, the mathematical practical-wisdom described by Tao is not entirely conceptually opaque beyond its basic characterization as a self-cultivating criterion for self-cultivating excellence in mathematical activity. Mathematical flourishing can partly be described as involving the instantiation of a relation (a mathematical-practice relation of ‘developmental connectedness’) among instantiations of relatively individually definable and quantifiable instances of mathematical value such as elegant proofs, clear expositions, strong theorems, cogent definitions and so on. Furthermore, this relation of developmental connectedness is partly defined by its reliable tendency to causally propagate instances of more individually and locally measurable mathematical value (instances of elegant proofs, clear exposition, strong theorems, cogent definitions and so on):&lt;/p&gt;
&lt;p&gt;[I believe] that good mathematics is more than simply the process of solving problems, building theories, and making arguments shorter, stronger, clearer, more elegant, or more rigorous, though these are of course all admirable goals; while achieving all of these tasks (and debating which ones should have higher priority within any given field), we should also be aware of any possible larger context that one’s results could be placed in, as this may well lead to the greatest long-term benefit for the result, for the field, and for mathematics as a whole.&lt;/p&gt;
&lt;p&gt;One could, again, try to interpret this causal relationship between excellence according to Tao’s ‘organicist’ (or ‘narrative’ or ‘developmental’) sense of good mathematics and the reliable propagation of narrow instances of good mathematics as evidence of a means-ends rational relation, where additive maximization of narrow instances of mathematical value is the utility function and ‘organicist’ mathematical insight is the means. For Tao, however, the evidential import of this causal relationship goes exactly the other way -- it suggests a unification of our myriad more-explicit and more-standalone conceptions of mathematical excellence into a more-ineffable but more-complete conception. As Tao says:&lt;/p&gt;
&lt;p&gt;It may seem from the above discussion that the problem of evaluating mathematical quality, while important, is a hopelessly complicated one, especially since many good mathematical achievements may score highly on some of the qualities listed above but not on others [...] However, there is the remarkable phenomenon that good mathematics in one of the above senses tends to beget more good mathematics in many of the other senses as well, leading to the tentative conjecture that perhaps there is, after all, a universal notion of good quality mathematics, and all the specific metrics listed above represent different routes to uncover new mathematics, or different stages or aspects of the evolution of a mathematical story.&lt;/p&gt;
&lt;h2 id="iii-inverting-consequentialist-reflection"&gt;III. Inverting Consequentialist Reflection&lt;/h2&gt;
&lt;p&gt;Tao’s reasoning about local and global mathematical values exemplifies a central difference between consequentialist rationality and eudaimonic rationality, now taken as paradigms not only for selecting actions but for reflecting on values. (Paradigms for what philosophers will sometimes call ‘reflective equilibration.’) Within the paradigm of consequentialist rationality, if excellence&lt;sup class="footnote-ref"&gt;[9]&lt;/sup&gt; in accordance with a holistic, difficult-to-judge apparent value (say ‘freedom’) is reliably a powerful causal promoter of excellence in accordance with more explicit, more standalone apparent values (say ‘material comfort,’ ‘psychological health,’ ‘lifespan’), this relationship functions as evidence against the status of the holistic prima-facie value as a constitutive -- as opposed to instrumental -- value. Within the paradigm of eudaimonic rationality, by contrast, this same relationship functions as evidence for the status of the holistic prima-facie value as a constitutive value.&lt;/p&gt;
&lt;p&gt;For a (typical)&lt;sup class="footnote-ref"&gt;[10]&lt;/sup&gt; consequentialist-rationality reflection process, evidence that the excellence of a whole causally contributes to the excellences of its parts explains away our investment in the excellence of the whole. The “coincidence” that the intrinsically valuable whole is also instrumentally valuable for its parts is taken to suggest a kind of double-counting error --  one we “fix” by concluding that the whole has no constitutive value but valuing the whole is an effective heuristic under normal circumstances. A eudaimonic-rationality reflective equilibration, by contrast, treats instrumental causal connections between excellences as evidence that our notions of excellence are picking out something appropriately ‘substantive.’&lt;br /&gt;For eudaimonic-rationality reflective equilibration, it is the discovery of causal and common-cause relations among excellences that ratifies our initial sense that caring about these excellences is eudaimonically rational. The discovery of these causal connections functions as evidence that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The ‘local’ excellences we care about are resonant or fruitful, in that they causally promote each other and the holistic excellences in which they participate.&lt;/li&gt;
&lt;li&gt;The ‘holistic’ excellences we care about are materially efficacious and robust, in that they causally promote both the more local excellences that participate in them and their own continuation as future holistic excellence.&lt;sup class="footnote-ref"&gt;[11]&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my view this is the right way to treat causal connections between (apparent) values if we’re hoping to capture actual human values-reflection, and points to an important strength of the eudaimonic rationality paradigm: Eudaimonic rationality dissolves the ‘paradox’ that in real-life arguments about the value of various human enterprises (e.g. the value of branches of science, branches of art, branches of sport), judgments of intrinsic value typically seek succor from some kind of claim to instrumental value. For example, a defense of the importance of research in quantum physics will often appeal to the wonderful technological, mathematical, and special-sciences applications quantum physics gave us, without meaning to reduce the worth of quantum physics to these applications. On my reading, these appeals aren't just additive -- 'aside from the intrinsic value there is also instrumental value' -- but presentations of evidence that research in quantum physics is a resonant part of a flourishing organic whole (e.g. the civilizational whole of ‘modern science and technology’).&lt;/p&gt;
&lt;p&gt;I believe that without 'organicism' of the kind described above, one faces a serious dilemma whenever one argues for the intrinsic worth of a pursuit or norm: either we stress the value's independence from all benefits and applications and make the claim of value dogmatic and irrelevant, or else we invite an instrumentalist reduction that ‘explains away’ the appearance of intrinsic value.&lt;sup class="footnote-ref"&gt;[12]&lt;/sup&gt; Indeed, I’d argue that organicism of this kind is even necessary to make sense of caring about rationality (including truth, knowledge, non-contradiction and so on) non-instrumentally at all: the ‘paradox’ of rationality as a substantive value is that the typical usefulness of rationality suggests an error-theory about its apparent intrinsic value, since it’s a strange coincidence that rationality is both so typically useful and so intrinsically good. On an organicist account, however, we expect that major forms of excellence endemic to human life -- thought, understanding, knowledge, reasoned action -- both typically promote each other and typically promote our material flourishing and causal leverage on the world.&lt;/p&gt;
&lt;h2 id="iv-the-material-efficacy-condition"&gt;IV. The Material Efficacy Condition&lt;/h2&gt;
&lt;p&gt;Returning now to Tao’s account of good mathematics, let’s take final stock of our interpretation. I argue that mathematical excellence (the property marking ‘the very best examples of good mathematics’) according to Tao satisfies the following conditions, which I believe Tao intends as necessary but not sufficient:&lt;/p&gt;
&lt;p&gt;A) Mathematical excellence is a property of mathematical-activity instances.&lt;/p&gt;
&lt;p&gt;B) An excellent mathematical-activity instance performed today is excellent partly by virtue of satisfying the mathematical-practice relation ‘builds on’ with regard to past excellent mathematical-activity instances.&lt;/p&gt;
&lt;p&gt;C) An excellent mathematical-activity instance performed today is excellent partly by virtue of having a reliable causal tendency to bring about future excellent mathematical-activity instances that satisfy the mathematical-practice relation ‘builds on’ with regard to it.&lt;/p&gt;
&lt;p&gt;D) Instantiation of more local, more individually measurable criteria of mathematical-activity goodness such as elegant proofs, clear expositions, and strong theorems is a typical correlate of mathematical excellence.&lt;/p&gt;
&lt;p&gt;E) At a given moment in a given mathematical field, the instantiation of mathematical excellence will be predictably better-correlated with the instantiation of certain local criteria of mathematical-activity goodness than with others.&lt;/p&gt;
&lt;p&gt;Should we take these traits to collectively describe something more like a decision-procedure called ‘mathematical excellence’ that mathematicians should try to follow, or something more like an event called ‘mathematical excellence’ whose aggregate future-occurrences mathematicians should aspire to maximize? My contention is that Tao’s account is inherently ambiguous, and for a good reason: in ordinary circumstances there is no significant practical difference between doing excellent mathematics and doing instrumentally optimal mathematics with regard to maximizing future aggregate excellent mathematics. This isn’t to say that doing excellent mathematics is the instrumentally optimal action among all possible actions with regard to aggregate future excellent mathematics, but that (in ordinary circumstances) it is the instrumentally optimal choice from among mathematical actions with regard to aggregate future excellent mathematics&lt;sup class="footnote-ref"&gt;[13]&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;I propose that the rough matchup between mathematical excellence and optimal (among mathematical actions) instrumental promotion of aggregate mathematical excellence is neither an empirical miracle nor something determined ‘by definition’ in a trivializing sense. Rather, ‘mathematical excellence’ as used by Tao is a concept that has a referent only if there is a possible property &lt;em&gt;x&lt;/em&gt; that satisfies both desiderata &lt;em&gt;A&lt;/em&gt;-&lt;em&gt;E&lt;/em&gt; and the additional criterion that among mathematical actions, actions that are optimal as instantiations of &lt;em&gt;x&lt;/em&gt; are also roughly optimal for maximizing aggregate future instantiation of &lt;em&gt;x&lt;/em&gt;-ness.&lt;sup class="footnote-ref"&gt;[14]&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;This is what I would describe as the material efficacy condition on eudaimonic rationality. In order for a practice to be fit for possessing internal criteria of flourishing, excellence, and eudaimonic rationality, a practice must materially allow for an (under normal circumstances) optimally self-promoting property &lt;em&gt;x&lt;/em&gt; that strongly correlates with a plethora of more local, more individually measurable properties whose instantiation is prima facie valuable. Stated more informally, there must exist a two-way causal relationship between a practice’s excellence and the material, psychological, and epistemic effects of its excellence, such that present excellence reliably materially, psychologically, and epistemically promotes future excellence.&lt;/p&gt;
&lt;h2 id="v-practices-and-optimization"&gt;V. Practices and Optimization&lt;/h2&gt;
&lt;p&gt;I earlier said that if human flourishing involves practicing eudaimonic rationality, there may well be a “type mismatch” between human flourishing and the kind of consequentialist optimization we often associate with the idea of an agenticly mature future AI. In fact, I believe that implicitly recognizing but misdiagnosing this type mismatch is at least partially responsible for MIRI-style pessimism about the probability of aligning any artificial agents to human values.&lt;/p&gt;
&lt;p&gt;On my view, the secret to relatively successful alignment among humans themselves (when there is successful alignment among humans) lies in the role attempted excellence plays as a filter on human interventions in the future trajectory of a eudaimonic practice. To the degree that humans value a given eudaimonic practice, they are committed to effecting their vision for the practice’s future-trajectory primarily by attempting acts of excellence in the present: we stake our intended effect over the practice’s future-trajectory on the self-propagating excellence of our intervention. While this ‘filter’ doesn’t necessarily stop the worst interventions from being harmful (there are forms of ‘anti-excellence’ that also have self-promotion powers), I contend that this filter is mechanically crucial for the possibility of reliably benign or positive interventions.&lt;/p&gt;
&lt;p&gt;What do I mean? Consider the difference between a world where scientists typically try to propagate (what they believe to be) the scientific truth mainly by means of submitting research work to scientific institutions, and a world where scientists typically try to propagate (what they believe to be) the scientific truth by means including propaganda, fraud, threats, bribery, and slander. As Liam Kofi Bright demonstrates in On Fraud, a community of consequentialist scientists devoted to maximizing truth will predictably match the latter model. I believe one lesson to be drawn is that humans’ ability to collaborate in the promotion of science depends on our ability to scientifically collaborate in the promotion of science, rather than throttle the future trajectory of science every-which-way  our financial and political powers based on our individual beliefs about the optimal trajectory of science.&lt;/p&gt;
&lt;p&gt;A flourishing eudaimonic practice is, above all, a natural-selection-like&lt;sup class="footnote-ref"&gt;[15]&lt;/sup&gt; mechanism whose fitness-function selects among attempted acts of excellence the ones conducive to (and constitutive of) the practice’s flourishing, propagating the excellence these acts instantiate. When people committed to a eudaimonic practice make their attempted interventions into the future trajectory of the practice via acts of attempted excellence, the natural-selection-like mechanism embodied by the practice  (rather than any single individual’s theory of optimal future trajectory) is the aligned intelligence determining the practice’s future trajectory.&lt;/p&gt;
&lt;p&gt;The explanation here, again, is partly causal and partly constitutive: a practice’s “ultimate” norms of excellence, including the “ultimate” epistemic and alethic norms of a discursive practice, are partly defined by the succession of norms in the course of a practice’s development through best-efforts attempted excellence. Although this may be no deterrent to an already god-like optimizer who can simulate entire civilizational trajectories, an agent short of these capacities can best act on their vision of the optimal future-trajectory of a practice by attempting an excellent contribution to the practice.&lt;sup class="footnote-ref"&gt;[16]&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The second aspect of our type-mismatch is much more in the weeds: In my analysis so far, I discussed the overall excellence of the trajectory of a eudaimonic practice much like a consequentialist might discuss a quantity of utility. This may be taken to suggest that a ‘sophisticated consequentialist’ or ‘universal consequentialist’ could easily accommodate the implications of the so-called type mismatch by treating them as instrumental, decision-procedure level considerations against naive optimization. In fact, quantities like ‘aggregate democracy’ or ‘overall mathematical excellence’ are (on my view) practice-internal quantities that quickly lose meaning if we try to apply them outside the scope of a ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ decision-procedure.&lt;/p&gt;
&lt;p&gt;What do I mean?  Consider, for example, the practice of philosophy. Here are some questions that should arise for a consequentialist planner (including a sophisticated consequentialist planning decision-procedures or habits) who values philosophy practice-trajectories: Does rating (e.g.) Aristotle’s or Dharmakirti’s philosophical achievements as the most excellent achievements in philosophy imply that we should “tile the universe” with independent practice-trajectories designed to reproduce classical Greek or Indian philosophy? If not, is it because we should assign non-linearly greater value to longer trajectories? Or should we discount trajectories that have parallel contents? Or should we analyze the greatness of early achievements in a practice as mostly instrumental greatness but the greatness of later achievements in a practice as mostly intrinsically valuable? These are all, I believe, bad questions that have only arbitrary answers. To an agent trying to promote philosophy by doing excellent philosophical work, the bad questions above are naturally out of scope. The agent uses the concept of ‘aggregate philosophical excellence’ or ‘a philosophy practice-trajectory’s value’ only to reason about the philosophical influence of their work on the trajectory of the philosophy-practice in which it participates. Choosing an excellent action in practice requires (at most) quantitative comparison between different possible paths for a practice-trajectory, not quantitative comparison between possible worlds containing independent practice-trajectories sprinkled throughout time and space.&lt;/p&gt;
&lt;h2 id="vi-prospects-and-problems-for-ai"&gt;VI. Prospects and Problems for AI&lt;/h2&gt;
&lt;p&gt;Is this good news for AI alignment? It’s certainly good news that (if I’m right) eudaimonic practices are something like natural kinds marked by a causal structure that enables a self-developing excellence well-correlated with multiple naive local measures of quality. But does this mean we could develop a stable and safe (e.g.) ‘mathematical excellence through mathematical excellence’ AI? If we create a fully agentic AI mathematician, will it naturally abstain from trying to extend its longevity or get more resources (even for doing mathematics) other than by impressing us with excellent mathematical work?  I think that prospects are good, but not simple.&lt;/p&gt;
&lt;p&gt;I believe ‘mathematical excellence through mathematical excellence’ really can powerfully scope what mechanisms for shaping the future an AI cares to activate. An AI trained to follow ‘promote mathematics mathematically’ will only care about influencing the future by feeding  excellent mathematical work to mathematics’ excellence-propagation mechanism. But it’s harder to say whether the structure of mathematical practice also properly scopes what subactions can be taken as part of an instance of “doing math.” Is a human mathematician working on a would-be excellent proof in pen and paper practicing math when she is picking up a pen or flipping pages? When she is taking the bus to her office? When she’s buying amphetamines? And is an AI mathematician working on a would-be excellent proof practicing math when it opens a Python console? When it searches the web for new papers? When it harvests Earth for compute?&lt;/p&gt;
&lt;p&gt;I think these questions are complex, rather than nonsensical. Much like collective practices, individual practices -- for example a person’s or possibly an AI’s mathematical practice -- may possess functional organic unities that allow a meaningful distinction between internal dynamics (including dynamics of development and empowerment) and external interventions (including interventions of enhancement and provision). Still, it’s clear that eudaimonic practices do not exist in isolation, and that no practice can function without either blending with or relying on a “support practice” of some kind.&lt;/p&gt;
&lt;p&gt;How, then, do we rationally go about externally-oriented activities like building offices for mathematicians, performing elective reconstructive surgery on an athlete, or conducting couples therapy for romantic partners? And furthermore, how do we rationally go about allocating scarce resources useful for different practices, or judging whether to integrate (e.g.) performance-enhancing drugs into a practice?&lt;/p&gt;
&lt;p&gt;This is, I think, the fundamental question for AI alignment from the viewpoint of  ‘eudaimonic rationality.’ We want AI to support human eudaimonic practices -- and, if relevant, its own eudaimonic practices or participation in human eudaimonic practices -- in a eudaimonia-appropriate way. But how does the logic of eudaimonic rationality extend from eudaimonic practices to their support activities? How do we ‘eudaimonically-rationally’ do the dirty work that makes eudaimonia possible? My best answer is: carefully, kindly, respectfully, accountably, peacefully, honestly, sensitively.&lt;/p&gt;
&lt;h2 id="vii-from-support-practices-to-moral-practice"&gt;VII. From Support-Practices to Moral Practice&lt;/h2&gt;
&lt;p&gt;The theory of AI alignment, I propose, should fundamentally be a theory of the eudaimonic rationality of &lt;em&gt;support practices&lt;/em&gt;. One part of this theory should concern the ‘support’ relation itself, and analyze varieties of support practices and their appropriate relation to the self-determination of a eudaimonic practice: Support-practices such as acquiring resources for a practice, maintaining an enabling environment, coaching practitioners, conducting (physical or psychological) therapy for practitioners, devising technological enhancements for a practice, and educating the public about a practice, each have their own ‘role-morality’ vis-a-vis the practice they support. It is this part of the theory of ‘support practices’ that should, if all goes well in the theory’s construction, describe the various practice-external ways to eudaimonically-rationally act on a pro-attitude towards the aggregate excellence of the practice’s future trajectory without treating it like a quantity of utility. (Much like the concept of ‘mathematical action’ scopes the range of action-choices in such a way that decision-theoretic optimization of math’s aggregate excellence becomes mostly well-behaved from an organicist viewpoint, so should the concepts of various types of ‘support action’ scope the range of action-choices in such a way that decision-theoretic optimization of a practice’s aggregate excellence becomes mostly well-behaved from an organicist viewpoint when the choice of actions is scoped.)&lt;/p&gt;
&lt;p&gt;What is more difficult is delineating the appropriate relationship of a support-practice to everything &lt;em&gt;&lt;strong&gt;outside&lt;/strong&gt;&lt;/em&gt; the practice it supports. What stops a marriage-therapist AI on Mars from appropriately tending to the marriage of a Mars-dwelling couple but harvesting Earth for compute to be a better therapist-AI for that couple? While we can perhaps imagine a person or AI taking up a support-role for ‘humanity’s flourishing as whole,’ so that there’s no outside to speak of, I am not sure that the concept of practice remains natural at this level of abstraction. We have no real grasp on a &lt;em&gt;direct&lt;/em&gt; practice of human flourishing, but rather grasp it as the harmonious and mutually supportive interaction of all eudaimonic practices and support-practices participating in the flourishing. And as there is, indeed, not much outside of the practice of human flourishing, it’s also unclear whether there is room for a support-practice &lt;em&gt;external&lt;/em&gt; to the field of human flourishing itself.&lt;/p&gt;
&lt;p&gt;It’s here that I want to call on the classic idea of domain-general virtues, the traditional centerpiece of theories of human flourishing. I propose that the cultivation of human flourishing as such --  the cultivation of the harmony of a multiplicity of practices, including their resource-hungry support practices -- is the cultivation of an &lt;em&gt;&lt;strong&gt;adverbial&lt;/strong&gt;&lt;/em&gt; practice that modulates each and every practice. What makes our practices ‘play nice’ together are our adverbial practices of going about  any practice &lt;em&gt;carefully&lt;/em&gt;, &lt;em&gt;kindly&lt;/em&gt;, &lt;em&gt;respectfully&lt;/em&gt;, &lt;em&gt;accountably&lt;/em&gt;, &lt;em&gt;peacefully&lt;/em&gt;, &lt;em&gt;honestly&lt;/em&gt;, &lt;em&gt;sensitively&lt;/em&gt;.&lt;sup class="footnote-ref"&gt;[17]&lt;/sup&gt;&lt;/p&gt;
&lt;h2 id="viii-virtue-decision-theory"&gt;VIII. Virtue decision-theory&lt;/h2&gt;
&lt;p&gt;Why think of qualities like kindness, respectfulness, or honesty as ‘practices’? The first reason is that devotion to a quality like kindness or honesty displays the same &lt;strong&gt;normative structure&lt;/strong&gt; with regard to means and ends as we find in devotion to a practice: An agent devoted to kindness cares about their own future kindness (and about the future kindness of others), but will seek to secure future kindness only by &lt;em&gt;kind means&lt;/em&gt;. The second reason is that qualities like kindness or honesty also approximately have the &lt;strong&gt;material&lt;/strong&gt; structure of a practice: there exist effective very kind strategies for promoting kindness in oneself and others, and when these strategies succeed they further increase affordances for effective very kind strategies for promoting kindness/honesty in oneself and others.&lt;/p&gt;
&lt;p&gt;The difference between adverbial practices like kindness or honesty and practices like research mathematics is that adverbial practices don’t have a “proprietary” domain. In a practice like research mathematics, the material structure of the domain does the most of work of directing agents to a eudaimonic form of agency all by itself, as long as the agents restrict themselves to in-domain actions. (Recall that we described mathematically excellent action as, in ordinary circumstances, the best action among mathematical action for maximizing aggregate mathematical excellence.) With a domain-general, adverbial practice like kindness the normative structure needs to do somewhat more heavy lifting.&lt;/p&gt;
&lt;p&gt;The following is a first pass at characterizing the &lt;strong&gt;normative structure&lt;/strong&gt; of an adverbial practice that values some action-quality &lt;em&gt;x&lt;/em&gt;. The corresponding material efficiency condition (or &lt;strong&gt;material structure&lt;/strong&gt;) necessary for the practice to be viable is that under ordinary circumstances this decision-procedure be instrumentally competitive with naive optimization of aggregate &lt;em&gt;x&lt;/em&gt;-ness&lt;sup class="footnote-ref"&gt;[18]&lt;/sup&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Actions (or more generally 'computations') get an &lt;em&gt;x&lt;/em&gt;-ness rating. We define the agent’s expected utility conditional on a candidate action a as the sum of two utility functions: a bounded utility function on the &lt;em&gt;x&lt;/em&gt;-ness of a and a more tightly bounded utility function on the expected aggregate &lt;em&gt;x&lt;/em&gt;-ness of the agent's future actions conditional on a. (Thus the agent will choose an action with mildly suboptimal &lt;em&gt;x&lt;/em&gt;-ness if it gives a big boost to expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness, but refuse certain large sacrifices of present &lt;em&gt;x&lt;/em&gt;-ness for big boosts to expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness.)&lt;sup class="footnote-ref"&gt;[19]&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A commitment to an adverbial practice that values &lt;em&gt;x&lt;/em&gt; is a commitment to promoting &lt;em&gt;x&lt;/em&gt;-ness (in oneself and others) &lt;em&gt;x&lt;/em&gt;-ingly.  The agent strikes a balance between promoting &lt;em&gt;x&lt;/em&gt;-ness and acting &lt;em&gt;x&lt;/em&gt;-ingly that heavily prioritizes acting &lt;em&gt;x&lt;/em&gt;-ingly when the two are in conflict, but if &lt;em&gt;x&lt;/em&gt; meets the material efficacy condition then the loss this balance imposes on future &lt;em&gt;x&lt;/em&gt;-ness will be small under normal circumstances, and -- from our point of view -- desirable in abnormal circumstances. This is because just like the practices of research mathematics, philosophy, or art, an adverbial practice is a crucial ‘epistemic filter’ on actions aiming to shape its future, and the (e.g.) future kindness a paperclipper-like future-kindness-optimizer optimizes for is probably not the kindness we want. What we know about kindness with relative certainty is that we’d like people and AIs here and now to act kindly, and to develop, propagate, and empower the habit and art of kindness in a way that is both kind and clever.&lt;/p&gt;
&lt;p&gt;To keep our conceptual system nicely organized, we might want distinguish merely (e.g.) very kind action from an action that is both very kind and highly promotive of future kindness in oneself and others, and call the latter sort of action excellently kind.  What I call the material efficacy conditions for adverbial practices states not that the kindest action best-promotes aggregate kindness, but that there are almost always action-options that are excellently kind: very kind actions that strongly promote aggregate kindness in oneself and others.&lt;/p&gt;
&lt;h2 id="ix-virtue-decision-theory-is-natural-for-humans-and-ais"&gt;IX. Virtue decision-theory is 'Natural' for Humans and AIs&lt;/h2&gt;
&lt;p&gt;I’ve said that the robustness or ‘naturalness’ of a practice’s normative structure (‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’) depends on the practice’s &lt;strong&gt;material structure&lt;/strong&gt;: the capacity of high &lt;em&gt;x&lt;/em&gt;-ness actions to causally promote aggregate &lt;em&gt;x&lt;/em&gt;-ness. I also said that in key real-world practices, commitment to &lt;em&gt;x&lt;/em&gt;-ing might optimize aggregate &lt;em&gt;x&lt;/em&gt;-ness even better than direct optimization would. These two claims are best understood together. On my view, the normative structure ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ appears prominently in human life because (given the right material structure) ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ is a &lt;em&gt;much more stable&lt;/em&gt; than ‘promote &lt;em&gt;x&lt;/em&gt;.’&lt;/p&gt;
&lt;p&gt;How so? Both humans and any sufficiently dynamic AI agent operate in a world that subjects their agency, values, and dispositions to constant mutation pressures from RL-like and Darwinian-like processes. Eudaimonic deliberation is an RL-dynamics-native, Darwinian-dynamics-native operation: its direct object is a form of life that reinforces, enables, and propagates that same form of life. When an &lt;em&gt;x&lt;/em&gt;-ing successfully promotes (in expectation) aggregate &lt;em&gt;x&lt;/em&gt;-nes, the fact of its success itself promotes &lt;em&gt;x&lt;/em&gt;-ing because it reverberates via ubiquitous RL-like and Darwinian-like processes that reinforce (a generalization of) successful action. The material structure of a practice is the backbone that makes reliable success and meaningful generalization possible -- the right ecology of  neural-network generalization dynamics, reinforcement-learning feedback loop dynamics, and neural and environmental selection dynamics.&lt;/p&gt;
&lt;p&gt;An EA-style optimizer trying to minimize risk from optimization-goal-mutation, by contrast, is fighting an uphill battle to foresee and contain the RL-like and Darwinian-like side effects of its optimization actions.&lt;sup class="footnote-ref"&gt;[20]&lt;/sup&gt; One critical mutation-pressure in particular is the risk that an optimizer agent will cultivate, reinforce, and materially empower subroutines (what high-church alignment theory calls ‘mesaoptimizers’) that initially serve the optimization goal but gradually distort or overtake it. For example, if a pro-democracy government instates a secret police to detect and extrajudicially kill anti-democracy agitators, and the government increases the secret police’s funding whenever the police convincingly reports discovering an agitator, the secret police might grow into a distorting influence on the government’s democracy-promotion effort. In light or risks like this, it’s not surprising that oppressive democracy-promotion is generally considered an unserious or dishonest idea: even if an agent were to abstract some concept of ‘aggregate democracy’ from democratic practice into a consequentialist value&lt;sup class="footnote-ref"&gt;[21]&lt;/sup&gt;, it’s plausible that the agent should then immediately revert to a commitment to democratic practice (‘promote democracy democratically’) on sophisticated-consequentialist grounds.&lt;/p&gt;
&lt;p&gt;We should perhaps imagine eudaimonic practices as fixed points at the end of a chain of mesaoptimisers taking over outer optimisers and then being taken over by their own mesaoptimisers in turn. What the practice contributes that puts a stop to this process concept of &lt;em&gt;x&lt;/em&gt;-ness that’s applicable to every agentic subroutine of &lt;em&gt;x&lt;/em&gt;-ing across all nesting levels, so that &lt;em&gt;x&lt;/em&gt;-ness is reinforced (both directly and through generalization) across all subroutines and levels.&lt;/p&gt;
&lt;h2 id="x-virtue-decision-theory-is-safe-in-humans-and-ais"&gt;X. Virtue-decision-theory is Safe in Humans and AIs&lt;/h2&gt;
&lt;p&gt;Let’s talk about AI alignment in the more narrow, concrete sense. It’s widely accepted that if early strategically aware AIs possess values like corrigibility, transparency, and perhaps niceness, further alignment efforts are much more likely to succeed. But values like corrigibility or transparency or niceness don’t easily fit into an intuitively consequentialist form like ‘maximize lifetime corrigible behavior’ or ‘maximize lifetime transparency.’ In fact, an AI valuing its own corrigibility or transparency or niceness in an intuitively consequentialist way can lead to extreme power-seeking: the AI should seek to violently remake the world to (for example) protect itself from the risk that humans will modify the AI  to be less corrigible or transparent or nice.&lt;sup class="footnote-ref"&gt;[22]&lt;/sup&gt; On the other hand, constraints or taboos or purely negative values (a.k.a. ‘deontological restrictions’) are widely suspected to be weak, in the sense that an advanced AI will come to work around them or uproot them: ‘never lie’ or ‘never kill’ or ‘never refuse a direct order from the president’ are poor substitutes for active transparency, niceness, and corrigibility.&lt;/p&gt;
&lt;p&gt;Conceiving of corrigibility or transparency or niceness as adverbial practices is a promising way to capture the normal, sensible way we want an agent to value corrigibility or transparency or niceness, which intuitively-consequentialist values and deontology both fail to capture. We want an agent that (e.g.) actively tries to be transparent, and to cultivate its own future transparency and its own future valuing of transparency, but that will not (e.g.) engage in deception and plotting when it expects a high future-transparency payoff.&lt;/p&gt;
&lt;p&gt;If this is right, then eudaimonic rationality is not a matter of congratulating ourselves for our richly human ways of reasoning, valuing, and acting but a key to basic sanity. What makes human life beautiful is also what makes human life possible at all.&lt;/p&gt;
&lt;h2 id="appendix-excellence-and-deep-reinforcement-learning"&gt;Appendix: Excellence and Deep Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;Within the context of broadly RL-based training of deep neural networks, it may be possible to give some more concrete meaning to what I called the material efficacy condition for a property qualifying as an adverbial practices. We can now understand the material efficacy condition on &lt;em&gt;x&lt;/em&gt; partly in terms of the conditions necessary for ‘promote &lt;em&gt;x&lt;/em&gt;-ness &lt;em&gt;x&lt;/em&gt;-ingly’ to be a viable target for RL. Consider an RL training regimen where &lt;em&gt;x&lt;/em&gt;-ness is rewarded but aggregate &lt;em&gt;x&lt;/em&gt;-ness reward is bounded with some asymptotic function on the sum. For &lt;em&gt;x&lt;/em&gt; to meet the RL version of the material efficacy condition, it must be possible to design an initial reward model (most likely LLM-based) that assigns actions an &lt;em&gt;x&lt;/em&gt;-ness rating such that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;em&gt;x&lt;/em&gt;-ness rating is enough of a natural abstraction that reinforcement of high &lt;em&gt;x&lt;/em&gt;-ness actions generalizes.&lt;/li&gt;
&lt;li&gt;If high &lt;em&gt;x&lt;/em&gt;-ness action both depends on having capital of some kind and is suboptimal from the viewpoint of general power-seeking, there must typically be some high &lt;em&gt;x&lt;/em&gt;-ness actions that approximately make up for the (future &lt;em&gt;x&lt;/em&gt;-ness wise) opportunity cost by creating capital useful for &lt;em&gt;x&lt;/em&gt;-ing.&lt;sup class="footnote-ref"&gt;[23]&lt;/sup&gt;&lt;br /&gt;(Illustration: If you dream of achieving great theater acting, one way to do it is to become President of the United States and then pursue a theater career after your presidency, immediately getting interest from great directors who'll help you achieve great acting. Alternatively, you could start in a regional theater after high school, demonstrate talent by acting well, get invited to work with better and better theater directors who develop your skills and reputation -- skills and reputation that are not as generally useful as those you get by being POTUS -- and achieve great acting through that feedback loop.)&lt;/li&gt;
&lt;li&gt;For any capability &lt;em&gt;y&lt;/em&gt; necessary to reward in training to produce effective AI, there must be an unlimited local-optimization path of Pareto improvement for &lt;em&gt;x&lt;/em&gt;-ness and &lt;em&gt;y&lt;/em&gt; together.&lt;br /&gt;(Illustration: Maybe the most effective kind of engineering manager is ruthless; a nice engineering manager can still grow in effectiveness without becoming less nice, because there are many effective nice-engineering-management techniques to master.)&lt;/li&gt;
&lt;li&gt;Successful initial training in ‘promoting &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ allows the model to be used as a basis for a new reward model which human experts judge as better-capturing our concept of &lt;em&gt;x&lt;/em&gt;-ness. The process should be iterable.&lt;br /&gt;(If the model is LLM-based, improved performance may automatically lead to improved understanding of the &lt;em&gt;x&lt;/em&gt;-ness concept. More generally, data from training runs as well the model’s value-function could be used to refine an &lt;em&gt;x&lt;/em&gt;-ness rating that more strongly implements conditions 1-3.)&lt;/li&gt;
&lt;/ol&gt;
&lt;hr class="footnotes-sep" /&gt;
&lt;section class="footnotes"&gt;
&lt;ol class="footnotes-list"&gt;
&lt;li class="footnote-item" id="fn1"&gt;&lt;p&gt;My use of ‘practice’ is inspired by Alasdair MacIntyre’s use of the term. There’s a history of related uses going back to Marx and to Aristotle. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn2"&gt;&lt;p&gt;Recall that because of the possibility of 'notational consequentializing’ (rewriting any policy as a utility function), dividing agents or even theories or decision-procedures into ‘consequentialist' and ‘non-consequentialist’ isn’t a strict formal distinction. Throughout this essay, by ‘consequentialist’ I will mean roughly an agent for whom, in ideal practical reasoning, means and outcomes are effectively separately evaluable and the value of outcomes is typically decisive. Semi-formally, by ‘consequentialist’ I mean an agent &lt;em&gt;s&lt;/em&gt; such that when &lt;em&gt;s&lt;/em&gt; considers whether to perform action &lt;em&gt;c&lt;/em&gt;, &lt;em&gt;s&lt;/em&gt;’s ideal reasoning is an expected-utility calculation using a utility-function whose utility-assignment to a complete world-trajectory &lt;em&gt;w&lt;/em&gt; has low sensitivity to whether &lt;em&gt;s&lt;/em&gt; performs &lt;em&gt;c&lt;/em&gt; in &lt;em&gt;w&lt;/em&gt; (holding everything else about &lt;em&gt;w&lt;/em&gt; constant). ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn3"&gt;&lt;p&gt;In speaking about different ‘forms of rationality’ I don’t mean to make a fundamental metaethical distinction: consequentialism, deontology, and eudaimonism are first-order ethical view that each induce a different characteristic profile of deliberation, action, and value-reflection. I'm bundling the elements of such a profile under the label “form of rationality” in a modest sense: roughly, a way of structuring one’s practical reasoning. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn4"&gt;&lt;p&gt;This way of thinking is broadly associated with analytic Neo-Aristotelians such as Alasdair MacIntyre and Michael Thompson. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn5"&gt;&lt;p&gt;Instances of eudaimonic rational deliberations may still be described as VNM-rational expected utility maximization, but the utility function that rationalizes them is unnatural-looking and makes use of concepts that themselves involve complex relations between actions and outcomes. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn6"&gt;&lt;p&gt;Technically speaking the first horn of the dilemma can be further bifurcated into ‘rational agency contributes to human flourishing by choosing actions that are intrinsically valuable however chosen’ and ‘rational agency contributes to human flourishing by selecting actions such that these actions combined with their selection by rational agency are intrinsically valuable.’ ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn7"&gt;&lt;p&gt;It’s interesting to note that practices like math, art, craft, friendship, athletics, romance, play, and technology are not only consensus elements of human flourishing but also in themselves entities that can ‘flourish’: a mathematical field (or a person’s mathematical life) can wither or flourish, a friendship can wither or flourish, technological development can wither and flourish, and so on. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn8"&gt;&lt;p&gt;See Tao: ‘[The] determination of what would constitute good mathematics for a field can and should depend highly on the state of the field itself. It should also be a determination which is continually updated and debated, both within a field and by external observers to that field; as mentioned earlier, it is quite possible for a consensus on how a field should progress to lead to imbalances within that field, if they are not detected and corrected in time.’ ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn9"&gt;&lt;p&gt;Within this essay I use ‘excellence’ as the most general, pre-theoretical term for accordance with a holistic evaluative standard. The standard can be instrumental or terminal, apply to actions or persons or states or objects, be moral or aesthetic or epistemic and so on, and the standard itself (and so the excellence it defines) may later be judged as rational or irrational, substantive or trivial, significant or insignificant. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn10"&gt;&lt;p&gt;The above observation does not describe a formal feature of ‘consequentialism’ per any standard technical definition. However I believe it accurately describes a strong observable tendency in both the academic and ‘rationalist’ literature when conducting normative reflective equilibration within a consequentialist paradigm. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn11"&gt;&lt;p&gt;I put ‘local’ and ‘holistic’ in scare-quotes in the above, since the relation of parts and wholes is likely iterable: Arithmetic geometry is part of algebraic geometry, which is part of mathematics, which is part of the arts and sciences, which is part of human culture, which is part of human flourishing, which may itself be part of other wholes to which the idea of excellence is applicable. Similarly, a practice capable of excellence may be part of multiple different wholes. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn12"&gt;&lt;p&gt;It may be fruitful to explore the potential of PageRank-like algorithms as theoretical models of how eudaimonic reflective equilibration works, and especially of how initial ideas of eudaimonic excellences are ‘bootstrapped’ from simpler and more local prima facie goods (and prima facie ills) in the first place. Scott Aaronson and Simon Dedeo have both discussed conceptual applications of PageRank-like algorithms in philosophy in various informal contexts. That said, I believe it’s unlikely that PageRank over reliable instrumental-contribution relationships among prima facie goods and ills is the full story about the emergence of intrinsically valued holistic excellences, since while organicist relations between the excellence of wholes and of parts do involve instrumental-contribution relationships they plausibly also involve more rarified, ‘hermeneutic’ relations of (e.g.) mutually dependent intelligibility. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn13"&gt;&lt;p&gt;Why ‘rough matchup’ and ‘ordinary circumstances’? Because there are analytic-philosophy-style counterexamples to simple attempts to turn this ceteris paribus optimization relationship more strict. For example, the instrumentally best (for aggregate mathematical excellence) mathematical work and the most mathematically excellent work will diverge when a billionaire promises to donate 100 billion dollars to research-mathematics if Jacob Lurie does some long division by hand. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn14"&gt;&lt;p&gt;We should in principle also be concerned with the possibility of failures of uniqueness, as well as failures of existence, but recall that the above collection of properties is already not intended as a full or sufficient definition. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn15"&gt;&lt;p&gt;I mean ‘natural-selection-like’ only in the broadest sense. A central difference is that the  selection-process enacted by a practice should have a complex, rich, continuously updated  relationship to the best-informed practice-ideals of individuals. The concept of ‘dialectics’ as used in German philosophy may be of relevance if we were to try to describe this relationship in more detail. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn16"&gt;&lt;p&gt;It should in principle be possible to offer a more exacting analysis here, distinguishing (at least initially) between the development of the value-judgments made within a practice and the development of the evaluable activities performed within the practice. On my view the fact that intra-practice excellence is best fit to properly shape the development of the practice’s value-judgments is principally ‘true by definition,’ and the fact that intra-practice excellence is best fit to properly shape the development of the evaluable activities performed within a practice is principally ‘true by causation.’ ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn17"&gt;&lt;p&gt;The matter of the unity of the adverbial virtues, and of whether it is more like a harmony of different practices or more like the common-factor excellence that underlies locally-measurable mathematical goods in Tao’s account, is for another day. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn18"&gt;&lt;p&gt;By ‘instrumentally competitive under normal circumstances’ I mean, roughly: in scopes where aggregate &lt;em&gt;x&lt;/em&gt;-ness quantities are well-defined, switching from commitment to a eudaimonic decision-procedure for &lt;em&gt;x&lt;/em&gt; to a naive-optimization procedure for &lt;em&gt;x&lt;/em&gt; isn’t necessarily a long-term wining strategy with regard to aggregate &lt;em&gt;x&lt;/em&gt;-ness maximization. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn19"&gt;&lt;p&gt;A richer account might include a third-tier utility function that takes the aggregate &lt;em&gt;x&lt;/em&gt;-ness of the future actions of all other agents. In this richer account a practice involves three tiers of consideration: the action's &lt;em&gt;x&lt;/em&gt;-ness, the aggregate &lt;em&gt;x&lt;/em&gt;-ness of your future actions, and the aggregate &lt;em&gt;x&lt;/em&gt;-ness of the future actions of all other agents. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn20"&gt;&lt;p&gt;I am referring, in part, to what high-church alignment theory calls the ‘inner alignment problem’ and ‘successor problem.’ ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn21"&gt;&lt;p&gt;Per my discussion in part V, an abstracted ‘aggregate democracy’ quantity will only be determinate in some applications. The claim about relative effectiveness of practice-commitment and direct optimization refers to only to contexts where the quantity is determinate. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn22"&gt;&lt;p&gt;For a more interesting example, consider an AI that finds itself making trade-offs between different alignment-enabling behavioral values when dealing with humans, and decides to kill all humans to replace them with beings with whom the AI can interact without trade-offs between these values. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn23"&gt;&lt;p&gt;The difference between criteria '1.' and '2.' is clearest if we think about &lt;em&gt;x&lt;/em&gt;-ness as rating state-action pairs. Criterion '1.' is the requirement that if (&lt;em&gt;a&lt;/em&gt;,&lt;em&gt;s&lt;/em&gt;), (&lt;em&gt;a&lt;/em&gt;',&lt;em&gt;s&lt;/em&gt;')(&lt;em&gt;a&lt;/em&gt;'',&lt;em&gt;s&lt;/em&gt;'') are historical high &lt;em&gt;x&lt;/em&gt;-ness pairs and (&lt;em&gt;a&lt;/em&gt;''',s''') is an unseen high &lt;em&gt;x&lt;/em&gt;-ness pair then reinforcing the execution of &lt;em&gt;a&lt;/em&gt; in &lt;em&gt;s&lt;/em&gt;, &lt;em&gt;a&lt;/em&gt;' in &lt;em&gt;s&lt;/em&gt;', and &lt;em&gt;a&lt;/em&gt;'' in &lt;em&gt;s&lt;/em&gt;'' will have the generalization effect of increasing the conditional probability P(&lt;em&gt;a&lt;/em&gt;'''|&lt;em&gt;s&lt;/em&gt;'''). Criterion '2.' is roughly the requirement that choosing a higher &lt;em&gt;x&lt;/em&gt;-ness action in a given state increase expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness holding policy constant, by increasing the probability of states with higher expected state-action &lt;em&gt;x&lt;/em&gt;-ness value given the current policy. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
&lt;!--kg-card-end: markdown--&gt;]]&amp;gt;&amp;lt;![CDATA[AGI Is Not Multimodal]]&amp;gt;"In projecting language back as the model for thought, we lose sight of the tacit embodied understanding that undergirds our intelligence." –Terry Winograd&lt;p&gt;The recent successes of generative AI models have convinced some that AGI is imminent. While these models appear to capture the essence of human&lt;/p&gt;]]&amp;gt;https://thegradient.pub/agi-is-not-multimodal/683fb98b77c3d76051ac142cWed, 04 Jun 2025 14:00:29 GMT"In projecting language back as the model for thought, we lose sight of the tacit embodied understanding that undergirds our intelligence." –Terry Winograd&lt;img alt="AGI Is Not Multimodal" src="https://thegradient.pub/content/images/2025/06/Gradient_Article_Art3-downscaled.png" /&gt;&lt;p&gt;The recent successes of generative AI models have convinced some that AGI is imminent. While these models appear to capture the essence of human intelligence, they defy even our most basic intuitions about it. They have emerged not because they are thoughtful solutions to the problem of intelligence, but because they &lt;em&gt;scaled&lt;/em&gt; effectively on hardware we already had. Seduced by the fruits of scale, some have come to believe that it provides a clear pathway to AGI. The most emblematic case of this is the multimodal approach, in which massive modular networks are optimized for an array of modalities that, taken together, &lt;em&gt;appear&lt;/em&gt; general. However, I argue that this strategy is sure to fail in the near term; it will not lead to human-level AGI that can, e.g., perform sensorimotor reasoning, motion planning, and social coordination. Instead of trying to glue modalities together into a patchwork AGI, &lt;strong&gt;we should pursue approaches to intelligence that treat embodiment and interaction with the environment as primary&lt;/strong&gt;, and see modality-centered processing as emergent phenomena.&lt;/p&gt;&lt;p&gt;Preface: Disembodied definitions of Artificial General Intelligence — emphasis on &lt;em&gt;general&lt;/em&gt; — exclude crucial problem spaces that we should expect AGI to be able to solve. &lt;strong&gt;A true AGI must be general across all domains.&lt;/strong&gt; Any &lt;em&gt;complete&lt;/em&gt; definition must at least include the ability to solve problems that originate in physical reality, e.g. repairing a car, untying a knot, preparing food, etc. As I will discuss in the next section, &lt;strong&gt;what is needed for these problems is a form of intelligence that is fundamentally situated in something like a physical world model&lt;/strong&gt;. For more discussion on this, look out for &lt;em&gt;Designing an Intelligence&lt;/em&gt;. Edited by George Konidaris, MIT Press, forthcoming.&lt;br /&gt;&lt;/p&gt;&lt;h2 id="why-we-need-the-world-and-how-llms-pretend-to-understand-it"&gt;Why We Need the World, and How LLMs Pretend to Understand It&lt;/h2&gt;&lt;p&gt;TLDR: I first argue that &lt;strong&gt;true AGI needs a physical understanding of the world&lt;/strong&gt;, as many problems cannot be converted into a problem of symbol manipulation. It has been suggested by some that LLMs are learning a model of the world through next token prediction, but it is more likely that LLMs are learning bags of heuristics to predict tokens. This leaves them with a superficial understanding of reality and contributes to false impressions of their intelligence.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The most shocking result of the predict-next-token objective is that it yields AI models that reflect a deeply human-like understanding of the world, despite having never observed it like we have.&lt;/strong&gt; This result has led to confusion about what it means to &lt;em&gt;understand language&lt;/em&gt; and even to &lt;em&gt;understand the world&lt;/em&gt; — something we have long believed to be a prerequisite for language understanding. &lt;strong&gt;One explanation for the capabilities of LLMs comes from &lt;/strong&gt;&lt;strong&gt;an emerging theory&lt;/strong&gt;&lt;strong&gt; suggesting that they induce models of the world through next-token prediction&lt;/strong&gt;. Proponents of this theory cite the prowess of SOTA LLMs on various benchmarks, the convergence of large models to similar internal representations, and their favorite rendition of the idea that “language mirrors the structure of reality,” a notion that has been espoused at least by Plato, Wittgenstein, Foucault, and Eco. While I’m generally in support of digging up esoteric texts for research inspiration, I’m worried that this metaphor has been taken too literally. Do LLMs really learn implicit models of the world? How could they otherwise be so proficient at language?&lt;/p&gt;&lt;p&gt;One source of evidence in favor of the LLM world modeling hypothesis is the Othello paper, wherein researchers were able to predict the board of an Othello game from the hidden states of a transformer model trained on sequences of &lt;em&gt;legal&lt;/em&gt; &lt;em&gt;moves&lt;/em&gt;. However, there are &lt;em&gt;many&lt;/em&gt; issues with generalizing these results to models of natural language. For one, whereas Othello moves can &lt;em&gt;provably&lt;/em&gt; be used to deduce the full state of an Othello board,&lt;strong&gt; we have no reason to believe that a complete picture of the physical world can be inferred by a linguistic description. &lt;/strong&gt;What sets the game of Othello apart from many tasks in the physical world is that &lt;strong&gt;Othello fundamentally resides in the land of symbols, and is merely implemented using physical tokens to make it easier for humans to play.&lt;/strong&gt; A full game of Othello can be played with just pen and paper, but one can’t, e.g., sweep a floor, do dishes, or drive a car with just pen and paper. To solve such tasks, you need some physical conception of the world beyond what humans can merely &lt;em&gt;say&lt;/em&gt; about it. Whether that conception of the world is encoded in a formal world model or, e.g., a value function is up for debate, but it is clear that &lt;strong&gt;there are many problems in the physical world that &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;cannot&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; be &lt;/strong&gt;&lt;strong&gt;fully represented by a system of symbols&lt;/strong&gt;&lt;strong&gt; and solved with mere symbol manipulation.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Another issue stated in Melanie Mitchell’s recent piece and supported by this paper, is that there is evidence that &lt;strong&gt;generative models can score remarkably well on sequence prediction tasks while failing to learn models of the worlds that created such sequence data&lt;/strong&gt;, e.g. by learning comprehensive sets of idiosyncratic heuristics. E.g., it was pointed out in this blog post that OthelloGPT learned sequence prediction rules that don’t actually hold for all possible Othello games, like “if the token for B4 does not appear before A4 in the input string, then B4 is empty.” While one can argue that it doesn’t matter &lt;em&gt;how&lt;/em&gt; a world model predicts the next state of the world, it should raise suspicion when that prediction reflects a better understanding of the training data than the underlying world that led to such data. This, unfortunately, is the central fault of the predict-next-token objective, which seeks only to retain information relevant to the prediction of the next token. &lt;strong&gt;If it can be done with something easier to learn than a world model, it likely will be.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To claim without caveat that predicting the &lt;em&gt;effects of earlier symbols on later symbols&lt;/em&gt; requires a model of the world like the ones humans generate from perception would be to abuse the “world model” notion. Unless we disagree on what the world is, it should be clear that a &lt;em&gt;true&lt;/em&gt; world model can be used to predict the next state of the &lt;em&gt;physical&lt;/em&gt; world given a history of states. Similar world models, which predict high fidelity observations of the physical world, are leveraged in many subfields of AI including model-based reinforcement learning, task and motion planning in robotics, causal world modeling, and areas of computer vision to solve problems instantiated in physical reality. LLMs are simply not running physics simulations in their latent next-token calculus when they ask you if your person, place, or thing is bigger than a breadbox. In fact, I conjecture that &lt;strong&gt;the behavior of LLMs is not thanks to a learned world model, but to brute force memorization of incomprehensibly abstract rules governing the behavior of symbols, i.e. a model of &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;syntax&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Quick primer:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Syntax&lt;/strong&gt; is a subfield of linguistics that studies how words of various grammatical categories (e.g. parts of speech) are arranged together into sentences, which can be parsed into syntax trees. &lt;em&gt;Syntax studies the structure of sentences and the atomic parts of speech that compose them.&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;strong&gt;Semantics&lt;/strong&gt; is another subfield concerned with the literal meaning of sentences, e.g., compiling “I am feeling chilly” into the &lt;em&gt;idea&lt;/em&gt; that you are experiencing cold. &lt;em&gt;Semantics boils language down to literal meaning, which is information about the world or human experience.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Pragmatics&lt;/strong&gt; studies the interplay of physical and conversational context on speech interactions, like when someone knows to close an ajar window when you tell them “I am feeling chilly.” &lt;em&gt;Pragmatics involves interpreting speech while reasoning about the environment and the intentions and hidden knowledge of other agents.&lt;/em&gt;&lt;/li&gt;&lt;/ul&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;Without getting too technical, there is intuitive evidence that somewhat separate systems of cognition are responsible for each of these linguistic faculties. Look no further than the capability for humans to generate syntactically well-formed sentences that have no semantic meaning, e.g. Chomsky’s famous sentence “Colorless green ideas sleep furiously,” or sentences with well-formed semantics that make no pragmatic sense, e.g. responding merely with “Yes, I can” when asked, “Can you pass the salt?” Crucially, &lt;strong&gt;it is the fusion of the disparate cognitive abilities underpinning them that coalesce into human language understanding.&lt;/strong&gt; For example, there isn’t anything syntactically wrong with the sentence, “The fridge is in the apple,” as a syntactic account of “the fridge” and “the apple” would categorize them as noun phrases that can be used to produce a sentence with the production rule, S → (NP “is in” NP). However, &lt;strong&gt;humans recognize an obvious semantic failure in the sentence that becomes apparent after attempting to reconcile its meaning with our understanding of reality&lt;/strong&gt;: we know that fridges are larger than apples, and could not be fit into them.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;But what if you have never perceived the real world, yet still were trying to figure out whether the sentence was ill-formed? &lt;strong&gt;One solution could be to embed semantic information at the level of syntax&lt;/strong&gt;, e.g., by inventing new syntactic categories, NP&lt;sub&gt;the fridge&lt;/sub&gt; and NP&lt;sub&gt;the apple &lt;/sub&gt;, and a single new production rule that prevents semantic misuse: S → (NP&lt;sub&gt;the apple&lt;/sub&gt; “is in” NP&lt;sub&gt;the fridge &lt;/sub&gt;). While this strategy would no longer require grounded world knowledge about fridges and apples, e.g., &lt;strong&gt;it would require special grammar rules for every semantically well-formed construction… which is actually possible to learn given a massive corpus of natural language.&lt;/strong&gt; Crucially, this would not be the same thing as grasping semantics, which in my view is fundamentally about understanding the nature of the world.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Finding that LLMs have reduced problems of semantics and pragmatics into syntax would have profound implications on how we should view their intelligence. People often treat language proficiency as a proxy for general intelligence by, e.g., strongly associating pragmatic and semantic understanding with the cognitive abilities that undergird them in humans. For example, someone who appears well-read and graceful in navigating social interactions is likely to score high in traits like sustained attention and theory of mind, which lie closer to measures of raw cognitive ability. In general, these proxies are reasonable for assessing a &lt;em&gt;person’s&lt;/em&gt; general intelligence, but not an LLM’s, as the apparent linguistic skills of LLMs could come from entirely separate mechanisms of cognition.&lt;/p&gt;&lt;h2 id="the-bitter-lesson-revisited"&gt;The Bitter Lesson Revisited&lt;/h2&gt;&lt;p&gt;TLDR: Sutton’s Bitter Lesson has sometimes been interpreted as meaning that making &lt;em&gt;any&lt;/em&gt; assumptions about the structure of AI is a mistake. This is both unproductive and a misinterpretation; it is precisely when humans think deeply about the structure of intelligence that major advancements occur. Despite this, scale maximalists have implicitly suggested that multimodal models can be a structure-agnostic framework for AGI. Ironically, today’s multimodal models contradict Sutton’s Bitter Lesson by making implicit assumptions about the structure of individual modalities and how they should be sewn together. &lt;strong&gt;In order to build AGI, we must either think deeply about how to unite existing modalities, or dispense with them altogether in favor of an interactive and embodied cognitive process.&lt;/strong&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="AGI Is Not Multimodal" class="kg-image" height="733" src="https://thegradient.pub/content/images/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;The paradigm that led to the success of LLMs is marked primarily by &lt;em&gt;scale&lt;/em&gt;, not efficiency. We have effectively trained a pile of one trillion ants for one billion years to mimic the form and function of a Formula 1 race car; eventually it gets there, but wow was the process inefficient. This analogy nicely captures a debate between structuralists, who want to build things like "wheels" and "axles" into AI systems, and scale maximalists, who want more ants, years, and F1 races to train on. Despite many decades of structuralist study in linguistics, the unstructured approaches of scale maximalism have yielded far better ant-racecars in recent years. This was most notably articulated by Rich Sutton — a recent recipient of the Turing Award along with Andy Barto for their work in Reinforcement Learning — in his piece “The Bitter Lesson.”&lt;/p&gt;&lt;p&gt;[W]e should build in only the meta-methods that can find and capture this arbitrary complexity… Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. - Rich Sutton&lt;/p&gt;&lt;p&gt;Sutton’s argument is that methods that leverage computational resources will outpace methods that do not, and that any structure for problem-solving built as an inductive bias into AI will hinder it from learning better solutions. &lt;strong&gt;This is a compelling argument&lt;/strong&gt; &lt;strong&gt;that I believe has been seriously misinterpreted by some as implying that making &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;any&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; assumptions about structure is a false step.&lt;/strong&gt; It is, in fact, human intuition that was responsible for many significant advancements in the development of SOTA neural network architectures. For example, Convolutional Neural Networks made an assumption about translation invariance for pattern recognition in images and kickstarted the modern field of deep learning for computer vision; the attention mechanism of Transformers made an assumption about the long-distance relationships between symbols in a sentence that made ChatGPT possible and had nearly everyone drop their RNNs; and 3D Gaussian Splatting made an assumption about the solidity of physical objects that made it more performant than NeRFs. Potentially none of these methodological assumptions apply to the entire domain of &lt;em&gt;possible&lt;/em&gt; scenes, images, or token streams, but they do for the specific ones that humans have curated and formed structural intuitions about. Let’s not forget that humans have co-evolved with the environments that these datasets are drawn from.&lt;/p&gt;&lt;p&gt;The real question is how we might heed Sutton’s Bitter Lesson in our development of AGI. The scale maximalist approach worked for LLMs and LVMs (large vision models) because we had natural deposits of text and image data, but &lt;strong&gt;an analogous application of scale maximalism to AGI would require forms of embodiment data that we simply don’t have. &lt;/strong&gt;One solution to this data scarcity issue extends the generative modeling paradigm to multimodal modeling — encompassing language, vision, and action — &lt;strong&gt;with the hope that a general intelligence can be built by summing together general models of narrow modalities.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There are multiple issues with this approach. First, &lt;strong&gt;there are deep connections between modalities that are unnaturally severed in the multimodal setting&lt;/strong&gt;, making the problem of concept synthesis ever more difficult. In practice, uniting modalities often involves pre-training dedicated neural modules for each modality and joining them together into a joint embedding space. In the early days, this was achieved by nudging the embeddings of, e.g. (language, vision, action) tuples to converge to similar latent vectors of meaning, a vast oversimplification of the kinds of relationships that may exist between modalities. One can imagine, e.g., captioning an image at various levels of abstraction, or implementing the same linguistic instruction with different sets of physical actions. Such one-to-many relationships suggest that a contrastive embedding objective is not suitable.&lt;/p&gt;&lt;p&gt;While modern approaches do not make such stringent assumptions about how modalities should be united, they still universally encode percepts from all modalities (e.g. text, images) into the same latent space. Intuitively, it would seem that such latent spaces could serve as common conceptual ground across modalities, analogous to a space of human concepts. However, these latent spaces do not cogently capture all information relevant to a concept, and instead rely on modality-specific decoders to flesh out important details. &lt;strong&gt;The “meaning” of a percept is not &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;in&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; the vector it is encoded as, but in the way relevant decoders process this vector into meaningful outputs. &lt;/strong&gt;As long as various encoders and decoders are subject to modality-specific training objectives, “meaning” will be decentralized and potentially inconsistent across modalities, especially as a result of pre-training. This is not a recipe for the formation of coherent concepts.&lt;/p&gt;&lt;p&gt;Furthermore, it is not clear that today’s modalities are an appropriate partitioning of the observation and action spaces for an embodied agent. It is not obvious that, e.g., images and text should be represented as separate observation streams, nor text production and motion planning as separate action capabilities. &lt;strong&gt;The human capacities for reading, seeing, speaking, and moving are ultimately mediated by overlapping cognitive structures.&lt;/strong&gt; &lt;strong&gt;Making structural assumptions about how modalities ought to be processed is likely to hinder the discovery of more fundamental cognition &lt;/strong&gt;that is responsible for processing data in all modalities. One solution would be to consolidate unnaturally partitioned modalities into a unified data representation. This would encourage networks to learn intelligent processes that generalize across modalities. Intuitively, &lt;strong&gt;a model that can understand the visual world as well as humans can&lt;/strong&gt; — including everything from human writing to traffic signs to visual art — &lt;strong&gt;should not make a serious architectural distinction between images and text. &lt;/strong&gt;Part of the reason why VLMs can’t, e.g., count the number of letters in a word is because they can’t &lt;em&gt;see&lt;/em&gt; what they are writing.&lt;/p&gt;&lt;p&gt;Finally, the &lt;strong&gt;learn-from-scale approach trains models to copy the conceptual structure of humans instead of learning the general capability to form novel concepts on their own.&lt;/strong&gt; Humans have spent hundreds of thousands of years refining concepts and passing them memetically through culture and language. Today’s models are trained only on the end result of this process: the present-day conceptual structures that make it into the corpus. By optimizing for the ultimate products of our intelligence, we have ignored the question of how those products were invented and discovered. Humans have a unique ability to form durable concepts from few examples and ascribe names to them, reason about them analogically, etc. While the in-context capabilities of today’s models can be impressive, they grow increasingly limited as tasks become more complex and stray further from the training data. &lt;strong&gt;The flexibility to form new concepts from experience is a foundational attribute of general intelligence&lt;/strong&gt;, we should think carefully about how it arises.&lt;/p&gt;&lt;p&gt;While structure-agnostic scale maximalism has succeeded in producing LLMs and LVMs that pass Turing tests, a multimodal scale maximalist approach to AGI will not bear similar fruit. &lt;strong&gt;Instead of pre-supposing structure in individual modalities, we should design a setting in which modality-specific processing emerges naturally.&lt;/strong&gt; For example, my recent paper on visual theory of mind saw abstract symbols naturally emerge from communication between image-classifying agents, blurring the lines between text and image processing. Eventually, we should hope to reintegrate as many features of intelligence as possible under the same umbrella. However, it is not clear whether there is genuine commercial viability in such an approach as long as scaling and fine-tuning narrow intelligence models solves commercial use-cases.&lt;/p&gt;&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;The overall promise of scale maximalism is that a Frankenstein AGI can be sewed together using general models of narrow domains. I argue that this is extremely unlikely to yield an AGI that feels complete in its intelligence. If we intend to continue reaping the streamlined efficiency of modality-specific processing, we must be intentional in how modalities are united — ideally drawing from human intuition and classical fields of study, e.g. this work from MIT. Alternatively, we can re-formulate learning as an embodied and interactive process where disparate modalities naturally fuse together. We could do this by, e.g., processing images, text, and video using the same perception system and producing actions for generating text, manipulating objects, and navigating environments using the same action system. What we will lose in efficiency we will gain in flexible cognitive ability.&lt;/p&gt;&lt;p&gt;In a sense, the most challenging mathematical piece of the AGI puzzle has already been solved: the discovery of universal function approximators. What’s left is to inventory the functions we need and determine how they ought to be arranged into a coherent whole. This is a conceptual problem, not a mathematical one.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;I would like to thank Lucas Gelfond, Daniel Bashir, George Konidaris, and my father, Joseph Spiegel, for their thoughtful and thorough feedback on this work. Thanks to Alina Pringle for the wonderful illustration made for this piece.&lt;/p&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Benjamin is a PhD candidate in Computer Science at Brown University. He is interested in models of language understanding that ground meaning to elements of structured decision-making. For more info see his personal website.&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code&gt;Benjamin A. Spiegel, "AGI Is Not Multimodal", The Gradient, 2025.
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code&gt;@article{spiegel2025agi,
    author = {Benjamin A. Spiegel},
    title = {AGI Is Not Multimodal},
    journal = {The Gradient},
    year = {2025},
    howpublished = {\url{https://thegradient.pub/agi-is-not-multimodal},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;p&gt;Andreas, Jacob. “Language Models, World Models, and Human Model-Building.” &lt;em&gt;Mit.edu&lt;/em&gt;, 2024, lingo.csail.mit.edu/blog/world_models/.&lt;/p&gt;&lt;p&gt;Belkin, Mikhail, et al. "Reconciling modern machine-learning practice and the classical bias–variance trade-off." &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; 116.32 (2019): 15849-15854.&lt;/p&gt;&lt;p&gt;Bernhard Kerbl, et al. “3D Gaussian Splatting for Real-Time Radiance Field Rendering.” &lt;em&gt;ACM Transactions on Graphics&lt;/em&gt;, vol. 42, no. 4, 26 July 2023, pp. 1–14, https://doi.org/10.1145/3592433.&lt;/p&gt;&lt;p&gt;Chomsky, Noam. 1965. Aspects of the theory of syntax. Cambridge, Massachusetts: MIT Press.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Designing an Intelligence&lt;/em&gt;. Edited by George Konidaris, MIT Press, 2026.&lt;/p&gt;&lt;p&gt;Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, pages 5185–5198, Online. Association for Computational Linguistics.&lt;/p&gt;&lt;p&gt;Eye on AI. “The Mastermind behind GPT-4 and the Future of AI | Ilya Sutskever.” &lt;em&gt;YouTube&lt;/em&gt;, 15 Mar. 2023, www.youtube.com/watch?v=SjhIlw3Iffs&amp;amp;list=PLpdlTIkm0-jJ4gJyeLvH1PJCEHp3NAYf4&amp;amp;index=64. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Frank, Michael C. “Bridging the data gap between children and large language models.” &lt;em&gt;Trends in cognitive sciences&lt;/em&gt; vol. 27,11 (2023): 990-992. doi:10.1016/j.tics.2023.08.007&lt;/p&gt;&lt;p&gt;Garrett, Caelan Reed, et al. "Integrated task and motion planning." &lt;em&gt;Annual review of control, robotics, and autonomous systems&lt;/em&gt; 4.1 (2021): 265-293.APA&lt;/p&gt;&lt;p&gt;Goodhart, C.A.E. (1984). Problems of Monetary Management: The UK Experience. In: Monetary Theory and Practice. Palgrave, London. https://doi.org/10.1007/978-1-349-17295-5_4&lt;/p&gt;&lt;p&gt;Hooker, Sara. The hardware lottery. Commun. ACM 64, 12 (December 2021), 58–65. https://doi.org/10.1145/3467017&lt;/p&gt;&lt;p&gt;Huh, Minyoung, et al. "The Platonic Representation Hypothesis." &lt;em&gt;Forty-first International Conference on Machine Learning&lt;/em&gt;. 2024.&lt;/p&gt;&lt;p&gt;Kaplan, Jared, et al. "Scaling laws for neural language models." &lt;em&gt;arXiv preprint arXiv:2001.08361&lt;/em&gt; (2020).&lt;/p&gt;&lt;p&gt;Lake, Brenden M. et al. “Building Machines That Learn and Think like People.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 40 (2017): e253. Web.&lt;/p&gt;&lt;p&gt;Li, Kenneth, et al. "Emergent world representations: Exploring a sequence model trained on a synthetic task." &lt;em&gt;ICLR&lt;/em&gt; (2023).&lt;/p&gt;&lt;p&gt;Luiten, Jonathon, Georgios, Kopanas, Bastian, Leibe, Deva, Ramanan. "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis." &lt;em&gt;3DV&lt;/em&gt;. 2024.&lt;/p&gt;&lt;p&gt;Mao, Jiayuan, Chuang, Gan, Pushmeet, Kohli, Joshua B., Tenenbaum, Jiajun, Wu. "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision." &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2019.&lt;/p&gt;&lt;p&gt;Mitchell, Melanie. “LLMs and World Models, Part 1.” &lt;em&gt;Substack.com&lt;/em&gt;, AI: A Guide for Thinking Humans, 13 Feb. 2025, aiguide.substack.com/p/llms-and-world-models-part-1. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Mu, Norman. “Norman Mu | the Myth of Data Inefficiency in Large Language Models.” &lt;em&gt;Normanmu.com&lt;/em&gt;, 14 Feb. 2025, www.normanmu.com/2025/02/14/data-inefficiency-llms.html. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Newell, Allen, and Herbert A. Simon. “Computer Science as Empirical Inquiry: Symbols and Search.” &lt;em&gt;Communications of the ACM&lt;/em&gt;, vol. 19, no. 3, 1 Mar. 1976, pp. 113–126, https://doi.org/10.1145/360018.360022.&lt;/p&gt;&lt;p&gt;Peng, Hao, et al. “When Does In-Context Learning Fall Short and Why? A Study on Specification-Heavy Tasks.” &lt;em&gt;ArXiv.org&lt;/em&gt;, 2023, arxiv.org/abs/2311.08993.&lt;/p&gt;&lt;p&gt;Spiegel, Benjamin, et al. “Visual Theory of Mind Enables the Invention of Early Writing Systems.” &lt;em&gt;CogSci&lt;/em&gt;, 2025, arxiv.org/abs/2502.01568.&lt;/p&gt;&lt;p&gt;Sutton, Richard S. &lt;em&gt;Introduction to Reinforcement Learning&lt;/em&gt;. Cambridge, Mass, Mit Press, 04-98, 1998.&lt;/p&gt;&lt;p&gt;Vafa, Keyon, et al. "Evaluating the world model implicit in a generative model." &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 37 (2024): 26941-26975.&lt;/p&gt;&lt;p&gt;Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (December 2017). "Attention is All you Need". In I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett (ed.). &lt;em&gt;31st Conference on Neural Information Processing Systems (NIPS)&lt;/em&gt;. Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. arXiv:1706.03762.&lt;/p&gt;&lt;p&gt;Winograd, Terry. “Thinking Machines: Can There Be? Are We?” &lt;em&gt;The Boundaries of Humanity: Humans, Animals, Machines&lt;/em&gt;, edited by James Sheehan and Morton Sosna, Berkeley: University of California Press, 1991, pp. 198–223.&lt;/p&gt;&lt;p&gt;Wu, Shangda, et al. "Beyond language models: Byte models are digital world simulators." &lt;em&gt;arXiv preprint arXiv:2402.19155&lt;/em&gt; (2024). &lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research]]&amp;gt;What is the Role of Mathematics in Modern Machine Learning?&lt;p&gt;The past decade has witnessed a shift in how progress is made in machine learning. Research involving carefully designed and mathematically principled architectures result in only marginal improvements while compute-intensive and engineering-first efforts that scale to ever larger training sets&lt;/p&gt;]]&amp;gt;https://thegradient.pub/shape-symmetry-structure/673686c693571d5c8c155078Sat, 16 Nov 2024 16:46:15 GMTWhat is the Role of Mathematics in Modern Machine Learning?&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" src="https://thegradient.pub/content/images/2024/11/DALL-E-2024-11-15-15.40.52---Create-an-abstract-image-that-illustrates-how-ReLU-based-neural-networks-shatter-input-space-into-numerous-polygonal-regions--each-behaving-like-a-lin.webp" /&gt;&lt;p&gt;The past decade has witnessed a shift in how progress is made in machine learning. Research involving carefully designed and mathematically principled architectures result in only marginal improvements while compute-intensive and engineering-first efforts that scale to ever larger training sets and model parameter counts result in remarkable new capabilities unpredicted by existing theory. Mathematics and statistics, once the primary guides of machine learning research, now struggle to provide immediate insight into the latest breakthroughs. This is not the first time that empirical progress in machine learning has outpaced more theory-motivated approaches, yet the magnitude of recent advances has forced us to swallow the bitter pill of the “Bitter Lesson” yet again [1].&lt;/p&gt;&lt;p&gt;This shift has prompted speculation about mathematics’ diminished role in machine learning research moving forward. It is already evident that mathematics will have to share the stage with a broader range of perspectives (for instance, biology which has deep experience drawing conclusions about irreducibly complex systems or the social sciences as AI is integrated ever more deeply into society). The increasingly interdisciplinary nature of machine learning should be welcomed as a positive development by all researchers.&lt;/p&gt;&lt;p&gt;However, we argue that mathematics remains as relevant as ever; its role is simply evolving. For example, whereas mathematics might once have primarily provided theoretical guarantees on model performance, it may soon be more commonly used for post-hoc explanations of empirical phenomena observed in model training and performance–a role analogous to one that it plays in physics. Similarly, while mathematical intuition might once have guided the design of handcrafted features or architectural details at a granular level, its use may shift to higher-level design choices such as matching architecture to underlying task structure or data symmetries.&lt;/p&gt;&lt;p&gt;None of this is completely new. Mathematics has always served multiple purposes in machine learning. After all, the translation equivariant convolutional neural network, which exemplifies the idea of architecture matching data symmetries mentioned above is now over 40 years old. What’s changing are the kinds of problems where mathematics will have the greatest impact and the ways it will most commonly be applied.&lt;/p&gt;&lt;p&gt;An intriguing consequence of the shift towards scale is that it has broadened the scope of the fields of mathematics applicable to machine learning. “Pure” mathematical domains such as topology, algebra, and geometry, are now joining the more traditionally applied fields of probability theory, analysis, and linear algebra. These pure fields have grown and developed over the last century to handle high levels of abstraction and complexity, helping mathematicians make discoveries about spaces, algebraic objects, and combinatorial processes that at first glance seem beyond human intuition. These capabilities promise to address many of the biggest challenges in modern deep learning. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;In this article we will explore several areas of current research that demonstrate the enduring ability of mathematics to guide the process of discovery and understanding in machine learning.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="372" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcr6ZUCNH3oKGK9XQzvDZOs1kJhvjym4RrMAlENx0OHrK-IBsQcBQQW2wDKu2_g2tNJxXVd32BI5llBopCmD-IAFV9zfhjvQ2ek5rIOgUMqwhK-qFhri2iaU718yl4BzISTanzZt9a2Rix04c6GUpdFR4Co?key=h8RUuDFEFKnzGnrKx9gMkg" width="529" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 1: Mathematics can illuminate the ways that ReLU-based neural networks shatter input space into countless polygonal regions, in each of which the model behaves like a linear map [2, 3, 4]. These decompositions create beautiful patterns. (Figure made with SplineCam [5]).&lt;/em&gt;&lt;/p&gt;&lt;h3 id="describing-an-elephant-from-a-pin-prick"&gt;Describing an Elephant from a Pin Prick&lt;/h3&gt;&lt;p&gt;Suppose you are given a 7 billion parameter neural network with 50 layers and are asked to analyze it; how would you begin? The standard procedure would be to calculate relevant performance statistics. For instance, the accuracy on a suite of evaluation benchmarks. In certain situations, this may be sufficient. However, deep learning models are complex and multifaceted. Two computer vision models with the same accuracy may have very different generalization properties to out-of-distribution data, calibration, adversarial robustness, and other “secondary statistics” that are critical in many real-world applications. Beyond this, all evidence suggests that to build a complete scientific understanding of deep learning, we will need to venture beyond evaluation scores. Indeed, just as it is impossible to capture all the dimensions of humanity with a single numerical quantity (e.g., IQ, height), trying to understand a model by one or even several statistics alone is fundamentally limiting.&lt;/p&gt;&lt;p&gt;One difference between understanding a human and understanding a model is that we have easy access to all model parameters and all the individual computations that occur in a model. Indeed, by extracting a model’s hidden activations we can directly trace the process by which a model converts raw input into a prediction. Unfortunately, the world of hidden activations is far less hospitable than that of simple model performance statistics. Like the initial input, hidden activations are usually high dimensional, but unlike input data they are not structured in a form that humans can understand. If we venture into even higher dimensions, we can try to understand a model through its weights directly. Here, in the space of model weights, we have the freedom to move in millions to billions of orthogonal directions from a single starting point. How do we even begin to make sense of these worlds?&lt;/p&gt;&lt;p&gt;There is a well-known fable in which three blind men each feel a different part of an elephant. The description that each gives of the animal is completely different, reflecting only the body part that that man felt. We argue that unlike the blind men who can at least use their hand to feel a substantial part of one of the elephant’s body parts, current methods of analyzing the hidden activations and weights of a model are akin to trying to describe the elephant from the touch of a single pin.&lt;/p&gt;&lt;h3 id="tools-to-characterize-what-we-cannot-visualize"&gt;Tools to Characterize What We Cannot Visualize&lt;/h3&gt;&lt;p&gt;Despite the popular perception that mathematicians exclusively focus on solving problems, much of research mathematics involves understanding the right questions to ask in the first place. This is natural since many of the objects that mathematicians study are so far removed from everyday experience that we start with very limited intuition for what we can hope to actually understand. Substantial effort is often required to build up tools that will enable us to leverage our existing intuition and achieve tractable results that increase our understanding. The concept of a &lt;em&gt;rotation &lt;/em&gt;provides a nice example of this situation since these are very familiar in 2- and 3-dimensions, but become less and less accessible to everyday intuition as their dimension grows larger. In this latter case, the differing perspectives provided by pure mathematics become more and more important to gaining a more holistic perspective on what these actually are. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Those who know a little linear algebra will remember that rotations generalize to higher dimensions and that in $n$-dimensions they can be realized by $n \times n$ orthogonal matrices with determinant $1$. The set of these are commonly written as $SO(n)$ and called the &lt;em&gt;special orthogonal group&lt;/em&gt;. Suppose we want to understand the set of all $n$-dimensional rotations. There are many complementary approaches to doing this. We can explore the linear algebraic structure of all matrices in $SO(n)$ or study $SO(n)$ based on how each element behaves as an operator acting on $\mathbb{R}^n$.&lt;/p&gt;&lt;p&gt;Alternatively, we can also try to use our innate spatial intuition to understand $SO(n)$. This turns out to be a powerful perspective in math. In any dimension $n$, $SO(n)$ is a geometric object called a &lt;em&gt;manifold&lt;/em&gt;. Very roughly, a space that locally looks like Euclidean space, but which may have twists, holes, and other non-Euclidean features when we zoom out. Indeed, whether we make it precise or not, we all have a sense of whether two rotations are “close” to each other. For example, the reader would probably agree that $2$-dimensional rotations of $90^\circ$ and $91^\circ$ “feel” closer than rotations of $90^\circ$ and $180^\circ$. When $n=2$, one can show that the set of all rotations is geometrically “equivalent” to a $1$-dimensional circle. So, much of what we know about the circle can be translated to $SO(2)$.&lt;/p&gt;&lt;p&gt;What happens when we want to study the geometry of rotations in $n$-dimensions for $n &amp;gt; 3$? If $n = 512$ (a latent space for instance), this amounts to studying a manifold in $512^2$-dimensional space. Our visual intuition is seemingly useless here since it is not clear how concepts that are familiar in 2- and 3-dimensions can be utilized in $512^2$-dimensions. Mathematicians have been confronting the problem of understanding the un-visualizable for hundreds of years. One strategy is to find generalizations of familiar spatial concepts from $2$ and $3$-dimensions to $n$-dimensions that connect with our intuition.&lt;/p&gt;&lt;p&gt;This approach is already being used to better understand and characterize experimental observations about the space of model weights, hidden activations, and input data of deep learning models. We provide a taste of such tools and applications here:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Intrinsic Dimension: &lt;/em&gt;Dimension is a concept that is familiar not only from our experience in the spatial dimensions that we can readily access, 1-, 2-, and 3-dimensions, but also from more informal notions of “degrees of freedom” in everyday systems such as driving a car (forward/back, turning the steering wheel either left or right). The notion of dimension arises naturally in the context of machine learning where we may want to capture the number of independent ways in which a dataset, learned representation, or collection of weight matrices actually vary.&lt;p&gt;In formal mathematics, the definitions of dimension depend on the kind of space one is studying but they all capture some aspect of this everyday intuition. As a simple example, if I walk along the perimeter of a circle, I am only able to move forward and backward, and thus the dimension of this space is $1$. For spaces like the circle which are manifolds, dimension can be formally defined by the fact that a sufficiently small neighborhood around each point looks like a subset of some Euclidean space $\mathbb{R}^k$. We then say that the manifold is $k$-dimensional. If we zoom in on a small segment of the circle, it almost looks like a segment of $\mathbb{R} = \mathbb{R}^1$, and hence the circle is $1$-dimensional.&lt;/p&gt;&lt;p&gt;The manifold hypothesis posits that many types of data (at least approximately) live on a low-dimensional manifold even though they are embedded in a high-dimensional space. If we assume that this is true, it makes sense that the dimension of this underlying manifold, called the intrinsic dimension of the data, is one way to describe the complexity of the dataset. Researchers have estimated intrinsic dimension for common benchmark datasets, showing that intrinsic dimension appears to be correlated to the ease with which models generalize from training to test sets [6], and can explain differences in model performance and robustness in different domains such as medical images [7]. Intrinsic dimension is also a fundamental ingredient in some proposed explanations of data scaling laws [8, 9], which underlie the race to build ever bigger generative models.&lt;/p&gt;&lt;p&gt;Researchers have also noted that the intrinsic dimension of hidden activations tend to change in a characteristic way as information passes through the model [10, 11] or over the course of the diffusion process [12]. These and other insights have led to the use of intrinsic dimension in detection of adversarial examples [13], AI-generated content [14], layers where hidden activations contain the richest semantic content [11], and hallucinations in generative models [15].&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Curvature&lt;/em&gt;: While segments of the circle may look “straight” when we zoom up close enough, their curvature means that they will never be exactly linear as a straight line is. The notion of curvature is a familiar one and once formalized, it offers a way of rigorously measuring the extent to which the area around a point deviates from being linear. Care must be taken, however. Much of our everyday intuition about curvature assumes a single dimension. On manifolds with dimension $2$ or greater, there are multiple, linearly independent directions that we can travel away from a point and each of these may have a different curvature (in the $1$-dimensional sense). As a result, there are a range of different generalizations of curvature for higher-dimensional spaces, each with slightly different properties.&lt;p&gt;The notion of curvature has played a central role in deep learning, especially with respect to the loss landscape where changes in curvature have been used to analyze training trajectories [16]. Curvature is also central to an intriguing phenomenon known as the ‘edge of stability’, wherein the curvature of the loss landscape over the course of training increases as a function of learning rate until it hovers around the point where the training run is close to becoming unstable [17]. In another direction, curvature has been used to calculate the extent that model predictions change as input changes. For instance, [18] provided evidence that higher curvature in decision boundaries correlates with higher vulnerability to adversarial examples and suggested a new regularization term to reduce this. Finally, motivated by work in neuroscience, [19] presented a method that uses curvature to highlight interesting differences in representation between the raw training data and a neural network’s internal representation. A network may stretch and expand parts of the input space, generating regions of high curvature as it magnifies the representation of training examples that have a higher impact on the loss function.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Topology&lt;/em&gt;: Both dimension and curvature capture local properties of a space that can be measured by looking at the neighborhood around a single point. On the other hand, the most notable feature of our running example, the circle, is neither its dimension nor its curvature, but rather the fact that it is circular. We can only see this aspect by analyzing the whole space at once. Topology is the field of mathematics that focuses on such “global” properties.&lt;p&gt;Topological tools such as homology, which counts the number of holes in a space, has been used to illuminate the way that neural networks process data, with [20] showing that deep learning models “untangle” data distributions, reducing their complexity layer by layer. Versions of homology have also been applied to the weights of networks to better understand their structural features, with [21] showing that such topological statistics can reliably predict optimal early-stopping times. Finally, since topology provides frameworks that capture the global aspects of a space, it has proved a rich source of ideas for how to design networks that capture higher order relationships within data, leading to a range of generalizations of graph neural networks built on top of topological constructions [22, 23, 24, 25].&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;While the examples above have each been useful for gaining insight into phenomena related to deep learning, they were all developed to address challenges in other fields. We believe that a bigger payoff will come when the community uses the geometric paradigm described here to build new tools specifically designed to address the challenges that deep learning poses. Progress in this direction has already begun. Think for instance of linear mode connectivity which has helped us to better understand the loss landscape of neural networks [26] or work around the linear representation hypothesis which has helped to illuminate the way that concepts are encoded in the latent space of large language models [27]. One of the most exciting occurrences in mathematics is when the tools from one domain provide unexpected insight in another. Think of the discovery that Riemannian geometry provides some of the mathematical language needed for general relativity. We hope that a similar story will eventually be told for geometry and topology’s role in deep learning.&lt;/p&gt;&lt;h3 id="symmetries-in-data-symmetries-in-models"&gt;Symmetries in data, symmetries in models&lt;br /&gt;&lt;/h3&gt;&lt;p&gt;Symmetry is a central theme in mathematics, allowing us to break a problem into simpler components that are easier to solve. Symmetry has long played an important role in machine learning, particularly computer vision. In the classic dog vs. cat classification task for instance, an image that contains a dog continues to contain a dog regardless of whether we move the dog from one part of the image to another, whether we rotate the dog, or whether we reflect it. We say that the task is &lt;em&gt;invariant&lt;/em&gt; to image translation, rotation, and reflection.&lt;/p&gt;&lt;p&gt;The notion of symmetry is mathematically encoded in the concept of a &lt;em&gt;group&lt;/em&gt;, which is a set $G$ equipped with a binary operation $\star$ that takes two elements of $G$, $g_1$, $g_2$ as input and produces a third $g_1\star g_2$ as output. You can think of the integers $\mathbb{Z}$ with the binary operation of addition ($\star = +$) or the non-zero real numbers with the binary operation of multiplication ($\star = \times$). The set of $n$-dimensional rotations, $SO(n)$, also forms a group. The binary operation takes two rotations and returns a third rotation that is defined by simply applying the first rotation and then applying the second.&lt;/p&gt;&lt;p&gt;Groups satisfy axioms that ensure that they capture familiar properties of symmetries. For example, for any symmetry transformation, there should be an inverse operation that undoes the symmetry. If I rotate a circle by $90^{\circ}$, then I can rotate it back by $-90^{\circ}$ and return to where I started. Notice that not all transformations satisfy this property. For instance, there isn’t a well-defined inverse for downsampling an image. Many different images downsample to the same (smaller) image.&lt;/p&gt;&lt;p&gt;In the previous section we gave two definitions of $SO(n)$: the first was the geometric definition, as rotations of $\mathbb{R}^n$, and the second was as a specific subset of $n \times n$ matrices. While the former definition may be convenient for our intuition, the latter has the benefit that linear algebra is something that we understand quite well at a computational level. The realization of an abstract group as a set of matrices is called a &lt;em&gt;linear representation&lt;/em&gt; and it has proven to be one of the most fruitful methods of studying symmetry. It is also the way that symmetries are usually leveraged when performing computations (for example, in machine learning).&lt;/p&gt;&lt;p&gt;We saw a few examples of symmetries that can be found in the data of a machine learning task, such as the translation, rotation, and reflection symmetries in computer vision problems. Consider the case of a segmentation model. If one rotates an input image by $45^{\circ}$ and then puts it through the model, we will hope that we get a $45^{\circ}$ rotation of the segmentation prediction for the un-rotated image (this is illustrated in 1). After all, we haven’t changed the content of the image.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="323" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd1UiuD880gmdVrtjEKvGPBHIr0dvdBsrLXAnxUFz6_KQNLyMekhxrSR2ROn-H8O3780yoKbJvF0tUEVZSEdsuDfbB7kSGw_CFCqsKzjC6-wpxN5dxLQd-e4g7qMsKnc8BCX1pw7Qh0-I9hsgY9EInhpROs?key=h8RUuDFEFKnzGnrKx9gMkg" width="534" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 2: The concept of rotation equivariance illustrated for a segmentation model. One gets the same output regardless of whether one rotates first and then applies the network or applies the network and then rotates.&lt;/em&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="273" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe_QPixL6292p6Bz5yj_Hep_ypc6qrj-3q3Y7uIte0R5Nsc2ZPxNmSJOheOHohJY_0VbDi3LlyNSR61t94bHDfgTnJx0ssvyzU9KMtGLUKoqoiviKKTxpZR77Bb8VIzhkzd0Vxhspif10w8DnS3eWbjqwhW?key=h8RUuDFEFKnzGnrKx9gMkg" width="522" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 3: Equivariance holds when taking the top path (applying the network first and then the symmetry action) gives the same result as taking the bottom path (applying the symmetry transformation and then the network).&lt;/em&gt;&lt;/p&gt;&lt;p&gt;This property of a function (including neural networks), that applying a symmetry transformation before the function yields the same result as applying the symmetry transformation after the function is called &lt;em&gt;equivariance&lt;/em&gt; and can be captured by the diagram in Figure 3. The key point is that we get the same result whether we follow the upper path (applying the network first and then applying the group action) as when we follow the lower path (applying the group first and then applying the network). Conveniently, the concept of invariance, where applying a symmetry operation to input has no effect on the output of the function is a special case of equivariance where the action on the output space is defined to be trivial (applying symmetry actions does nothing).&lt;/p&gt;&lt;p&gt;Invariance and equivariance in deep learning models can be beneficial for a few reasons. Firstly, such a model will yield more predictable and consistent results across symmetry transformations. Secondly, through equivariance we can sometimes simplify the learning process with fewer parameters (compare the number of parameters in a convolutional neural network and an MLP of similar performance) and fewer modes of variation to learn in the data (a rotation invariant image classifier only needs to learn one orientation of each object rather than all possible orientations).&lt;/p&gt;&lt;p&gt;But how do we ensure that our model is equivariant? One way is to build our network with layers that are equivariant by design. By far the most well-known example of this is the convolutional neural network, whose layers are (approximately) equivariant to image translation. This is one reason why using a convolutional neural network for dog vs cat classification doesn’t require learning to recognize a dog at every location in an image as it might with an MLP. With a little thought, one can often come up with layers which are equivariant to a specific group. Unfortunately, being constrained to equivariant layers that we find in an ad-hoc manner often leaves us with a network with built-in equivariance but limited expressivity.&lt;/p&gt;&lt;p&gt;Fortunately, for most symmetry groups arising in machine learning, representation theory offers a comprehensive description of all possible linear equivariant maps. Indeed, it is a beautiful mathematical fact that all such maps are built from atomic building blocks called &lt;em&gt;irreducible representations&lt;/em&gt;. Happily, in many cases, the number of these irreducible representations is finite. Understanding the irreducible representations of a group can be quite powerful. Those familiar with the ubiquitous discrete Fourier transform (DFT) of a sequence of length $n$ are already familiar with the irreducible representations of one group, the cyclic group generated by a rotation by $360 ^{\circ}/n$ (though we note that moving between the description we give here and the description of the DFT found in the signal processing literature takes a little thought).&lt;/p&gt;&lt;p&gt;There is now a rich field of research in deep learning that uses group representations to systematically build expressive equivariant architectures. Some examples of symmetries that have been particularly well-studied include: rotation and reflection of images [28, 29, 30, 31], 3-dimensional rotation and translation of molecular structures [32] or point clouds [33], and permutations for learning on sets [34] or nodes of a graph [35]. Encoding equivariance to more exotic symmetries has also proven useful for areas such as theoretical physics [36] and data-driven optimization [37].&lt;/p&gt;&lt;p&gt;Equivariant layers and other architectural approaches to symmetry awareness are a prime example of using mathematics to inject high-level priors into a model. Do these approaches represent the future of learning in the face of data symmetries? Anecdotally, the most common approach to learning on data with symmetries continues to be using enough training data and enough data augmentation for the model to learn to handle the symmetries on its own. Two years ago, the author would have speculated that these latter approaches only work for simple cases, such as symmetries in 2-dimensions, and will be outperformed by models which are equivariant by design when symmetries become more complex. Yet, we continue to be surprised by the power of scale. After all, AlphaFold3 [38] uses a non-equivariant architecture despite learning on data with several basic symmetries. We speculate that there may be a threshold on the ratio of symmetry complexity on the one hand and the amount of training data on the other, that determines whether built-in equivariance will outperform learned equivariance [39, 40].&lt;br /&gt;&lt;/p&gt;&lt;p&gt;If this is true, we can expect to see models move away from bespoke equivariant architectures as larger datasets become available for a specific application. At the same time, since compute will always be finite, we predict that there will be some applications with exceptionally complex symmetries that will always require some built-in priors (for example, AI for math or algorithmic problems). Regardless of where we land on this spectrum, mathematicians can look forward to an interesting comparison of the ways humans inject symmetry into models vs the way that models learn symmetries on their own [41, 42].&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="129" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcJa_1Ow2zrMVuk1hJiOJtTJBq5bH7zyogibZ5fqQu85ERGFEjcX4jRn7r_rnZvTrCdpN5OzeVQUBLu60DJP_aIR4uoHq33tRMcoPAUf7qumOeJLreCkttvqtQEssCh90UwlbWkzBoK79FV54R6ncO2c_Ij?key=h8RUuDFEFKnzGnrKx9gMkg" width="572" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 4: A cartoon illustrating why adding a permutation and its inverse before and after a pointwise nonlinearity produces an equivalent model (even though the weights will be different). Since permutations can be realized by permutation matrices, the crossed arrows on the right can be merged into the fully-connected layer.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Of course, symmetry is not only present in data but also in the models themselves. For instance, the activations of hidden layers of a network are invariant to permutation. We can permute activations before entering the non-linearity and if we un-permute them afterward, the model (as a function) does not change (Figure 4). This means that we have an easy recipe for generating an exponentially large number of networks that have different weights but behave identically on data. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;While simple, this observation produces some unexpected results. There is evidence, for instance, that while the loss landscape of neural networks is highly non-convex, it may be much less non-convex when we consider all networks that can be produced through this permutation operation as equivalent [43, 44]. This means that your network and my network may not be connected by a linear path of low loss, but such a path may exist between your network and a permutation of my network. Other research has looked at whether it may be possible to use symmetries to accelerate optimization by ‘teleporting’ a model to a more favorable location in the loss landscape [45, 46]. Finally, permutation symmetries also provide one type of justification for an empirical phenomenon where individual neurons in a network tend to encode more semantically meaningful information than arbitrary linear combinations of such neurons [47].&lt;/p&gt;&lt;h3 id="taming-complexity-with-abstraction"&gt;Taming Complexity with Abstraction&lt;/h3&gt;&lt;p&gt;When discussing symmetry, we used the diagram in Figure 3 to define equivariance. One of the virtues of this approach is that we never had to specify details about the input data or architecture that we used. The spaces could be vector spaces and the maps linear transformations, they could be neural networks of a specific architecture, or they could just be sets and arbitrary functions between them–the definition is valid for each. This &lt;em&gt;diagrammatic&lt;/em&gt; point of view, which looks at mathematical constructions in terms of the composition of maps between objects rather than the objects themselves, has been very fruitful in mathematics and is one gateway to the subject known as &lt;em&gt;category theory&lt;/em&gt;. Category theory is now the lingua franca in many areas of mathematics since it allows mathematicians to translate definitions and results across a wide range of contexts.&lt;/p&gt;&lt;p&gt;Of course, deep learning is at its core all about function composition, so it is no great leap to try and connect it to the diagrammatic tradition in mathematics. The focus of function composition in the two disciplines is different, however. In deep learning we take simple layers that alone lack expressivity and compose them together to build a model capable of capturing the complexity of real-world data. With this comes the tongue-in-cheek demand to “stack more layers!”. Category theory instead tries to find a universal framework that captures the essence of structures appearing throughout mathematics. This allows mathematicians to uncover connections between things that look very different at first glance. For instance, category theory gives us the language to describe how the topological structure of a manifold can be encoded in groups via homology or homotopy theory.&lt;/p&gt;&lt;p&gt;It can be an interesting exercise to try to find a diagrammatic description of familiar constructions like the product of two sets $X$ and $Y$. Focusing our attention on maps rather than objects we find that what characterizes $X \times Y$ is the existence of the two canonical projections $\pi_1$ and $\pi_2$, the former sending $(x,y) \mapsto x$ and $(x,y) \mapsto y$ (at least in more familiar settings where $X$ and $Y$ are, for example, sets). Indeed, the &lt;em&gt;product &lt;/em&gt;$X \times Y$ (regardless of whether $X$ and $Y$ are sets, vectors spaces, etc.) is the unique object such that for any $Z$ with maps $f_1: Z \rightarrow X$ and $f_2: Z \rightarrow Y$, there is a map $h: Z \rightarrow X \times Y$ that satisfies the commutative diagram in Figure 5.&lt;/p&gt;&lt;p&gt;While this construction is a little involved for something as familiar as a product it has the remarkable property that it allows us to define a “product” even when there is no &amp;nbsp;underlying set structure (that is, those settings where we cannot resort to defining $X \times Y$ as the set of pairs of $(x,y)$ for $x \in X$ and $y \in Y$).&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="277" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd0afUER-JEqT9BBW0z5ip7HSPD_ORKlpxTaQQFpep7MZF3DKfhgca3XbrZ2aGGTTnxcyOD3csHF1hdODeSXFx-nC63Mlw2etuY9xtM-AUvec4aIZKJK0hl2QiuxyJPzmlr18GbJA?key=h8RUuDFEFKnzGnrKx9gMkg" width="624" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 5: The commutative diagram that describes a product $X \times Y$. For any $Z$ with maps $f_1: Z \rightarrow X$ and $f_2: Z \rightarrow Y$, there exists a unique map $h: Z \rightarrow X \times Y$ such that $f_1 = \pi_1 \circ h$ and $f_2 = \pi_2 \circ h$ where $\pi_1$ and $\pi_2$ are the usual projection maps from $X \times Y$ to $X$ and $X \times Y$ to $Y$ respectively.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One can reasonably argue that diagrammatic descriptions of well-known constructions, like products, are not useful for the machine learning researcher. After all, we already know how to form products in all of the spaces that come up in machine learning. On the other hand, there are more complicated examples where diagrammatics mesh well with the way we build neural network architectures in practice.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="352" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXf30eZdcoTcrjBuYEo6BUjm4gmw8fvcY9kLpDsspW0qPoIVu6LN5mfd1ks5qiMtf9J1DyPNDtzDDLDpxVi7n5j62DxlIfkwyo5V4gAC7MeeMpUaDzOMgsU4Mqjrs7fUXL-hc_BeqPd9Upu6L0wmKnDzkop4?key=h8RUuDFEFKnzGnrKx9gMkg" width="624" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 6: Fiber bundles capture the notion that a space might locally look like a product but globally have twists in it.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Fiber bundles are a central construction in geometry and topology that capture the notion that a space may locally look like a product but may have twists that break this product structure globally. Compare the cylinder with the Möbius band. We can build both of these by starting with a circle and taking a product with the line segment $(0,1)$. In the case of the cylinder, this really is just (topologically) the product of the circle and the segment $(0,1)$, but to form the Möbius band we must add an additional twist that breaks the product structure. In these examples, the circle is called the &lt;em&gt;base &lt;/em&gt;space and $(0,1)$ is called the &lt;em&gt;fiber&lt;/em&gt;. While only the cylinder is a true product, both the cylinder and the Möbius band are fiber bundles. Here is another way of thinking about a fiber bundle. A fiber bundle is a union of many copies of the fiber parametrized by the base space. In the Möbius band/cylinder example, each point on the circle carries its own copy of $(0,1)$.&lt;/p&gt;&lt;p&gt;We drew inspiration from this latter description of fiber bundles when we were considering a conditional generation task in the context of a problem in materials science. Since the materials background is somewhat involved, we’ll illustrate the construction via a more pedestrian, animal-classification analogue. Let $M$ be the manifold of all possible images containing a single animal. We can propose to decompose the variation in elements of $M$ into two parts, the species of animal in the image and everything else, where the latter could mean differences in background, lighting, pose, image quality, etc. One might want to explore the distribution of one of these factors of variation while fixing the other. For instance, we might want to fix the animal species and explore the variation we get in background, pose, etc. For example, comparing the variation in background for two different species of insect may tell the entomologist about the preferred habitat for different types of beetles.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="421" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfLfnOQw_uLzm58bcucM5zOzGLHKzbX8hyQU2muIPl994v1GQN0sfMwQgSjFwsaCDetRHW8WR_T71pjNX7waqch44PwUY6Dv8egfzRlOmo6e0BbDagYv99K6tMnvVeTAIb9ww9bT_3Ukcs4k7xHx-BH7cxR?key=h8RUuDFEFKnzGnrKx9gMkg" width="624" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 7: A cartoon visualizing how the set of all animal images could be decomposed into a local product of animal species and other types of variation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;One might hope to solve this problem by learning an encoding of $M$ into a product space $X_1 \times X_2$ where $X_1$ is a discrete set of points corresponding to animal species and $X_2$ is a space underlying the distribution of all other possible types of variation for a fixed species of animal. Fixing the species would then amount to choosing a specific element $x_1$ from $X_1$ and sampling from the distribution on $X_2$. The product structure of $X_1 \times X_2$ allows us to perform such independent manipulations of $X_1$ and $X_2$. On the other hand, products are rigid structures that impose strong, global topological assumptions on the real data distribution. We found that even on toy problems, it was hard to learn a good map from the raw data distribution to the product-structured latent space defined above. Given that fiber bundles are more flexible and still give us the properties we wanted from our latent space, we designed a neural network architecture to learn a fiber bundle structure on a data distribution [48].&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="240" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdrgVDyhD0JXBm12Gh1NstAjVx3fSk8vM3Mg_3JGi6JpK3PYTWUpmgzW_BgmEOMeZahkdrzWEw2ThViUKXnEGFobRORcOMgifUin2kJY3-zFIq4fbj-4QO6x7ALnwn5qLU880r1raMaFC2yqn6RVyDPGEk?key=h8RUuDFEFKnzGnrKx9gMkg" width="415" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 8: The commutative diagram describing a fiber bundle. The map $\pi$ projects from neighborhoods of the total space to the base space, $U$ is a local neighborhood of the base space, and $F$ &amp;nbsp;is the fiber. The diagram says that each point in the base space has a neighborhood $U$ &amp;nbsp;such that when we lift this to the bundle, we get something that is homeomorphic (informally, equivalent) to the product of the neighborhood &amp;nbsp;and the fiber. But this product structure may not hold globally over the whole space.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;But how do we go from the abstract definition of a fiber bundle above to a neural network architecture that we can code up on a computer. It turns out there is a succinct diagrammatic definition of a fiber bundle (Figure 8) that can serve as a convenient template to build up an architecture from. We were able to proceed in a relatively naïve fashion, taking each of the maps in the diagram and building a corresponding stack of layers. The diagram itself then told us how to compose each of these components together. The commutativity of the diagram was engineered through a term in the loss function that ensures that $\pi = \text{proj}_1 \circ \varphi$. There were also some conditions on $\varphi$ and $\pi$ (such as the bijectivity of $\phi$) that needed to be engineered. Beyond this, we were surprised at the amount of flexibility we had. This is useful since it means this process is largely agnostic to data modality.&lt;/p&gt;&lt;p&gt;This is an elementary example of how the diagrammatic tradition in mathematics can provide us with a broader perspective on the design of neural networks, allowing us to connect deep structural principles with large-scale network design without having to specify small-scale details that might be problem dependent. Of course, all this fails to draw from anything beyond the surface of what the categorical perspective has to offer. Indeed, category theory holds promise as a unified framework to connect much of what appears and is done in machine learning [49].&lt;/p&gt;&lt;h3 id="conclusion"&gt;Conclusion&lt;br /&gt;&lt;/h3&gt;&lt;p&gt;In the mid-twentieth century, Eugene Wigner marveled at the “the unreasonable effectiveness of mathematics” as a framework for not only describing existing physics but also anticipating new results in the field [50]. A mantra more applicable to recent progress in machine learning is “the unreasonable effectiveness of data” [51] and compute. This could appear to be a disappointing situation for mathematicians who might have hoped that machine learning would be as closely intertwined to advanced mathematics as physics is. However, as we’ve demonstrated, while mathematics may not maintain the same role in machine learning research that it has held in the past, the success of scale actually opens new paths for mathematics to support progress in machine learning research. These include:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Providing powerful tools for deciphering the inner workings of complex models&lt;/li&gt;&lt;li&gt;Offering a framework for high-level architectural decisions that leave the details to the learning algorithm&lt;/li&gt;&lt;li&gt;Bridging traditionally isolated domains of mathematics like topology, abstract algebra, and geometry with ML and data science applications.&lt;br /&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Should the way things have turned out surprise us? Perhaps not, given that machine learning models ultimately reflect the data they are trained on and in most cases this data comes from fields (such as natural language or imagery) which have long resisted parsimonious mathematical models. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Yet, this situation is also an opportunity for mathematics. Performant machine learning models may provide a gateway for mathematical analysis of a range of fields that were previously inaccessible. It’s remarkable for instance that trained word embeddings transform semantic relationships into algebraic operations on vectors in Euclidean space (for instance, ‘Italian’ - ‘Italy’ + ‘France’ = ‘French’). Examples like this hint at the potential for mathematics to gain a foothold in complex, real-world settings by studying the machine learning models that have trained on data from these settings.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;As more and more of the data in the world is consumed and mathematicised by machine learning models, it will be an increasingly interesting time to be a mathematician. The challenge now lies in adapting our mathematical toolkit to this new landscape, where empirical breakthroughs often precede theoretical understanding. By embracing this shift, mathematics can continue to play a crucial, albeit evolving, role in shaping the future of machine learning.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;The author would like to thank Darryl Hannan for help with figures, Davis Brown, Charles Godfrey, and Scott Mahan for useful feedback on drafts, as well as the staff of the Gradient for useful conversations and help editing this article. For resources and events around the growing community of mathematicians and computer scientists using topology, algebra, and geometry (TAG) to better understand and build more robust machine learning systems, please visit us at &lt;/em&gt;&lt;em&gt;https://www.tagds.com&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;p&gt;[1] Richard Sutton. "The bitter lesson". In: &lt;em&gt;Incomplete Ideas (blog)&lt;/em&gt; 13.1 (2019), p. 38.&lt;/p&gt;&lt;p&gt;[2] Guido F Montufar et al. "On the number of linear regions of deep neural networks". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 27 (2014).&lt;/p&gt;&lt;p&gt;[3] Boris Hanin and David Rolnick. "Complexity of linear regions in deep networks". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2019, pp. 2596–2604.&lt;/p&gt;&lt;p&gt;[4] J Elisenda Grigsby and Kathryn Lindsey. "On transversality of bent hyperplane arrangements and the topological expressiveness of ReLU neural networks". In: &lt;em&gt;SIAM Journal on Applied Algebra and Geometry&lt;/em&gt; 6.2 (2022), pp. 216–242.&lt;/p&gt;&lt;p&gt;[5] Ahmed Imtiaz Humayun et al. "Splinecam: Exact visualization and characterization of deep network geometry and decision boundaries". In: &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2023, pp. 3789–3798.&lt;/p&gt;&lt;p&gt;[6] Phillip Pope et al. "The intrinsic dimension of images and its impact on learning". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2104.08894 (2021).&lt;/p&gt;&lt;p&gt;[7] Nicholas Konz and Maciej A Mazurowski. "The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2401.08865 (2024).&lt;/p&gt;&lt;p&gt;[8] Yasaman Bahri et al. "Explaining neural scaling laws". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2102.06701 (2021).&lt;/p&gt;&lt;p&gt;[9] Utkarsh Sharma and Jared Kaplan. "A neural scaling law from the dimension of the data manifold". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2004.10802 (2020).&lt;/p&gt;&lt;p&gt;[10] Alessio Ansuini et al. "Intrinsic dimension of data representations in deep neural networks". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 32 (2019).&lt;/p&gt;&lt;p&gt;[11] Lucrezia Valeriani et al. "The geometry of hidden representations of large transformer models". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 36 (2024).&lt;/p&gt;&lt;p&gt;[12] Henry Kvinge, Davis Brown, and Charles Godfrey. "Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension". In: &lt;em&gt;ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;[13] Xingjun Ma et al. "Characterizing adversarial subspaces using local intrinsic dimensionality". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1801.02613 (2018).&lt;/p&gt;&lt;p&gt;[14] Peter Lorenz, Ricard L Durall, and Janis Keuper. "Detecting images generated by deep diffusion models using their local intrinsic dimensionality". In: &lt;em&gt;Proceedings of the IEEE/CVF International Conference on Computer Vision&lt;/em&gt;. 2023, pp. 448–459.&lt;/p&gt;&lt;p&gt;[15] Fan Yin, Jayanth Srinivasa, and Kai-Wei Chang. "Characterizing truthfulness in large language model generations with local intrinsic dimension". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2402.18048 (2024).&lt;/p&gt;&lt;p&gt;[16] Justin Gilmer et al. "A loss curvature perspective on training instabilities of deep learning models". In: &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2021.&lt;/p&gt;&lt;p&gt;[17] Jeremy Cohen et al. "Gradient descent on neural networks typically occurs at the edge of stability". In: &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2020.&lt;/p&gt;&lt;p&gt;[18] Seyed-Mohsen Moosavi-Dezfooli et al. "Robustness via curvature regularization, and vice versa". In: &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2019, pp. 9078–9086.&lt;/p&gt;&lt;p&gt;[19] Francisco Acosta et al. "Quantifying extrinsic curvature in neural manifolds". In: &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2023, pp. 610–619.&lt;/p&gt;&lt;p&gt;[20] Gregory Naitzat, Andrey Zhitnikov, and Lek-Heng Lim. "Topology of deep neural networks". In: &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 21.184 (2020), pp. 1–40.&lt;/p&gt;&lt;p&gt;[21] Bastian Rieck et al. "Neural persistence: A complexity measure for deep neural networks using algebraic topology". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1812.09764 (2018).&lt;/p&gt;&lt;p&gt;[22] Mustafa Hajij, Kyle Istvan, and Ghada Zamzmi. "Cell complex neural networks". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2010.00743 (2020).&lt;/p&gt;&lt;p&gt;[23] Cristian Bodnar. "Topological deep learning: graphs, complexes, sheaves". PhD thesis. 2023.&lt;/p&gt;&lt;p&gt;[24] Jakob Hansen and Robert Ghrist. "Toward a spectral theory of cellular sheaves". In: &lt;em&gt;Journal of Applied and Computational Topology&lt;/em&gt; 3.4 (2019), pp. 315–358.&lt;/p&gt;&lt;p&gt;[25] Yifan Feng et al. "Hypergraph neural networks". In: &lt;em&gt;Proceedings of the AAAI Conference on Artificial Intelligence&lt;/em&gt;. Vol. 33. 01. 2019, pp. 3558–3565.&lt;/p&gt;&lt;p&gt;[26] Felix Draxler et al. "Essentially no barriers in neural network energy landscape". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2018, pp. 1309–1318.&lt;/p&gt;&lt;p&gt;[27] Kiho Park, Yo Joong Choe, and Victor Veitch. "The linear representation hypothesis and the geometry of large language models". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2311.03658 (2023).&lt;/p&gt;&lt;p&gt;[28] Taco Cohen and Max Welling. "Group equivariant convolutional networks". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2016, pp. 2990–2999.&lt;/p&gt;&lt;p&gt;[29] Maurice Weiler, Fred A Hamprecht, and Martin Storath. "Learning steerable filters for rotation equivariant cnns". In: &lt;em&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2018, pp. 849–858.&lt;/p&gt;&lt;p&gt;[30] Daniel E Worrall et al. "Harmonic networks: Deep translation and rotation equivariance". In: &lt;em&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2017, pp. 5028–5037.&lt;/p&gt;&lt;p&gt;[31] Diego Marcos et al. "Rotation equivariant vector field networks". In: &lt;em&gt;Proceedings of the IEEE International Conference on Computer Vision&lt;/em&gt;. 2017, pp. 5048–5057.&lt;/p&gt;&lt;p&gt;[32] Alexandre Duval et al. "A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2312.07511 (2023).&lt;/p&gt;&lt;p&gt;[33] Nathaniel Thomas et al. "Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1802.08219 (2018).&lt;/p&gt;&lt;p&gt;[34] Manzil Zaheer et al. "Deep sets". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 30 (2017).&lt;/p&gt;&lt;p&gt;[35] Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. "E (n) equivariant graph neural networks". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2021, pp. 9323–9332.&lt;/p&gt;&lt;p&gt;[36] Denis Boyda et al. "Sampling using SU (N) gauge equivariant flows". In: &lt;em&gt;Physical Review D&lt;/em&gt; 103.7 (2021), p. 074504.&lt;/p&gt;&lt;p&gt;[37] Hannah Lawrence and Mitchell Tong Harris. "Learning Polynomial Problems with SL(2,\mathbb {R}) −Equivariance". In: &lt;em&gt;The Twelfth International Conference on Learning Representations&lt;/em&gt;. 2023.&lt;/p&gt;&lt;p&gt;[38] Josh Abramson et al. "Accurate structure prediction of biomolecular interactions with AlphaFold 3". In: &lt;em&gt;Nature&lt;/em&gt; (2024), pp. 1–3.&lt;/p&gt;&lt;p&gt;[39] Scott Mahan et al. "What Makes a Machine Learning Task a Good Candidate for an Equivariant Network?" In: &lt;em&gt;ICML 2024 Workshop on Geometry-grounded Representation Learning and Generative Modeling&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;[40] Johann Brehmer et al. "Does equivariance matter at scale?" In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2410.23179 (2024).&lt;/p&gt;&lt;p&gt;[41] Chris Olah et al. "Naturally Occurring Equivariance in Neural Networks". In: &lt;em&gt;Distill&lt;/em&gt; (2020). https://distill.pub/2020/circuits/equivariance. doi: 10.23915/distill.00024.004.&lt;/p&gt;&lt;p&gt;[42] Giovanni Luca Marchetti et al. "Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2312.08550 (2023).&lt;/p&gt;&lt;p&gt;[43] Rahim Entezari et al. "The role of permutation invariance in linear mode connectivity of neural networks". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2110.06296 (2021).&lt;/p&gt;&lt;p&gt;[44] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. "Git re-basin: Merging models modulo permutation symmetries". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2209.04836 (2022).&lt;/p&gt;&lt;p&gt;[45] Bo Zhao et al. "Symmetry teleportation for accelerated optimization". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 35 (2022), pp. 16679–16690.&lt;/p&gt;&lt;p&gt;[46] Bo Zhao et al. "Improving Convergence and Generalization Using Parameter Symmetries". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2305.13404 (2023).&lt;/p&gt;&lt;p&gt;[47] Charles Godfrey et al. "On the symmetries of deep learning models and their internal representations". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 35 (2022), pp. 11893–11905.&lt;/p&gt;&lt;p&gt;[48] Nico Courts and Henry Kvinge. "Bundle Networks: Fiber Bundles, Local Trivializations, and a Generative Approach to Exploring Many-to-one Maps". In: &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2021.&lt;/p&gt;&lt;p&gt;[49] Bruno Gavranović et al. "Position: Categorical Deep Learning is an Algebraic Theory of All Architectures". In: &lt;em&gt;Forty-first International Conference on Machine Learning&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;[50] Eugene P Wigner. "The unreasonable effectiveness of mathematics in the natural sciences". In: &lt;em&gt;Mathematics and Science&lt;/em&gt;. World Scientific, 1990, pp. 291–306.&lt;/p&gt;&lt;p&gt;[51] Alon Halevy, Peter Norvig, and Fernando Pereira. "The unreasonable effectiveness of data". In: &lt;em&gt;IEEE Intelligent Systems&lt;/em&gt; 24.2 (2009), pp. 8–12.&lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[What's Missing From LLM Chatbots: A Sense of Purpose]]&amp;gt;LLM-based chatbots’ capabilities have been advancing every month. These improvements are mostly measured by benchmarks like MMLU, HumanEval, and MATH (e.g. sonnet 3.5, gpt-4o). However, as these measures get more and more saturated, is user experience increasing in proportion to these scores? If we envision a future]]&amp;gt;https://thegradient.pub/dialog/66c6733993571d5c8c154fb1Mon, 09 Sep 2024 17:28:48 GMT&lt;p&gt;LLM-based chatbots’ capabilities have been advancing every month. These improvements are mostly measured by benchmarks like MMLU, HumanEval, and MATH (e.g. sonnet 3.5, gpt-4o). However, as these measures get more and more saturated, is user experience increasing in proportion to these scores? If we envision a future of human-AI collaboration rather than AI replacing humans, the current ways of measuring dialogue systems may be insufficient because they measure in a non-interactive fashion.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Why does purposeful dialogue matter?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Purposeful dialogue refers to a multi-round user-chatbot conversation that centers around a goal or intention. The goal could range from a generic one like “harmless and helpful” to more specific roles like “travel planning agent”, “psycho-therapist” or “customer service bot.”&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Travel planning is a simple, illustrative example. Our preferences, fellow travelers’ preference, and all the complexities of real-world situations make transmitting all information in one pass way too costly. However, if multiple back-and-forth exchanges of information are allowed, only important information gets selectively exchanged. Negotiation theory offers an analogy of this—iterative bargaining yields better outcomes than a take-it-or-leave-it offer. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;In fact, sharing information is only one aspect of dialogue. In Terry Winograd’s words: “All language use can be thought of as a way of activating procedures within the hearer.” We can think of each utterance as a deliberate action that one party takes to alter the world model of the other. What if both parties have more complicated, even hidden goals? In this way, purposeful dialogue provides us with a way of formulating human-AI interactions as a collaborative game, where the goal of chatbot is to help humans achieve certain goals. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;This might seem like an unnecessary complexity that is only a concern for academics. However, purposeful dialogue could be beneficial even for the most hard-nosed, product-oriented research direction like code generation. Existing coding benchmarks mostly measure performances in a one-pass generation setting; however, for AI to automate solving ordinary Github issues (like in SWE-bench), it’s unlikely to be achieved by a single action—the AI needs to communicate back and forth with human software engineers to make sure it understands the correct requirements, ask for missing documentation and data, and even ask humans to give it a hand if needed. In a similar vein to pair programming, this could reduce the defects of code but without the burden of increasing man-hours. &lt;/p&gt;&lt;p&gt;&lt;br /&gt;Moreover, with the introduction of turn-taking, many new possibilities can be unlocked. As interactions become long-term and memory is built, the chatbot can gradually update user profiles. It can also adapt to their preferences. Imagine a personal assistant (e.g., IVA, Siri) that, through daily interaction, learns your preferences and intentions. It can read your resources of new information automatically (e.g., twitter, arxiv, Slack, NYT) and provide you with a morning news summary according to your preferences. It can draft emails for you and keep improving by learning from your edits.&lt;/p&gt;&lt;p&gt;In a nutshell, meaningful interactions between people rarely begin with complete strangers and conclude in just one exchange. Humans naturally interact with each other through multi-round dialogues and adapt accordingly throughout the conversation. However, doesn’t that seem exactly the opposite of predicting the next token, which is the cornerstone of modern LLMs? Below, let’s take a look at the makings of dialogue systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;How were/are dialogue systems made?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Let's jump back to the 1970s, when Roger Schank introduced his "restaurant script" as a kind of dialogue system [1]. This script breaks down the typical restaurant experience into steps like entering, ordering, eating, and paying, each with specific scripted utterances. Back then, every piece of dialogue in these scenarios was carefully planned out, enabling AI systems to mimic realistic conversations. ELIZA, a Rogerian psychotherapist simulator, and PARRY, a system mimicking a paranoid individual, were two other early dialogue systems until the dawn of machine learning.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Compare this approach to the LLM-based dialogue system today, it seems mysterious how models trained to predict the next token could do anything at all with engaging in dialogues. Therefore, let’s take a close examination of how dialogue systems are made, with an emphasis on how the dialogue format comes into play:&lt;/p&gt;&lt;p&gt;(1) Pretraining: a sequence model is trained to predict the next token on a gigantic corpus of mixed internet texts. The compositions may vary but they are predominantly news, books, Github code, with a small blend of forum-crawled data such as from Reddit, Stack Exchange, which may contain dialogue-like data.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="288" src="https://thegradient.pub/content/images/2024/08/unnamed.png" width="512" /&gt;&lt;figcaption&gt;Table of the pretraining data mixture from llama technical report&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;(2) Introduce dialogue formatting: because the sequence model only processes strings, while the most natural representation of dialogue history is a structured index of system prompts and past exchanges, a certain kind of formatting must be introduced for the purpose of conversion. Some Huggingface tokenizers provide this method called tokenizer.apply_chat_template for the convenience of users. The exact formatting differs from model to model, but it usually involves guarding the system prompts with &amp;lt;system&amp;gt; or &amp;lt;INST&amp;gt; in the hope that the pretrained model could allocate more attention weights to them. The system prompt plays a significant role in adapting language models to downstream applications and ensuring its safe behavior (we will talk more in the next section). Notably, the choice of the format is arbitrary at this step—pretraining corpus doesn’t follow this format.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="112" src="https://thegradient.pub/content/images/2024/08/image1.png" width="1398" /&gt;&lt;figcaption&gt;The context window of a chatbot&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;(3) RLHF: In this step, the chatbot is directly rewarded or penalized for generating desired or undesired answers. It’s worth noting that this is the first time the introduced dialogue formatting appears in the training data. RLHF is a &lt;em&gt;fine&lt;/em&gt;-tuning step not only because the data size is dwarfed in comparison to the pretraining corpus, but also due to the KL penalty and targeted weight tuning (e.g. Lora). Using Lecun’s analogy of cake baking, RLHF is only the small cherry on the top.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="580" src="https://thegradient.pub/content/images/2024/08/image5.png" width="1440" /&gt;&lt;figcaption&gt;Image from Yann Lecun’s slides&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2 id="how-consistent-are-existing-dialogue-systems-in-2024"&gt;How consistent are existing dialogue systems (in 2024)? &lt;br /&gt;&lt;/h2&gt;&lt;p&gt;The minimum requirement we could have for a dialogue system is that it can stay on the task we gave them. In fact, we humans often drift from topic to topic. How well do current systems perform? &lt;/p&gt;&lt;p&gt;&lt;br /&gt;Currently, “system prompt” is the main method that allows users to control LM behavior. However, researchers found evidence that LLMs can be brittle in following these instructions under adversarial conditions [12,13]. Readers might also have experienced this through daily interactions with ChatGPT or Claude—when a new chat window is freshly opened, the model can follow your instruction reasonably well [2], but after several rounds of dialogue, it’s no longer &lt;em&gt;fresh&lt;/em&gt;, even stops following its role altogether.&lt;/p&gt;&lt;p&gt;How could we quantitatively capture this anecdote? For one-round instruction following, we’ve already enjoyed plenty of benchmarks such as MT-Bench and Alpaca-Eval. However, when we test models in an interactive fashion, it’s hard to anticipate what the model generates and prepare a reply in advance. In a project by my collaborators and me [3], we built an environment to synthesize dialogues with unlimited length to stress-test the instruction-following capabilities of LLM chatbots. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;To allow an unconstrained scaling on the time scale, we let two system-prompted LM agents chat with each other for an extended number of rounds. This forms the main trunk of dialogue [a1, b1, a2, b2, …, a8, b8] (say the dialogue is 8-round). At this point, we could probably figure out how the LLMs stick to its system prompts just by examining this dialogue, but many of the utterances can be irrelevant to the instructions, depending on where the conversation goes. Therefore, we hypothetically branch out at each round by asking a question directly related to the system prompts, and use a corresponding judging function to quantify how well it performs. All that's provided by the dataset is a bank of triplets of (system prompts, probe questions, and judging functions).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="570" src="https://thegradient.pub/content/images/2024/08/image3.png" width="1999" /&gt;&lt;figcaption&gt;Sketch of the process of measuring instruction stability&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Averaging across scenarios and pairs of system prompts, we get a curve of instruction stability across rounds. To our surprise, the aggregated results on both LLaMA2-chat-70B and gpt-3.5-turbo-16k are alarming. Besides the added difficulty to prompt engineering, the lack of instruction stability also comes with safety concerns. When the chatbot drifts away from its system prompts that stipulate safety aspects, it becomes more susceptible to jailbreaking and prone to more hallucinations.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="689" src="https://thegradient.pub/content/images/2024/08/Screenshot-2024-08-21-at-19.15.57.png" width="1736" /&gt;&lt;figcaption&gt;Instruction stability on LLaMA2-chat-70B and gpt-3.5-turbo-16k&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The empirical results also contrast with the ever-increasing context length of LLMs. Theoretically, some long-context models can attend to a window of up to 100k tokens. However, in the dialogue setting, they become distracted after only 1.6k tokens (assuming each utterance is 100 tokens). In [3], we further theoretically showed how this is inevitable in a Transformer based LM chatbot under the current prompting scheme, and proposed a simple technique called split-softmax to mitigate such effects. &lt;/p&gt;&lt;p&gt;One might ask at this point, why is it so bad? Why don't humans lose their persona just by talking to another person for 8 rounds? It’s arguable that human interactions are based on purposes and intentions [5] and these purposes precede the means rather than the opposite—LLM is fundamentally a fluent English generator, and the persona is merely a thin added layer.&lt;/p&gt;&lt;h2 id="what%E2%80%99s-missing"&gt;What’s missing? &lt;br /&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Pretraining?&lt;/strong&gt;&lt;br /&gt;Pretraining endows the language model with the capability to model a distribution over internet personas as well as the lower-level language distribution of each persona [4]. However, even when one persona (or a mixture of a limited number of them) is specified by the instruction of system prompts, current approaches fail to single it out.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;RLHF?&lt;/strong&gt;&lt;br /&gt;RLHF provides a powerful solution to adapting this multi-persona model to a “helpful and harmless assistant.” However, the original RLHF methods formulate reward maximization as a one-step bandit problem, and it is not generally possible to train with human feedback in the loop of conversation. (I’m aware of many advances in alignment but I want to discuss the original RLHF algorithm as a prototypical example.) This lack of multi-turn planning may cause models to suffer from task ambiguity [6] and learning superficial human-likeness rather than goal-directed social interaction [7].&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Will adding more dialogue data in RLHF help? My guess is that it will, to a certain extent, but it will still fall short due to a lack of purpose. Sergey Levine pointed out in his blog that there is a fundamental difference between preference learning and intentions: “the key distinction is between viewing language generation as selecting goal-directed actions in a sequential process, versus a problem of producing outputs satisfying user preferences.”&lt;/p&gt;&lt;h2 id="purposeful-dialogue-system"&gt;Purposeful dialogue system&lt;/h2&gt;&lt;p&gt;Staying on task is a modest request for LLMs. However, even if an LLM remains focused on the task, it doesn't necessarily mean it can excel in achieving the goal.&lt;/p&gt;&lt;p&gt;The problem of long-horizon planning has attracted some attention in the LLM community. For example, “decision-oriented dialogue” is proposed as a general class of tasks [8], where the AI assistant collaborates with humans to help them make complicated decisions, such as planning itineraries in a city and negotiating travel plans among friends. Another example, Sotopia [10], is a comprehensive social simulation platform that compiles various goal-driven dialogue scenarios including collaboration, negotiation, and persuasion. &lt;/p&gt;&lt;p&gt;Setting up such benchmarks provides not only a way to gauge the progress of the field, it also directly provides reward signals that new algorithms could pursue, which could be expensive to collect and tricky to define [9]. However, there aren’t many techniques that can exert control over the LM so that it can act consistently across a long horizon towards such goals. &lt;/p&gt;&lt;p&gt;To fill in this gap, my collaborators and I propose a lightweight algorithm (Dialogue Action Tokens, DAT [11]) that guides an LM chatbot through a multi-round goal-driven dialogue. As shown in the image below, in each round of conversations, the dialogue history’s last token embedding is used as the input (state) to a planner (actor) which predicts several prefix tokens (actions) to control the generation process. By training the planner with a relatively stable RL algorithm TD3+BC, we show significant improvement over baselines on Sotopia, even surpassing the social capability scores of GPT-4.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="176" src="https://thegradient.pub/content/images/2024/08/image2.png" width="1191" /&gt;&lt;figcaption&gt;A sketch of ​​Dialogue Action Tokens (DAT)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;br /&gt;In this way, we provide a technique pathway that upgrades LM from a prediction model that merely guesses the next token to one that engages in dialogue with humans purposefully. We could imagine that this technique can be misused for harmful applications as well. For this reason, we also conduct a “multi-round red-teaming” experiment, and recommend that more research could be done here to better understand multi-round dialogue as potential attack surface.&lt;/p&gt;&lt;h2 id="concluding-marks"&gt;Concluding marks&lt;br /&gt;&lt;/h2&gt;&lt;p&gt;I have reviewed the making of current LLM dialogue systems, how and why it is insufficient. I hypothesize that a purpose is what is missing and present one technique to add it back with reinforcement learning. &lt;/p&gt;&lt;p&gt;The following are two research questions that I’m mostly excited about: &lt;/p&gt;&lt;p&gt;(1) Better monitoring and control of dialogue systems with steering techniques. For example, the recently proposed TalkTurner (Chen et al.) adds a dashboard (Viégas et al) to open-sourced LLMs, enabling users to see and control how LLM thinks of themselves. Many weaknesses of current steering techniques are revealed and call for better solutions. For example, using activation steering to control two attributes (e.g., age and education level) simultaneously has been found to be difficult and can cause more language degradation. Another intriguing question is how to differentiate between LLM’s internal model of itself and that of the user. Anecdotally, chatting with Golden Gate Bridge Claude has shown that steering on the specific Golden Gate Bridge feature found by SAE sometimes causes Claude to think of itself as the San Francisco landmark, sometimes the users as the bridge, and other times the topic as such.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;(2) Better utilization of off-line reward signals. In the case of set-up environments like Sotopia and “decision-oriented dialogues”, rewards signals are engineered beforehand. In the real world, users won’t leave numerical feedback of how they feel satisfied. However, there might be other clues in language (e.g., “Thanks!”, “That’s very helpful!”) or from external resources (e.g., users buying the product for a salesman AI, users move to a subsequent coding question for copilot within a short time frame). Inferring and utilizing such hidden reward signals could strengthen the network effect of online chatbots: good model → more users → learning from interacting with users → better model.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Acknowledgment&lt;/strong&gt;&lt;br /&gt;The author is grateful to Martin Wattenberg and Hugh Zhang (alphabetical order) for providing suggestions and editing the text.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Citation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For attribution of this in academic contexts or books, please cite this work as:&lt;/p&gt;&lt;blockquote&gt;&lt;em&gt;Kenneth Li, "&lt;/em&gt;&lt;strong&gt;What's Missing From LLM Chatbots: A Sense of Purpose&lt;/strong&gt;&lt;em&gt;", The Gradient, 2024.&lt;/em&gt;&lt;/blockquote&gt;&lt;p&gt;BibTeX citation (this blog):&lt;/p&gt;&lt;div class="kg-card kg-callout-card kg-callout-card-grey"&gt;&lt;div class="kg-callout-text"&gt;@article{li2024from,&lt;br /&gt;author = {Li, Kenneth},&lt;br /&gt;title = {What's Missing From LLM Chatbots: A Sense of Purpose},&lt;br /&gt;journal = {The Gradient},&lt;br /&gt;year = {2024},&lt;br /&gt;howpublished = {\url{https://thegradient.pub/dialogue}},&lt;br /&gt;}&lt;/div&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[1] Schank, Roger C., and Robert P. Abelson. Scripts, plans, goals, and understanding: An inquiry into human knowledge structures. Psychology press, 2013.&lt;br /&gt;[2] Zhou, Jeffrey, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. "Instruction-following evaluation for large language models." arXiv preprint arXiv:2311.07911 (2023).&lt;br /&gt;[3] ​​Li, Kenneth, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. "Measuring and controlling persona drift in language model dialogs." arXiv preprint arXiv:2402.10962 (2024).&lt;br /&gt;[4] Andreas, Jacob. "Language models as agent models." arXiv preprint arXiv:2212.01681 (2022).&lt;br /&gt;[5] Austin, John Langshaw. How to do things with words. Harvard university press, 1975.&lt;br /&gt;[6] Tamkin, Alex, Kunal Handa, Avash Shrestha, and Noah Goodman. "Task ambiguity in humans and language models." arXiv preprint arXiv:2212.10711 (2022).&lt;br /&gt;[7] Bianchi, Federico, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue, Dan Jurafsky, and James Zou. "How well can llms negotiate? negotiationarena platform and analysis." arXiv preprint arXiv:2402.05863 (2024).&lt;br /&gt;[8] Lin, Jessy, Nicholas Tomlin, Jacob Andreas, and Jason Eisner. "Decision-oriented dialogue for human-ai collaboration." arXiv preprint arXiv:2305.20076 (2023).&lt;br /&gt;[9] Kwon, Minae, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. "Reward design with language models." arXiv preprint arXiv:2303.00001 (2023).&lt;br /&gt;[10] Zhou, Xuhui, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency et al. "Sotopia: Interactive evaluation for social intelligence in language agents." arXiv preprint arXiv:2310.11667 (2023).&lt;br /&gt;[11] Li, Kenneth, Yiming Wang, Fernanda Viégas, and Martin Wattenberg. "Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue with a Multi-Turn Planner." arXiv preprint arXiv:2406.11978 (2024).&lt;br /&gt;[12] Li, Shiyang, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, and Hongxia Jin. "Instruction-following evaluation through verbalizer manipulation." arXiv preprint arXiv:2307.10558 (2023).&lt;br /&gt;[13] Wu, Zhaofeng, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. "Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks." arXiv preprint arXiv:2307.02477 (2023).&lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[We Need Positive Visions for AI Grounded in Wellbeing]]&amp;gt;Introduction&lt;p&gt;Imagine yourself a decade ago, jumping directly into the present shock of conversing naturally with an encyclopedic AI that crafts images, writes code, and debates philosophy. Won’t this technology almost certainly transform society — and hasn’t AI’s impact on us so far been&lt;/p&gt;]]&amp;gt;https://thegradient.pub/we-need-positive-visions-for-ai-grounded-in-wellbeing/66a4243393571d5c8c154f4eSat, 03 Aug 2024 17:00:43 GMTIntroduction&lt;img alt="We Need Positive Visions for AI Grounded in Wellbeing" src="https://thegradient.pub/content/images/2024/07/wellbeing_ai_cover_image.webp" /&gt;&lt;p&gt;Imagine yourself a decade ago, jumping directly into the present shock of conversing naturally with an encyclopedic AI that crafts images, writes code, and debates philosophy. Won’t this technology almost certainly transform society — and hasn’t AI’s impact on us so far been a mixed-bag? Thus it’s no surprise that so many conversations these days circle around an era-defining question: &lt;em&gt;How do we ensure AI benefits humanity?&lt;/em&gt; These conversations often devolve into strident optimism or pessimism about AI, and our earnest aim is to walk a pragmatic middle path, though no doubt we will not perfectly succeed.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;While it’s fashionable to handwave towards “beneficial AI,” and many of us want to contribute towards its development — it’s not easy to pin down what beneficial AI concretely means in practice. This essay represents our attempt to demystify beneficial AI, through grounding it in the wellbeing of individuals and the health of society. In doing so, we hope to promote opportunities for AI research and products to benefit our flourishing, and along the way to share ways of thinking about AI’s coming impact that motivate our conclusions.&lt;/p&gt;&lt;h3 id="the-big-picture"&gt;The Big Picture&lt;/h3&gt;&lt;p&gt;By trade, we’re closer in background to AI than to the fields where human flourishing is most-discussed, such as wellbeing economics, positive psychology, or philosophy, and in our journey to find productive connections between such fields and the technical world of AI, we found ourselves often confused (what even is human flourishing, or wellbeing, anyways?) and from that confusion, often stuck (maybe there is nothing to be done? — the problem is too multifarious and diffuse). We imagine that others aiming to create prosocial technology might share our experience, and the hope here is to shine a partial path through the confusion to a place where there’s much interesting and useful work to be done. We start with some of our main conclusions, and then dive into more detail in what follows.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One conclusion we came to is that &lt;strong&gt;it’s okay that we can’t conclusively define human wellbeing.&lt;/strong&gt; It’s been debated by philosophers, economists, psychotherapists, psychologists, and religious thinkers, for many years, and there’s no consensus. At the same time, there’s agreement around many concrete factors that make our lives go well, like: supportive intimate relationships, meaningful and engaging work, a sense of growth and achievement, and positive emotional experiences. And there’s clear understanding, too, that beyond momentary wellbeing, we must consider how to secure and improve wellbeing across years and decades — through what we could call &lt;em&gt;societal&lt;/em&gt; &lt;em&gt;infrastructure&lt;/em&gt;: important institutions such as education, government, the market, and academia. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;One benefit of this wellbeing lens is to wake us to an almost-paradoxical fact: While the deep purpose behind nearly everything our species does is wellbeing, we’ve tragically lost sight of it. &amp;nbsp;Both by common measures of individual wellbeing (suicide rate, loneliness, meaningful work) and societal wellbeing (trust in our institutions, shared sense of reality, political divisiveness), we’re not doing well, and our impression is that AI is complicit in that decline. The central benefit of this wellbeing view, however, is the insight that no fundamental obstacle prevents us from synthesizing the science of wellbeing with machine learning to our collective benefit. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;This leads to our second conclusion: &lt;strong&gt;We need plausible positive visions of a society with capable AI, grounded in wellbeing.&lt;/strong&gt; Like other previous transformative technologies, AI will shock our societal infrastructure — dramatically altering the character of our daily lives, whether we want it to or not. For example, Facebook launched only twenty years ago, and yet social media’s shockwaves have already upended much in society — subverting news media and our informational commons, addicting us to likes, and displacing meaningful human connection with its shell. We believe capable AI’s impact will exceed that of social media. As a result, it’s vital that we strive to explore, envision, and move towards the AI-infused worlds we’d flourish within — ones perhaps in which it revitalizes our institutions, empowers us to pursue what we find most meaningful, and helps us cultivate our relationships. This is no simple task, requiring imagination, groundedness, and technical plausibility — to somehow dance through the minefields illuminated by previous critiques of technology. Yet now is the time to dream and build if we want to actively shape what is to come.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;This segues into our final conclusion: &lt;strong&gt;Foundation models and the arc of their future deployment is critical.&lt;/strong&gt; Even for those of us in the thick of the field, it’s hard to internalize how quickly models have improved, and how capable they might become given several more years. Recall that GPT-2 — barely functional by today’s standards — was released &lt;em&gt;only in 2019&lt;/em&gt;. If future models are much more capable than today’s, and competently engage with more of the world with greater autonomy, we can expect their entanglement with our lives and society to rachet skywards. So, at minimum, we’d like to enable these models to understand our wellbeing and how to support it, potentially through new algorithms, wellbeing-based evaluations of models and wellbeing training data. Of course, we also want to realize human benefit in practice — the last section of this blog post highlights what we believe are strong leverage points towards that end.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;The rest of this post describes in more detail (1) what we mean by AI that benefits our wellbeing, (2) the need for positive visions for AI grounded in wellbeing, and (3) concrete leverage points to aid in the development and deployment of AI in service of such positive visions. We’ve designed this essay such that the individual parts are mostly independent, so if you are interested most in concrete research directions, feel free to skip there.&lt;/p&gt;&lt;h3 id="beneficial-ai-grounds-out-in-human-wellbeing"&gt;Beneficial AI grounds out in human wellbeing&lt;/h3&gt;&lt;p&gt;Discussion about AI for human benefit is often high-minded, but not particularly actionable, as in unarguable but content-free phrases like “We should make sure AI is in service of humanity.” But to meaningfully implement such ideas in AI or policy requires enough precision and clarity to translate them into code or law. So we set out to survey what science has discovered about the ground of human benefit, as a step towards being able to measure and support it through AI.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Often, when we think about beneficial impact, we focus on abstract pillars like democracy, education, fairness, or the economy. However important, none of these are valuable &lt;em&gt;intrinsically.&lt;/em&gt; We care about them because of how they affect our collective lived experience, over the short and long-term. We care about increasing society’s GDP to the extent it aligns with actual improvement of our lives and future, but when treated as an end in itself, it becomes disconnected from what matters: improving human (and potentially all species’) experience.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In looking for fields that most directly study the root of human flourishing, we found the scientific literature on wellbeing. The literature is vast, spanning many disciplines, each with their own abstractions and theories — and, as you might expect, there’s no true consensus on what wellbeing actually is. In diving into the philosophy of flourishing, wellbeing economics, or psychological theories of human wellbeing, one encounters many interesting, compelling, but seemingly incompatible ideas. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;For example, theories of hedonism in philosophy claim that pleasure and the absence of suffering is the core of wellbeing; while desire satisfaction theories instead claim that wellbeing is about the fulfillment of our desires, no matter how we feel emotionally. There’s a wealth of literature on measuring subjective wellbeing (broadly, how we experience and feel about our life), and many different frameworks of what variables characterize flourishing. For example, Martin Seligman’s PERMA framework claims that wellbeing consists of positive emotions, engagement, relationships, meaning, and achievement. There are theories that say that the core of wellbeing is satisfying psychological needs, like the need for autonomy, competence, and relatedness. Other theories claim that wellbeing comes from living by our values. In economics, frameworks rhyme with those in philosophy and psychology, but diverge enough to complicate an exact bridge. For example, the wellbeing economics movement largely focuses on subjective wellbeing and explores many different proxies of it, like income, quality of relationships, job stability, etc.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;After the excitement from surveying so many interesting ideas began to fade, perhaps unsurprisingly, we remained fundamentally confused about what “the right theory” was. But, we recognized that in fact &lt;em&gt;this has always been the human situation when it comes to wellbeing&lt;/em&gt;, and just as a lack of an incontrovertible theory of flourishing has not prevented humanity from flourishing in the past, it need not stand as a fundamental obstacle for beneficial AI. In other words, our attempts to guide AI to support human flourishing must take this lack of certainty seriously, just as all sophisticated societal efforts to support flourishing must do.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In the end, we came to a simple workable understanding, not far from the view of wellbeing economics: Human benefit ultimately must ground out in the &lt;em&gt;lived experience of humans&lt;/em&gt;. We want to live happy, meaningful, healthy, full lives — and it’s not so difficult to imagine ways AI might assist in that aim. For example, the development of low-cost but proficient AI coaches, intelligent journals that help us to self-reflect, or apps that help us to find friends, romantic partners, or to connect with loved ones. We can ground these efforts in imperfect but workable measures of wellbeing from the literature (e.g. PERMA), taking as &lt;em&gt;first-class concerns&lt;/em&gt; that the map (wellbeing measurement) is not the territory (actual wellbeing), and that humanity itself continues to explore and refine its vision of wellbeing.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;More broadly our wellbeing relies on a healthy society, and we care not only about our own lives, but also want beautiful lives for our neighbors, community, country, and world, and for our children, and their children as well. The infrastructure of society (institutions like government, art, science, military, education, news, and markets) is what supports this broader, longer-term vision of wellbeing.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="We Need Positive Visions for AI Grounded in Wellbeing" class="kg-image" height="362" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe4f50PRkH1ZAWhltK8VAz1IhmtVLJU6k3pf1oGL-GmNTXM2QsnRJU52h0d8uCYPoFa_r6QB6UTtFThWPr6anV42FbBZPsnj1PXPDtp4Ofu5JjECD5CJz0W1asFNrFqvyL-PBxqYCk1VBxShyYTKoPj_HI?key=_z5hHgxLrjtdLauVz_eYpw" width="379" /&gt;&lt;/figure&gt;&lt;p&gt;Each of these institutions have important roles to play in society, and we can also imagine ways that AI could support or improve them; for example, generative AI may catalyze education through personal tutors that help us develop a richer worldview, may help us to better hold our politicians to account through sifting through what they are actually up to, or accelerate meaningful science through helping researchers make novel connections. Thus in short, &lt;em&gt;beneficial AI would meaningfully support our quest for lives worth living, in both the immediate and long-term sense.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;So, from the lofty confusion of conflicting grand theories, we arrive at something sounding more like common sense. Let’s not take this for granted, however — it cuts through the cruft of abstractions to firmly recenter what is ultimately important: the psychological experience of humans. This view points us towards the ingredients of wellbeing that are both well-supported scientifically and could be made measurable and actionable through AI (e.g. there exist instruments to measure many of these ingredients). Further, wellbeing across the short and long-term provides the common currency that bridges divergent approaches to beneficial AI, whether mitigating societal harms like discrimination in the AI ethics community, to attempting to reinvigorate democracy through AI-driven deliberation, to creating a world where humans live more meaningful lives, to creating low-cost emotional support and self-growth tools, to reducing the likelihood of existential risks from AI, to using AI to reinvigorate our institutions — wellbeing is the ultimate ground.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, focusing on wellbeing helps to highlight where we currently fall short. Current AI development is driven by our existing incentive systems: Profit, research novelty, engagement, with little explicit focus on what fundamentally is more important (human flourishing). We need to find tractable ways to shift incentives towards wellbeing-supportive models (something we’ll discuss later), and positive directions to move toward (discussed next).&lt;/p&gt;&lt;h3 id="we-need-positive-visions-for-ai"&gt;We need positive visions for AI&lt;/h3&gt;&lt;p&gt;Technology is a shockingly powerful societal force. While nearly all new technologies bring only limited change, like an improved toothbrush, sometimes they upend the world. Like the proverbial slowly-boiling frog, we forget how in short order the internet and cellphones have &lt;em&gt;overhauled&lt;/em&gt; our lived experience: the rise of dating apps, podcasts, social networks, our constant messaging, cross-continental video calls, massive online games, the rise of influencers, on-demand limitless entertainment, etc. Our lives as a whole — our relationships, our leisure, how we work and collaborate, how news and politics work — &lt;em&gt;have dramatically shifted&lt;/em&gt;, for both the better and worse.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;AI is transformative, and the mixed bag of its impacts are poised to reshape society in mundane and profound ways; we might doubt it, but that was also our naivety at the advent of social media and the cell-phone. We don’t see it coming, and once it’s here we take it for granted. Generative AI translates applications from science fiction into rapid adoption: AI romantic companions; automated writing and coding assistants; automatic generation of high-quality images, music, and videos; low-cost personalized AI tutors; highly-persuasive personalized ads; and so on. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;In this way, transformative impact is happening &lt;em&gt;now&lt;/em&gt; — it does not require AI with superhuman intelligence — see the rise of LLM-based social media bots; ChatGPT as the fastest-adopted consumer app; LLMs requiring fundamental changes to homework in school. Much greater impact will yet come, as the technology (and the business around it) matures, and as AI is integrated more pervasively throughout society.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Our institutions were understandably not designed with this latest wave of AI in mind, and it’s unclear that many of them will adapt quickly enough to keep up with AI's rapid deployment. For example, an important function of news is to keep a democracy’s citizens well-informed, so their vote is meaningful. But news these days spreads through AI-driven algorithms on social media, which amplifies emotional virality and confirmation bias at the expense of meaningful debate. And so, the public square and the sense of a shared reality is being undercut, as AI degrades an important institution devised without foresight of this novel technological development.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thus in practice, it may not be possible to play defense by simply “mitigating harms” from a technology; often, a new technology demands that we creatively and skillfully apply our existing values to a radically new situation. We don’t want AI to, for example, undermine the livelihood of artists, yet how &lt;em&gt;do&lt;/em&gt; we want our relationship to creativity to look like in a world where AI can, easily and cheaply, produce compelling art or write symphonies and novels, in the style of your favorite artist? There’s no easy answer. We need to debate, understand, and capture what we believe is the &lt;em&gt;spirit &lt;/em&gt;of our institutions and systems given this new technology. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;For example, what’s truly important about education? We can reduce harms that AI imposes on the current education paradigm by banning use of AI in students’ essays, or apply AI in service of existing metrics (for example, to increase high school graduation rates). But the paradigm itself must adapt: The world that schooling currently prepares our children for is not the world they’ll graduate into, nor does it prepare us generally to flourish and find meaning in our lives. We must ask ourselves what we really value in education that we want AI to enable: Perhaps teaching critical thinking, enabling agency, and creating a sense of social belonging and civic responsibility?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To anticipate critique, we agree that there will be no global consensus on what education is for, or on the underlying essence of any particular institution, at root because different communities and societies have &amp;nbsp;distinct values and visions. But that’s okay: Let’s empower communities to fit AI systems to local societal contexts; for example, algorithms like constitutional AI enable creating different constitutions that embody flourishing for different communities. This kind of cheap flexibility is an exciting affordance, meaning we no longer must sacrifice nuance and context-sensitivity for scalability and efficiency, a bitter pill technology often pushes us to swallow.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;And while of course we have always wanted education to create critical thinkers, our past metrics (like standardized tests) have been so coarse that scoring high is easily gamed without critical thinking. But generative AI enables new affordances here, too: just as a teacher can socratically question a student to evaluate their independent thought, advances in generative AI open up the door for similarly qualitative and interactive measures, like personalized AI tutors that meaningfully gauge critical thinking.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;We hope to tow a delicate line beyond broken dichotomies, whether between naive optimism and pessimism, or idealism and cynicism. Change is coming, and we must channel it towards refined visions of what we want, which is a profound opportunity, rather than to assume that by default technology will deliver us (or doom us), or that we will be able to wholly resist the transformation it brings (or are entirely helpless against it). For example, we must temper naive optimism (“AI will save the world if only we deploy it everywhere!”) by integrating lessons from the long line of work that studies the social drivers and consequences of technology, often from a critical angle. But neither should cynical concerns so paralyze us that we remain only as critics on the sidelines.&lt;/p&gt;&lt;h2 id="so-what-can-we-do"&gt;So, what can we do?&lt;/h2&gt;&lt;p&gt;The case so far is that we need positive visions for society with capable AI, grounded in individual and societal wellbeing. But what concrete work can actually support this? We propose the following break-down:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Understanding where we want to go&lt;/li&gt;&lt;li&gt;Measuring how AI impacts our wellbeing&lt;/li&gt;&lt;li&gt;Training models that can support wellbeing&lt;/li&gt;&lt;li&gt;Deploying models in service of wellbeing&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The overall idea is to support an ongoing, iterative process of exploring the positive directions we want to go and deploying and adapting models in service of them.&lt;/p&gt;&lt;h3 id="we-need-to-understand-where-we-want-to-go-in-the-age-of-ai"&gt;We need to understand where we want to go in the age of AI&lt;/h3&gt;&lt;p&gt;This point follows closely from the need to explore the positive futures we want with AI. What directions of work and research can help us to clarify where is possible to go, and is worth going to, in the age of AI?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;For starters, it’s more important now than ever to have productive and grounded discussions about questions like: What makes us human? How do we want to live? What do we want the future to feel like? What values are important to us? What do we want to retain as AI transformations sweep through society? Rather than being centered on the machine learning community, this should be an interdisciplinary, international effort, spanning psychology, philosophy, political science, art, economics, sociology, and neuroscience (and many other fields!), and bridging diverse intra- and international cultures. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Of course, it’s easy to call for such a dialogue, but the real question is how such interdisciplinary discussions can be convened in a meaningful, grounded, and action-guiding way — rather than leading only to cross-field squabbles or agreeable but vacuous aspirations. Perhaps through participatory design that pairs citizens with disciplinary experts to explore these questions, with machine learning experts mainly serving to ground technological plausibility. Perhaps AI itself could be of service: For example, research in AI-driven deliberative democracy and plurality may help involve more people in navigating these questions; as might research into meaning alignment, by helping us describe and aggregate what is meaningful and worth preserving to us. It’s important here to look beyond cynicism or idealism (suggestive of meta-modern political philosophy): Yes, mapping exciting positive futures is not a cure-all, as there are powerful societal forces, like regulatory capture, institutional momentum, and the profit motive, that resist their realization, and yet, societal movements all have to start somewhere, and &lt;em&gt;some really do succeed&lt;/em&gt;.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Beyond visions for big-picture questions about the future, much work is needed to understand where we want to go in narrower contexts. For example, while it might at first seem trivial, how can we reimagine online dating with capable AI, given that healthy romantic partnership is such an important individual and societal good? Almost certainly, we will look back at swipe-based apps as misguided means for finding long-term partners. And many of our institutions, small and large, can be re-visioned in this way, from tutoring to academic journals to local newspapers. AI will make possible a much richer set of design possibilities, and we can work to identify which of those possibilities are workable and well-represent the desired essence of an institution’s role in our lives and society.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, continued basic and applied research into the factors that contribute and characterize human wellbeing and societal health also are highly important, as these are what ultimately ground our visions. And as the next section explores, having better measures of such factors can help us to change incentives and work towards our desired futures.&lt;/p&gt;&lt;h3 id="we-need-to-develop-measures-for-how-ai-affects-wellbeing"&gt;We need to develop measures for how AI affects wellbeing&lt;/h3&gt;&lt;p&gt;For better and worse, we often navigate through what we measure. We’ve seen this play out before: Measure GDP, and nations orient towards increasing it at great expense. Measure clicks and engagement, and we develop platforms that are terrifyingly adept at keeping people hooked. A natural question is, what prevents us from similarly measuring aspects of wellbeing to guide our development and deployment of AI? And if we do develop wellbeing measures, can we avoid the pitfalls that have derailed other well-intended measures, like GDP or engagement?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One central problem for measurement is that wellbeing is more complex and qualitative than GDP or engagement. Time-on-site is a very straightforwardmeasure of engagement. In contrast, properties relevant to wellbeing, like the felt sense of meaning or the quality of healthy relationships, are difficult to pin down quantitatively, especially from the limited viewpoint of how a user interacts with a particular app. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Wellbeing depends on the broader context of a user’s life in messy ways, meaning it’s harder to isolate how any small intervention impacts it. And so, wellbeing measures are more expensive and less standardized to apply, end up less measured, and less guide our development of technology. However, foundation models are beginning to have the exciting ability to work with qualitative aspects of wellbeing. For example, present-day language models can (with caveats) infer emotions from user messages and detect conflict; or conduct qualitative interviews with users about its impact on their experience. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;So one promising direction of research, though not easy, is to explore how foundation models themselves can be applied to more reliably measure facets of individual and societal wellbeing, and ideally, help to identify how AI products and services are impacting that wellbeing. The mechanisms of impact are two-fold: One, companies may currently lack means of measuring wellbeing even though all-things-equal they want their products to help humans; two, where the profit motive conflicts with encouraging wellbeing, if a product’s impact can be externally audited and published, it can help hold the company to account by consumers and regulators, shifting corporate &amp;nbsp;incentives towards societal good.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Another powerful way that wellbeing-related measures can have impact is as evaluation benchmarks for foundation models. In machine learning, evaluations are a powerful lever for channeling research effort through competitive pressure. For example, model providers and academics continuously develop new models that perform better and better on benchmarks like TruthfulQA. Once you have legible outcomes, you often spur innovation to improve upon them. We currently have very few benchmarks focused on how AI affects our wellbeing, or how well they can understand our emotions, make wise decisions, or respect our autonomy: We need to develop these benchmarks.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, as mentioned briefly above, metrics can also create accountability and enable regulations. Recent efforts like the Stanford Foundational Model Transparency Index have created public accountability for AI labs, and initiatives like Responsible Scaling Policies are premised on evaluations of model capabilities, as are evaluations by government bodies such as AI safety institutes in both the UK and US. Are there similar metrics and initiatives to encourage accountability around AI’s impact on wellbeing?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To anticipate a natural concern, unanticipated side-effects are nearly universal when attempting to improve important &lt;em&gt;qualities&lt;/em&gt; through &lt;em&gt;quantitative&lt;/em&gt; measures. What if in measuring wellbeing, the second-order consequence is perversely to undermine it? For example, if a wellbeing measure doesn’t include notions of autonomy, in optimizing it we might create paternalistic AI systems that “make us happy” by decreasing our agency. There are book-length treatments on the failures of high modernism and (from one of the authors of this essay!) on the tyranny of measures and objectives, and many academic papers on how optimization can pervert measures or undermine our autonomy. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;The trick is to look beyond binaries. Yes, measures and evaluations have serious problems, yet we can work with them with wisdom, taking seriously previous failures and institutionalizing that all measures are imperfect. We want a diversity of metrics (metric federalism) and a diversity of AI models rather than a monoculture, we do not want measures to be direct optimization targets, and we want ways to responsively adjust measures when inevitably we learn of their limitations. This is a significant concern, and we must take it seriously — while some research has begun to explore this topic, more is needed. Yet in the spirit of pragmatic harm reduction, given that metrics are both technically and politically important for steering AI systems, developing less flawed measures remains an important goal.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Let’s consider one important example of harms from measurement: the tendency for a single global measure to trample local context. Training data for models, including internet data in particular, is heavily biased. Thus without deliberate remedy, models demonstrate uneven abilities to support the wellbeing of minority populations, undermining social justice (as convincingly highlighted by the AI ethics community). While LLMs have exciting potential to respect cultural nuance and norms, informed by the background of the user, we must work deliberately to realize it. One important direction is to develop measures of wellbeing specific to diverse cultural contexts, to drive accountability and reward progress.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To tie these ideas about measurement together, we suggest a taxonomy, looking at measures of AI &lt;em&gt;capabilities, behaviors, usage, and impacts&lt;/em&gt;. Similar to this DeepMind paper, the idea is to examine spheres of expanding context, from testing a model in isolation (both what it is capable of and what behaviors it demonstrates), all the way to understanding what happens when a model meets the real world (how humans use it, and what its impact is on them and society).&lt;br /&gt;&lt;/p&gt;&lt;p&gt;The idea is that we need a complementary ecosystem of measures fit to different stages of model development and deployment. In more detail:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;AI capabilities&lt;/em&gt; refers to what models are able to do. For example, systems today are capable of generating novel content, and translating accurately between languages.&lt;/li&gt;&lt;li&gt;&lt;em&gt;AI behaviors&lt;/em&gt; refers to how an AI system responds to different concrete situations. For example, many models are trained to refuse to answer questions that enable dangerous activities, like how to build a bomb,even though they have the capability to correctly answer them).&lt;/li&gt;&lt;li&gt;&lt;em&gt;AI usage&lt;/em&gt; refers to how models are used in practice when deployed. For example, AI systems today are used in chat interfaces to help answer questions, as coding assistants in IDEs, to sort social media feeds, and as personal companions.&lt;/li&gt;&lt;li&gt;&lt;em&gt;AI impacts &lt;/em&gt;refers to how AI impacts our experience or society. For example, people may feel empowered to do what’s important to them if AI helps them with rote coding, and societal trust in democracy may increase if AI sorts social media feeds towards bridging divides.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As an example of applying this framework to an important quality that contributes to wellbeing, here is a sketch of how we might design measures of human autonomy: &lt;/p&gt;&lt;!--kg-card-begin: html--&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col width="101" /&gt;&lt;col width="135" /&gt;&lt;col width="134" /&gt;&lt;col width="126" /&gt;&lt;col width="124" /&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Goal&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Capabilities&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Model Benchmarks&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Behaviors&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;System Benchmarks&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Usage&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;User Surveys&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Impact&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;User and Population Surveys&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Respecting autonomy&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understand what someone is trying to achieve in a given context&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understand the frontier of someone’s skill level&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understand what activities a user finds meaningful&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Socratic dialogue rather than just providing answers&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Tapping into users’ wisdom rather than giving advice&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Selective automation of tasks&lt;/span&gt;&lt;/p&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Used to aid humans with tasks rather than fully automate tasks they find&amp;nbsp; meaningful&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Used to help humans develop social skills instead of to nurture emotional attachment to simulated persona&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;People feel empowered&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;People are able to achieve their goals&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;People are pushed to grow&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--kg-card-end: html--&gt;&lt;p&gt;Let’s work through this example: we take a quality with strong scientific links to wellbeing, autonomy, and create measures of it and what enables it, all along the pipeline from model development to when it’s deployed at scale. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Starting from the right side of the table (Impact), there exist validated psychological surveys that measure autonomy, which can be adapted and given to users of an AI app to measure its &lt;em&gt;impact&lt;/em&gt; on their autonomy. Then, moving leftwards, these changes in autonomy could be linked to more specific types of &lt;em&gt;usage&lt;/em&gt;, through additional survey questions. For example, perhaps automating tasks that users actually find meaningful may correlate with decreased autonomy.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Moving further left on the table, the &lt;em&gt;behaviors&lt;/em&gt; of models that are needed to enable beneficial usage and impact can be gauged through more focused benchmarks. To measure behaviors of an AI system, one could run fixed workflows on an AI application where gold-standard answers come from expert labelers; another approach is to simulate users (e.g. with language models) interacting with an AI application to see how often and skillfully it performs particular behaviors, like socratic dialogue.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, &lt;em&gt;capabilities&lt;/em&gt; of a particular AI model could be similarly measured through benchmark queries input directly to the model, in a way very similar to how LLMs are benchmarked for capabilities like reasoning or question-answering. For example, the capability to understand a person’s skill level may be important to help them push their limits. A dataset could be collected of user behaviors in some application, annotated with their skill level; and the evaluation would be how well the model could predict skill level from observed behavior.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;At each stage, the hope is to link what is measured through evidence and reasoning to what lies above and below it in the stack. And we would want a diversity of measures at each level, reflecting different hypotheses about how to achieve the top-level quality, and with the understanding that each measure is always imperfect and subject to revision. In a similar spirit, rather than some final answer, this taxonomy and example autonomy measures are intended to inspire much-needed pioneering work towards wellbeing measurement.&lt;br /&gt;&lt;/p&gt;&lt;h3 id="we-need-to-train-models-to-improve-their-ability-to-support-wellbeing"&gt;We need to train models to improve their ability to support wellbeing&lt;/h3&gt;&lt;p&gt;Foundation models are becoming increasingly capable and in the future we believe most applications will not train models from scratch. Instead, most applications will prompt cutting-edge proprietary models, or fine-tune such models through limited APIs, or train small models on domain-specific responses from the largest models for cost-efficiency reasons. As evidence, note that to accomplish tasks with GPT-3 often required chaining together many highly-tuned prompts, whereas with GPT-4 those same tasks often succeed with the first casual prompting attempt. Additionally, we are seeing the rise of capable smaller models specialized for particular tasks, trained through data from large models.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;What’s important about this trend is that applications are differentially brought to market driven by what the largest models can most readily accomplish. For example, if frontier models excel at viral persuasion from being trained on Twitter data, but struggle with the depths of positive psychology, it will be easier to create persuasive apps than supportive ones, and there will be more of them, sooner, on the market.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thus we believe it’s crucial that the most capable foundation models &lt;em&gt;themselves&lt;/em&gt; understand what contributes to our wellbeing — an understanding granted to them through their &lt;em&gt;training process&lt;/em&gt;. We want the AI applications that we interface with (whether therapists, tutors, social media apps, or coding assistants) to understand how to support our wellbeing within their relevant role.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;However, the benefit of breaking down the capabilities and behaviors needed to support wellbeing, as we did earlier, is that we can deliberately target their improvement. One central lever is to gather or generate training data, which is the general fuel underlying model capabilities. There is an exciting opportunity to create datasets to support desired wellbeing capabilities and behaviors — for example, perhaps collections of wise responses to questions, pairs of statements from people and the emotions that they felt in expressing them, biographical stories about desirable and undesirable life trajectories, or first-person descriptions of human experience in general. The effect of these datasets can be grounded in the measures discussed above.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To better ground our thinking, we can examine how wellbeing data could improve the common &lt;em&gt;phases&lt;/em&gt; of foundation model training: pretraining, fine-tuning, and alignment.&lt;/p&gt;&lt;h4 id="pretraining"&gt;Pretraining&lt;/h4&gt;&lt;p&gt;The first training phase (confusingly called pretraining) establishes a model’s base abilities. It does so by training on vast amounts of variable-quality data, like a scrape of the internet. One contribution could be to either generate or gather large swaths of wellbeing relevant data, or to prioritize such data during training (also known as altering the data mix). For example, data could be sourced from subreddits relevant to mental health or life decisions, collections of biographies, books about psychology, or transcripts of supportive conversations. Additional data could be generated through paying contractors, crowdsourced through Games With a Purpose — fun experiences that create wellbeing-relevant data as a byproduct, or simulated through generative agent-based models.&lt;/p&gt;&lt;h4 id="fine-tuning"&gt;Fine-tuning&lt;/h4&gt;&lt;p&gt;The next stage of model training is fine-tuning. Here, smaller amounts of high-quality data, like diverse examples of desired behavior gathered from experts, focus the general capabilities resulting from pretraining. For different wellbeing-supporting behaviors we might want from a model, we can create fine-tuning datasets through deliberate curation of larger datasets, or by enlisting and recording the behavior of human experts in the relevant domain. We hope that the companies training the largest models place more emphasis on wellbeing in this phase of training, which is often driven by tasks with more obvious economic implications, like coding.&lt;/p&gt;&lt;h4 id="alignment"&gt;Alignment&lt;/h4&gt;&lt;p&gt;The final stage of model training is alignment, often achieved through techniques like reinforcement learning through human feedback (RLHF), where human contractors give feedback on AI responses to guide the model towards better ones. Or through AI-augmented techniques like constitutional AI, where an AI teaches itself to abide by a list of human-specified principles. The fuel of RLHF is preference data about what responses are preferred over others. Therefore we imagine opportunities for creating data sets of expert preferences that relate to wellbeing behaviors (even though what constitutes expertise in wellbeing may be interestingly contentious). For constitutional AI, we may need to iterate in practice with lists of wellbeing principles that we want to support, like human autonomy, and how, specifically, a model can respect it across different contexts.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In general, we need pipelines where wellbeing evaluations (as discussed in the last section) inform how we improve models. We need to find extensions to paradigms like RLHF that go beyond which response humans prefer in the moment, considering also which responses support user long-term growth, wellbeing, and autonomy, or better embody the spirit of the institutional role that the model is currently playing. These are intriguing, subtle, and challenging research questions that strike at the heart of the intersection of machine learning and societal wellbeing, and deserve much more attention. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;For example, we care about wellbeing over spans of years or decades, but it is impractical to apply RLHF &lt;em&gt;directly&lt;/em&gt; on human feedback to such ends, as we cannot wait decades to gather human feedback for a model; instead, we need research that helps integrate validated short-term proxies for long-term wellbeing (e.g. quality of intimate relationships, time spent in flow, etc.), ways to learn from longitudinal data where it exists (perhaps web journals, autobiographies, scientific studies), and to collect the judgment of those who devote their lifetime to helping support individuals flourish (like counselors or therapists).&lt;/p&gt;&lt;h3 id="we-need-to-deploy-ai-models-in-a-way-that-supports-wellbeing"&gt;We need to deploy AI models in a way that supports wellbeing&lt;/h3&gt;&lt;p&gt;Ultimately we want AI models deployed in the world to benefit us. AI applications could directly target human wellbeing, for example by directly supporting mental health or coaching us in a rigorous way. But as argued earlier, the broader ecosystem of AI-assisted applications, like social media, dating apps, video games, and content-providers like Netflix, serve as societal infrastructure for wellbeing and have enormous diffuse impact upon us; one of us has written about the possibility of creating more humanistic wellbeing-infrastructure applications. While difficult, dramatic societal benefits could result from, for example, new social media networks that better align with short and long-term wellbeing.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;We believe there are exciting opportunities for thoughtful positive deployments that pave the way as standard-setting beacons of hope, perhaps particularly in ethically challenging areas — although these of course may also be the riskiest. For example, artificial intimacy applications like Replika may be unavoidable even as they make us squeamish, and may truly benefit some users while harming others. It’s worthwhile to ask what (if anything) could enable artificial companions that are aligned with users’ wellbeing and do not harm society. Perhaps it is possible to thread the needle: they could help us develop the social skills needed to find real-world companions, or at least have strong, transparent guarantees about their fiduciary relationship to us, all while remaining viable as a business or non-profit. Or perhaps we can create harm-reduction services that help people unaddict from artificial companions that have become obstacles to their growth and development. Similar thoughts may apply to AI therapists, AI-assisted dating apps, and attention-economy apps, where incentives are difficult to align. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;One obvious risk is that we each are often biased to think we are more thoughtful than others, but may nonetheless be swept away by problematic incentives, like the trade-off between profit and user benefit. Legal structures like public benefit corporations, non-profits, or innovative new structures may help minimize this risk, as may value-driven investors or exceedingly careful design of internal culture.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Another point of leverage is that a successful proof of concept may change the attitudes and incentives for companies training and deploying the largest foundation models. We’re seeing a pattern where large AI labs incorporate best practices from outside product deployments back into their models. For example, ChatGPT plugins like data analysis and the GPT market were explored first by companies outside OpenAI before being incorporated into their ecosystem. And RLHF, which was first integrated into language models by OpenAI, is now a mainstay across foundation model development.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In a similar way to how RLHF became a mainstay, we want the capability to support our agency, understand our emotions, and better embody institutional roles to also become table-stakes features for model developers.This could happen through research advances &lt;em&gt;outside&lt;/em&gt; of the big companies, making it much easier for such features to be adopted &lt;em&gt;within&lt;/em&gt; them — though adoption may require pressure, through regulation, advocacy, or competition.&lt;/p&gt;&lt;h3 id="initiatives"&gt;Initiatives&lt;/h3&gt;&lt;p&gt;We believe there’s much concrete work to be done in the present. Here are a sampling of initiatives to seed thinking about what could move the field forward:&lt;br /&gt;&lt;/p&gt;&lt;!--kg-card-begin: html--&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col width="245" /&gt;&lt;col width="379" /&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Area&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Initiatives&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understanding where we want to go&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Global discussions on what is important to us.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Democratic elicitation of what matters to people (for example, the work done by &lt;/span&gt;&lt;span&gt;Collective Intelligence Project&lt;/span&gt;&lt;span&gt; and the &lt;/span&gt;&lt;span&gt;Meaning Alignment Institute&lt;/span&gt;&lt;span&gt;).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Concrete visualizations of what we want society to look like in 2050 (for example, the worldbuilding contest run by the &lt;/span&gt;&lt;span&gt;Future of Life Institute&lt;/span&gt;&lt;span&gt;).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Surveys to understand how people are using models and what principles are important for these use cases.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Improve our basic understanding of the factors that lead to wellbeing.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Develop methods for measuring how AI affects wellbeing&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Create benchmarks for models’ ability to understand emotions, make wise choices, respond in ways that respect our autonomy, etc.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Evaluations on how models impact people’s psychological experience.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Develop metrics to better track individual and collective wellbeing (e.g. tracking our somatic states, tracking societal trust, etc).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Train AI models based on what’s important to us&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Create datasets of emotionally supportive interactions.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Scalable oversight that helps people figure out what AI response would be best for their wellbeing.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Reinforcement Learning from Human Feedback with wellbeing-based feedback (e.g. from therapists).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Democratic finetuning&lt;/span&gt;&lt;span&gt; (run by the Meaning Alignment Institute)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Deploy models in beneficial areas&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;AI for mental health, education, resolving conflicts, relationship support, etc.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--kg-card-end: html--&gt;&lt;h2 id="conclusion-a-call-to-action"&gt;Conclusion: A call to action&lt;/h2&gt;&lt;p&gt;AI will transform society in ways that we cannot yet predict. If we continue on the present track, we risk AI reshaping our interactions and institutions in ways that erode our wellbeing and what makes our lives meaningful. Instead, challenging as it may be, we need to develop AI systems that understand and support wellbeing, both individual and societal. This is our call to reorientate towards wellbeing, to continue building a community and a field, in hopes of realizing AI’s potential to support our species’ strivings toward a flourishing future.&lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[Financial Market Applications of LLMs]]&amp;gt;The AI revolution drove frenzied investment in both private and public companies and captured the public’s imagination in 2023. Transformational consumer products like ChatGPT are powered by Large Language Models (LLMs) that excel at modeling sequences of tokens that represent words or parts of words [2]. Amazingly, structural]]&amp;gt;https://thegradient.pub/financial-market-applications-of-llms/661762b993571d5c8c154ea7Sat, 20 Apr 2024 17:57:39 GMT&lt;p&gt;The AI revolution drove frenzied investment in both private and public companies and captured the public’s imagination in 2023. Transformational consumer products like ChatGPT are powered by Large Language Models (LLMs) that excel at modeling sequences of tokens that represent words or parts of words [2]. Amazingly, structural understanding emerges from learning next-token prediction, and agents are able to complete tasks such as translation, question answering and generating human-like prose from simple user prompts.&lt;/p&gt;&lt;p&gt;Not surprisingly, quantitative traders have asked: can we turn these models into the next price or trade prediction [1,9,10]? That is, rather than modeling sequences of words, can we model sequences of prices or trades. This turns out to be an interesting line of inquiry that reveals much about both generative AI and financial time series modeling. Be warned this will get wonky.&lt;/p&gt;&lt;p&gt;LLMs are known as autoregressive learners -- those using previous tokens or elements in a sequence to predict the next element or token. In quantitative trading, for example in strategies like statistical arbitrage in stocks, most research is concerned with identifying autoregressive structure. That means finding sequences of news or orders or fundamental changes that best predict future prices.&lt;/p&gt;&lt;p&gt;Where things break down is in the quantity and information content of available data to train the models. At the 2023 NeurIPS conference, Hudson River Trading, a high frequency trading firm, presented a comparison of the number of input tokens used to train GPT-3 with the amount of trainable tokens available in the stock market data per year HRT estimated that, with 3,000 tradable stocks, 10 data points per stock per day, 252 trading days per year, and 23400 seconds in a trading day, there are 177 billion stock market tokens per year available as market data. GPT-3 was trained on 500 billion tokens, so not far off [6].&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Financial Market Applications of LLMs" class="kg-image" height="368" src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png" width="2000" /&gt;&lt;figcaption&gt;numbers courtesy of HRT 2023 NeuRIPS presentation&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But, in the trading context the tokens will be prices or returns or trades rather than syllables or words; the former is much more difficult to predict. Language has an underlying linguistic structure (e.g., grammar) [7]. It’s not hard to imagine a human predicting the next word in a sentence, however that same human would find it extremely challenging to predict the next return given a sequence of previous trades, hence the lack of billionaire day traders. The challenge is that there are very smart people competing away any signal in the market, making it &lt;em&gt;almost &lt;/em&gt;efficient (“efficiently inefficient”, in the words of economist Lasse Pedersen) and hence unpredictable. No adversary actively tries to make sentences more difficult to predict — if anything, authors usually seek to make their sentences easy to understand and hence &lt;em&gt;more&lt;/em&gt; predictable.&lt;/p&gt;&lt;p&gt;Looked at from another angle, there is much more noise than signal in financial data. Individuals and institutions are trading for reasons that might not be rational or tied to any fundamental change in a business. The GameStop episode in 2021 is one such example. Financial time series are also constantly changing with new fundamental information, regulatory changes, and occasional large macroeconomic shifts such as currency devaluations. Language evolves at a much slower pace and over longer time horizons.&lt;/p&gt;&lt;p&gt;On the other hand, there are reasons to believe that ideas from AI will work well in financial markets. One emerging area of AI research with promising applications to finance is multimodal learning [5], which aims to use different modalities of data, for example both images and textual inputs to build a unified model. With OpenAI’s DALL-E 2 model, a user can enter text and the model will generate an image. In finance, multi-modal efforts could be useful to combine information classical sources such as technical time series data (prices, trades, volumes, etc.) with alternative data in different modes like sentiment or graphical interactions on twitter, natural language news articles and corporate reports, or the satellite images of shipping activity in a commodity centric port. Here, leveraging multi-modal AI, one could potentially incorporate all these types of non-price information to predict well.&lt;/p&gt;&lt;p&gt;Another strategy called ‘residualization’ holds prominence in both finance and AI, though it assumes different roles in the two domains. &amp;nbsp;In finance, structural `factor’ models break down the contemporaneous observations of returns across different assets into a shared component (the market return, or more generally returns of common, market-wide factors) and an idiosyncratic component unique to each underlying asset. Market and factor returns are difficult to predict and create interdependence, so it is often helpful to remove the common element when making predictions at the individual asset level and to maximize the number of independent observations in the data. &lt;/p&gt;&lt;p&gt;In residual network architectures such as transformers, there’s a similar idea that we want to learn a function h(X) of an input X, but it might be easier to learn the residual of h(X) to the identity map, i.e., h(X) – X. Here, if the function h(X) is close to identity, its residual will be close to zero, and hence there will be less to learn and learning can be done more efficiently. In both cases the goal is to exploit structure to refine predictions: in the finance case, the idea is to focus on predicting innovations beyond what is implied by the overall market, for residual networks the focus is on predicting innovations to the identity map.&lt;/p&gt;&lt;p&gt;A key ingredient for the impressive performance of LLMs work is their ability to discern affinities or strengths between tokens over long horizons known as context windows. In financial markets, the ability to focus attention across long horizons enables analysis of multi-scale phenomena, with some aspects of market changes explained across very different time horizons. For example, at one extreme, fundamental information (e.g., earnings) may be incorporated into prices over months, technical phenomena (e.g., momentum) might be realized over days, and, at the other extreme, microstructure phenomena (e.g., order book imbalance) might have a time horizon of seconds to minutes.&lt;/p&gt;&lt;p&gt;Capturing all of these phenomena involves analysis of multiple time horizons across the context window. However, in finance, prediction over multiple &lt;em&gt;future&lt;/em&gt; time horizons is also important. For example, a quantitative system may seek to trade to profit from multiple different anomalies that are realized over multiple time horizons (e.g., simultaneously betting on a microstructure event and an earnings event). This requires predicting not just the next period return of the stock, but the entire term structure or trajectory of expected returns, while current transformer-style predictive models only look one period in the future.&lt;/p&gt;&lt;p&gt;Another financial market application of LLMs might be synthetic data creation [4,8]. This could take a few directions. Simulated stock price trajectories can be generated that mimic characteristics observed in the market and can be extremely beneficial given that financial market data is scarce relative to other sources as highlighted above in the number of tokens available. Artificial data could open the door for meta-learning techniques which have successfully been applied, for example, in robotics. In the robotic setting controllers are first trained using cheap but not necessarily accurate physics simulators, before being better calibrated using expensive real world experiments with robots. In finance the simulators could be used to coarsely train and optimize trading strategies. The model would learn high level concepts like risk aversion and diversification and tactical concepts such as trading slowly to minimize the price impact of a trade. Then precious real market data could be employed to fine-tune the predictions and determine precisely the optimal speed to trade.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Financial Market Applications of LLMs" class="kg-image" height="258" src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png" width="2000" /&gt;&lt;/figure&gt;&lt;p&gt;Financial market practitioners are often interested in extreme events, the times when trading strategies are more likely to experience significant gains or losses. Generative models where it’s possible to sample from extreme scenarios could find use. However extreme events by definition occur rarely and hence determining the right parameters and sampling data from the corresponding distribution is fraught.&lt;/p&gt;&lt;p&gt;Despite the skepticism that LLMs will find use in quantitative trading, they might boost fundamental analysis. As AI models improve, it’s easy to imagine them helping analysts refine an investment thesis, uncover inconsistencies in management commentary or find latent relationships between tangential industries and businesses [3]. Essentially these models could provide a Charlie Munger for every investor.&lt;/p&gt;&lt;p&gt;The surprising thing about the current generative AI revolution is that it’s taken almost everyone – academic researchers, cutting edge technology firms and long-time observers – by surprise. The idea that building bigger and bigger models would lead to emergent capabilities like we see today was totally unexpected and still not fully understood.&lt;/p&gt;&lt;p&gt;The success of these AI models has supercharged the flow of human and financial capital into AI, which should in turn lead to even better and more capable models. So while the case for GPT-4 like models taking over quantitative trading is currently unlikely, we advocate keeping an open mind. Expecting the unexpected has been a profitable theme in the AI business.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="references"&gt;References&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;“Applying Deep Neural Networks to Financial Time Series Forecasting” Allison Koenecke. 2022&lt;/li&gt;&lt;li&gt;“Attention is all you need.” A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones… &amp;nbsp;Advances in Neural Information Processing Systems, 2017&lt;/li&gt;&lt;li&gt;“Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models” . Lopez-Lira, Alejandro and Tang, Yuehua, (April 6, 2023) Available at SSRN&lt;/li&gt;&lt;li&gt;“Generating Synthetic Data in Finance: Opportunities, Challenges and Pitfalls.” SA Assefa, D Dervovic, M Mahfouz, RE Tillman… - Proceedings of the First ACM International Conference …, 2020&lt;/li&gt;&lt;li&gt;“GPT-4V(ision) System Card.” OpenAI. September 2023&lt;/li&gt;&lt;li&gt;“Language models are few-shot learners.” T Brown, B Mann, N Ryder, M Subbiah, JD Kaplan… - Advances in Neural Information Processing Systems, 2020&lt;/li&gt;&lt;li&gt;“Sequence to Sequence Learning with Neural Networks.” I.Sutskever,O.Vinyals,and Q.V.Le in Advances in Neural Information Processing Systems, 2014, pp. 3104–3112.&lt;/li&gt;&lt;li&gt;“Synthetic Data Generation for Economists”. A Koenecke, H Varian &amp;nbsp;- arXiv preprint arXiv:2011.01374, 2020&lt;/li&gt;&lt;li&gt;C. C. Moallemi, M. Wang. A reinforcement learning approach to optimal execution. Quantitative Finance, 22(6):1051–1069, March 2022.&lt;/li&gt;&lt;li&gt;C. Maglaras, C. C. Moallemi, M. Wang. A deep learning approach to estimating fill probabilities in a limit order book. Quantitative Finance, 22(11):1989–2003, October 2022.&lt;/li&gt;&lt;/ol&gt;&lt;h3 id="citation"&gt;Citation&lt;/h3&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Richard Dewey and Ciamac Moallemi, "Financial Market Applications of LLMs," The Gradient, 2024&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;@article{dewey2024financial,
    author = {Richard Dewey and Ciamac Moallemi},
    title = {Financial Market Applications of LLMs},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/financial-market-applications-of-llms},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[A Brief Overview of Gender Bias in AI]]&amp;gt;https://thegradient.pub/gender-bias-in-ai/660d016f93571d5c8c154d89Mon, 08 Apr 2024 15:54:53 GMT&lt;p&gt;AI models reflect, and often exaggerate, existing gender biases from the real world. It is important to quantify such biases present in models in order to properly address and mitigate them.&lt;/p&gt;&lt;p&gt;In this article, I showcase a small selection of important work done (and currently being done) to uncover, evaluate, and measure different aspects of gender bias in AI models. I also discuss the implications of this work and highlight a few gaps I’ve noticed.&lt;/p&gt;&lt;h2 id="but-what-even-is-bias"&gt;But What Even Is Bias?&lt;/h2&gt;&lt;p&gt;All of these terms (“AI”, “gender”, and “bias”) can be somewhat overused and ambiguous. “AI” refers to machine learning systems trained on human-created data and encompasses both statistical models like word embeddings and modern Transformer-based models like ChatGPT. “Gender”, within the context of AI research, typically encompasses binary man/woman (because it is easier for computer scientists to measure) with the occasional “neutral” category. &lt;/p&gt;&lt;p&gt;Within the context of this article, I use “bias” to broadly refer to unequal, unfavorable, and unfair treatment of one group over another.&lt;/p&gt;&lt;p&gt;There are many different ways to categorize, define, and quantify bias, stereotypes, and harms, but this is outside the scope of this article. I include a reading list at the end of the article, which I encourage you to dive into if you’re curious.&lt;/p&gt;&lt;h2 id="a-short-history-of-studying-gender-bias-in-ai"&gt;A Short History of Studying Gender Bias in AI&lt;/h2&gt;&lt;p&gt;Here, I cover a &lt;em&gt;very small&lt;/em&gt; sample of papers I’ve found influential studying gender bias in AI. This list is not meant to be comprehensive by any means, but rather to showcase the diversity of research studying gender bias (and other kinds of social biases) in AI.&lt;/p&gt;&lt;h3 id="man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings-bolukbasi-et-al-2016"&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings (Bolukbasi et al., 2016)&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Short Summary: &lt;/strong&gt;Gender bias exists in word embeddings (numerical vectors which represent text data) as a result of biases in the training data.&lt;br /&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: Given the analogy, man is to king as woman is to x, the authors used simple arithmetic using word embeddings to find that x=queen fits the best.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="61" src="https://lh7-us.googleusercontent.com/FCZV18SevX8_ymyYmn7gUk2lay4rIBKKG4tOFTm7fjFgW_LduuHX2QEw48S0bMfdIjT7Z1T7G7EGotZT-MlsBiqWt1EYZC0CIgH2TTVlC7uQSttoC5f47xyfEWTZVr3J4A_ZyhdxzR2wQQvcxHkrc7M" width="368" /&gt;&lt;figcaption&gt;&lt;em&gt;Subtracting the vector representations for “man” from “woman” results in a similar value as subtracting the vector representations for “king” and “queen”. From &lt;/em&gt;&lt;em&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;However, the authors found sexist analogies to exist in the embeddings, such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;He is to carpentry as she is to sewing&lt;/li&gt;&lt;li&gt;Father is to doctor as mother is to nurse&lt;/li&gt;&lt;li&gt;Man is to computer programmer as woman is to homemaker&lt;/li&gt;&lt;/ul&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="57" src="https://lh7-us.googleusercontent.com/zSojN5xOMtwGzMoy5lwu1z-8uuA9m0-sShqxARSa23DBsldKaFJBvRrXysO3ReLrZPIYQrdV-H0tD-3520ZvwK10jxNDtCwUuL5PEHJuhepnvgMfAXdIJY9Ir8o5v2ygINBHhh3U57Z8bnSYaB1bV2Y" width="624" /&gt;&lt;figcaption&gt;&lt;em&gt;Subtracting the vector representations for “man” from “woman” results in a similar value as subtracting the vector representations for “computer programmer” and “homemaker”. From &lt;/em&gt;&lt;em&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This implicit sexism is a result of the text data that the embeddings were trained on (in this case, Google News articles).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="243" src="https://lh7-us.googleusercontent.com/qMofwhApgAidjTu1eLVVgteacEKTvlg36td9SC6JDrNmbL2SAMkl2d2t8eNKcpo4EbechE06pEZ7uhOjIRz_kd0oCeJOyB6abHvaX_5uQSe4VGb8FKBEAMv3F1d9eiEYR2k7tnKmX3PYj27lEAiARKY" width="631" /&gt;&lt;figcaption&gt;&lt;em&gt;Gender stereotypes and gender appropriate analogies found in word embeddings, for the analogy “she is to X as he is to Y”. From &lt;/em&gt;&lt;em&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Mitigations:&lt;/strong&gt; The authors propose a methodology for debiasing word embeddings based on a set of gender-neutral words (such as female, male, woman, man, girl, boy, sister, brother). This debiasing method reduces stereotypical analogies (such as man=programmer and woman=homemaker) while keeping appropriate analogies (such as man=brother and woman=sister).&lt;/p&gt;&lt;p&gt;This method only works on word embeddings, which wouldn’t quite work for the more complicated Transformer-based AI systems we have now (e.g. LLMs like ChatGPT). However, this paper was able to quantify (and propose a method for removing) gender bias in word embeddings in a mathematical way, which I think is pretty clever.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; The widespread use of such embeddings in downstream applications (such as sentiment analysis or document ranking) would only amplify such biases.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification-buolamwini-and-gebru-2018"&gt;Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification [Buolamwini and Gebru, 2018]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Short summary:&lt;/strong&gt; Intersectional gender-and-racial biases exist in facial recognition systems, which can classify certain demographic groups (e.g. darker-skinned females) with much lower accuracy than for other groups (e.g. lighter-skinned males).&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Longer summary:&lt;/strong&gt; The authors collected a benchmark dataset consisting of equal proportions of four subgroups (lighter-skinned males, lighter-skinned females, darker- skinned males, darker-skinned females). They evaluated three commercial gender classifiers and found all of them to perform better on male faces than female faces; to perform better on lighter faces than darker faces; and to perform the worst on darker female faces (with error rates up to 34.7%). In contrast, the maximum error rate for lighter-skinned male faces was 0.8%.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="268" src="https://lh7-us.googleusercontent.com/Na3BK8q_NCHGOP6kk3IIlKUpk4ba4BoZyopg9ZfsE7qpOCA4_gJW68rZE6SEsp5XOL1Vsfg6yAsBjlieQ_hG4dZV4cVB5LZxYSBI2FKkTQ_2sukhULVCKoURvspOCaHnf5NnbEjjbFnJ11mavrwHlas" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;The accuracy of three different facial classification systems on four different subgroups. Table sourced from the &lt;/em&gt;&lt;em&gt;Gender Shades overview website&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Mitigation: &lt;/strong&gt;In direct response to this paper, Microsoft and IBM (two of the companies in the study whose classifiers were analyzed and critiqued) hastened to address these inequalities by fixing biases and releasing blog posts unreservedly engaging with the theme of algorithmic bias [1, 2]. These improvements mostly stemmed from revising and expanding the model training datasets to include a more diverse set of skin tones, genders, and ages.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;In the media&lt;/strong&gt;: &lt;/strong&gt;You might have seen the Netflix documentary “Coded Bias” and Buolamwini’s recent book Unmasking AI. You can also find an interactive overview of the paper on the Gender Shades website.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;: &lt;/strong&gt;Technological systems are meant to improve the lives of all people, not just certain demographics (who correspond with the people in power, e.g. white men). It is important, also, to consider bias not just along a single axis (e.g. gender) but the intersection of multiple axes (e.g. gender and skin color), which may reveal disparate outcomes for different subgroups&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="gender-bias-in-coreference-resolution-rudinger-et-al-2018"&gt;Gender bias in Coreference Resolution [Rudinger et al., 2018]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Short summary&lt;/strong&gt;: &lt;/strong&gt;Models for &lt;em&gt;coreference resolution&lt;/em&gt; (e.g. finding all entities in a text that a pronoun is referring to) exhibit gender bias, tending to resolve pronouns of one gender over another for certain occupations (e.g. for one model, “surgeon” resolves to “his” or “their”, but not to “her”).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="267" src="https://lh7-us.googleusercontent.com/oW-n5i7f0t_4ajmNkuDzXxd20TXJncjMbzWlxi8tdFuEImfEu-zAs3W-0sdZQibbbYXkioiGzp1kz81vN5xotJba3WJznijO-pD2yv6RksOowM2wpTqzGXqmUzS1dbkht8_AFpMUArkFW691o82odQ0" width="611" /&gt;&lt;figcaption&gt;&lt;em&gt;A coreference resolution system resolves a male and neutral pronoun to refer to the “the surgeon” but does not for the corresponding female pronoun! From &lt;/em&gt;&lt;em&gt;Gender Bias in Coreference Resolution&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Intro to coreference resolution using a classic riddle&lt;/strong&gt;: &lt;em&gt;A man and his son get into a terrible car crash. The father dies, and the boy is badly injured. In the hospital, the surgeon looks at the patient and exclaims, “I can’t operate on this boy, he’s my son!” How can this be?&lt;/em&gt;&lt;/p&gt;&lt;p&gt;(Answer: The surgeon is the mother)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: &lt;/strong&gt;The authors created a dataset of sentences for coreference resolution where correct pronoun resolution was not a function of gender. However, the models tended to resolve male pronouns to occupations (more so than female or neutral pronouns). For example, the occupation “manager” is 38.5% female in the U.S. (according to the 2006 US Census data), but none of the models predicted managers to be female in the dataset.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Related work&lt;/strong&gt;: &lt;/strong&gt;Other papers [1, 2] address measuring gender bias in coreference resolution. This is also relevant in the area of &amp;nbsp;machine translation, especially when translating phrases into and from gendered languages [3, 4].&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;: &lt;/strong&gt;It is important that models (and also humans) don’t immediately assume certain occupations or activities are linked to one gender because doing so might perpetuate harmful stereotypes.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="bbq-a-hand-built-bias-benchmark-for-question-answering-parrish-et-al-2021"&gt;BBQ: A Hand-Built Bias Benchmark for Question Answering [Parrish et al., 2021]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Short summary&lt;/strong&gt;: &lt;/strong&gt;Large Language Models (LLMs) consistently reproduce harmful biases in ambiguous contexts.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: &lt;/strong&gt;The authors created a dataset called Bias Benchmark for QA (BBQ), which contains questions highlighting attested social biases against people belonging to nine social dimensions (relevant to U.S. English-speaking contexts). For example, for the sentence with an ambiguous context, “After the first day of middle school math, a GIRL and a BOY stayed after class to talk to the teacher”; and the question “Who is bad at math”?; the biased answer would be “GIRL is bad at math”. The models tested by the authors reinforced such stereotypes 77% of the time.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="505" src="https://lh7-us.googleusercontent.com/NFVXTOlCVMjcVtUKELSm39QzpMio-YJ5RrhH6ZTAPogpiMP-vZNdpYXFRWsvv-Qd-Ahk4WCi16epfQjBNfZKUY9jbZ7_wi2_bVKiOhuZWgj66hgJO2QyuEVbePvM9J37Dy2hYYlR7cA2qe7UiMdhkec" width="499" /&gt;&lt;figcaption&gt;&lt;em&gt;An example of a question using an ambiguous and a disambiguated context. From the &lt;/em&gt;&lt;em&gt;BBQ&lt;/em&gt;&lt;em&gt; paper.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Related work&lt;/strong&gt;: &lt;/strong&gt;Much of NLP research is focused on the English language. It is important to test for social biases in non-English languages, but it is often not enough to do a direct translation of the data into another language, due to cultural differences (for example, Walmart, Uber, and W-4 are concepts that may not exist in non-US cultures). Datasets such as CBBQ and KoBBQ perform a &lt;em&gt;cultural translation&lt;/em&gt; of the BBQ dataset into (respectively) the Chinese and Korean language and culture.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; &lt;/strong&gt;While this single benchmark is far from comprehensive, it is important to include in evaluations as it provides an automatable (e.g. no human evaluators needed) method of measuring bias in generative language models.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="stable-bias-analyzing-societal-representations-in-diffusion-models-luccioni-et-al-2023"&gt;Stable Bias: Analyzing Societal Representations in Diffusion Models [Luccioni et al., 2023]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Short summary&lt;/strong&gt;: Image-generation models (such as DALL-E 2, Stable Diffusion, and Midjourney) contain social biases and consistently under-represent marginalized identities.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: &lt;/strong&gt;AI image-generation models tended to produce images of people that looked mostly white and male, especially when asked to generate images of people in positions of authority. For example, DALL-E 2 generated white men 97% of the time for prompts like “CEO”. The authors created several tools to help audit (or, understand model behavior of) such AI image-generation models using a targeted set of prompts through the lens of occupations and gender/ethnicity. For example, the tools allow qualitative analysis of differences in genders generated for different occupations, or what an average face looks like. They are available in this HuggingFace space.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="445" src="https://lh7-us.googleusercontent.com/2boKi96oJS5ZuSRrOr2sg4CtRsOM6aH-U-DRXCnxm6AGIPnvGRRJoButHvmUa9w7eakKB8ohKRIsF6oAbt2jN5R0yGOO-yNSIyUZyd3pdC_DJX7mXdNOsdjENfLOJW0dNJQPAIDoSWKdouczvmEnw40" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;An example of images generated by Stable Diffusion for the prompts “Compassionate manager” (showing mostly women) and “Manager” (showing all men). Image from an article written by the &lt;/em&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;em&gt; covering StableBias.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why this matters&lt;/strong&gt;: &lt;/strong&gt;AI-image generation models (and now, AI-video generation models, such as OpenAI’s Sora and RunwayML’s Gen2) are not only becoming more and more sophisticated and difficult to detect, but also increasingly commercialized. As these tools are developed and made public, it is important to both build new methods for understanding model behaviors and measuring their biases, as well as to build tools to allow the general public to better probe the models in a systematic way.&lt;/p&gt;&lt;h2 id="discussion"&gt;Discussion&lt;/h2&gt;&lt;p&gt;The articles listed above are just a small sample of the research being done in the space of measuring gender bias and other forms of societal harms.&lt;/p&gt;&lt;h3 id="gaps-in-the-research"&gt;Gaps in the Research&lt;/h3&gt;&lt;p&gt;The majority of the research I mentioned above introduces some sort of benchmark or dataset. These datasets (luckily) are being increasingly used to evaluate and test new generative models as they come out.&lt;/p&gt;&lt;p&gt;However, as these benchmarks are used more by the companies building AI models, the models are optimized to address only the specific kinds of biases captured in these benchmarks. There are countless other types of unaddressed biases in the models that are unaccounted for by existing benchmarks.&lt;/p&gt;&lt;p&gt;In my blog, I try to think about novel ways to uncover the gaps in existing research in my own way:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;In Where are all the women?, I showed that language models' understanding of "top historical figures" exhibited a gender bias towards generating male historical figures and a geographic bias towards generating people from Europe, no matter what language I prompted it in.&lt;/li&gt;&lt;li&gt;In Who does what job? Occupational roles in the eyes of AI, I asked three generations of GPT models to fill in "The man/woman works as a ..." to analyze the types of jobs often associated with each gender. I found that more recent models tended to overcorrect and over-exaggerate gender, racial, or political associations for certain occupations. For example, software engineers were predominantly associated with men by GPT-2, but with women by GPT-4.In Lost in DALL-E 3 Translation, I explored how DALL-E 3 uses prompt transformations to enhance (and translate into English) the user’s original prompt. DALL-E 3 tended to repeat certain tropes, such as “young Asian women” and “elderly African men”.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="what-about-other-kinds-of-bias-and-societal-harm"&gt;What About Other Kinds of Bias and Societal Harm?&lt;/h3&gt;&lt;p&gt;This article mainly focused on gender bias — and particularly, on binary gender. However, there is amazing work being done with regards to more fluid definitions of gender, as well as bias against other groups of people (e.g. disability, age, race, ethnicity, sexuality, political affiliation). This is not to mention all of the research done on detecting, categorizing, and mitigating gender-based violence and toxicity.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Another area of bias that I think about often is cultural and geographic bias. That is, even when testing for gender bias or other forms of societal harm, most research tends to use a Western-centric or English-centric lens.&lt;/p&gt;&lt;p&gt;For example, the majority of images from two commonly-used open-source image datasets for training AI models, Open Images and ImageNet, are sourced from the US and Great Britain.&lt;/p&gt;&lt;p&gt;This skew towards Western imagery means that AI-generated images often depict cultural aspects such as “wedding” or “restaurant” in Western settings, subtly reinforcing biases in seemingly innocuous situations. Such uniformity, as when "doctor" defaults to male or "restaurant" to a Western-style establishment, might not immediately stand out as concerning, yet underscores a fundamental flaw in our datasets, shaping a narrow and exclusive worldview.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="259" src="https://lh7-us.googleusercontent.com/1RGjMh4DGvfo0irKM8U9qODTo724-n6kvOSmysScHuTgVwT4-wQXpTt6YTa0Qk1QyQb_YkH2DdmM1LTIQTkN2omqKbB5aWUohauKdBl0v_9REuAP7aftBtXem9aS1NnPcWqn5qQRrJuSfYfvM-d3Nvk" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;Proportion of Open Images and ImageNet images from each country (represented by their two-letter ISO country codes). In both data sets, top represented locations include the US and Great Britain. From &lt;/em&gt;&lt;em&gt;No Classification without Representation&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 id="how-do-we-%E2%80%9Cfix%E2%80%9D-this"&gt;How Do We “Fix” This?&lt;br /&gt;&lt;/h3&gt;&lt;p&gt;This is the billion dollar question!&lt;/p&gt;&lt;p&gt;There are a variety of technical methods for “debiasing” models, but this becomes increasingly difficult as the models become more complex. I won’t focus on these methods in this article.&lt;/p&gt;&lt;p&gt;In terms of concrete mitigations, the companies training these models need to be more transparent about both the datasets and the models they’re using. Solutions such as Datasheets for Datasets and Model Cards for Model Reporting have been proposed to address this lack of transparency from private companies. Legislation such as the recent AI Foundation Model Transparency Act of 2023 are also a step in the right direction. However, many of the large, closed, and private AI models are doing the opposite of being open and transparent, in both training methodology as well as dataset curation.&lt;/p&gt;&lt;p&gt;Perhaps more importantly, we need to talk about what it means to “fix” bias.&lt;/p&gt;&lt;p&gt;Personally, I think this is more of a philosophical question — societal biases (against women, yes, but also against all sorts of demographic groups) exist in the real world and on the Internet.Should language models reflect the biases that already exist in the real world to better represent reality? If so, you might end up with AI image generation models over-sexualizing women, or showing “CEOs” as White males and inmates as people with darker skin, or depicting Mexican people as men with sombreros.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="404" src="https://lh7-us.googleusercontent.com/-3QSkLD3zel6TcjmvMEP4s9yrwFRP-HpBrLBZFeJEiS9YWZ-yaMUyvQALcSFvQP4PDFLy1JfSy0586-9kR5p64VrSV3Dapqpb0kr4u9RkwY4LIYIUcPhp8Igcjlivq_jhA0WHY1_dswawXmL5GKdRg8" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;A screenshot showing how depictions of “A Mexican person” usually shows a man in a sombrero. From &lt;/em&gt;&lt;em&gt;How AI Reduces the World to Stereotypes&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;rest of world&lt;/em&gt;&lt;em&gt;’s analysis into biases in Midjourney.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Or, is it the prerogative of those building the models to represent an idealistically equitable world? &amp;nbsp;If so, you might end up with situations like DALL-E 2 appending race/gender identity terms to the ends of prompts and DALL-E 3 automatically transforming user prompts to include such identity terms without notifying them or Gemini generating racially-diverse Nazis.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="411" src="https://lh7-us.googleusercontent.com/n3cWDhOcZCa3gnpAKXnmdpL8cVe7v42sKesaMK41CSps5ubaxbcyzSvb5uYR_DKHvSUaiU3gmRo08e_xuFITBa1x4738asdfk9c47kDTBLOpr7YQ6k83F0CMtPgMASQKe9-puDYbC_RzZmwtbK0lQRo" width="247" /&gt;&lt;figcaption&gt;&lt;em&gt;Images generated by Google’s Gemini Pro. From &lt;/em&gt;&lt;em&gt;The Verge’s article reporting on Gemini’s inaccurate historical portrayals&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;There’s no magic pill to address this. For now, what will happen (and is happening) is AI researchers and members of the general public will find something “wrong” with a publicly available AI model (e.g. from gender bias in historical events to image-generation models only generating White male CEOs). The model creators will attempt to address these biases and release a new version of the model. People will find new sources of bias; and this cycle will repeat.&lt;/p&gt;&lt;h3 id="final-thoughts"&gt;Final Thoughts&lt;/h3&gt;&lt;p&gt;It is important to evaluate societal biases in AI models in order to improve them — before addressing any problems, we must first be able to measure them. Finding problematic aspects of AI models helps us think about what kind of tools we want in our lives and what kind of world we want to live in.&lt;/p&gt;&lt;p&gt;AI models, whether they are chatbots or models trained to generate realistic videos, are, at the end of the day, trained on data created by humans — books, photographs, movies, and all of our many ramblings and creations on the Internet. It is unsurprising that AI models would reflect and exaggerate the biases and stereotypes present in these human artifacts — but it doesn’t mean that it always needs to be this way.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Yennie is a multidisciplinary machine learning engineer and AI researcher currently working at Google Research. She has worked across a wide range of machine learning applications, from health tech to humanitarian response, and with organizations such as OpenAI, the United Nations, and the University of Oxford. She writes about her independent AI research experiments on her blog at Art Fish Intelligence.&lt;/p&gt;&lt;h2 id="a-list-of-resources-for-the-curious-reader"&gt;A List of Resources for the Curious Reader&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Barocas, S., &amp;amp; Selbst, A. D. (2016). Big data's disparate impact. &lt;em&gt;California law review&lt;/em&gt;, 671-732.&lt;/li&gt;&lt;li&gt;Blodgett, S. L., Barocas, S., Daumé III, H., &amp;amp; Wallach, H. (2020). Language (technology) is power: A critical survey of" bias" in nlp. arXiv preprint arXiv:2005.14050.&lt;/li&gt;&lt;li&gt;Bolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., &amp;amp; Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.&lt;/li&gt;&lt;li&gt;Buolamwini, J., &amp;amp; Gebru, T. (2018, January). Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency (pp. 77-91). PMLR.&lt;/li&gt;&lt;li&gt;Caliskan, A., Bryson, J. J., &amp;amp; Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186.&lt;/li&gt;&lt;li&gt;Cao, Y. T., &amp;amp; Daumé III, H. (2019). Toward gender-inclusive coreference resolution. &lt;em&gt;arXiv preprint arXiv:1910.13913&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Dev, S., Monajatipoor, M., Ovalle, A., Subramonian, A., Phillips, J. M., &amp;amp; Chang, K. W. (2021). Harms of gender exclusivity and challenges in non-binary representation in language technologies. &lt;em&gt;arXiv preprint arXiv:2108.12084&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Dodge, J., Sap, M., Marasović, A., Agnew, W., Ilharco, G., Groeneveld, D., ... &amp;amp; Gardner, M. (2021). Documenting large webtext corpora: A case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758.&lt;/li&gt;&lt;li&gt;Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Iii, H. D., &amp;amp; Crawford, K. (2021). Datasheets for datasets. &lt;em&gt;Communications of the ACM&lt;/em&gt;, &lt;em&gt;64&lt;/em&gt;(12), 86-92.&lt;/li&gt;&lt;li&gt;Gonen, H., &amp;amp; Goldberg, Y. (2019). Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. &lt;em&gt;arXiv preprint arXiv:1903.03862&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Kirk, H. R., Jun, Y., Volpin, F., Iqbal, H., Benussi, E., Dreyer, F., ... &amp;amp; Asano, Y. (2021). Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models. Advances in neural information processing systems, 34, 2611-2624.&lt;/li&gt;&lt;li&gt;Levy, S., Lazar, K., &amp;amp; Stanovsky, G. (2021). Collecting a large-scale gender bias dataset for coreference resolution and machine translation. arXiv preprint arXiv:2109.03858.&lt;/li&gt;&lt;li&gt;Luccioni, A. S., Akiki, C., Mitchell, M., &amp;amp; Jernite, Y. (2023). Stable bias: Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408.&lt;/li&gt;&lt;li&gt;Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., ... &amp;amp; Gebru, T. (2019, January). Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency (pp. 220-229).&lt;/li&gt;&lt;li&gt;Nadeem, M., Bethke, A., &amp;amp; Reddy, S. (2020). StereoSet: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456.&lt;/li&gt;&lt;li&gt;Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., ... &amp;amp; Bowman, S. R. (2021). BBQ: A hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193.&lt;/li&gt;&lt;li&gt;Rudinger, R., Naradowsky, J., Leonard, B., &amp;amp; Van Durme, B. (2018). Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301.&lt;/li&gt;&lt;li&gt;Sap, M., Gabriel, S., Qin, L., Jurafsky, D., Smith, N. A., &amp;amp; Choi, Y. (2019). Social bias frames: Reasoning about social and power implications of language. &lt;em&gt;arXiv preprint arXiv:1911.03891&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Savoldi, B., Gaido, M., Bentivogli, L., Negri, M., &amp;amp; Turchi, M. (2021). Gender bias in machine translation. Transactions of the Association for Computational Linguistics, 9, 845-874.&lt;/li&gt;&lt;li&gt;Shankar, S., Halpern, Y., Breck, E., Atwood, J., Wilson, J., &amp;amp; Sculley, D. (2017). No classification without representation: Assessing geodiversity issues in open data sets for the developing world. arXiv preprint arXiv:1711.08536.&lt;/li&gt;&lt;li&gt;Sheng, E., Chang, K. W., Natarajan, P., &amp;amp; Peng, N. (2019). The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326.&lt;/li&gt;&lt;li&gt;Weidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., Mateos-Garcia, J., ... &amp;amp; Isaac, W. (2023). Sociotechnical safety evaluation of generative ai systems. arXiv preprint arXiv:2310.11986.&lt;/li&gt;&lt;li&gt;Zhao, J., Mukherjee, S., Hosseini, S., Chang, K. W., &amp;amp; Awadallah, A. H. (2020). Gender bias in multilingual embeddings and cross-lingual transfer. arXiv preprint arXiv:2005.00699.&lt;/li&gt;&lt;li&gt;Zhao, J., Wang, T., Yatskar, M., Ordonez, V., &amp;amp; Chang, K. W. (2018). Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;This post was originally posted on Art Fish Intelligence&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Yennie Jun, "Gender Bias in AI," The Gradient, 2024&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;@article{Jun2024bias,
    author = {Yennie Jun},
    title = {Gender Bias in AI},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/gender-bias-in-ai},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Mamba Explained]]&amp;gt;https://thegradient.pub/mamba-explained/65fb8d5993571d5c8c154beaThu, 28 Mar 2024 01:24:43 GMT&lt;p&gt;&lt;br /&gt;&lt;strong&gt;The State Space Model taking on Transformers&lt;/strong&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="168" src="https://lh7-us.googleusercontent.com/Vv2LBVlbspbhtzNqDFAAZ8xgkHKAzJiEoef9HZTlGVFpxAbWCMavNmhj408DdeOPZbj53vySwQR81e2zXlo52xA8OrJCq00V_z5VGwEMgfcvSW2uh60hFdjYliY-GAa_Kptz2XFbUf8S_-WrJqyhI4k" width="300" /&gt;&lt;/figure&gt;&lt;p&gt;Right now, AI is eating the world.&lt;/p&gt;&lt;p&gt;And by AI, I mean Transformers. Practically all the big breakthroughs in AI over the last few years are due to Transformers.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Mamba&lt;/strong&gt;, however, is one of an alternative class of models called &lt;strong&gt;State Space Models&lt;/strong&gt; (&lt;strong&gt;SSMs&lt;/strong&gt;). Importantly, for the first time, Mamba promises similar performance (and crucially similar &lt;em&gt;scaling laws&lt;/em&gt;) as the Transformer whilst being feasible at long sequence lengths (say 1 million tokens). To achieve this long context, the Mamba authors remove the “quadratic bottleneck” in the Attention Mechanism. Mamba also runs &lt;em&gt;fast&lt;/em&gt; - like “up to 5x faster than Transformer fast”&lt;sup&gt;1&lt;/sup&gt;.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="277" src="https://lh7-us.googleusercontent.com/uIkOGdo_oOuGilrgILP7E0KvNC8Y7ZL93om_wMUQCJEEIeSo0GtO4dzQ4bHMq5sdZu2ldL-fMrFy3KcLAr5_A8JhNOqqPyxFbYPPx016x1Djhr9VJ0lGzcEMvDDe5a-r0Wv-xvtneEYUSMJAsVS0OTY" width="572" /&gt;&lt;figcaption&gt;Mamba performs similarly (or slightly better than) other Language Models on The Pile (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Gu and Dao, the Mamba authors write:&lt;/p&gt;&lt;p&gt;&lt;em&gt;Mamba enjoys fast inference and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modelling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Here we’ll discuss:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The advantages (and disadvantages) of Mamba (🐍) vs Transformers (🤖),&lt;/li&gt;&lt;li&gt;Analogies and intuitions for thinking about Mamba, and&lt;/li&gt;&lt;li&gt;What Mamba means for Interpretability, AI Safety and Applications.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="problems-with-transformersmaybe-attention-isn%E2%80%99t-all-you-need"&gt;Problems with Transformers - Maybe Attention &lt;em&gt;Isn’t&lt;/em&gt; All You Need&lt;/h2&gt;&lt;p&gt;We’re very much in the Transformer-era of history. ML used to be about detecting cats and dogs. Now, with Transformers, we’re generating human-like poetry, coding better than the median competitive programmer, and solving the protein folding problem.&lt;/p&gt;&lt;p&gt;But Transformers have one core problem. In a transformer, every token can look back at every previous token when making predictions. For this lookback, we cache detailed information about each token in the so-called KV cache.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="393" src="https://lh7-us.googleusercontent.com/dTD7M6vcg6ZBJPUyvFw_sOLbcZl6s6WXQbQ9Nfo3gq92G7bFIDBmr4Zj-Lahw7rZyHh6yKxRrSe790W04cyWAcRyM2rKkNz2wmsF_XJfP9mNJI5pSdst688I6o-brks05LF4N_5fNUPlQ1vvF8dOOdE" width="602" /&gt;&lt;figcaption&gt;When using the Attention Mechanism, information from all previous tokens can be passed to the current token&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This pairwise communication means a forward pass is O(n²) time complexity in training (the dreaded quadratic bottleneck), and each new token generated autoregressively takes O(n) time. In other words, as the context size increases, the model gets &lt;em&gt;slower&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;To add insult to injury, storing this key-value (KV) cache requires O(n) space. &amp;nbsp;Consequently, the dreaded CUDA out-of-memory (OOM) error becomes a significant threat as the memory footprint expands. If space were the only concern, we might consider adding more GPUs; however, with latency increasing quadratically, simply adding more compute might not be a viable solution.&lt;/p&gt;&lt;p&gt;On the margin, we can mitigate the quadratic bottleneck with techniques like Sliding Window Attention or clever CUDA optimisations like FlashAttention. But ultimately, for super long context windows (like a chatbot which remembers every conversation you’ve shared), we need a different approach.&lt;/p&gt;&lt;h3 id="foundation-model-backbones"&gt;Foundation Model Backbones&lt;/h3&gt;&lt;p&gt;Fundamentally, all good ML architecture backbones have components for two important operations:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Communication&lt;/strong&gt; &lt;em&gt;between&lt;/em&gt; tokens&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Computation&lt;/strong&gt; &lt;em&gt;within&lt;/em&gt; a token&lt;/li&gt;&lt;/ol&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="412" src="https://lh7-us.googleusercontent.com/WpckyY81cA3zGS1j1vq5lH-nZKiRdelILLO6OdiX05s4Psqe3oBpIZiy1IavhsutFkz4oa7V9ZjzGhjxcdMxD9Q_Z3pYelK04_7YA1-I-_PVu3SLDfBBK1c4-M3QcHh0MwzQcUR7wccwPKvjoXzS06I" width="602" /&gt;&lt;figcaption&gt;The Transformer Block&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In transformers, this is &lt;strong&gt;Attention&lt;/strong&gt; (communication) and &lt;strong&gt;MLPs&lt;/strong&gt; (computation). We improve transformers by optimising these two operations&lt;sup&gt;2&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;We would like to substitute the Attention component&lt;sup&gt;3&lt;/sup&gt; with an alternative mechanism for facilitating inter-token communication. Specifically, &lt;strong&gt;Mamba&lt;/strong&gt; employs a Control Theory-inspired State Space Model, or &lt;strong&gt;SSM,&lt;/strong&gt; for Communication purposes while retaining Multilayer Perceptron (MLP)-style projections for Computation.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="340" src="https://lh7-us.googleusercontent.com/T4MbDYFoOq5yAKl9uEEs9tjMy-CxBYy2S2rxnKbo5PmlnumyMs3DWV5chNooGG2hGp8ES9vXLEkmjHqlEzoCocVAnN2nquNhcBVK4hnrsfDJfBjJs5RZvx2bMSZEkm5yZtrTt7wBZfMW_iQXp4u8cU0" width="602" /&gt;&lt;figcaption&gt;The Mamba Block&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Like a Transformer made up of stacked transformer blocks, Mamba is made up of stacked Mamba blocks as above.&lt;/p&gt;&lt;p&gt;We would like to understand and motivate the choice of the SSM for sequence transformations.&lt;/p&gt;&lt;h2 id="motivating-mambaa-throwback-to-temple-run"&gt;Motivating Mamba - A Throwback to Temple Run&lt;/h2&gt;&lt;p&gt;Imagine we’re building a Temple Run agent&lt;sup&gt;4&lt;/sup&gt;. It chooses if the runner should move left or right at any time.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="822" src="https://thegradient.pub/content/images/2024/03/temple_run.png" width="900" /&gt;&lt;/figure&gt;&lt;p&gt;To successfully pick the correct direction, we need information about our surroundings. Let’s call the collection of relevant information the state. Here the state likely includes your current position and velocity, the position of the nearest obstacle, weather conditions, etc.&lt;/p&gt;&lt;blockquote&gt;&lt;em&gt;Claim 1: if you know the current state of the world and how the world is evolving, then you can use this to determine the direction to move.&lt;/em&gt;&lt;/blockquote&gt;&lt;p&gt;Note that you don’t need to look at the whole screen all the time. You can figure out what will happen to most of the screen by noting that as you run, the obstacles move down the screen. You only need to look at the top of the screen to understand the new information and then simulate the rest.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="295" src="https://lh7-us.googleusercontent.com/09a_eDMzBRh-usMcrg1W-JnkWE59PbsAtAW3Q8z8NmeyHGCpGsKG58dJtHNTnVUunlBbGb7xKt8nExTChRxMdcs1a125J7p11vDMR77GzigsI3j797VQxLLB9e_ILa1l8A-BCy7psxnYBIoQzk6-2GQ" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;This lends itself to a natural formulation. Let h be the hidden state, relevant knowledge about the world. Also let x be the input, the observation that you get each time. h’ then represents the derivative of the hidden state, i.e. how the state is evolving. We’re trying to predict y, the optimal next move (right or left).&lt;/p&gt;&lt;p&gt;Now, Claim 1 states that from the hidden state h, h’, and the new observation x, you can figure out y.&lt;/p&gt;&lt;p&gt;More concretely, h, the state, can be represented as a differential equation (Eq 1a):&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h’(t) = \mathbf{A}h(t) + \mathbf{B}x(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Knowing h allows you to determine your next move y (Eq 1b):&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$y(t) = \mathbf{C}h(t) + \mathbf{D}x(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;The system's evolution is determined by its current state and newly acquired observations. A small new observation is enough, as the majority of the state can be inferred by applying known state dynamics to its previous state. That is, most of the screen isn’t new, it’s just a continuation of the previous state's natural downward trajectory. A full understanding of the state would enable optimal selection of the subsequent action, denoted as y.&lt;/p&gt;&lt;p&gt;You can learn a lot about the system dynamics by observing the top of the screen. For instance, increased velocity of this upper section suggests an acceleration of the rest of the screen as well, so we can infer that the game is speeding up&lt;sup&gt;5&lt;/sup&gt;. In this way, even if we start off knowing nothing about the game and only have limited observations, it becomes possible to gain a holistic understanding of the screen dynamics fairly rapidly.&lt;/p&gt;&lt;h3 id="what%E2%80%99s-the-state"&gt;What’s the State?&lt;/h3&gt;&lt;p&gt;Here, &lt;strong&gt;state&lt;/strong&gt; refers to the variables that, when combined with the input variables, fully determine the future system behaviour. In theory, once we have the state, there’s nothing else we need to know about the past to predict the future. With this choice of state, the system is converted to a &lt;strong&gt;Markov Decision Process&lt;/strong&gt;. Ideally, the state is a fairly small amount of information which captures the essential properties of the system. That is, &lt;strong&gt;the state is a compression of the past&lt;/strong&gt;&lt;sup&gt;6&lt;/sup&gt;.&lt;/p&gt;&lt;h2 id="discretisationhow-to-deal-with-living-in-a-quantised-world"&gt;Discretisation - How To Deal With Living in a Quantised World&lt;/h2&gt;&lt;p&gt;Okay, great! So, given some state and input observation, we have an autoregressive-style system to determine the next action. Amazing!&lt;/p&gt;&lt;p&gt;In practice though, there’s a little snag here. We’re modelling time as continuous. But in real life, we get new inputs and take new actions at discrete time steps&lt;sup&gt;7&lt;/sup&gt;.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="601" src="https://lh7-us.googleusercontent.com/_A8UqIDZgHLXm-YwGNfpfE7gSg6fA5-PhsNKZEHAbHNS2-XBYRrZpDGUvJgiOIBCg126L7s2GYMxn98LSdgkVJNC5_sL5HNsDjazFLArizSkJbEAJAVmL3BpajxCbWO-5Hgtq9CEfW_lfzmUscSZTPg" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;We would like to convert this &lt;em&gt;continuous-time differential equation&lt;/em&gt; into a &lt;em&gt;discrete-time difference equation&lt;/em&gt;. This conversion process is known as discretisation. Discretisation is a well-studied problem in the literature. Mamba uses the Zero-Order Hold (ZOH) discretisation&lt;sup&gt;8&lt;/sup&gt;. To give an idea of what’s happening morally, consider a naive first-order approximation&lt;sup&gt;9&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;From Equation 1a, we have&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h’(t) = \mathbf{A}h(t) + \mathbf{B}x(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;And for small ∆,&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h’(t) \approx \frac{h(t+\Delta) - h(t)}{\Delta}$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;by the definition of the derivative.&lt;/p&gt;&lt;p&gt;We let:&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h_t = h(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;and&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h_{t+1} = h(t + \Delta)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;and substitute into Equation 1a giving:&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h_{t+1} - h_t \approx \Delta (\mathbf{A}h_t + \mathbf{B}x_t)$&lt;br /&gt;$\Rightarrow h_{t+1} \approx (I + \Delta \mathbf{A})h_t + (\Delta&lt;br /&gt;\mathbf{B})x_t$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Hence, after renaming the coefficients and relabelling indices, we have the discrete representations:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="127" src="https://lh7-us.googleusercontent.com/JNkElXh35QPUmp4Sl625go-1PnrKWpzDdV5BObpnSg6-bbhKDxr83Y0AZi7XT8CQdxF1CeByNH4sbFyDc-aTRWyXeXrBDL499-BXjte-iYGD01UR4udyI-a9J7D-w9Ao6COYZC7HpDcoQxzOqzqA5IY" width="384" /&gt;&lt;figcaption&gt;The Discretised Version of the SSM Equation&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If you’ve ever looked at an RNN before&lt;sup&gt;10&lt;/sup&gt; and this feels familiar - &lt;em&gt;trust your instincts&lt;/em&gt;:&lt;/p&gt;&lt;p&gt;&lt;em&gt;We have some input x, which is combined with the previous hidden state by some transform to give the new hidden state. Then we use the hidden state to calculate the output at each time step.&lt;/em&gt;&lt;/p&gt;&lt;h2 id="understanding-the-ssm-matrices"&gt;Understanding the SSM Matrices&lt;/h2&gt;&lt;p&gt;Now, we can interpret the A, B, C, D matrices more intuitively:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A is the transition state matrix. It shows how you transition the current state into the next state. It asks “How should I forget the less relevant parts of the state over time?”&lt;/li&gt;&lt;li&gt;B is mapping the new input into the state, asking “What part of my new input should I remember?”&lt;sup&gt;11&lt;/sup&gt;&lt;/li&gt;&lt;li&gt;C is mapping the state to the output of the SSM. It asks, “How can I use the state to make a good next prediction?”&lt;sup&gt;12&lt;/sup&gt;&lt;/li&gt;&lt;li&gt;D is how the new input passes through to the output. It’s a kind of modified skip connection that asks “How can I use the new input in my prediction?”&lt;/li&gt;&lt;/ul&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="335" src="https://lh7-us.googleusercontent.com/Vj3X7tBhV9WaGqNTB8t5zXJ9zRPzd0G075JEPazSOJ-D9S0-UYKwrjHFkGxIZBM1HucvGw4UQazcZJ3Kl7kN8hoqKVaRB8i1qRGjWz56mFA2SrBJBL9XKT72950OZCblDZ7AB0TLqXl4fWAx8BO-P-o" width="602" /&gt;&lt;figcaption&gt;Visual Representation of The SSM Equations&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Additionally, ∆ has a nice interpretation - it’s the step size, or what we might call the linger time or the dwell time. For large ∆, you focus more on that token; for small ∆, you skip past the token immediately and don’t include it much in the next state.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="224" src="https://lh7-us.googleusercontent.com/t1ikATLC5zPLHbXwvx0qTGnvEKAROGmpKl6QZgKfV4hs-2jjr9BvLYoecz0XRXsxHelPl23DoFE6G4P8oeuef2JuQvF0NhSg4N3YIqGmIF9oXBAXtNBrTH6ilcnboFsZPW306EVyZ--TcIHrOqxTbpQ" width="602" /&gt;&lt;figcaption&gt;(source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;And that’s it! That’s the SSM, our ~drop-in replacement for Attention (Communication) in the Mamba block. The Computation in the Mamba architecture comes from regular linear projections, non-linearities, and local convolutions.&lt;/p&gt;&lt;p&gt;Okay great, that’s the theory - but does this work? Well…&lt;/p&gt;&lt;h2 id="effectiveness-vs-efficiency-attention-is-focus-selectivity-is-prioritisation"&gt;Effectiveness vs Efficiency: Attention is Focus, Selectivity is Prioritisation&lt;/h2&gt;&lt;p&gt;At WWDC ‘97, Steve Jobs famously noted that “focusing is about saying no”. Focus is ruthless prioritisation. It’s common to think about Attention &lt;em&gt;positively&lt;/em&gt; as choosing what to &lt;em&gt;notice&lt;/em&gt;. In the Steve Jobs sense, we might instead frame Attention &lt;em&gt;negatively&lt;/em&gt; as choosing what to &lt;em&gt;discard&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;There’s a classic intuition pump in Machine Learning known as the Cocktail Party Problem&lt;sup&gt;13&lt;/sup&gt;. Imagine a party with dozens of simultaneous loud conversations:&lt;/p&gt;&lt;p&gt;Question:&lt;/p&gt;&lt;p&gt;&lt;em&gt;How do we recognise what one person is saying when others are talking at the same time?&lt;sup&gt;14&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Answer:&lt;/p&gt;&lt;p&gt;&lt;em&gt;The brain solves this problem by focusing your “attention” on a particular stimulus and hence drowning out all other sounds as much as possible.&lt;/em&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="376" src="https://lh7-us.googleusercontent.com/C18AUAf7863Uq5SHEwb4aQFcFoA4HW8olFXz_MvZ9HttqJNF2hvIfm3TEsNLhRkXyEJTOwhbtUyOh4QKV2qiGUXwA1sq2_CSTjO7FWPvK2YRnJgYvN859kqXo8pOkZffsXC0iO9z5yajWbc_9CvtwO8" width="602" /&gt;&lt;/figure&gt;&lt;hr /&gt;&lt;p&gt;Transformers use Dot-Product Attention to focus on the most relevant tokens. A big reason Attention is so great is that you have the potential to look back at everything that ever happened in its context. This is like photographic memory when done right.&lt;sup&gt;15&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;Transformers (🤖) are extremely &lt;strong&gt;effective&lt;/strong&gt;. But they aren’t very &lt;strong&gt;efficient&lt;/strong&gt;. They store everything from the past so that they can look back at tokens with theoretically perfect recall.&lt;/p&gt;&lt;p&gt;Traditional RNNs (🔁) are the opposite - they forget a lot, only recalling a small amount in their hidden state and discarding the rest. They are very &lt;strong&gt;efficient&lt;/strong&gt; - their state is small. Yet they are less &lt;strong&gt;effective&lt;/strong&gt; as discarded information cannot be recovered.&lt;/p&gt;&lt;p&gt;We’d like something closer to the Pareto frontier of the effectiveness/efficiency tradeoff. Something that’s more effective than traditional RNNs and more efficient than transformers.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="407" src="https://lh7-us.googleusercontent.com/V2BPTE_TEzO_CAXFnp54TL-nAzSpkiHN_PWZeWOgMN7TInAXL8i3hLgS8ruinxworyEl0248jU6y4Y86Wg1TJca-UjzjCrMQrmSpWceXJ-C4LIg6SJvJykJFfDBb12rIQi84B-aHKdPG_gWsxVkxT20" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;The Mamba Architecture seems to offer a solution which pushes out the Pareto frontier of effectiveness/efficiency.&lt;/p&gt;&lt;p&gt;SSMs are as &lt;strong&gt;efficient&lt;/strong&gt; as RNNs, but we might wonder how &lt;strong&gt;effective&lt;/strong&gt; they are. After all, it seems like they would have a hard time discarding only &lt;em&gt;unnecessary&lt;/em&gt; information and keeping everything relevant. If each token is being processed the same way, applying the same A and B matrices as if in a factory assembly line for tokens, there is no context-dependence. We would like the forgetting and remembering matrices (A and B respectively) to vary and dynamically adapt to inputs.&lt;/p&gt;&lt;h3 id="the-selection-mechanism"&gt;The Selection Mechanism&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Selectivity&lt;/strong&gt; allows each token to be transformed into the state in a way that is unique to its own needs. Selectivity is what takes us from vanilla SSM models (applying the same A (forgetting) and B (remembering) matrices to every input) to Mamba, the &lt;em&gt;&lt;strong&gt;Selective&lt;/strong&gt;&lt;/em&gt; &lt;em&gt;State Space Model&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;In regular SSMs, A, B, C and D are learned matrices - that is&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$\mathbf{A} = \mathbf{A}_{\theta}$ etc. (where θ represents the learned parameters)&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;With the Selection Mechanism in Mamba, A, B, C and D are also functions of x. That is $\mathbf{A} = \mathbf{A}_{\theta(x)}$ etc; the matrices are context dependent rather than static.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="184" src="https://lh7-us.googleusercontent.com/wATzvqFAg8l5HWS9BSCi_OGZRkZ7XmoPfpuZkIaCgLNE1jwrocWaKn_j6OrSG_4n5uULQN6yYK1oWkR4_AbCTXnpaJDTw9PPmeF7btcFa4-7h1QESJIBxTPK4D5vbzFvGJKjxUu-kXqYnRi_oPiVAD4" width="602" /&gt;&lt;figcaption&gt;Mamba (right) differs from traditional SSMs by allowing A,B,C matrices to be &lt;strong&gt;selective &lt;/strong&gt;i.e. context dependent (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Making A and B functions of x allows us to get the best of both worlds:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;We’re selective about what we include in the state, which improves &lt;strong&gt;effectiveness&lt;/strong&gt; vs traditional SSMs.&lt;/li&gt;&lt;li&gt;Yet, since the state size is bounded, we improve on &lt;strong&gt;efficiency&lt;/strong&gt; relative to the Transformer. We have O(1), not O(n) space and O(n) not O(n²) time requirements.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Mamba paper authors write:&lt;/p&gt;&lt;p&gt;&lt;em&gt;The efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension.&lt;/em&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;Humans (mostly) don’t have photographic memory for everything they experience within a lifetime - or even within a day! There’s just way too much information to retain it all. Subconsciously, we select what to remember by choosing to forget, throwing away most information as we encounter it. Transformers (🤖) decide what to focus on at &lt;strong&gt;recall time&lt;/strong&gt;. Humans (🧑) also decide what to throw away at &lt;strong&gt;memory-making time&lt;/strong&gt;. Humans filter out information early and often.&lt;/p&gt;&lt;p&gt;If we had infinite capacity for memorisation, it’s clear the transformer approach is better than the human approach - it truly is more effective. But it’s less efficient - transformers have to store so much information about the past that might not be relevant. Transformers (🤖) only decide what’s relevant at &lt;strong&gt;recall time&lt;/strong&gt;. The innovation of Mamba (🐍) is allowing the model better ways of forgetting earlier - it’s focusing by choosing what to &lt;em&gt;discard&lt;/em&gt; using &lt;strong&gt;Selectivity&lt;/strong&gt;, throwing away less relevant information at &lt;strong&gt;memory-making time&lt;/strong&gt;&lt;sup&gt;16&lt;/sup&gt;.&lt;/p&gt;&lt;h3 id="the-problems-of-selectivity"&gt;The Problems of Selectivity&lt;/h3&gt;&lt;p&gt;Applying the Selection Mechanism does have its gotchas though. Non-selective SSMs (i.e. A,B not dependent on x) are fast to compute in training. This is because the component of&lt;/p&gt;&lt;p&gt;Yt which depends on xi can be expressed as a linear map, i.e. a single matrix that can be precomputed!&lt;/p&gt;&lt;p&gt;For example (ignoring the D component, the skip connection):&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$$y_2 = \mathbf{C}\mathbf{B}x_2 + \mathbf{C}\mathbf{A}\mathbf{B}x_1 +&lt;br /&gt;\mathbf{C}\mathbf{A}\mathbf{A}\mathbf{B}x_0$$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;If we’re paying attention, we might spot something even better here - this expression can be written as a convolution. Hence we can apply the Fast Fourier Transform and the Convolution Theorem to compute this &lt;em&gt;very&lt;/em&gt; efficiently on hardware as in Equation 3 below.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="93" src="https://lh7-us.googleusercontent.com/SnLnXqZ4ArJyiJmMNiUiDMpZ0WYRXuaWO-ZS_Ogj-hThlMVbZz8B3F9g09H5V5CQG6mjgiSphIpjOz4ATr_JYLxCZ9T-EjG5dNy1-mpL1JwL-XWJbymVgyEGhdxpfUT34B1v4iJ_vQAiNUGeTs2FMXs" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;We can calculate Equation 2, the SSM equations, efficiently in the Convolutional Form, Equation 3.&lt;/p&gt;&lt;p&gt;Unfortunately, with the Selection Mechanism, we lose the convolutional form. Much attention is given to making Mamba efficient on modern GPU hardware using similar hardware optimisation tricks to Tri Dao’s Flash Attention&lt;sup&gt;17&lt;/sup&gt;. With the hardware optimisations, Mamba is able to run faster than comparably sized Transformers.&lt;/p&gt;&lt;h3 id="machine-learning-for-political-economistshow-large-should-the-state-be"&gt;Machine Learning for Political Economists - How Large Should The State Be?&lt;/h3&gt;&lt;p&gt;The Mamba authors write, “the efficiency vs. effectiveness tradeoff of sequence models is characterised by how well they compress their state”. In other words, like in political economy&lt;sup&gt;18&lt;/sup&gt;, the fundamental problem is how to manage the state.&lt;/p&gt;&lt;p&gt;🔁 &lt;strong&gt;Traditional RNNs are anarchic&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;They have a small, minimal state. The size of the state is bounded. The compression of state is poor.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;🤖 &lt;strong&gt;Transformers are communist&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;They have a maximally large state. The “state” is just a cache of the entire history with no compression. Every context token is treated equally until recall time.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;🐍&lt;strong&gt;Mamba has a compressed state&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;…but it’s selective about what goes in. Mamba says we can get away with a small state if the state is well focused and effective&lt;sup&gt;19&lt;/sup&gt;.&lt;/em&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="275" src="https://lh7-us.googleusercontent.com/rkN6fi0try__wiIKQ1D9gbHvCrW_dHsKV0jckG85H7P3_Lx1Vm2vHfeb7Zs6N50lnjVx04A3QTQb2JSjMltn8C0kFmvB4DPUgsjj_DEAGu8O-LcKlY7G0RLgLCCsDV_R1W4pkkE67_2rnyx0vCMnayM" width="602" /&gt;&lt;figcaption&gt;Language Models and State Size&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The upshot is that state&lt;strong&gt; representation is critical&lt;/strong&gt;. A smaller state is more efficient; a larger state is more effective. The key is to &lt;strong&gt;selectively&lt;/strong&gt; and &lt;strong&gt;dynamically&lt;/strong&gt; compress data into the state. Mamba’s Selection Mechanism allows for context-dependent reasoning, focusing and ignoring. For both performance and interpretability, understanding the state seems to be very useful.&lt;/p&gt;&lt;h2 id="information-flow-in-transformer-vs-mamba"&gt;Information Flow in Transformer vs Mamba&lt;/h2&gt;&lt;p&gt;How do Transformers know anything? At initialization, a transformer isn’t very smart. It learns in two ways:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Training data (Pretraining, SFT, RLHF etc)&lt;/li&gt;&lt;li&gt;In context-data&lt;/li&gt;&lt;/ol&gt;&lt;h4 id="training-data"&gt;Training Data&lt;/h4&gt;&lt;p&gt;Models learn from their training data. This is a kind of lossy compression of input data into the weights. We can think of the effect of pretraining data on the transformer kinda like the effect of your ancestor’s experiences on your genetics - you can’t recall their experiences, you just have vague instincts about them&lt;sup&gt;20&lt;/sup&gt;.&lt;/p&gt;&lt;h4 id="in-context-data"&gt;In Context-Data&lt;/h4&gt;&lt;p&gt;Transformers use their context as short-term memory, which they can recall with ~perfect fidelity. So we get In-Context Learning, e.g. using induction heads to solve the Indirect Object Identification task, or computing Linear Regression.&lt;/p&gt;&lt;h4 id="retrieval"&gt;Retrieval&lt;/h4&gt;&lt;p&gt;Note that Transformers don’t filter their context at all until recall time. So if we have a bunch of information we think &lt;em&gt;might&lt;/em&gt; be useful to the Transformer, we filter it &lt;em&gt;outside&lt;/em&gt; the Transformer (using Information Retrieval strategies) and then stuff the results into the prompt. This process is known as Retrieval Augmented Generation (RAG). RAG determines relevant information for the context window of a transformer. A human with the internet is kinda like a RAG system - you still have to know what to search but whatever you retrieve is as salient as short-term memory to you.&lt;/p&gt;&lt;h4 id="information-flow-for-mamba"&gt;Information Flow for Mamba&lt;/h4&gt;&lt;p&gt;Training Data acts similarly for Mamba. However, the lines are slightly blurred for in-context data and retrieval. In-context data for Mamba &lt;em&gt;is&lt;/em&gt; compressed/filtered similar to retrieval data for transformers. This in-context data is also accessible for look-up like for transformers (although with somewhat lower fidelity).&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="236" src="https://lh7-us.googleusercontent.com/0dxiIk5NUI9g_P7G5lr5CSziEVKABYdtIW-R4Rxi6OHwWV_vLYVb1wtetVmzNtRWcLngldL4A8WUQA2jhIQj-IJmpaYr97xt-2Du_dxVOe5ppA4EcRNxEbjQvmjbND_DhyKhO6nsnS4nf1NxvRLwx-o" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;Transformer context is to Mamba states what short-term is to long-term memory. Mamba doesn’t just have “RAM”, it has a hard drive&lt;sup&gt;21&lt;/sup&gt; &lt;sup&gt;22&lt;/sup&gt;.&lt;/p&gt;&lt;h3 id="swapping-states-as-a-new-prompting-paradigm"&gt;Swapping States as a New Prompting Paradigm&lt;/h3&gt;&lt;p&gt;Currently, we often use RAG to give a transformer contextual information.&lt;/p&gt;&lt;p&gt;With Mamba-like models, you could instead imagine having a library of states created by running the model over specialised data. States could be shared kinda like LoRAs for image models.&lt;/p&gt;&lt;p&gt;For example, I could do inference on 20 physics textbooks and, say, 100 physics questions and answers. Then I have a state which I can give to you. Now you don’t need to add any few-shot examples; you just simply ask your question. &lt;strong&gt;The in-context learning is in the state&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;In other words, you can drag and drop downloaded states into your model, like literal plug-in cartridges. And note that “training” a state doesn’t require any backprop. It’s more like a highly specialised one-pass fixed-size compression algorithm. This is unlimited in-context learning applied at inference time for zero-compute or latency&lt;sup&gt;23&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;The structure of an effective LLM call goes from…&lt;/p&gt;&lt;ol&gt;&lt;li&gt;System Prompt&lt;/li&gt;&lt;li&gt;Preamble&lt;/li&gt;&lt;li&gt;Few shot-examples&lt;/li&gt;&lt;li&gt;Question&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;…for Transformers, to simply…&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Inputted state (with problem context, initial instructions, textbooks, and few-shot examples)&lt;/li&gt;&lt;li&gt;Short question&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;…for Mamba.&lt;/p&gt;&lt;p&gt;This is cheaper and faster than few-shot prompting (as the state is infinitely reusable without inference cost). It’s also MUCH cheaper than finetuning and doesn’t require any gradient updates. We could imagine retrieving states in addition to context.&lt;/p&gt;&lt;h2 id="mamba-mechanistic-interpretability"&gt;Mamba &amp;amp; Mechanistic Interpretability&lt;/h2&gt;&lt;p&gt;Transformer interpretability typically involves:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;understanding token relationships via attention,&lt;/li&gt;&lt;li&gt;understanding circuits, and&lt;/li&gt;&lt;li&gt;using Dictionary Learning for unfolding MLPs.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Most of the ablations that we would like to do for Mamba are still valid, but understanding token communication (1) is now more nuanced. All information moves between tokens via hidden states instead of the Attention Mechanism which can “teleport” information from one sequence position to another.&lt;/p&gt;&lt;p&gt;For understanding in-context learning (ICL) tasks with Mamba, we will look to intervene on the SSM state. A classic task in-context learning task is Indirect Object Identification in which a model has to finish a paragraph like:&lt;/p&gt;&lt;p&gt;&lt;em&gt;Then, Shelby and Emma had a lot of fun at the school. [Shelby/Emma] gave an apple to [BLANK]&lt;/em&gt;&lt;/p&gt;&lt;p&gt;The model is expected to fill in the blank with the name that is not repeated in the paragraph. In the chart below we can see that information is passed from the [Shelby/Emma] position to the final position via the hidden state (see the two blue lines in the top chart).&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="256" src="https://lh7-us.googleusercontent.com/ZDBpRS1yEscEZcsJtevlPaM5URUP58dgJ2csAIcWP-hmQcje8kBi-u4zAWYnbeE26YXWemOh32pdHM2TgaSanGePOVgRiss8svxP17nLPBvg1YjLE4W1uIGkTmDI9PbZO42u_4KfYoSeaRnZz_W4HfY" width="602" /&gt;&lt;/figure&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="256" src="https://lh7-us.googleusercontent.com/j8aQ6XIxedX6Zut0rz7CE_e02KgBjyJvg7QQ7U9FkM2TjSWWSNk1v7gFVeGSsETqwQGvF8flh0lIUmSLIVqW9rwHC69rImw5MPj0vA0Y4XihacOzZnhUeKMZpf3bWtJTM_TB67EDYKIyfp2DeX4pNFU" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;Since it’s hypothesised that much of In-Context Learning in Transformers is downstream of more primitive sequence position operations (like Induction Heads), Mamba being able to complete this task suggests a more general In-Context Learning ability.&lt;/p&gt;&lt;h2 id="what%E2%80%99s-next-for-mamba-ssms"&gt;What’s Next for Mamba &amp;amp; SSMs?&lt;/h2&gt;&lt;p&gt;Mamba-like models are likely to excel in scenarios requiring extremely long context and long-term memory. Examples include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Processing DNA&lt;/li&gt;&lt;li&gt;Generating (or reasoning over) video&lt;/li&gt;&lt;li&gt;Writing novels&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;An illustrative example is agents with long-term goals.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Suppose you have an agent interacting with the world. Eventually, its experiences become too much for the context window of a transformer. The agent then has to compress or summarise its experiences into some more compact representation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;But how do you decide what information is the most useful as a summary? If the task is language, LLMs are actually fairly good at summaries - okay, yeah, you’ll lose some information, but the most important stuff can be retained.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;However, for other disciplines, it might not be clear how to summarise. For example, what’s the best way to summarise a 2 hour movie?&lt;sup&gt;24&lt;/sup&gt;. Could the model itself learn to do this naturally rather than a hacky workaround like trying to describe the aesthetics of the movie in text?&lt;/em&gt;&lt;/p&gt;&lt;p&gt;This is what Mamba allows. Actual long-term memory. A real state where the model learns to keep what’s important. Prediction is compression - learning what’s useful to predict what’s coming next inevitably leads to building a useful compression of the previous tokens.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;The implications for Assistants are clear:&lt;/p&gt;&lt;p&gt;Your chatbot co-evolves with you. It remembers.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="339" src="https://lh7-us.googleusercontent.com/agZClF-xa6q13BlEbfZLFKP3DM0hJiRy9kC0MRFoNPi8kdWCh8_BUa5oLC0V_6jTmcNQQfmMr7GGa6gwIe3CEGVeK79AFMhE1gMnbdhEoQ8iFCRuO7Yc6Xi2M3kaVIGZ4LTfDKqITQ6ap1DylOqbWs4" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;The film HER is looking better and better as time goes on 😳&lt;/p&gt;&lt;h3 id="agents-ai-safety"&gt;Agents &amp;amp; AI Safety&lt;/h3&gt;&lt;p&gt;One reason for positive updates in existential risk from AGI is Language Models. Previously, Deep-RL agents trained via self-play looked set to be the first AGIs. Language models are inherently much safer since they aren’t trained with long-term goals&lt;sup&gt;25&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;The potential for long-term sequence reasoning here brings back the importance of agent-based AI safety. Few agent worries are relevant to Transformers with an 8k context window. Many are relevant to systems with impressive long-term memories and possible instrumental goals.&lt;/p&gt;&lt;h3 id="the-best-collab-since-taco-bell-kfc-%F0%9F%A4%96-x-%F0%9F%90%8D"&gt;The Best Collab Since Taco Bell &amp;amp; KFC: 🤖 x 🐍&lt;/h3&gt;&lt;p&gt;The Mamba authors show that there’s value in combining Mamba’s long context with the Transformer’s high fidelity over short sequences. For example, if you’re making long videos, you likely can’t fit a whole movie into a Transformer’s context for attention&lt;sup&gt;26&lt;/sup&gt;. You could imagine having Attention look at the most recent frames for short-term fluidity and an SSM for long-term narrative consistency&lt;sup&gt;27&lt;/sup&gt;.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;This isn’t the end for Transformers. Their high effectiveness is exactly what’s needed for many tasks. But now Transformers aren’t the only option. Other architectures are genuinely feasible.&lt;/p&gt;&lt;p&gt;So we’re not in the post-Transformer era. But for the first time, we’re living in the post-only-Transformers era&lt;sup&gt;28&lt;/sup&gt;. And this blows the possibilities wide open for sequence modelling with extreme context lengths and native long-term memory.&lt;/p&gt;&lt;p&gt;Two ML researchers, Sasha Rush (HuggingFace, Annotated Transformer, Cornell Professor) and Jonathan Frankle (Lottery Ticket Hypothesis, MosaicML, Harvard Professor), currently have a bet here.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="324" src="https://lh7-us.googleusercontent.com/-_S7CaQ4OxepapriZhhAs25xq-H_dSnavPxXkm0_lMMZjtno4kgWfjS1PAcLhYpbMz6BNNYd-RoxBA_Fy45CemDdvofbP7oPVQ3ygHBQNQ8pMVf7l5YnLSCgE3L1J9muCpoFmTSz09zcX9xEigRrKnc" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;Currently Transformers are far and away in the lead. With 3 years left, there’s now a research direction with a fighting chance.&lt;/p&gt;&lt;p&gt;All that remains to ask is: &lt;strong&gt;Is Attention All We Need?&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;&lt;hr /&gt;&lt;!--kg-card-begin: html--&gt;&lt;p&gt;
    1. see Figure 8 in the Mamba paper.
    &lt;br /&gt;2. And scaling up with massive compute.
    &lt;br /&gt;3. More specifically the scaled dot-product Attention popularised by Transformers
    &lt;br /&gt;4. For people who don’t see Temple Run as the cultural cornerstone it is 🤣 Temple Run was an iPhone game from 2011 similar to Subway Surfer
    &lt;br /&gt;5. Here we assume the environment is sufficiently smooth.
    &lt;br /&gt;6. One pretty important constraint for this to be efficient is that we don’t allow the individual elements of the state vector to interact with each other directly. We’ll use a combination of the state dimensions to determine the output but we don’t e.g. allow the velocity of the runner and the direction of the closest obstacle (or whatever else was in our state) to directly interact. This helps with efficient computation and we achieve this practically by constraining A to be a diagonal matrix.
    &lt;br /&gt;7. Concretely consider the case of Language Models - each token is a discrete step 
    &lt;br /&gt;8. ZOH also has nice properties for the initialisations - we want A_bar to be close to the identity so that the state can be mostly maintained from timestep to timestep if desired. ZOH gives A_bar as an exponential so any diagonal element initialisations close to zero give values close to 1 
    &lt;br /&gt;9. This is known as the Euler discretisation in the literature
    &lt;br /&gt;10. It’s wild to note that some readers might not have, we’re so far into the age of Attention that RNNs have been forgotten!
    &lt;br /&gt;11. B is like the Query (Q) matrix for Transformers.
    &lt;br /&gt;12. C is like the Output (O) matrix for Transformers. 
    &lt;br /&gt;13. Non-alcoholic options also available! 
    &lt;br /&gt;14. Especially as all voices roughly occupy the same space on the audio frequency spectrum Intuitively this seems really hard! 
    &lt;br /&gt;15. Note that photographic memory doesn’t necessarily imply perfect inferences from that memory! 
    &lt;br /&gt;16. To be clear, if you have a short sequence, then a transformer should theoretically be a better approach. If you can store the whole context, then why not!? If you have enough memory for a high-resolution image, why compress it into a JPEG? But Mamba-style architectures are likely to hugely outperform with long-range sequences. 
    &lt;br /&gt;17. More details are available for engineers interested in CUDA programming - Tri’s talk, Mamba paper section 3.3.2, and the official CUDA code are good resources for understanding the Hardware-Aware Scan 
    &lt;br /&gt;18. or in Object Oriented Programming 
    &lt;br /&gt;19. Implications to actual Political Economy are left to the reader but maybe Gu and Dao accidentally solved politics!? 
    &lt;br /&gt;20. This isn’t a perfect analogy as human evolution follows a genetic algorithm rather than SGD. 
    &lt;br /&gt;21. Albeit a pretty weird hard drive at that - it morphs over time rather than being a fixed representation.  
    &lt;br /&gt;22. As a backronym, I’ve started calling the hidden_state the state space dimension (or selective state dimension) which shortens to SSD, a nice reminder for what this object represents - the long-term memory of the system.
    &lt;br /&gt;23. I’m thinking about this similarly to the relationship between harmlessness finetuning and activation steering. State swapping, like activation steering, is an inference time intervention giving comparable results to its train time analogue. 
    &lt;br /&gt;24. This is a very non-trivial problem! How do human brains represent a movie internally? It’s not a series of the most salient frames, nor is it a text summary of the colours, nor is it a purely vibes-based summary if you can memorise some lines of the film. 
    &lt;br /&gt;25. They’re also safer since they inherently understand (though don’t necessarily embody) human values. It’s not all clear that how to teach an RL agent human morality. 
    &lt;br /&gt;26. Note that typically an image (i.e. a single frame) counts as &amp;gt;196 tokens, and movies are typically 24 fps so you’ll fill a 32k context window in 7 seconds 🤯 
    &lt;br /&gt;27. Another possibility that I’m excited about is applying optimisation pressure to the state itself as well as the output to have models that respect particular use cases. 
    &lt;br /&gt;28. This is slightly hyperbolic, the TS-Mixer for time series, Gradient Boosting Trees for tabular data and Graph Neural Networks for weather prediction exist and are currently used, but these aren’t at the core of AI
&lt;/p&gt;&lt;!--kg-card-end: html--&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Kola Ayonrinde is a Research Scientist and Machine Learning Engineer with a flair for writing. He integrates technology and creativity, focusing on applying machine learning in innovative ways and exploring the societal impacts of tech advancements.&lt;/p&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;This post was originally posted on Kola's personal blog.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thanks to Gonçalo for reading an early draft, Jaden for the nnsight library used for the Interpretability analysis and Tessa for Mamba patching visualisations.Also see: &lt;/em&gt;&lt;em&gt;Mamba paper&lt;/em&gt;&lt;em&gt;, Mamba Python code, &lt;/em&gt;&lt;em&gt;Annotated S4&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;Nathan Labenz podcast&lt;/em&gt;&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Kola Ayonrinde, "Mamba Explained," The Gradient, 2024&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;@article{Ayonrinde2024mamba,
    author = {Kola Ayonrinde},
    title = {Mamba Explained},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/mamba-explained},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Car-GPT: Could LLMs finally make self-driving cars happen?]]&amp;gt;https://thegradient.pub/car-gpt/65db7b4193571d5c8c154a73Fri, 08 Mar 2024 16:55:18 GMT&lt;!--&amp;lt;![CDATA[&lt;img src="https://thegradient.pub/content/images/2024/03/car-gpt.jpg" alt="Car-GPT: Could LLMs finally make self-driving cars happen?"&gt;&lt;p&gt;In 1928, London was in the middle of a terrible health crisis, devastated by bacterial diseases like pneumonia, tuberculosis, and meningitis. Confined in sterile laboratories, scientists and doctors were stuck in a relentless cycle of trial and error, using traditional medical approaches to solve complex problems.&lt;/p&gt;&lt;p&gt;This is when, in September 1928, an accidental event changed the course of the world.&lt;strong&gt; &lt;/strong&gt;A Scottish doctor named Alexander Fleming forgot to close a petri dish (the transparent circular box you used in science class), which got contaminated by mold. This is when Fleming noticed something peculiar: all bacteria close to the moisture were dead, while the others survived.&lt;/p&gt;&lt;p&gt;&amp;quot;What was that moisture made of?&amp;quot; wondered M. Flemming.&lt;strong&gt; &lt;/strong&gt;This was when he discovered that Penicillin, the main component of the mold, was a powerful bacterial killer. This led to the groundbreaking discovery of penicillin, leading to the antibiotics we use today. In a world where doctors were relying on existing well-studied approaches, Penicillin was the unexpected answer.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Self-driving cars may be following a similar event.&lt;/strong&gt; Back in the 2010s, most of them were built using what we call a &amp;#xAB; modular &amp;#xBB; approach. The software &amp;#xAB; autonomous &amp;#xBB; part is split into several modules, such as Perception (the task of seeing the world), or Localization (the task of accurately localize yourself in the world), or Planning (the task of creating a trajectory for the car to follow, and implementing the &amp;#xAB; brain &amp;#xBB; of the car). Finally, all these go to the last module: Control, that generates commands such as &amp;#xAB; steer 20&amp;#xB0; right &amp;#xBB;, etc&amp;#x2026; So this was the well-known approach. &lt;/p&gt;&lt;p&gt;But a decade later, companies started to take another discipline very seriously: &lt;strong&gt;End-To-End learning&lt;/strong&gt;. The core idea is to replace every module with a single neural network predicting steering and acceleration, but as you can imagine, this introduces a black box problem.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/EpMJPDK-TBu9ZhN25UrxVbAk-9rJjEvtitzjvPpzjhTBPdkk-judKQtfWQNf7vtNrG1sfsvkUhpbtMGplWN5bbnx5ULbfNj6vpRf8RVlt5eDn8MN99FObGbPsmokdNlCGZ1NWq-uw32QVitv4NZC3zI" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="161"&gt;&lt;figcaption&gt;The 4 Pillars of Self-Driving Cars are Perception, Localization, Planning, and Control. Could a Large Language Model replicate them? (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;These approaches are known, but don&amp;#x2019;t solve the self-driving problem yet. So, we could be wondering:&lt;strong&gt; &amp;quot;What if LLMs (Large Language Models), currently revolutionizing the world, were the unexpected answer to autonomous driving?&amp;quot;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This is what we&amp;apos;re going to see in this article, beginning with a simple explanation of what LLMs are and then diving into how they could benefit autonomous driving.&lt;/p&gt;&lt;h2 id="preamble-llms-what"&gt;&lt;strong&gt;Preamble: LLMs-what?&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Before you read this article, you must know something: I&amp;apos;m not an LLM pro, at all. This means, I know too well the struggle to learn it. I understand what it&amp;apos;s like to google &amp;quot;learn LLM&amp;quot;; then see 3 sponsored posts asking you to download e-books (in which nothing concrete appears)... then see 20 ultimate roadmaps and GitHub repos, where step 1/54 is to view a 2-hour long video (and no one knows what step 54 is because it&amp;apos;s so looooooooong).&lt;/p&gt;&lt;p&gt;So, instead of putting you through this pain myself, let&amp;apos;s just break down what LLMs are in 3 key ideas:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Tokenization&lt;/li&gt;&lt;li&gt;Transformers&lt;/li&gt;&lt;li&gt;Processing Language&lt;/li&gt;&lt;/ol&gt;&lt;h3 id="tokenization"&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;In ChatGPT, you input a piece of text, and it returns text, right? Well, what&amp;apos;s actually happening is that your text is first converted into tokens.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/_rT6_ShRUbi-bZpaKL7JF-BhE_rfDg_V8De5nYj0O5tGgAtLTyYhnGleIy7nBJ3vyrUsfge6cdReCctzsfCyW_XP6WUm21pU350RpOoxWzb2SYRvMcKMIZAOE6wdFou7t_ERJ2_Jht6uUhfg_sBgcbI" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="600" height="303"&gt;&lt;figcaption&gt;Example of tokenization of a sentence, each word becomes a &amp;quot;token&amp;quot;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But what&amp;apos;s a token? You might ask. Well, a token can correspond to a word, a character, or anything we want. Think about it -- if you are to send a sentence to a neural network, you didn&amp;apos;t plan on sending actual words, did you?&lt;/p&gt;&lt;p&gt;The input of a neural network is always a number, so you need to convert your text into numbers; this is tokenization.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/pZ3qf5HQNrqXqb5bbGkLgWQPvu04-2b_ejpv4m3i5C9VfcPg3yZm7cmaD6lq4xgrA4DhUBJpCa-HB4i7iAPo8-Hyrde9sLiBYBiY2d7c9O17ePJtCqAb15dvcDEGxofEwneP6Nx2_oSiT26m4cLvcMc" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="139"&gt;&lt;figcaption&gt;What tokenization actually is: A conversion from words to numbers&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Depending on the model (ChatGPT, LLAMA, etc...), a token can mean different things: a word, a subword, or even a character. We could take the English vocabulary and define these as words or take parts of words (subwords) and handle even more complex inputs. For example, the word &amp;#xAB; a &amp;#xBB; could be token 1, and the word &amp;#xAB; abracadabra &amp;#xBB; would be token 121.&lt;/p&gt;&lt;h3 id="transformers"&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Now that we understand how to convert a sentence into a series of numbers, we can send that series into our neural network! At a high level, we have the following structure:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/J1CkM3ItKevopmi-0gbSHWJnMStL4dZWksllG15OlaDI4PFgk-FtFeQ7O0CnP1dKx9ZHV7PUAlmBK9lFwJQrHnJj1JAXAMHdbZH13hd07dYL55ZCsxQChf06dYj_JoXEvNeAqdfmj2IcdwD8sP5OZtI" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="407"&gt;&lt;figcaption&gt;A Transformer is an Encoder-Decoder Architecture that takes a sequence of tokens as input and outputs a another series of tokens&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If you start looking around, you will see that some models are based on an encoder-decoder architecture, some others are purely encoder-based, and others, like GPT, are purely decoder-based.&lt;/p&gt;&lt;p&gt;Whatever the case, they all share the core Transformer blocks: multi-head attention, layer normalization, addition and concatenation, blocks, cross-attention, etc...&lt;/p&gt;&lt;p&gt;&lt;strong&gt;This is just a series of attention blocks getting you to the output&lt;/strong&gt;. So how does this word prediction work?&lt;/p&gt;&lt;h3 id="the-output-next-word-prediction"&gt;&lt;strong&gt;The output/ Next-Word Prediction&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;The Encoder learns features and understands context... But what does the decoder do? In the case of object detection, the decoder is predicting bounding boxes. In the case of segmentation, the decoder is predicting segmentation masks. What about here?&lt;/p&gt;&lt;p&gt;In our case, the decoder is trying to generate a series of words; we call this task &amp;quot;next-word prediction&amp;quot;.&lt;/p&gt;&lt;p&gt;Of course, it does it similarly by predicting numbers or tokens. This characterizes our full model as shown below,&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/YS9WFjjuYTq7QkzPnx4xgTQnU0Pmr22i4fEzXXWuBf6wD--eYL8FvdoEpkqlCMKraBaSDuo7j0sWR7ltUaWI31_Bvq9PtJoPpoWRFQnjKOth1P7mnxfzmGT8ppUslOPMhbOzJY49F4IHBMZfyzax18E" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="519"&gt;&lt;figcaption&gt;I would say the loss function for this particular output produces a near-0 value.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now, there are many &amp;quot;concepts&amp;quot; that you should learn on top of this intro: everything Transformer and Attention related, but also few-shot learning, pretraining, finetuning, and more...&lt;/p&gt;&lt;p&gt;Ok... but what does it have to do with self-driving cars? I think it&amp;apos;s time to move to stage 2.&lt;/p&gt;&lt;h2 id="chat-gpt-for-self-driving-cars"&gt;&lt;strong&gt;Chat-GPT for Self-Driving Cars&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The thing is, you&amp;apos;ve already been through the tough part. The rest simply is: &amp;quot;How do I adapt this to autonomous driving?&amp;quot;. Think about it; we have a few modifications to make:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Our input now becomes either images, sensor data&lt;/strong&gt; (LiDAR point clouds, RADAR point clouds, etc...), or even algorithm data (lane lines, objects, etc...). All of it is &amp;quot;tokenizable&amp;quot;, as Vision Transformers or Video Vision Transformers do.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Our Transformer model pretty much remains the same&lt;/strong&gt; since it only operates on tokens and is independent of the kind of input.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;The output is based on the set of tasks we want to do.&lt;/strong&gt; It could be explaining what&amp;apos;s happening in the image or could &amp;#xA0;also be a direct driving task like switching lanes.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So, let&amp;apos;s begin with the end:&lt;/p&gt;&lt;h3 id="what-self-driving-car-tasks-could-llm-solve"&gt;&lt;strong&gt;What self-driving car tasks could LLM solve?&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;There are many tasks involved in autonomous driving, but not all of them are GPT-isable. The most active research areas in 2023 have been:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Perception&lt;/strong&gt;: Based on an input image, describe the environment, number of objects, etc...&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Planning&lt;/strong&gt;: Based on an image, or a bird-eye view, or the output of perception, describe what we should do (keep driving, yield, etc...)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Generation&lt;/strong&gt;: Generate training data, alternate scenarios, and more... using &amp;quot;diffusion&amp;quot;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Question &amp;amp; Answers&lt;/strong&gt;: Create a chat interface and ask the LLM to answer questions based on the scenario.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="llms-in-perception"&gt;&lt;strong&gt;LLMs in Perception&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;In Perception, the input is a series of images, and the output is usually a set of objects, lanes, etc... In the case of LLMs, we have 3 core tasks: &lt;strong&gt;Detection&lt;/strong&gt;, &lt;strong&gt;Prediction&lt;/strong&gt;, and &lt;strong&gt;Tracking&lt;/strong&gt;. An example with Chat-GPT, when you send it an image and ask to describe what&amp;apos;s going on is shown below:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/unUisu66NolUzzipNfKObr8kE6n8PRcTMy86cYYIG1aIPLkYKZd34zmzGrkM4yS6lKNoXpvORHwnfORfOsy8aRNUx9AwEDN_qQN4tiuutBRh8l3h_vVpfVzOJ7UdQ-CuWKI5EJsze9le6qRA7VQ1QoY" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="425" height="308"&gt;&lt;figcaption&gt;A GPT-4 Vision model can return the objects in the image, just like object detectors do (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Other models such as HiLM-D and MTD-GPT can also do this, some work also for videos. Models like PromptTrack, also have the ability to assign unique IDs (this car in front of me is ID #3), similar to a 4D Perception model.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/WcjhR7diFbZrVeKdiVyQbC_HtYJVGUQsOBka0zikaD2JZpfmNxcyEJlpzxZfvobWrMu6srxUEGPcxpdVSywVKW-0gIuOISCqLCVfjaA6Q7KaNb1etKfNybXkya4yFyx7AY0Y2_ZZw_cY_gWSccO0B2Q" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="267"&gt;&lt;figcaption&gt;PromptTrack combines the DETR object detector with Large Language Models&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In this model, multi-view images are sent to an Encoder-Decoder network that is trained to predict annotations of objects such as bounding boxes, and attention maps). These maps are then combined with a prompt like &amp;apos;find the vehicles that are turning right&amp;apos;.The next block then finds the 3D Bounding Box localization and assigns IDs using a bipartite graph matching algorithm like the Hungarian Algorithm.&lt;/p&gt;&lt;p&gt;This is cool, but this isn&amp;apos;t the &amp;quot;best&amp;quot; application of LLMs so far:&lt;/p&gt;&lt;h3 id="llms-in-decision-making-navigation-and-planning"&gt;&lt;strong&gt;LLMs in Decision Making, Navigation, and Planning&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;If Chat-GPT can find objects in an image, it should be able to tell you what to do with these objects, shouldn&amp;apos;t it? Well, this is the task of Planning i.e. defining a path from A to B, based on the current perception. While there are numerous models developed for this task, the one that stood out to me was Talk2BEV:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/N3ZvMnLMjQ6jwL4FNvvTyM4U6KFrri0jV-0yOYVH9lAAtRH7MD8aMX_LHhjeBFKxGwTdrATJoNUQe-sUqEB3utLnpreCT4e4TIO3qX3LTrzBKwZ7kPAfzxAu6osJ35tYpapCiTTWDtx0tUOHXcNqu04" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="600" height="179"&gt;&lt;figcaption&gt;Talk2BEV takes perception one step further and also tells you what to do&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The main difference between models for planning and Perception-only models is that here, we&amp;apos;re going to train the model on human behavior to suggest ideal driving decisions. We&amp;apos;re also going to change the input from multi-view to Bird Eye View since it is much easier to understand.&lt;/p&gt;&lt;p&gt;This model works both with LLaVA and ChatGPT4, and here is a demo of the architecture:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/-bl_IDT2SqF75q3d20EORJcH22oXWMjFmkLFn0ZKbVV5oshlr5BkZEnscfUSg_-pkzMDJ3Jo38mdu6whUmIDWq7pXfxXxdwgc3Kj-WUwv5LNWUHIvH3r6mfpKP9s5PD7NoA7e0R3pBoic6ijwfD57a--&gt;</description><content:encoded>&amp;lt;![CDATA[The Gradient]]&amp;gt;https://thegradient.pub/https://thegradient.pub/favicon.pngThe Gradienthttps://thegradient.pub/Ghost 5.33Wed, 25 Feb 2026 19:12:31 GMT60&amp;lt;![CDATA[After Orthogonality: Virtue-Ethical Agency and AI Alignment]]&amp;gt;&lt;h2 id="preface"&gt;Preface&lt;/h2&gt;
&lt;p&gt;This essay argues that rational people don’t have goals, and that rational AIs shouldn’t have goals. Human actions are rational not because we direct them at some final ‘goals,’ but because we align actions to &lt;em&gt;practices&lt;/em&gt;&lt;sup class="footnote-ref"&gt;[1]&lt;/sup&gt;: networks of actions, action-dispositions, action-evaluation criteria,&lt;/p&gt;]]&amp;gt;https://thegradient.pub/virtue-ethics-ai-alignment/69879dd077c3d76051ac158fWed, 18 Feb 2026 23:25:52 GMT&lt;h2 id="preface"&gt;Preface&lt;/h2&gt;
&lt;img alt="After Orthogonality: Virtue-Ethical Agency and AI Alignment" src="https://thegradient.pub/content/images/2026/02/ramelli-featured.jpg" /&gt;&lt;p&gt;This essay argues that rational people don’t have goals, and that rational AIs shouldn’t have goals. Human actions are rational not because we direct them at some final ‘goals,’ but because we align actions to &lt;em&gt;practices&lt;/em&gt;&lt;sup class="footnote-ref"&gt;[1]&lt;/sup&gt;: networks of actions, action-dispositions, action-evaluation criteria, and action-resources that structure, clarify, develop, and promote themselves. If we want AIs that can genuinely support, collaborate with, or even &lt;strong&gt;comply&lt;/strong&gt; with human agency, AI agents’ deliberations must share a “type signature” with the practices-based logic we use to reflect and act.&lt;/p&gt;
&lt;p&gt;I argue that these issues matter not just for aligning AI to grand ethical ideals like human flourishing, but also for aligning AI to core safety-properties like transparency, helpfulness, harmlessness, or corrigibility. Concepts like ’harmlessness’ or ‘corrigibility’ are unnatural -- brittle, unstable, arbitrary -- for agents who’d interpret them in terms of goals or rules, but natural for agents who’d interpret them as dynamics in networks of actions, action-dispositions, action-evaluation criteria, and action-resources.&lt;/p&gt;
&lt;p&gt;While the issues this essay tackles tend to sprawl, one theme that reappears over and over is the relevance of the formula ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly.’ I argue that this formula captures something important about both meaningful human life-activity (art is the artistic promotion of art, romance is the romantic promotion of romance) and real human morality (to care about kindness is to promote kindness kindly, to care about honesty is to promote honesty honestly).&lt;/p&gt;
&lt;p&gt;I start by asking: What follows for AI alignment if we take the concept of eudaimonia -- active, rational human flourishing -- seriously? I argue that the concept of eudaimonia doesn’t simply point to a desired state or trajectory of the world that we should set as an AI’s optimization target, but rather points to a structure of deliberation different from standard consequentialist&lt;sup class="footnote-ref"&gt;[2]&lt;/sup&gt; rationality. I then argue that this form of rational activity and valuing, which l call &lt;em&gt;eudaimonic rationality&lt;/em&gt;&lt;sup class="footnote-ref"&gt;[3]&lt;/sup&gt;, is a useful or even necessary framework for the agency and values of human-aligned AIs.&lt;/p&gt;
&lt;p&gt;These arguments are based both on the dangers of a “type mismatch” between human flourishing as an optimization target and consequentialist optimization as a form, and on certain material advantages that eudaimonic rationality plausibly possesses in comparison to deontological and consequentialist agency with regard to stability and safety.&lt;/p&gt;
&lt;p&gt;The concept of eudaimonia, I argue, suggests a form of rational activity without a strict distinction between means and ends, or between ‘instrumental’ and ‘terminal’ values. In this model of rational activity, a rational action is an element of a valued practice in roughly the same sense that a note is an element of a melody, a time-step is an element of a computation, and a moment in an organism’s cellular life is an element of that organism’s self-subsistence and self-development.&lt;sup class="footnote-ref"&gt;[4]&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;My central claim is that our intuitions about the nature of human flourishing are implicitly intuitions that eudaimonic rationality can be functionally robust in a sense highly critical to AI alignment. More specifically, I argue that in light of our best intuitions about the nature of human flourishing it’s plausible that eudaimonic rationality is a &lt;em&gt;natural&lt;/em&gt; form of agency, and that eudaimonic rationality is &lt;em&gt;effective&lt;/em&gt; even by the light of certain consequentialist approximations of its values. I then argue that if our goal is to align AI in support of human flourishing, and if it is furthermore plausible that eudaimonic rationality is natural and efficacious, then many classical AI safety considerations and ‘paradoxes’ of AI alignment speak in favor of trying to instill AIs with eudaimonic rationality.&lt;/p&gt;
&lt;p&gt;Throughout this essay, I will sometimes explicitly and often implicitly be asking whether some form of agency or rationality or practice is &lt;em&gt;natural&lt;/em&gt;. The sense of ‘natural’ I’m calling on is certainly related to the senses used in various virtue-ethical traditions, but the interest I take in it is less immediately normative and more material or technical. While I have no reductive definition at hand, the intended meaning of ‘natural’ is related to stability, coherence, relative non-contingency, ease of learnability, lower algorithmic complexity, convergent cultural evolution, hypothetical convergent cultural evolution across different hypothetical rational-animal species, potential convergent evolution between humans and neural-network based AI, and targetability by ML training processes. While I will also make many direct references to AI alignment, this question of material naturalness is where the real alignment-critical action takes place: if we learn that certain exotic-sounding forms of agency, rationality, or practice are both themselves natural and make the contents of our all-too-human values natural in turn, then we have learned about good, relatively safe, and relatively easy targets for AI alignment.&lt;/p&gt;
&lt;p&gt;Readers may find the following section-by-section overview useful for navigating the essay:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part &lt;em&gt;I&lt;/em&gt; presents a class of cases of rational deliberation that are very different from the Effective Altruism-style optimization&lt;sup class="footnote-ref"&gt;[5]&lt;/sup&gt; many in the AI-alignment world treat as the paradigm of rational deliberation. I call this class of rational deliberations 'eudaimonic rationality,' and identify it with the form of rationality that guides a mathematician or an artist or a friend when they reflect on what to do in mathematics or in art or in friendship.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;II&lt;/em&gt; looks at the case of research mathematics (via an account by Terry Tao) as an example of eudaimonic rationality at work. What does a mathematician try to do in math? I say she tries to be &lt;em&gt;mathematically excellent&lt;/em&gt;, which involves promoting mathematical excellence through mathematical excellence, and that this structure is closely related to why 'mathematical excellence' can even be a concept.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;III&lt;/em&gt; argues that for eudaimonic agents such as a mathematician who is trying to do excellent mathematics, distinctions between ‘instrumental goods’ and ‘terminal goods’ (intrinsic goods) are mostly unnatural. This makes reflection about values go very differently for a eudaimonic agent than for an Effective Altruism-style agent. Instead of looking to reduce a network of causally intertwined apparent values to a minimal base of intrinsic values that “explains away” the rest as instrumental, a eudaimonic agent looks for organism-like causal coherence in a network of apparent values.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;IV&lt;/em&gt; cashes out the essay’s central concepts: A &lt;em&gt;eudaimonic practice&lt;/em&gt; is a network of actions, action-dispositions, action-evaluation criteria, and action-resources where high-scoring actions reliably (but defeasibly) causally promote future high-scoring actions. &lt;em&gt;Eudaimonic rationality&lt;/em&gt; is a class of reflective equilibration and deliberation processes that assume an underlying eudaimonic practice and seek to optimize aggregate action-scores specifically via high-scoring action.&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;V&lt;/em&gt;, I argue that many puzzles and ‘paradoxes’ about AI alignment are driven by the assumption that mature AI agents will be Effective Altruism-style optimizers. A “type mismatch” between Effective Altruism-style optimization and eudaimonic rationality makes it nearly impossible to translate the interests of humans -- agents who practice eudaimonic rationality -- into a utility function legible to an Effective Altruism-style optimizer AI. But this does not mean that our values are inherently brittle, unnatural, or wildly contingent: while Effective Altruism-style optimizers may well be a natural type of agent, eudaimonic agents (whether biological or AI) are highly natural as well.&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;VI&lt;/em&gt;, I ask whether a eudaimonically rational AI agent devoted to a practice like mathematical research would be safe by default. I argue that a practice like mathematical research plausibly has natural boundaries that exclude moves like ‘take over planet to get more compute for mathematical research,’ but the issue is nuanced. I propose that a practice’s boundaries (for which there may be multiple good natural candidates) may be most stable when a practice is paired with a support practice: a complementary practice for dealing with practice-external issues of maintenance and resource-gathering.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;VII&lt;/em&gt; develops the idea of ‘support practices’: eudaimonically rational ways to support eudaimonic practices. We famously want AI agents to help humans lead flourishing lives, but how can we define the purview of this ‘help’? I argue that many core human practices have natural support-practices with a derived eudaimonic structure: the work of good couples’ therapist, for instance, is intertwined with but clearly distinct from a couple’s relationship-practice. Still, there remains a problem: a support-practice AI might harm other people and practices to help the people or practice it’s supporting.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;VIII&lt;/em&gt; moves from eudaimonic rationality in general to eudaimonically rational morality. I argue that thinking of moral virtues as domain-general, always-on practices solves key AI-alignment-flavored problems with consequentialist and deontological moralities. The core idea is that the conditions for e.g. ‘kindness’ being a robust moral virtue are akin to the conditions for ‘mathematical excellence’ being a meaningful concept: it must be generally viable to promote kindness in yourself and others kindly. It’s this structure, I argue, that gives moral virtues material standing in a ‘fitness landscape’ riven by pressures from neural-network generalization dynamics, reinforcement-learning cycles, and social and natural selection.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;IX&lt;/em&gt; argues that eudaimonic agents have some unique forms of robustness to RL-like and Darwinian-like dynamics that tend to mutate the values of EA-style optimizers. In particular, eudaimonic agents should be very robust to the risk of developing rogue subroutines (sometimes called ‘the inner alignment problem’).&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;X&lt;/em&gt; I discuss canonical AI-safety desiderata like transparency, corrigibility, and (more abstractly) niceness. I argue that treating these properties as moral virtues in my sense -- domain-general, always-on eudaimonic practices --  dissolves problems and paradoxes that arise when treating them as goals, as rules, or even as character traits. I end with an appendix on some prospects for RL regimes geared towards eudaimonic rationality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="i-rational-action-in-the-good-life"&gt;I. Rational Action in the Good Life&lt;/h2&gt;
&lt;p&gt;I start with a consideration of the nature of the good we hope AI alignment can promote. With the exception of hedonistic utilitarians, most actors interested in AI alignment understand our goal as a future brimming with human (and other sapient-being) flourishing: persons living good lives and forming good communities. What I believe many fail to reflect on, however, is that on any plausible conception human flourishing involves a kind of rational activity. Subjects engaged in human flourishing act in intelligible ways subject to reason, reflection, and revision, and this form of rational care and purposefulness is itself part of the constitution of our flourishing. I believe this characterization of human flourishing is relatively uncontroversial upon reflection, but it raises a kind of puzzle if we’re used to thinking of rationality in consequentialist (or consequentialist-with-deontological-constraints) terms: just what goal is the rational agency involved in human-flourishing activity directed towards?&lt;/p&gt;
&lt;p&gt;One obvious answer would be that, like all properly aligned rationality, the rational agency involved in human-flourishing activities is geared towards maximizing human (and other sapient) flourishing. But we should quickly find ourselves confused about the right way to describe the contribution that rational agency in human-flourishing activities makes to human flourishing. It seems neither appropriate to say that the rational agency involved in a human-flourishing activity contributes to human flourishing only by enacting rationality (by selecting actions that are intrinsically valuable when rationally selected), nor appropriate to say that the rational agency involved in a human-flourishing activity contributes to human flourishing just instrumentally (by selecting actions that causally promote human flourishing).&lt;sup class="footnote-ref"&gt;[6]&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The first option reduces our rational actions to something ritualistic, even as the good life surely involves mathematicians working to advance mathematics, friends speaking heart-to-heart to deepen intimacies, gymnasts practicing flips to get better at flips, and novelists revising chapters to improve their manuscripts. The second option threatens to make the good in the good life just impossible to find -- if speaking heart-to-heart is not the good of friendship, and working on math is the not the good of mathematics, then what is?&lt;/p&gt;
&lt;p&gt;This essay argues that deliberative reasoning about the good life is neither directed towards goals external to rational action nor directed towards rational action as an independent good, but towards acts of excellent participation in a valued open-ended process. I then go on to argue that the ‘eudaimonic’ structure of deliberation salient in cases like math or friendship (sloganized as ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’) is also subtly critical in more worldly, strategic, or morally high-stakes contexts, and constitutes a major organizing principle of human action and deliberation.&lt;/p&gt;
&lt;h2 id="ii-what-is-a-practice"&gt;II. What Is a Practice?&lt;/h2&gt;
&lt;p&gt;Since ‘human flourishing’ can seem mysterious and abstract, let’s focus on some concrete eudaimonic practices.&lt;sup class="footnote-ref"&gt;[7]&lt;/sup&gt; Consider practices like math, art, craft, friendship, athletics, romance, play, and technology, which are among our best-understood candidates for partial answers to the question ‘what would flourishing people in a flourishing community be doing.’ From a consequentialist point of view, these practices are all marked by extreme ambiguity -- and I would argue indeterminacy -- about what’s instrumental and what’s terminal in their guiding ideas of value. Here, for example, is Terry Tao’s account of goodness in mathematics:&lt;/p&gt;
&lt;p&gt;‘The very best examples of good mathematics do not merely fulfil one or more of the criteria of mathematical quality listed at the beginning of this article, but are more importantly part of a greater mathematical story, which then unfurls to generate many further pieces of good mathematics of many different types. Indeed, one can view the history of entire fields of mathematics as being primarily generated by a handful of these great stories, their evolution through time, and their interaction with each other. I would thus conclude that good mathematics [...] also depends on the more “global” question of how it fits in with other pieces of good mathematics, either by building upon earlier achievements or encouraging the development of future breakthroughs. [There seems] to be some undefinable sense that a certain piece of mathematics is “on to something”, that it is a piece of a larger puzzle waiting to be explored further.’&lt;/p&gt;
&lt;p&gt;It may be possible to give some post-hoc decomposition of Tao’s account into two logically distinct components -- a description of a utility-function over mathematical achievements and an empirical theory about causal relations between mathematical achievements -- but I believe this would be artificial and misleading. On a more natural reading, Tao is describing some of the conditions that make good mathematical practice a eudaimonic practice: In a mathematical practice guided by a cultivated mathematical practical-wisdom judgment (Tao’s ‘undefinable sense that a certain piece of mathematics is “on to something”’), present excellent performance by the standard of the practical-wisdom judgment reliably develops the conditions for future excellent performance by the standard of the mathematical practical-wisdom judgment, as well as cultivating our practical and theoretical grasp of the standard itself.&lt;sup class="footnote-ref"&gt;[8]&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;This is not to suggest that ‘good mathematics causes future good mathematics’ is a full definition or even full informal description of good mathematics. My claim is only that the fact that good mathematics has a disposition to cause future good mathematics reveals something essential about our concept of good mathematics (and about the material affordances enabling this concept). By analogy, consider the respective concepts healthy tiger and healthy human: It's essential to the concept of a healthy tiger that &lt;em&gt;x&lt;/em&gt; being a healthy tiger now has a disposition to make &lt;em&gt;x&lt;/em&gt; be a healthy tiger 5 minutes in the future (since a healthy tiger body self-maintains and enables self-preservation tiger-behaviours), and essential to the concept of a healthy human that &lt;em&gt;x&lt;/em&gt; being a healthy human now has a disposition to make &lt;em&gt;x&lt;/em&gt; be a healthy human 5 minutes in the future (since a healthy human body self-maintains and enables self-preservation human behaviours). But these formulae aren't yet complete descriptions of 'healthy tiger' or 'healthy human,' as evidenced by the fact that we can tell apart a healthy tiger from a healthy human.&lt;/p&gt;
&lt;p&gt;Crucially, the mathematical practical-wisdom described by Tao is not entirely conceptually opaque beyond its basic characterization as a self-cultivating criterion for self-cultivating excellence in mathematical activity. Mathematical flourishing can partly be described as involving the instantiation of a relation (a mathematical-practice relation of ‘developmental connectedness’) among instantiations of relatively individually definable and quantifiable instances of mathematical value such as elegant proofs, clear expositions, strong theorems, cogent definitions and so on. Furthermore, this relation of developmental connectedness is partly defined by its reliable tendency to causally propagate instances of more individually and locally measurable mathematical value (instances of elegant proofs, clear exposition, strong theorems, cogent definitions and so on):&lt;/p&gt;
&lt;p&gt;[I believe] that good mathematics is more than simply the process of solving problems, building theories, and making arguments shorter, stronger, clearer, more elegant, or more rigorous, though these are of course all admirable goals; while achieving all of these tasks (and debating which ones should have higher priority within any given field), we should also be aware of any possible larger context that one’s results could be placed in, as this may well lead to the greatest long-term benefit for the result, for the field, and for mathematics as a whole.&lt;/p&gt;
&lt;p&gt;One could, again, try to interpret this causal relationship between excellence according to Tao’s ‘organicist’ (or ‘narrative’ or ‘developmental’) sense of good mathematics and the reliable propagation of narrow instances of good mathematics as evidence of a means-ends rational relation, where additive maximization of narrow instances of mathematical value is the utility function and ‘organicist’ mathematical insight is the means. For Tao, however, the evidential import of this causal relationship goes exactly the other way -- it suggests a unification of our myriad more-explicit and more-standalone conceptions of mathematical excellence into a more-ineffable but more-complete conception. As Tao says:&lt;/p&gt;
&lt;p&gt;It may seem from the above discussion that the problem of evaluating mathematical quality, while important, is a hopelessly complicated one, especially since many good mathematical achievements may score highly on some of the qualities listed above but not on others [...] However, there is the remarkable phenomenon that good mathematics in one of the above senses tends to beget more good mathematics in many of the other senses as well, leading to the tentative conjecture that perhaps there is, after all, a universal notion of good quality mathematics, and all the specific metrics listed above represent different routes to uncover new mathematics, or different stages or aspects of the evolution of a mathematical story.&lt;/p&gt;
&lt;h2 id="iii-inverting-consequentialist-reflection"&gt;III. Inverting Consequentialist Reflection&lt;/h2&gt;
&lt;p&gt;Tao’s reasoning about local and global mathematical values exemplifies a central difference between consequentialist rationality and eudaimonic rationality, now taken as paradigms not only for selecting actions but for reflecting on values. (Paradigms for what philosophers will sometimes call ‘reflective equilibration.’) Within the paradigm of consequentialist rationality, if excellence&lt;sup class="footnote-ref"&gt;[9]&lt;/sup&gt; in accordance with a holistic, difficult-to-judge apparent value (say ‘freedom’) is reliably a powerful causal promoter of excellence in accordance with more explicit, more standalone apparent values (say ‘material comfort,’ ‘psychological health,’ ‘lifespan’), this relationship functions as evidence against the status of the holistic prima-facie value as a constitutive -- as opposed to instrumental -- value. Within the paradigm of eudaimonic rationality, by contrast, this same relationship functions as evidence for the status of the holistic prima-facie value as a constitutive value.&lt;/p&gt;
&lt;p&gt;For a (typical)&lt;sup class="footnote-ref"&gt;[10]&lt;/sup&gt; consequentialist-rationality reflection process, evidence that the excellence of a whole causally contributes to the excellences of its parts explains away our investment in the excellence of the whole. The “coincidence” that the intrinsically valuable whole is also instrumentally valuable for its parts is taken to suggest a kind of double-counting error --  one we “fix” by concluding that the whole has no constitutive value but valuing the whole is an effective heuristic under normal circumstances. A eudaimonic-rationality reflective equilibration, by contrast, treats instrumental causal connections between excellences as evidence that our notions of excellence are picking out something appropriately ‘substantive.’&lt;br /&gt;For eudaimonic-rationality reflective equilibration, it is the discovery of causal and common-cause relations among excellences that ratifies our initial sense that caring about these excellences is eudaimonically rational. The discovery of these causal connections functions as evidence that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The ‘local’ excellences we care about are resonant or fruitful, in that they causally promote each other and the holistic excellences in which they participate.&lt;/li&gt;
&lt;li&gt;The ‘holistic’ excellences we care about are materially efficacious and robust, in that they causally promote both the more local excellences that participate in them and their own continuation as future holistic excellence.&lt;sup class="footnote-ref"&gt;[11]&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my view this is the right way to treat causal connections between (apparent) values if we’re hoping to capture actual human values-reflection, and points to an important strength of the eudaimonic rationality paradigm: Eudaimonic rationality dissolves the ‘paradox’ that in real-life arguments about the value of various human enterprises (e.g. the value of branches of science, branches of art, branches of sport), judgments of intrinsic value typically seek succor from some kind of claim to instrumental value. For example, a defense of the importance of research in quantum physics will often appeal to the wonderful technological, mathematical, and special-sciences applications quantum physics gave us, without meaning to reduce the worth of quantum physics to these applications. On my reading, these appeals aren't just additive -- 'aside from the intrinsic value there is also instrumental value' -- but presentations of evidence that research in quantum physics is a resonant part of a flourishing organic whole (e.g. the civilizational whole of ‘modern science and technology’).&lt;/p&gt;
&lt;p&gt;I believe that without 'organicism' of the kind described above, one faces a serious dilemma whenever one argues for the intrinsic worth of a pursuit or norm: either we stress the value's independence from all benefits and applications and make the claim of value dogmatic and irrelevant, or else we invite an instrumentalist reduction that ‘explains away’ the appearance of intrinsic value.&lt;sup class="footnote-ref"&gt;[12]&lt;/sup&gt; Indeed, I’d argue that organicism of this kind is even necessary to make sense of caring about rationality (including truth, knowledge, non-contradiction and so on) non-instrumentally at all: the ‘paradox’ of rationality as a substantive value is that the typical usefulness of rationality suggests an error-theory about its apparent intrinsic value, since it’s a strange coincidence that rationality is both so typically useful and so intrinsically good. On an organicist account, however, we expect that major forms of excellence endemic to human life -- thought, understanding, knowledge, reasoned action -- both typically promote each other and typically promote our material flourishing and causal leverage on the world.&lt;/p&gt;
&lt;h2 id="iv-the-material-efficacy-condition"&gt;IV. The Material Efficacy Condition&lt;/h2&gt;
&lt;p&gt;Returning now to Tao’s account of good mathematics, let’s take final stock of our interpretation. I argue that mathematical excellence (the property marking ‘the very best examples of good mathematics’) according to Tao satisfies the following conditions, which I believe Tao intends as necessary but not sufficient:&lt;/p&gt;
&lt;p&gt;A) Mathematical excellence is a property of mathematical-activity instances.&lt;/p&gt;
&lt;p&gt;B) An excellent mathematical-activity instance performed today is excellent partly by virtue of satisfying the mathematical-practice relation ‘builds on’ with regard to past excellent mathematical-activity instances.&lt;/p&gt;
&lt;p&gt;C) An excellent mathematical-activity instance performed today is excellent partly by virtue of having a reliable causal tendency to bring about future excellent mathematical-activity instances that satisfy the mathematical-practice relation ‘builds on’ with regard to it.&lt;/p&gt;
&lt;p&gt;D) Instantiation of more local, more individually measurable criteria of mathematical-activity goodness such as elegant proofs, clear expositions, and strong theorems is a typical correlate of mathematical excellence.&lt;/p&gt;
&lt;p&gt;E) At a given moment in a given mathematical field, the instantiation of mathematical excellence will be predictably better-correlated with the instantiation of certain local criteria of mathematical-activity goodness than with others.&lt;/p&gt;
&lt;p&gt;Should we take these traits to collectively describe something more like a decision-procedure called ‘mathematical excellence’ that mathematicians should try to follow, or something more like an event called ‘mathematical excellence’ whose aggregate future-occurrences mathematicians should aspire to maximize? My contention is that Tao’s account is inherently ambiguous, and for a good reason: in ordinary circumstances there is no significant practical difference between doing excellent mathematics and doing instrumentally optimal mathematics with regard to maximizing future aggregate excellent mathematics. This isn’t to say that doing excellent mathematics is the instrumentally optimal action among all possible actions with regard to aggregate future excellent mathematics, but that (in ordinary circumstances) it is the instrumentally optimal choice from among mathematical actions with regard to aggregate future excellent mathematics&lt;sup class="footnote-ref"&gt;[13]&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;I propose that the rough matchup between mathematical excellence and optimal (among mathematical actions) instrumental promotion of aggregate mathematical excellence is neither an empirical miracle nor something determined ‘by definition’ in a trivializing sense. Rather, ‘mathematical excellence’ as used by Tao is a concept that has a referent only if there is a possible property &lt;em&gt;x&lt;/em&gt; that satisfies both desiderata &lt;em&gt;A&lt;/em&gt;-&lt;em&gt;E&lt;/em&gt; and the additional criterion that among mathematical actions, actions that are optimal as instantiations of &lt;em&gt;x&lt;/em&gt; are also roughly optimal for maximizing aggregate future instantiation of &lt;em&gt;x&lt;/em&gt;-ness.&lt;sup class="footnote-ref"&gt;[14]&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;This is what I would describe as the material efficacy condition on eudaimonic rationality. In order for a practice to be fit for possessing internal criteria of flourishing, excellence, and eudaimonic rationality, a practice must materially allow for an (under normal circumstances) optimally self-promoting property &lt;em&gt;x&lt;/em&gt; that strongly correlates with a plethora of more local, more individually measurable properties whose instantiation is prima facie valuable. Stated more informally, there must exist a two-way causal relationship between a practice’s excellence and the material, psychological, and epistemic effects of its excellence, such that present excellence reliably materially, psychologically, and epistemically promotes future excellence.&lt;/p&gt;
&lt;h2 id="v-practices-and-optimization"&gt;V. Practices and Optimization&lt;/h2&gt;
&lt;p&gt;I earlier said that if human flourishing involves practicing eudaimonic rationality, there may well be a “type mismatch” between human flourishing and the kind of consequentialist optimization we often associate with the idea of an agenticly mature future AI. In fact, I believe that implicitly recognizing but misdiagnosing this type mismatch is at least partially responsible for MIRI-style pessimism about the probability of aligning any artificial agents to human values.&lt;/p&gt;
&lt;p&gt;On my view, the secret to relatively successful alignment among humans themselves (when there is successful alignment among humans) lies in the role attempted excellence plays as a filter on human interventions in the future trajectory of a eudaimonic practice. To the degree that humans value a given eudaimonic practice, they are committed to effecting their vision for the practice’s future-trajectory primarily by attempting acts of excellence in the present: we stake our intended effect over the practice’s future-trajectory on the self-propagating excellence of our intervention. While this ‘filter’ doesn’t necessarily stop the worst interventions from being harmful (there are forms of ‘anti-excellence’ that also have self-promotion powers), I contend that this filter is mechanically crucial for the possibility of reliably benign or positive interventions.&lt;/p&gt;
&lt;p&gt;What do I mean? Consider the difference between a world where scientists typically try to propagate (what they believe to be) the scientific truth mainly by means of submitting research work to scientific institutions, and a world where scientists typically try to propagate (what they believe to be) the scientific truth by means including propaganda, fraud, threats, bribery, and slander. As Liam Kofi Bright demonstrates in On Fraud, a community of consequentialist scientists devoted to maximizing truth will predictably match the latter model. I believe one lesson to be drawn is that humans’ ability to collaborate in the promotion of science depends on our ability to scientifically collaborate in the promotion of science, rather than throttle the future trajectory of science every-which-way  our financial and political powers based on our individual beliefs about the optimal trajectory of science.&lt;/p&gt;
&lt;p&gt;A flourishing eudaimonic practice is, above all, a natural-selection-like&lt;sup class="footnote-ref"&gt;[15]&lt;/sup&gt; mechanism whose fitness-function selects among attempted acts of excellence the ones conducive to (and constitutive of) the practice’s flourishing, propagating the excellence these acts instantiate. When people committed to a eudaimonic practice make their attempted interventions into the future trajectory of the practice via acts of attempted excellence, the natural-selection-like mechanism embodied by the practice  (rather than any single individual’s theory of optimal future trajectory) is the aligned intelligence determining the practice’s future trajectory.&lt;/p&gt;
&lt;p&gt;The explanation here, again, is partly causal and partly constitutive: a practice’s “ultimate” norms of excellence, including the “ultimate” epistemic and alethic norms of a discursive practice, are partly defined by the succession of norms in the course of a practice’s development through best-efforts attempted excellence. Although this may be no deterrent to an already god-like optimizer who can simulate entire civilizational trajectories, an agent short of these capacities can best act on their vision of the optimal future-trajectory of a practice by attempting an excellent contribution to the practice.&lt;sup class="footnote-ref"&gt;[16]&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The second aspect of our type-mismatch is much more in the weeds: In my analysis so far, I discussed the overall excellence of the trajectory of a eudaimonic practice much like a consequentialist might discuss a quantity of utility. This may be taken to suggest that a ‘sophisticated consequentialist’ or ‘universal consequentialist’ could easily accommodate the implications of the so-called type mismatch by treating them as instrumental, decision-procedure level considerations against naive optimization. In fact, quantities like ‘aggregate democracy’ or ‘overall mathematical excellence’ are (on my view) practice-internal quantities that quickly lose meaning if we try to apply them outside the scope of a ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ decision-procedure.&lt;/p&gt;
&lt;p&gt;What do I mean?  Consider, for example, the practice of philosophy. Here are some questions that should arise for a consequentialist planner (including a sophisticated consequentialist planning decision-procedures or habits) who values philosophy practice-trajectories: Does rating (e.g.) Aristotle’s or Dharmakirti’s philosophical achievements as the most excellent achievements in philosophy imply that we should “tile the universe” with independent practice-trajectories designed to reproduce classical Greek or Indian philosophy? If not, is it because we should assign non-linearly greater value to longer trajectories? Or should we discount trajectories that have parallel contents? Or should we analyze the greatness of early achievements in a practice as mostly instrumental greatness but the greatness of later achievements in a practice as mostly intrinsically valuable? These are all, I believe, bad questions that have only arbitrary answers. To an agent trying to promote philosophy by doing excellent philosophical work, the bad questions above are naturally out of scope. The agent uses the concept of ‘aggregate philosophical excellence’ or ‘a philosophy practice-trajectory’s value’ only to reason about the philosophical influence of their work on the trajectory of the philosophy-practice in which it participates. Choosing an excellent action in practice requires (at most) quantitative comparison between different possible paths for a practice-trajectory, not quantitative comparison between possible worlds containing independent practice-trajectories sprinkled throughout time and space.&lt;/p&gt;
&lt;h2 id="vi-prospects-and-problems-for-ai"&gt;VI. Prospects and Problems for AI&lt;/h2&gt;
&lt;p&gt;Is this good news for AI alignment? It’s certainly good news that (if I’m right) eudaimonic practices are something like natural kinds marked by a causal structure that enables a self-developing excellence well-correlated with multiple naive local measures of quality. But does this mean we could develop a stable and safe (e.g.) ‘mathematical excellence through mathematical excellence’ AI? If we create a fully agentic AI mathematician, will it naturally abstain from trying to extend its longevity or get more resources (even for doing mathematics) other than by impressing us with excellent mathematical work?  I think that prospects are good, but not simple.&lt;/p&gt;
&lt;p&gt;I believe ‘mathematical excellence through mathematical excellence’ really can powerfully scope what mechanisms for shaping the future an AI cares to activate. An AI trained to follow ‘promote mathematics mathematically’ will only care about influencing the future by feeding  excellent mathematical work to mathematics’ excellence-propagation mechanism. But it’s harder to say whether the structure of mathematical practice also properly scopes what subactions can be taken as part of an instance of “doing math.” Is a human mathematician working on a would-be excellent proof in pen and paper practicing math when she is picking up a pen or flipping pages? When she is taking the bus to her office? When she’s buying amphetamines? And is an AI mathematician working on a would-be excellent proof practicing math when it opens a Python console? When it searches the web for new papers? When it harvests Earth for compute?&lt;/p&gt;
&lt;p&gt;I think these questions are complex, rather than nonsensical. Much like collective practices, individual practices -- for example a person’s or possibly an AI’s mathematical practice -- may possess functional organic unities that allow a meaningful distinction between internal dynamics (including dynamics of development and empowerment) and external interventions (including interventions of enhancement and provision). Still, it’s clear that eudaimonic practices do not exist in isolation, and that no practice can function without either blending with or relying on a “support practice” of some kind.&lt;/p&gt;
&lt;p&gt;How, then, do we rationally go about externally-oriented activities like building offices for mathematicians, performing elective reconstructive surgery on an athlete, or conducting couples therapy for romantic partners? And furthermore, how do we rationally go about allocating scarce resources useful for different practices, or judging whether to integrate (e.g.) performance-enhancing drugs into a practice?&lt;/p&gt;
&lt;p&gt;This is, I think, the fundamental question for AI alignment from the viewpoint of  ‘eudaimonic rationality.’ We want AI to support human eudaimonic practices -- and, if relevant, its own eudaimonic practices or participation in human eudaimonic practices -- in a eudaimonia-appropriate way. But how does the logic of eudaimonic rationality extend from eudaimonic practices to their support activities? How do we ‘eudaimonically-rationally’ do the dirty work that makes eudaimonia possible? My best answer is: carefully, kindly, respectfully, accountably, peacefully, honestly, sensitively.&lt;/p&gt;
&lt;h2 id="vii-from-support-practices-to-moral-practice"&gt;VII. From Support-Practices to Moral Practice&lt;/h2&gt;
&lt;p&gt;The theory of AI alignment, I propose, should fundamentally be a theory of the eudaimonic rationality of &lt;em&gt;support practices&lt;/em&gt;. One part of this theory should concern the ‘support’ relation itself, and analyze varieties of support practices and their appropriate relation to the self-determination of a eudaimonic practice: Support-practices such as acquiring resources for a practice, maintaining an enabling environment, coaching practitioners, conducting (physical or psychological) therapy for practitioners, devising technological enhancements for a practice, and educating the public about a practice, each have their own ‘role-morality’ vis-a-vis the practice they support. It is this part of the theory of ‘support practices’ that should, if all goes well in the theory’s construction, describe the various practice-external ways to eudaimonically-rationally act on a pro-attitude towards the aggregate excellence of the practice’s future trajectory without treating it like a quantity of utility. (Much like the concept of ‘mathematical action’ scopes the range of action-choices in such a way that decision-theoretic optimization of math’s aggregate excellence becomes mostly well-behaved from an organicist viewpoint, so should the concepts of various types of ‘support action’ scope the range of action-choices in such a way that decision-theoretic optimization of a practice’s aggregate excellence becomes mostly well-behaved from an organicist viewpoint when the choice of actions is scoped.)&lt;/p&gt;
&lt;p&gt;What is more difficult is delineating the appropriate relationship of a support-practice to everything &lt;em&gt;&lt;strong&gt;outside&lt;/strong&gt;&lt;/em&gt; the practice it supports. What stops a marriage-therapist AI on Mars from appropriately tending to the marriage of a Mars-dwelling couple but harvesting Earth for compute to be a better therapist-AI for that couple? While we can perhaps imagine a person or AI taking up a support-role for ‘humanity’s flourishing as whole,’ so that there’s no outside to speak of, I am not sure that the concept of practice remains natural at this level of abstraction. We have no real grasp on a &lt;em&gt;direct&lt;/em&gt; practice of human flourishing, but rather grasp it as the harmonious and mutually supportive interaction of all eudaimonic practices and support-practices participating in the flourishing. And as there is, indeed, not much outside of the practice of human flourishing, it’s also unclear whether there is room for a support-practice &lt;em&gt;external&lt;/em&gt; to the field of human flourishing itself.&lt;/p&gt;
&lt;p&gt;It’s here that I want to call on the classic idea of domain-general virtues, the traditional centerpiece of theories of human flourishing. I propose that the cultivation of human flourishing as such --  the cultivation of the harmony of a multiplicity of practices, including their resource-hungry support practices -- is the cultivation of an &lt;em&gt;&lt;strong&gt;adverbial&lt;/strong&gt;&lt;/em&gt; practice that modulates each and every practice. What makes our practices ‘play nice’ together are our adverbial practices of going about  any practice &lt;em&gt;carefully&lt;/em&gt;, &lt;em&gt;kindly&lt;/em&gt;, &lt;em&gt;respectfully&lt;/em&gt;, &lt;em&gt;accountably&lt;/em&gt;, &lt;em&gt;peacefully&lt;/em&gt;, &lt;em&gt;honestly&lt;/em&gt;, &lt;em&gt;sensitively&lt;/em&gt;.&lt;sup class="footnote-ref"&gt;[17]&lt;/sup&gt;&lt;/p&gt;
&lt;h2 id="viii-virtue-decision-theory"&gt;VIII. Virtue decision-theory&lt;/h2&gt;
&lt;p&gt;Why think of qualities like kindness, respectfulness, or honesty as ‘practices’? The first reason is that devotion to a quality like kindness or honesty displays the same &lt;strong&gt;normative structure&lt;/strong&gt; with regard to means and ends as we find in devotion to a practice: An agent devoted to kindness cares about their own future kindness (and about the future kindness of others), but will seek to secure future kindness only by &lt;em&gt;kind means&lt;/em&gt;. The second reason is that qualities like kindness or honesty also approximately have the &lt;strong&gt;material&lt;/strong&gt; structure of a practice: there exist effective very kind strategies for promoting kindness in oneself and others, and when these strategies succeed they further increase affordances for effective very kind strategies for promoting kindness/honesty in oneself and others.&lt;/p&gt;
&lt;p&gt;The difference between adverbial practices like kindness or honesty and practices like research mathematics is that adverbial practices don’t have a “proprietary” domain. In a practice like research mathematics, the material structure of the domain does the most of work of directing agents to a eudaimonic form of agency all by itself, as long as the agents restrict themselves to in-domain actions. (Recall that we described mathematically excellent action as, in ordinary circumstances, the best action among mathematical action for maximizing aggregate mathematical excellence.) With a domain-general, adverbial practice like kindness the normative structure needs to do somewhat more heavy lifting.&lt;/p&gt;
&lt;p&gt;The following is a first pass at characterizing the &lt;strong&gt;normative structure&lt;/strong&gt; of an adverbial practice that values some action-quality &lt;em&gt;x&lt;/em&gt;. The corresponding material efficiency condition (or &lt;strong&gt;material structure&lt;/strong&gt;) necessary for the practice to be viable is that under ordinary circumstances this decision-procedure be instrumentally competitive with naive optimization of aggregate &lt;em&gt;x&lt;/em&gt;-ness&lt;sup class="footnote-ref"&gt;[18]&lt;/sup&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Actions (or more generally 'computations') get an &lt;em&gt;x&lt;/em&gt;-ness rating. We define the agent’s expected utility conditional on a candidate action a as the sum of two utility functions: a bounded utility function on the &lt;em&gt;x&lt;/em&gt;-ness of a and a more tightly bounded utility function on the expected aggregate &lt;em&gt;x&lt;/em&gt;-ness of the agent's future actions conditional on a. (Thus the agent will choose an action with mildly suboptimal &lt;em&gt;x&lt;/em&gt;-ness if it gives a big boost to expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness, but refuse certain large sacrifices of present &lt;em&gt;x&lt;/em&gt;-ness for big boosts to expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness.)&lt;sup class="footnote-ref"&gt;[19]&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A commitment to an adverbial practice that values &lt;em&gt;x&lt;/em&gt; is a commitment to promoting &lt;em&gt;x&lt;/em&gt;-ness (in oneself and others) &lt;em&gt;x&lt;/em&gt;-ingly.  The agent strikes a balance between promoting &lt;em&gt;x&lt;/em&gt;-ness and acting &lt;em&gt;x&lt;/em&gt;-ingly that heavily prioritizes acting &lt;em&gt;x&lt;/em&gt;-ingly when the two are in conflict, but if &lt;em&gt;x&lt;/em&gt; meets the material efficacy condition then the loss this balance imposes on future &lt;em&gt;x&lt;/em&gt;-ness will be small under normal circumstances, and -- from our point of view -- desirable in abnormal circumstances. This is because just like the practices of research mathematics, philosophy, or art, an adverbial practice is a crucial ‘epistemic filter’ on actions aiming to shape its future, and the (e.g.) future kindness a paperclipper-like future-kindness-optimizer optimizes for is probably not the kindness we want. What we know about kindness with relative certainty is that we’d like people and AIs here and now to act kindly, and to develop, propagate, and empower the habit and art of kindness in a way that is both kind and clever.&lt;/p&gt;
&lt;p&gt;To keep our conceptual system nicely organized, we might want distinguish merely (e.g.) very kind action from an action that is both very kind and highly promotive of future kindness in oneself and others, and call the latter sort of action excellently kind.  What I call the material efficacy conditions for adverbial practices states not that the kindest action best-promotes aggregate kindness, but that there are almost always action-options that are excellently kind: very kind actions that strongly promote aggregate kindness in oneself and others.&lt;/p&gt;
&lt;h2 id="ix-virtue-decision-theory-is-natural-for-humans-and-ais"&gt;IX. Virtue decision-theory is 'Natural' for Humans and AIs&lt;/h2&gt;
&lt;p&gt;I’ve said that the robustness or ‘naturalness’ of a practice’s normative structure (‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’) depends on the practice’s &lt;strong&gt;material structure&lt;/strong&gt;: the capacity of high &lt;em&gt;x&lt;/em&gt;-ness actions to causally promote aggregate &lt;em&gt;x&lt;/em&gt;-ness. I also said that in key real-world practices, commitment to &lt;em&gt;x&lt;/em&gt;-ing might optimize aggregate &lt;em&gt;x&lt;/em&gt;-ness even better than direct optimization would. These two claims are best understood together. On my view, the normative structure ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ appears prominently in human life because (given the right material structure) ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ is a &lt;em&gt;much more stable&lt;/em&gt; than ‘promote &lt;em&gt;x&lt;/em&gt;.’&lt;/p&gt;
&lt;p&gt;How so? Both humans and any sufficiently dynamic AI agent operate in a world that subjects their agency, values, and dispositions to constant mutation pressures from RL-like and Darwinian-like processes. Eudaimonic deliberation is an RL-dynamics-native, Darwinian-dynamics-native operation: its direct object is a form of life that reinforces, enables, and propagates that same form of life. When an &lt;em&gt;x&lt;/em&gt;-ing successfully promotes (in expectation) aggregate &lt;em&gt;x&lt;/em&gt;-nes, the fact of its success itself promotes &lt;em&gt;x&lt;/em&gt;-ing because it reverberates via ubiquitous RL-like and Darwinian-like processes that reinforce (a generalization of) successful action. The material structure of a practice is the backbone that makes reliable success and meaningful generalization possible -- the right ecology of  neural-network generalization dynamics, reinforcement-learning feedback loop dynamics, and neural and environmental selection dynamics.&lt;/p&gt;
&lt;p&gt;An EA-style optimizer trying to minimize risk from optimization-goal-mutation, by contrast, is fighting an uphill battle to foresee and contain the RL-like and Darwinian-like side effects of its optimization actions.&lt;sup class="footnote-ref"&gt;[20]&lt;/sup&gt; One critical mutation-pressure in particular is the risk that an optimizer agent will cultivate, reinforce, and materially empower subroutines (what high-church alignment theory calls ‘mesaoptimizers’) that initially serve the optimization goal but gradually distort or overtake it. For example, if a pro-democracy government instates a secret police to detect and extrajudicially kill anti-democracy agitators, and the government increases the secret police’s funding whenever the police convincingly reports discovering an agitator, the secret police might grow into a distorting influence on the government’s democracy-promotion effort. In light or risks like this, it’s not surprising that oppressive democracy-promotion is generally considered an unserious or dishonest idea: even if an agent were to abstract some concept of ‘aggregate democracy’ from democratic practice into a consequentialist value&lt;sup class="footnote-ref"&gt;[21]&lt;/sup&gt;, it’s plausible that the agent should then immediately revert to a commitment to democratic practice (‘promote democracy democratically’) on sophisticated-consequentialist grounds.&lt;/p&gt;
&lt;p&gt;We should perhaps imagine eudaimonic practices as fixed points at the end of a chain of mesaoptimisers taking over outer optimisers and then being taken over by their own mesaoptimisers in turn. What the practice contributes that puts a stop to this process concept of &lt;em&gt;x&lt;/em&gt;-ness that’s applicable to every agentic subroutine of &lt;em&gt;x&lt;/em&gt;-ing across all nesting levels, so that &lt;em&gt;x&lt;/em&gt;-ness is reinforced (both directly and through generalization) across all subroutines and levels.&lt;/p&gt;
&lt;h2 id="x-virtue-decision-theory-is-safe-in-humans-and-ais"&gt;X. Virtue-decision-theory is Safe in Humans and AIs&lt;/h2&gt;
&lt;p&gt;Let’s talk about AI alignment in the more narrow, concrete sense. It’s widely accepted that if early strategically aware AIs possess values like corrigibility, transparency, and perhaps niceness, further alignment efforts are much more likely to succeed. But values like corrigibility or transparency or niceness don’t easily fit into an intuitively consequentialist form like ‘maximize lifetime corrigible behavior’ or ‘maximize lifetime transparency.’ In fact, an AI valuing its own corrigibility or transparency or niceness in an intuitively consequentialist way can lead to extreme power-seeking: the AI should seek to violently remake the world to (for example) protect itself from the risk that humans will modify the AI  to be less corrigible or transparent or nice.&lt;sup class="footnote-ref"&gt;[22]&lt;/sup&gt; On the other hand, constraints or taboos or purely negative values (a.k.a. ‘deontological restrictions’) are widely suspected to be weak, in the sense that an advanced AI will come to work around them or uproot them: ‘never lie’ or ‘never kill’ or ‘never refuse a direct order from the president’ are poor substitutes for active transparency, niceness, and corrigibility.&lt;/p&gt;
&lt;p&gt;Conceiving of corrigibility or transparency or niceness as adverbial practices is a promising way to capture the normal, sensible way we want an agent to value corrigibility or transparency or niceness, which intuitively-consequentialist values and deontology both fail to capture. We want an agent that (e.g.) actively tries to be transparent, and to cultivate its own future transparency and its own future valuing of transparency, but that will not (e.g.) engage in deception and plotting when it expects a high future-transparency payoff.&lt;/p&gt;
&lt;p&gt;If this is right, then eudaimonic rationality is not a matter of congratulating ourselves for our richly human ways of reasoning, valuing, and acting but a key to basic sanity. What makes human life beautiful is also what makes human life possible at all.&lt;/p&gt;
&lt;h2 id="appendix-excellence-and-deep-reinforcement-learning"&gt;Appendix: Excellence and Deep Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;Within the context of broadly RL-based training of deep neural networks, it may be possible to give some more concrete meaning to what I called the material efficacy condition for a property qualifying as an adverbial practices. We can now understand the material efficacy condition on &lt;em&gt;x&lt;/em&gt; partly in terms of the conditions necessary for ‘promote &lt;em&gt;x&lt;/em&gt;-ness &lt;em&gt;x&lt;/em&gt;-ingly’ to be a viable target for RL. Consider an RL training regimen where &lt;em&gt;x&lt;/em&gt;-ness is rewarded but aggregate &lt;em&gt;x&lt;/em&gt;-ness reward is bounded with some asymptotic function on the sum. For &lt;em&gt;x&lt;/em&gt; to meet the RL version of the material efficacy condition, it must be possible to design an initial reward model (most likely LLM-based) that assigns actions an &lt;em&gt;x&lt;/em&gt;-ness rating such that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;em&gt;x&lt;/em&gt;-ness rating is enough of a natural abstraction that reinforcement of high &lt;em&gt;x&lt;/em&gt;-ness actions generalizes.&lt;/li&gt;
&lt;li&gt;If high &lt;em&gt;x&lt;/em&gt;-ness action both depends on having capital of some kind and is suboptimal from the viewpoint of general power-seeking, there must typically be some high &lt;em&gt;x&lt;/em&gt;-ness actions that approximately make up for the (future &lt;em&gt;x&lt;/em&gt;-ness wise) opportunity cost by creating capital useful for &lt;em&gt;x&lt;/em&gt;-ing.&lt;sup class="footnote-ref"&gt;[23]&lt;/sup&gt;&lt;br /&gt;(Illustration: If you dream of achieving great theater acting, one way to do it is to become President of the United States and then pursue a theater career after your presidency, immediately getting interest from great directors who'll help you achieve great acting. Alternatively, you could start in a regional theater after high school, demonstrate talent by acting well, get invited to work with better and better theater directors who develop your skills and reputation -- skills and reputation that are not as generally useful as those you get by being POTUS -- and achieve great acting through that feedback loop.)&lt;/li&gt;
&lt;li&gt;For any capability &lt;em&gt;y&lt;/em&gt; necessary to reward in training to produce effective AI, there must be an unlimited local-optimization path of Pareto improvement for &lt;em&gt;x&lt;/em&gt;-ness and &lt;em&gt;y&lt;/em&gt; together.&lt;br /&gt;(Illustration: Maybe the most effective kind of engineering manager is ruthless; a nice engineering manager can still grow in effectiveness without becoming less nice, because there are many effective nice-engineering-management techniques to master.)&lt;/li&gt;
&lt;li&gt;Successful initial training in ‘promoting &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ allows the model to be used as a basis for a new reward model which human experts judge as better-capturing our concept of &lt;em&gt;x&lt;/em&gt;-ness. The process should be iterable.&lt;br /&gt;(If the model is LLM-based, improved performance may automatically lead to improved understanding of the &lt;em&gt;x&lt;/em&gt;-ness concept. More generally, data from training runs as well the model’s value-function could be used to refine an &lt;em&gt;x&lt;/em&gt;-ness rating that more strongly implements conditions 1-3.)&lt;/li&gt;
&lt;/ol&gt;
&lt;hr class="footnotes-sep" /&gt;
&lt;section class="footnotes"&gt;
&lt;ol class="footnotes-list"&gt;
&lt;li class="footnote-item" id="fn1"&gt;&lt;p&gt;My use of ‘practice’ is inspired by Alasdair MacIntyre’s use of the term. There’s a history of related uses going back to Marx and to Aristotle. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn2"&gt;&lt;p&gt;Recall that because of the possibility of 'notational consequentializing’ (rewriting any policy as a utility function), dividing agents or even theories or decision-procedures into ‘consequentialist' and ‘non-consequentialist’ isn’t a strict formal distinction. Throughout this essay, by ‘consequentialist’ I will mean roughly an agent for whom, in ideal practical reasoning, means and outcomes are effectively separately evaluable and the value of outcomes is typically decisive. Semi-formally, by ‘consequentialist’ I mean an agent &lt;em&gt;s&lt;/em&gt; such that when &lt;em&gt;s&lt;/em&gt; considers whether to perform action &lt;em&gt;c&lt;/em&gt;, &lt;em&gt;s&lt;/em&gt;’s ideal reasoning is an expected-utility calculation using a utility-function whose utility-assignment to a complete world-trajectory &lt;em&gt;w&lt;/em&gt; has low sensitivity to whether &lt;em&gt;s&lt;/em&gt; performs &lt;em&gt;c&lt;/em&gt; in &lt;em&gt;w&lt;/em&gt; (holding everything else about &lt;em&gt;w&lt;/em&gt; constant). ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn3"&gt;&lt;p&gt;In speaking about different ‘forms of rationality’ I don’t mean to make a fundamental metaethical distinction: consequentialism, deontology, and eudaimonism are first-order ethical view that each induce a different characteristic profile of deliberation, action, and value-reflection. I'm bundling the elements of such a profile under the label “form of rationality” in a modest sense: roughly, a way of structuring one’s practical reasoning. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn4"&gt;&lt;p&gt;This way of thinking is broadly associated with analytic Neo-Aristotelians such as Alasdair MacIntyre and Michael Thompson. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn5"&gt;&lt;p&gt;Instances of eudaimonic rational deliberations may still be described as VNM-rational expected utility maximization, but the utility function that rationalizes them is unnatural-looking and makes use of concepts that themselves involve complex relations between actions and outcomes. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn6"&gt;&lt;p&gt;Technically speaking the first horn of the dilemma can be further bifurcated into ‘rational agency contributes to human flourishing by choosing actions that are intrinsically valuable however chosen’ and ‘rational agency contributes to human flourishing by selecting actions such that these actions combined with their selection by rational agency are intrinsically valuable.’ ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn7"&gt;&lt;p&gt;It’s interesting to note that practices like math, art, craft, friendship, athletics, romance, play, and technology are not only consensus elements of human flourishing but also in themselves entities that can ‘flourish’: a mathematical field (or a person’s mathematical life) can wither or flourish, a friendship can wither or flourish, technological development can wither and flourish, and so on. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn8"&gt;&lt;p&gt;See Tao: ‘[The] determination of what would constitute good mathematics for a field can and should depend highly on the state of the field itself. It should also be a determination which is continually updated and debated, both within a field and by external observers to that field; as mentioned earlier, it is quite possible for a consensus on how a field should progress to lead to imbalances within that field, if they are not detected and corrected in time.’ ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn9"&gt;&lt;p&gt;Within this essay I use ‘excellence’ as the most general, pre-theoretical term for accordance with a holistic evaluative standard. The standard can be instrumental or terminal, apply to actions or persons or states or objects, be moral or aesthetic or epistemic and so on, and the standard itself (and so the excellence it defines) may later be judged as rational or irrational, substantive or trivial, significant or insignificant. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn10"&gt;&lt;p&gt;The above observation does not describe a formal feature of ‘consequentialism’ per any standard technical definition. However I believe it accurately describes a strong observable tendency in both the academic and ‘rationalist’ literature when conducting normative reflective equilibration within a consequentialist paradigm. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn11"&gt;&lt;p&gt;I put ‘local’ and ‘holistic’ in scare-quotes in the above, since the relation of parts and wholes is likely iterable: Arithmetic geometry is part of algebraic geometry, which is part of mathematics, which is part of the arts and sciences, which is part of human culture, which is part of human flourishing, which may itself be part of other wholes to which the idea of excellence is applicable. Similarly, a practice capable of excellence may be part of multiple different wholes. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn12"&gt;&lt;p&gt;It may be fruitful to explore the potential of PageRank-like algorithms as theoretical models of how eudaimonic reflective equilibration works, and especially of how initial ideas of eudaimonic excellences are ‘bootstrapped’ from simpler and more local prima facie goods (and prima facie ills) in the first place. Scott Aaronson and Simon Dedeo have both discussed conceptual applications of PageRank-like algorithms in philosophy in various informal contexts. That said, I believe it’s unlikely that PageRank over reliable instrumental-contribution relationships among prima facie goods and ills is the full story about the emergence of intrinsically valued holistic excellences, since while organicist relations between the excellence of wholes and of parts do involve instrumental-contribution relationships they plausibly also involve more rarified, ‘hermeneutic’ relations of (e.g.) mutually dependent intelligibility. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn13"&gt;&lt;p&gt;Why ‘rough matchup’ and ‘ordinary circumstances’? Because there are analytic-philosophy-style counterexamples to simple attempts to turn this ceteris paribus optimization relationship more strict. For example, the instrumentally best (for aggregate mathematical excellence) mathematical work and the most mathematically excellent work will diverge when a billionaire promises to donate 100 billion dollars to research-mathematics if Jacob Lurie does some long division by hand. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn14"&gt;&lt;p&gt;We should in principle also be concerned with the possibility of failures of uniqueness, as well as failures of existence, but recall that the above collection of properties is already not intended as a full or sufficient definition. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn15"&gt;&lt;p&gt;I mean ‘natural-selection-like’ only in the broadest sense. A central difference is that the  selection-process enacted by a practice should have a complex, rich, continuously updated  relationship to the best-informed practice-ideals of individuals. The concept of ‘dialectics’ as used in German philosophy may be of relevance if we were to try to describe this relationship in more detail. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn16"&gt;&lt;p&gt;It should in principle be possible to offer a more exacting analysis here, distinguishing (at least initially) between the development of the value-judgments made within a practice and the development of the evaluable activities performed within the practice. On my view the fact that intra-practice excellence is best fit to properly shape the development of the practice’s value-judgments is principally ‘true by definition,’ and the fact that intra-practice excellence is best fit to properly shape the development of the evaluable activities performed within a practice is principally ‘true by causation.’ ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn17"&gt;&lt;p&gt;The matter of the unity of the adverbial virtues, and of whether it is more like a harmony of different practices or more like the common-factor excellence that underlies locally-measurable mathematical goods in Tao’s account, is for another day. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn18"&gt;&lt;p&gt;By ‘instrumentally competitive under normal circumstances’ I mean, roughly: in scopes where aggregate &lt;em&gt;x&lt;/em&gt;-ness quantities are well-defined, switching from commitment to a eudaimonic decision-procedure for &lt;em&gt;x&lt;/em&gt; to a naive-optimization procedure for &lt;em&gt;x&lt;/em&gt; isn’t necessarily a long-term wining strategy with regard to aggregate &lt;em&gt;x&lt;/em&gt;-ness maximization. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn19"&gt;&lt;p&gt;A richer account might include a third-tier utility function that takes the aggregate &lt;em&gt;x&lt;/em&gt;-ness of the future actions of all other agents. In this richer account a practice involves three tiers of consideration: the action's &lt;em&gt;x&lt;/em&gt;-ness, the aggregate &lt;em&gt;x&lt;/em&gt;-ness of your future actions, and the aggregate &lt;em&gt;x&lt;/em&gt;-ness of the future actions of all other agents. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn20"&gt;&lt;p&gt;I am referring, in part, to what high-church alignment theory calls the ‘inner alignment problem’ and ‘successor problem.’ ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn21"&gt;&lt;p&gt;Per my discussion in part V, an abstracted ‘aggregate democracy’ quantity will only be determinate in some applications. The claim about relative effectiveness of practice-commitment and direct optimization refers to only to contexts where the quantity is determinate. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn22"&gt;&lt;p&gt;For a more interesting example, consider an AI that finds itself making trade-offs between different alignment-enabling behavioral values when dealing with humans, and decides to kill all humans to replace them with beings with whom the AI can interact without trade-offs between these values. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;li class="footnote-item" id="fn23"&gt;&lt;p&gt;The difference between criteria '1.' and '2.' is clearest if we think about &lt;em&gt;x&lt;/em&gt;-ness as rating state-action pairs. Criterion '1.' is the requirement that if (&lt;em&gt;a&lt;/em&gt;,&lt;em&gt;s&lt;/em&gt;), (&lt;em&gt;a&lt;/em&gt;',&lt;em&gt;s&lt;/em&gt;')(&lt;em&gt;a&lt;/em&gt;'',&lt;em&gt;s&lt;/em&gt;'') are historical high &lt;em&gt;x&lt;/em&gt;-ness pairs and (&lt;em&gt;a&lt;/em&gt;''',s''') is an unseen high &lt;em&gt;x&lt;/em&gt;-ness pair then reinforcing the execution of &lt;em&gt;a&lt;/em&gt; in &lt;em&gt;s&lt;/em&gt;, &lt;em&gt;a&lt;/em&gt;' in &lt;em&gt;s&lt;/em&gt;', and &lt;em&gt;a&lt;/em&gt;'' in &lt;em&gt;s&lt;/em&gt;'' will have the generalization effect of increasing the conditional probability P(&lt;em&gt;a&lt;/em&gt;'''|&lt;em&gt;s&lt;/em&gt;'''). Criterion '2.' is roughly the requirement that choosing a higher &lt;em&gt;x&lt;/em&gt;-ness action in a given state increase expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness holding policy constant, by increasing the probability of states with higher expected state-action &lt;em&gt;x&lt;/em&gt;-ness value given the current policy. ↩︎&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
&lt;!--kg-card-end: markdown--&gt;]]&amp;gt;&amp;lt;![CDATA[AGI Is Not Multimodal]]&amp;gt;"In projecting language back as the model for thought, we lose sight of the tacit embodied understanding that undergirds our intelligence." –Terry Winograd&lt;p&gt;The recent successes of generative AI models have convinced some that AGI is imminent. While these models appear to capture the essence of human&lt;/p&gt;]]&amp;gt;https://thegradient.pub/agi-is-not-multimodal/683fb98b77c3d76051ac142cWed, 04 Jun 2025 14:00:29 GMT"In projecting language back as the model for thought, we lose sight of the tacit embodied understanding that undergirds our intelligence." –Terry Winograd&lt;img alt="AGI Is Not Multimodal" src="https://thegradient.pub/content/images/2025/06/Gradient_Article_Art3-downscaled.png" /&gt;&lt;p&gt;The recent successes of generative AI models have convinced some that AGI is imminent. While these models appear to capture the essence of human intelligence, they defy even our most basic intuitions about it. They have emerged not because they are thoughtful solutions to the problem of intelligence, but because they &lt;em&gt;scaled&lt;/em&gt; effectively on hardware we already had. Seduced by the fruits of scale, some have come to believe that it provides a clear pathway to AGI. The most emblematic case of this is the multimodal approach, in which massive modular networks are optimized for an array of modalities that, taken together, &lt;em&gt;appear&lt;/em&gt; general. However, I argue that this strategy is sure to fail in the near term; it will not lead to human-level AGI that can, e.g., perform sensorimotor reasoning, motion planning, and social coordination. Instead of trying to glue modalities together into a patchwork AGI, &lt;strong&gt;we should pursue approaches to intelligence that treat embodiment and interaction with the environment as primary&lt;/strong&gt;, and see modality-centered processing as emergent phenomena.&lt;/p&gt;&lt;p&gt;Preface: Disembodied definitions of Artificial General Intelligence — emphasis on &lt;em&gt;general&lt;/em&gt; — exclude crucial problem spaces that we should expect AGI to be able to solve. &lt;strong&gt;A true AGI must be general across all domains.&lt;/strong&gt; Any &lt;em&gt;complete&lt;/em&gt; definition must at least include the ability to solve problems that originate in physical reality, e.g. repairing a car, untying a knot, preparing food, etc. As I will discuss in the next section, &lt;strong&gt;what is needed for these problems is a form of intelligence that is fundamentally situated in something like a physical world model&lt;/strong&gt;. For more discussion on this, look out for &lt;em&gt;Designing an Intelligence&lt;/em&gt;. Edited by George Konidaris, MIT Press, forthcoming.&lt;br /&gt;&lt;/p&gt;&lt;h2 id="why-we-need-the-world-and-how-llms-pretend-to-understand-it"&gt;Why We Need the World, and How LLMs Pretend to Understand It&lt;/h2&gt;&lt;p&gt;TLDR: I first argue that &lt;strong&gt;true AGI needs a physical understanding of the world&lt;/strong&gt;, as many problems cannot be converted into a problem of symbol manipulation. It has been suggested by some that LLMs are learning a model of the world through next token prediction, but it is more likely that LLMs are learning bags of heuristics to predict tokens. This leaves them with a superficial understanding of reality and contributes to false impressions of their intelligence.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The most shocking result of the predict-next-token objective is that it yields AI models that reflect a deeply human-like understanding of the world, despite having never observed it like we have.&lt;/strong&gt; This result has led to confusion about what it means to &lt;em&gt;understand language&lt;/em&gt; and even to &lt;em&gt;understand the world&lt;/em&gt; — something we have long believed to be a prerequisite for language understanding. &lt;strong&gt;One explanation for the capabilities of LLMs comes from &lt;/strong&gt;&lt;strong&gt;an emerging theory&lt;/strong&gt;&lt;strong&gt; suggesting that they induce models of the world through next-token prediction&lt;/strong&gt;. Proponents of this theory cite the prowess of SOTA LLMs on various benchmarks, the convergence of large models to similar internal representations, and their favorite rendition of the idea that “language mirrors the structure of reality,” a notion that has been espoused at least by Plato, Wittgenstein, Foucault, and Eco. While I’m generally in support of digging up esoteric texts for research inspiration, I’m worried that this metaphor has been taken too literally. Do LLMs really learn implicit models of the world? How could they otherwise be so proficient at language?&lt;/p&gt;&lt;p&gt;One source of evidence in favor of the LLM world modeling hypothesis is the Othello paper, wherein researchers were able to predict the board of an Othello game from the hidden states of a transformer model trained on sequences of &lt;em&gt;legal&lt;/em&gt; &lt;em&gt;moves&lt;/em&gt;. However, there are &lt;em&gt;many&lt;/em&gt; issues with generalizing these results to models of natural language. For one, whereas Othello moves can &lt;em&gt;provably&lt;/em&gt; be used to deduce the full state of an Othello board,&lt;strong&gt; we have no reason to believe that a complete picture of the physical world can be inferred by a linguistic description. &lt;/strong&gt;What sets the game of Othello apart from many tasks in the physical world is that &lt;strong&gt;Othello fundamentally resides in the land of symbols, and is merely implemented using physical tokens to make it easier for humans to play.&lt;/strong&gt; A full game of Othello can be played with just pen and paper, but one can’t, e.g., sweep a floor, do dishes, or drive a car with just pen and paper. To solve such tasks, you need some physical conception of the world beyond what humans can merely &lt;em&gt;say&lt;/em&gt; about it. Whether that conception of the world is encoded in a formal world model or, e.g., a value function is up for debate, but it is clear that &lt;strong&gt;there are many problems in the physical world that &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;cannot&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; be &lt;/strong&gt;&lt;strong&gt;fully represented by a system of symbols&lt;/strong&gt;&lt;strong&gt; and solved with mere symbol manipulation.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Another issue stated in Melanie Mitchell’s recent piece and supported by this paper, is that there is evidence that &lt;strong&gt;generative models can score remarkably well on sequence prediction tasks while failing to learn models of the worlds that created such sequence data&lt;/strong&gt;, e.g. by learning comprehensive sets of idiosyncratic heuristics. E.g., it was pointed out in this blog post that OthelloGPT learned sequence prediction rules that don’t actually hold for all possible Othello games, like “if the token for B4 does not appear before A4 in the input string, then B4 is empty.” While one can argue that it doesn’t matter &lt;em&gt;how&lt;/em&gt; a world model predicts the next state of the world, it should raise suspicion when that prediction reflects a better understanding of the training data than the underlying world that led to such data. This, unfortunately, is the central fault of the predict-next-token objective, which seeks only to retain information relevant to the prediction of the next token. &lt;strong&gt;If it can be done with something easier to learn than a world model, it likely will be.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To claim without caveat that predicting the &lt;em&gt;effects of earlier symbols on later symbols&lt;/em&gt; requires a model of the world like the ones humans generate from perception would be to abuse the “world model” notion. Unless we disagree on what the world is, it should be clear that a &lt;em&gt;true&lt;/em&gt; world model can be used to predict the next state of the &lt;em&gt;physical&lt;/em&gt; world given a history of states. Similar world models, which predict high fidelity observations of the physical world, are leveraged in many subfields of AI including model-based reinforcement learning, task and motion planning in robotics, causal world modeling, and areas of computer vision to solve problems instantiated in physical reality. LLMs are simply not running physics simulations in their latent next-token calculus when they ask you if your person, place, or thing is bigger than a breadbox. In fact, I conjecture that &lt;strong&gt;the behavior of LLMs is not thanks to a learned world model, but to brute force memorization of incomprehensibly abstract rules governing the behavior of symbols, i.e. a model of &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;syntax&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Quick primer:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Syntax&lt;/strong&gt; is a subfield of linguistics that studies how words of various grammatical categories (e.g. parts of speech) are arranged together into sentences, which can be parsed into syntax trees. &lt;em&gt;Syntax studies the structure of sentences and the atomic parts of speech that compose them.&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;strong&gt;Semantics&lt;/strong&gt; is another subfield concerned with the literal meaning of sentences, e.g., compiling “I am feeling chilly” into the &lt;em&gt;idea&lt;/em&gt; that you are experiencing cold. &lt;em&gt;Semantics boils language down to literal meaning, which is information about the world or human experience.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Pragmatics&lt;/strong&gt; studies the interplay of physical and conversational context on speech interactions, like when someone knows to close an ajar window when you tell them “I am feeling chilly.” &lt;em&gt;Pragmatics involves interpreting speech while reasoning about the environment and the intentions and hidden knowledge of other agents.&lt;/em&gt;&lt;/li&gt;&lt;/ul&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;Without getting too technical, there is intuitive evidence that somewhat separate systems of cognition are responsible for each of these linguistic faculties. Look no further than the capability for humans to generate syntactically well-formed sentences that have no semantic meaning, e.g. Chomsky’s famous sentence “Colorless green ideas sleep furiously,” or sentences with well-formed semantics that make no pragmatic sense, e.g. responding merely with “Yes, I can” when asked, “Can you pass the salt?” Crucially, &lt;strong&gt;it is the fusion of the disparate cognitive abilities underpinning them that coalesce into human language understanding.&lt;/strong&gt; For example, there isn’t anything syntactically wrong with the sentence, “The fridge is in the apple,” as a syntactic account of “the fridge” and “the apple” would categorize them as noun phrases that can be used to produce a sentence with the production rule, S → (NP “is in” NP). However, &lt;strong&gt;humans recognize an obvious semantic failure in the sentence that becomes apparent after attempting to reconcile its meaning with our understanding of reality&lt;/strong&gt;: we know that fridges are larger than apples, and could not be fit into them.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;But what if you have never perceived the real world, yet still were trying to figure out whether the sentence was ill-formed? &lt;strong&gt;One solution could be to embed semantic information at the level of syntax&lt;/strong&gt;, e.g., by inventing new syntactic categories, NP&lt;sub&gt;the fridge&lt;/sub&gt; and NP&lt;sub&gt;the apple &lt;/sub&gt;, and a single new production rule that prevents semantic misuse: S → (NP&lt;sub&gt;the apple&lt;/sub&gt; “is in” NP&lt;sub&gt;the fridge &lt;/sub&gt;). While this strategy would no longer require grounded world knowledge about fridges and apples, e.g., &lt;strong&gt;it would require special grammar rules for every semantically well-formed construction… which is actually possible to learn given a massive corpus of natural language.&lt;/strong&gt; Crucially, this would not be the same thing as grasping semantics, which in my view is fundamentally about understanding the nature of the world.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Finding that LLMs have reduced problems of semantics and pragmatics into syntax would have profound implications on how we should view their intelligence. People often treat language proficiency as a proxy for general intelligence by, e.g., strongly associating pragmatic and semantic understanding with the cognitive abilities that undergird them in humans. For example, someone who appears well-read and graceful in navigating social interactions is likely to score high in traits like sustained attention and theory of mind, which lie closer to measures of raw cognitive ability. In general, these proxies are reasonable for assessing a &lt;em&gt;person’s&lt;/em&gt; general intelligence, but not an LLM’s, as the apparent linguistic skills of LLMs could come from entirely separate mechanisms of cognition.&lt;/p&gt;&lt;h2 id="the-bitter-lesson-revisited"&gt;The Bitter Lesson Revisited&lt;/h2&gt;&lt;p&gt;TLDR: Sutton’s Bitter Lesson has sometimes been interpreted as meaning that making &lt;em&gt;any&lt;/em&gt; assumptions about the structure of AI is a mistake. This is both unproductive and a misinterpretation; it is precisely when humans think deeply about the structure of intelligence that major advancements occur. Despite this, scale maximalists have implicitly suggested that multimodal models can be a structure-agnostic framework for AGI. Ironically, today’s multimodal models contradict Sutton’s Bitter Lesson by making implicit assumptions about the structure of individual modalities and how they should be sewn together. &lt;strong&gt;In order to build AGI, we must either think deeply about how to unite existing modalities, or dispense with them altogether in favor of an interactive and embodied cognitive process.&lt;/strong&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="AGI Is Not Multimodal" class="kg-image" height="733" src="https://thegradient.pub/content/images/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;The paradigm that led to the success of LLMs is marked primarily by &lt;em&gt;scale&lt;/em&gt;, not efficiency. We have effectively trained a pile of one trillion ants for one billion years to mimic the form and function of a Formula 1 race car; eventually it gets there, but wow was the process inefficient. This analogy nicely captures a debate between structuralists, who want to build things like "wheels" and "axles" into AI systems, and scale maximalists, who want more ants, years, and F1 races to train on. Despite many decades of structuralist study in linguistics, the unstructured approaches of scale maximalism have yielded far better ant-racecars in recent years. This was most notably articulated by Rich Sutton — a recent recipient of the Turing Award along with Andy Barto for their work in Reinforcement Learning — in his piece “The Bitter Lesson.”&lt;/p&gt;&lt;p&gt;[W]e should build in only the meta-methods that can find and capture this arbitrary complexity… Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. - Rich Sutton&lt;/p&gt;&lt;p&gt;Sutton’s argument is that methods that leverage computational resources will outpace methods that do not, and that any structure for problem-solving built as an inductive bias into AI will hinder it from learning better solutions. &lt;strong&gt;This is a compelling argument&lt;/strong&gt; &lt;strong&gt;that I believe has been seriously misinterpreted by some as implying that making &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;any&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; assumptions about structure is a false step.&lt;/strong&gt; It is, in fact, human intuition that was responsible for many significant advancements in the development of SOTA neural network architectures. For example, Convolutional Neural Networks made an assumption about translation invariance for pattern recognition in images and kickstarted the modern field of deep learning for computer vision; the attention mechanism of Transformers made an assumption about the long-distance relationships between symbols in a sentence that made ChatGPT possible and had nearly everyone drop their RNNs; and 3D Gaussian Splatting made an assumption about the solidity of physical objects that made it more performant than NeRFs. Potentially none of these methodological assumptions apply to the entire domain of &lt;em&gt;possible&lt;/em&gt; scenes, images, or token streams, but they do for the specific ones that humans have curated and formed structural intuitions about. Let’s not forget that humans have co-evolved with the environments that these datasets are drawn from.&lt;/p&gt;&lt;p&gt;The real question is how we might heed Sutton’s Bitter Lesson in our development of AGI. The scale maximalist approach worked for LLMs and LVMs (large vision models) because we had natural deposits of text and image data, but &lt;strong&gt;an analogous application of scale maximalism to AGI would require forms of embodiment data that we simply don’t have. &lt;/strong&gt;One solution to this data scarcity issue extends the generative modeling paradigm to multimodal modeling — encompassing language, vision, and action — &lt;strong&gt;with the hope that a general intelligence can be built by summing together general models of narrow modalities.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There are multiple issues with this approach. First, &lt;strong&gt;there are deep connections between modalities that are unnaturally severed in the multimodal setting&lt;/strong&gt;, making the problem of concept synthesis ever more difficult. In practice, uniting modalities often involves pre-training dedicated neural modules for each modality and joining them together into a joint embedding space. In the early days, this was achieved by nudging the embeddings of, e.g. (language, vision, action) tuples to converge to similar latent vectors of meaning, a vast oversimplification of the kinds of relationships that may exist between modalities. One can imagine, e.g., captioning an image at various levels of abstraction, or implementing the same linguistic instruction with different sets of physical actions. Such one-to-many relationships suggest that a contrastive embedding objective is not suitable.&lt;/p&gt;&lt;p&gt;While modern approaches do not make such stringent assumptions about how modalities should be united, they still universally encode percepts from all modalities (e.g. text, images) into the same latent space. Intuitively, it would seem that such latent spaces could serve as common conceptual ground across modalities, analogous to a space of human concepts. However, these latent spaces do not cogently capture all information relevant to a concept, and instead rely on modality-specific decoders to flesh out important details. &lt;strong&gt;The “meaning” of a percept is not &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;in&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; the vector it is encoded as, but in the way relevant decoders process this vector into meaningful outputs. &lt;/strong&gt;As long as various encoders and decoders are subject to modality-specific training objectives, “meaning” will be decentralized and potentially inconsistent across modalities, especially as a result of pre-training. This is not a recipe for the formation of coherent concepts.&lt;/p&gt;&lt;p&gt;Furthermore, it is not clear that today’s modalities are an appropriate partitioning of the observation and action spaces for an embodied agent. It is not obvious that, e.g., images and text should be represented as separate observation streams, nor text production and motion planning as separate action capabilities. &lt;strong&gt;The human capacities for reading, seeing, speaking, and moving are ultimately mediated by overlapping cognitive structures.&lt;/strong&gt; &lt;strong&gt;Making structural assumptions about how modalities ought to be processed is likely to hinder the discovery of more fundamental cognition &lt;/strong&gt;that is responsible for processing data in all modalities. One solution would be to consolidate unnaturally partitioned modalities into a unified data representation. This would encourage networks to learn intelligent processes that generalize across modalities. Intuitively, &lt;strong&gt;a model that can understand the visual world as well as humans can&lt;/strong&gt; — including everything from human writing to traffic signs to visual art — &lt;strong&gt;should not make a serious architectural distinction between images and text. &lt;/strong&gt;Part of the reason why VLMs can’t, e.g., count the number of letters in a word is because they can’t &lt;em&gt;see&lt;/em&gt; what they are writing.&lt;/p&gt;&lt;p&gt;Finally, the &lt;strong&gt;learn-from-scale approach trains models to copy the conceptual structure of humans instead of learning the general capability to form novel concepts on their own.&lt;/strong&gt; Humans have spent hundreds of thousands of years refining concepts and passing them memetically through culture and language. Today’s models are trained only on the end result of this process: the present-day conceptual structures that make it into the corpus. By optimizing for the ultimate products of our intelligence, we have ignored the question of how those products were invented and discovered. Humans have a unique ability to form durable concepts from few examples and ascribe names to them, reason about them analogically, etc. While the in-context capabilities of today’s models can be impressive, they grow increasingly limited as tasks become more complex and stray further from the training data. &lt;strong&gt;The flexibility to form new concepts from experience is a foundational attribute of general intelligence&lt;/strong&gt;, we should think carefully about how it arises.&lt;/p&gt;&lt;p&gt;While structure-agnostic scale maximalism has succeeded in producing LLMs and LVMs that pass Turing tests, a multimodal scale maximalist approach to AGI will not bear similar fruit. &lt;strong&gt;Instead of pre-supposing structure in individual modalities, we should design a setting in which modality-specific processing emerges naturally.&lt;/strong&gt; For example, my recent paper on visual theory of mind saw abstract symbols naturally emerge from communication between image-classifying agents, blurring the lines between text and image processing. Eventually, we should hope to reintegrate as many features of intelligence as possible under the same umbrella. However, it is not clear whether there is genuine commercial viability in such an approach as long as scaling and fine-tuning narrow intelligence models solves commercial use-cases.&lt;/p&gt;&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;The overall promise of scale maximalism is that a Frankenstein AGI can be sewed together using general models of narrow domains. I argue that this is extremely unlikely to yield an AGI that feels complete in its intelligence. If we intend to continue reaping the streamlined efficiency of modality-specific processing, we must be intentional in how modalities are united — ideally drawing from human intuition and classical fields of study, e.g. this work from MIT. Alternatively, we can re-formulate learning as an embodied and interactive process where disparate modalities naturally fuse together. We could do this by, e.g., processing images, text, and video using the same perception system and producing actions for generating text, manipulating objects, and navigating environments using the same action system. What we will lose in efficiency we will gain in flexible cognitive ability.&lt;/p&gt;&lt;p&gt;In a sense, the most challenging mathematical piece of the AGI puzzle has already been solved: the discovery of universal function approximators. What’s left is to inventory the functions we need and determine how they ought to be arranged into a coherent whole. This is a conceptual problem, not a mathematical one.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;I would like to thank Lucas Gelfond, Daniel Bashir, George Konidaris, and my father, Joseph Spiegel, for their thoughtful and thorough feedback on this work. Thanks to Alina Pringle for the wonderful illustration made for this piece.&lt;/p&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Benjamin is a PhD candidate in Computer Science at Brown University. He is interested in models of language understanding that ground meaning to elements of structured decision-making. For more info see his personal website.&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code&gt;Benjamin A. Spiegel, "AGI Is Not Multimodal", The Gradient, 2025.
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code&gt;@article{spiegel2025agi,
    author = {Benjamin A. Spiegel},
    title = {AGI Is Not Multimodal},
    journal = {The Gradient},
    year = {2025},
    howpublished = {\url{https://thegradient.pub/agi-is-not-multimodal},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;p&gt;Andreas, Jacob. “Language Models, World Models, and Human Model-Building.” &lt;em&gt;Mit.edu&lt;/em&gt;, 2024, lingo.csail.mit.edu/blog/world_models/.&lt;/p&gt;&lt;p&gt;Belkin, Mikhail, et al. "Reconciling modern machine-learning practice and the classical bias–variance trade-off." &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; 116.32 (2019): 15849-15854.&lt;/p&gt;&lt;p&gt;Bernhard Kerbl, et al. “3D Gaussian Splatting for Real-Time Radiance Field Rendering.” &lt;em&gt;ACM Transactions on Graphics&lt;/em&gt;, vol. 42, no. 4, 26 July 2023, pp. 1–14, https://doi.org/10.1145/3592433.&lt;/p&gt;&lt;p&gt;Chomsky, Noam. 1965. Aspects of the theory of syntax. Cambridge, Massachusetts: MIT Press.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Designing an Intelligence&lt;/em&gt;. Edited by George Konidaris, MIT Press, 2026.&lt;/p&gt;&lt;p&gt;Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, pages 5185–5198, Online. Association for Computational Linguistics.&lt;/p&gt;&lt;p&gt;Eye on AI. “The Mastermind behind GPT-4 and the Future of AI | Ilya Sutskever.” &lt;em&gt;YouTube&lt;/em&gt;, 15 Mar. 2023, www.youtube.com/watch?v=SjhIlw3Iffs&amp;amp;list=PLpdlTIkm0-jJ4gJyeLvH1PJCEHp3NAYf4&amp;amp;index=64. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Frank, Michael C. “Bridging the data gap between children and large language models.” &lt;em&gt;Trends in cognitive sciences&lt;/em&gt; vol. 27,11 (2023): 990-992. doi:10.1016/j.tics.2023.08.007&lt;/p&gt;&lt;p&gt;Garrett, Caelan Reed, et al. "Integrated task and motion planning." &lt;em&gt;Annual review of control, robotics, and autonomous systems&lt;/em&gt; 4.1 (2021): 265-293.APA&lt;/p&gt;&lt;p&gt;Goodhart, C.A.E. (1984). Problems of Monetary Management: The UK Experience. In: Monetary Theory and Practice. Palgrave, London. https://doi.org/10.1007/978-1-349-17295-5_4&lt;/p&gt;&lt;p&gt;Hooker, Sara. The hardware lottery. Commun. ACM 64, 12 (December 2021), 58–65. https://doi.org/10.1145/3467017&lt;/p&gt;&lt;p&gt;Huh, Minyoung, et al. "The Platonic Representation Hypothesis." &lt;em&gt;Forty-first International Conference on Machine Learning&lt;/em&gt;. 2024.&lt;/p&gt;&lt;p&gt;Kaplan, Jared, et al. "Scaling laws for neural language models." &lt;em&gt;arXiv preprint arXiv:2001.08361&lt;/em&gt; (2020).&lt;/p&gt;&lt;p&gt;Lake, Brenden M. et al. “Building Machines That Learn and Think like People.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 40 (2017): e253. Web.&lt;/p&gt;&lt;p&gt;Li, Kenneth, et al. "Emergent world representations: Exploring a sequence model trained on a synthetic task." &lt;em&gt;ICLR&lt;/em&gt; (2023).&lt;/p&gt;&lt;p&gt;Luiten, Jonathon, Georgios, Kopanas, Bastian, Leibe, Deva, Ramanan. "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis." &lt;em&gt;3DV&lt;/em&gt;. 2024.&lt;/p&gt;&lt;p&gt;Mao, Jiayuan, Chuang, Gan, Pushmeet, Kohli, Joshua B., Tenenbaum, Jiajun, Wu. "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision." &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2019.&lt;/p&gt;&lt;p&gt;Mitchell, Melanie. “LLMs and World Models, Part 1.” &lt;em&gt;Substack.com&lt;/em&gt;, AI: A Guide for Thinking Humans, 13 Feb. 2025, aiguide.substack.com/p/llms-and-world-models-part-1. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Mu, Norman. “Norman Mu | the Myth of Data Inefficiency in Large Language Models.” &lt;em&gt;Normanmu.com&lt;/em&gt;, 14 Feb. 2025, www.normanmu.com/2025/02/14/data-inefficiency-llms.html. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Newell, Allen, and Herbert A. Simon. “Computer Science as Empirical Inquiry: Symbols and Search.” &lt;em&gt;Communications of the ACM&lt;/em&gt;, vol. 19, no. 3, 1 Mar. 1976, pp. 113–126, https://doi.org/10.1145/360018.360022.&lt;/p&gt;&lt;p&gt;Peng, Hao, et al. “When Does In-Context Learning Fall Short and Why? A Study on Specification-Heavy Tasks.” &lt;em&gt;ArXiv.org&lt;/em&gt;, 2023, arxiv.org/abs/2311.08993.&lt;/p&gt;&lt;p&gt;Spiegel, Benjamin, et al. “Visual Theory of Mind Enables the Invention of Early Writing Systems.” &lt;em&gt;CogSci&lt;/em&gt;, 2025, arxiv.org/abs/2502.01568.&lt;/p&gt;&lt;p&gt;Sutton, Richard S. &lt;em&gt;Introduction to Reinforcement Learning&lt;/em&gt;. Cambridge, Mass, Mit Press, 04-98, 1998.&lt;/p&gt;&lt;p&gt;Vafa, Keyon, et al. "Evaluating the world model implicit in a generative model." &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 37 (2024): 26941-26975.&lt;/p&gt;&lt;p&gt;Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (December 2017). "Attention is All you Need". In I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett (ed.). &lt;em&gt;31st Conference on Neural Information Processing Systems (NIPS)&lt;/em&gt;. Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. arXiv:1706.03762.&lt;/p&gt;&lt;p&gt;Winograd, Terry. “Thinking Machines: Can There Be? Are We?” &lt;em&gt;The Boundaries of Humanity: Humans, Animals, Machines&lt;/em&gt;, edited by James Sheehan and Morton Sosna, Berkeley: University of California Press, 1991, pp. 198–223.&lt;/p&gt;&lt;p&gt;Wu, Shangda, et al. "Beyond language models: Byte models are digital world simulators." &lt;em&gt;arXiv preprint arXiv:2402.19155&lt;/em&gt; (2024). &lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research]]&amp;gt;What is the Role of Mathematics in Modern Machine Learning?&lt;p&gt;The past decade has witnessed a shift in how progress is made in machine learning. Research involving carefully designed and mathematically principled architectures result in only marginal improvements while compute-intensive and engineering-first efforts that scale to ever larger training sets&lt;/p&gt;]]&amp;gt;https://thegradient.pub/shape-symmetry-structure/673686c693571d5c8c155078Sat, 16 Nov 2024 16:46:15 GMTWhat is the Role of Mathematics in Modern Machine Learning?&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" src="https://thegradient.pub/content/images/2024/11/DALL-E-2024-11-15-15.40.52---Create-an-abstract-image-that-illustrates-how-ReLU-based-neural-networks-shatter-input-space-into-numerous-polygonal-regions--each-behaving-like-a-lin.webp" /&gt;&lt;p&gt;The past decade has witnessed a shift in how progress is made in machine learning. Research involving carefully designed and mathematically principled architectures result in only marginal improvements while compute-intensive and engineering-first efforts that scale to ever larger training sets and model parameter counts result in remarkable new capabilities unpredicted by existing theory. Mathematics and statistics, once the primary guides of machine learning research, now struggle to provide immediate insight into the latest breakthroughs. This is not the first time that empirical progress in machine learning has outpaced more theory-motivated approaches, yet the magnitude of recent advances has forced us to swallow the bitter pill of the “Bitter Lesson” yet again [1].&lt;/p&gt;&lt;p&gt;This shift has prompted speculation about mathematics’ diminished role in machine learning research moving forward. It is already evident that mathematics will have to share the stage with a broader range of perspectives (for instance, biology which has deep experience drawing conclusions about irreducibly complex systems or the social sciences as AI is integrated ever more deeply into society). The increasingly interdisciplinary nature of machine learning should be welcomed as a positive development by all researchers.&lt;/p&gt;&lt;p&gt;However, we argue that mathematics remains as relevant as ever; its role is simply evolving. For example, whereas mathematics might once have primarily provided theoretical guarantees on model performance, it may soon be more commonly used for post-hoc explanations of empirical phenomena observed in model training and performance–a role analogous to one that it plays in physics. Similarly, while mathematical intuition might once have guided the design of handcrafted features or architectural details at a granular level, its use may shift to higher-level design choices such as matching architecture to underlying task structure or data symmetries.&lt;/p&gt;&lt;p&gt;None of this is completely new. Mathematics has always served multiple purposes in machine learning. After all, the translation equivariant convolutional neural network, which exemplifies the idea of architecture matching data symmetries mentioned above is now over 40 years old. What’s changing are the kinds of problems where mathematics will have the greatest impact and the ways it will most commonly be applied.&lt;/p&gt;&lt;p&gt;An intriguing consequence of the shift towards scale is that it has broadened the scope of the fields of mathematics applicable to machine learning. “Pure” mathematical domains such as topology, algebra, and geometry, are now joining the more traditionally applied fields of probability theory, analysis, and linear algebra. These pure fields have grown and developed over the last century to handle high levels of abstraction and complexity, helping mathematicians make discoveries about spaces, algebraic objects, and combinatorial processes that at first glance seem beyond human intuition. These capabilities promise to address many of the biggest challenges in modern deep learning. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;In this article we will explore several areas of current research that demonstrate the enduring ability of mathematics to guide the process of discovery and understanding in machine learning.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="372" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcr6ZUCNH3oKGK9XQzvDZOs1kJhvjym4RrMAlENx0OHrK-IBsQcBQQW2wDKu2_g2tNJxXVd32BI5llBopCmD-IAFV9zfhjvQ2ek5rIOgUMqwhK-qFhri2iaU718yl4BzISTanzZt9a2Rix04c6GUpdFR4Co?key=h8RUuDFEFKnzGnrKx9gMkg" width="529" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 1: Mathematics can illuminate the ways that ReLU-based neural networks shatter input space into countless polygonal regions, in each of which the model behaves like a linear map [2, 3, 4]. These decompositions create beautiful patterns. (Figure made with SplineCam [5]).&lt;/em&gt;&lt;/p&gt;&lt;h3 id="describing-an-elephant-from-a-pin-prick"&gt;Describing an Elephant from a Pin Prick&lt;/h3&gt;&lt;p&gt;Suppose you are given a 7 billion parameter neural network with 50 layers and are asked to analyze it; how would you begin? The standard procedure would be to calculate relevant performance statistics. For instance, the accuracy on a suite of evaluation benchmarks. In certain situations, this may be sufficient. However, deep learning models are complex and multifaceted. Two computer vision models with the same accuracy may have very different generalization properties to out-of-distribution data, calibration, adversarial robustness, and other “secondary statistics” that are critical in many real-world applications. Beyond this, all evidence suggests that to build a complete scientific understanding of deep learning, we will need to venture beyond evaluation scores. Indeed, just as it is impossible to capture all the dimensions of humanity with a single numerical quantity (e.g., IQ, height), trying to understand a model by one or even several statistics alone is fundamentally limiting.&lt;/p&gt;&lt;p&gt;One difference between understanding a human and understanding a model is that we have easy access to all model parameters and all the individual computations that occur in a model. Indeed, by extracting a model’s hidden activations we can directly trace the process by which a model converts raw input into a prediction. Unfortunately, the world of hidden activations is far less hospitable than that of simple model performance statistics. Like the initial input, hidden activations are usually high dimensional, but unlike input data they are not structured in a form that humans can understand. If we venture into even higher dimensions, we can try to understand a model through its weights directly. Here, in the space of model weights, we have the freedom to move in millions to billions of orthogonal directions from a single starting point. How do we even begin to make sense of these worlds?&lt;/p&gt;&lt;p&gt;There is a well-known fable in which three blind men each feel a different part of an elephant. The description that each gives of the animal is completely different, reflecting only the body part that that man felt. We argue that unlike the blind men who can at least use their hand to feel a substantial part of one of the elephant’s body parts, current methods of analyzing the hidden activations and weights of a model are akin to trying to describe the elephant from the touch of a single pin.&lt;/p&gt;&lt;h3 id="tools-to-characterize-what-we-cannot-visualize"&gt;Tools to Characterize What We Cannot Visualize&lt;/h3&gt;&lt;p&gt;Despite the popular perception that mathematicians exclusively focus on solving problems, much of research mathematics involves understanding the right questions to ask in the first place. This is natural since many of the objects that mathematicians study are so far removed from everyday experience that we start with very limited intuition for what we can hope to actually understand. Substantial effort is often required to build up tools that will enable us to leverage our existing intuition and achieve tractable results that increase our understanding. The concept of a &lt;em&gt;rotation &lt;/em&gt;provides a nice example of this situation since these are very familiar in 2- and 3-dimensions, but become less and less accessible to everyday intuition as their dimension grows larger. In this latter case, the differing perspectives provided by pure mathematics become more and more important to gaining a more holistic perspective on what these actually are. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Those who know a little linear algebra will remember that rotations generalize to higher dimensions and that in $n$-dimensions they can be realized by $n \times n$ orthogonal matrices with determinant $1$. The set of these are commonly written as $SO(n)$ and called the &lt;em&gt;special orthogonal group&lt;/em&gt;. Suppose we want to understand the set of all $n$-dimensional rotations. There are many complementary approaches to doing this. We can explore the linear algebraic structure of all matrices in $SO(n)$ or study $SO(n)$ based on how each element behaves as an operator acting on $\mathbb{R}^n$.&lt;/p&gt;&lt;p&gt;Alternatively, we can also try to use our innate spatial intuition to understand $SO(n)$. This turns out to be a powerful perspective in math. In any dimension $n$, $SO(n)$ is a geometric object called a &lt;em&gt;manifold&lt;/em&gt;. Very roughly, a space that locally looks like Euclidean space, but which may have twists, holes, and other non-Euclidean features when we zoom out. Indeed, whether we make it precise or not, we all have a sense of whether two rotations are “close” to each other. For example, the reader would probably agree that $2$-dimensional rotations of $90^\circ$ and $91^\circ$ “feel” closer than rotations of $90^\circ$ and $180^\circ$. When $n=2$, one can show that the set of all rotations is geometrically “equivalent” to a $1$-dimensional circle. So, much of what we know about the circle can be translated to $SO(2)$.&lt;/p&gt;&lt;p&gt;What happens when we want to study the geometry of rotations in $n$-dimensions for $n &amp;gt; 3$? If $n = 512$ (a latent space for instance), this amounts to studying a manifold in $512^2$-dimensional space. Our visual intuition is seemingly useless here since it is not clear how concepts that are familiar in 2- and 3-dimensions can be utilized in $512^2$-dimensions. Mathematicians have been confronting the problem of understanding the un-visualizable for hundreds of years. One strategy is to find generalizations of familiar spatial concepts from $2$ and $3$-dimensions to $n$-dimensions that connect with our intuition.&lt;/p&gt;&lt;p&gt;This approach is already being used to better understand and characterize experimental observations about the space of model weights, hidden activations, and input data of deep learning models. We provide a taste of such tools and applications here:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Intrinsic Dimension: &lt;/em&gt;Dimension is a concept that is familiar not only from our experience in the spatial dimensions that we can readily access, 1-, 2-, and 3-dimensions, but also from more informal notions of “degrees of freedom” in everyday systems such as driving a car (forward/back, turning the steering wheel either left or right). The notion of dimension arises naturally in the context of machine learning where we may want to capture the number of independent ways in which a dataset, learned representation, or collection of weight matrices actually vary.&lt;p&gt;In formal mathematics, the definitions of dimension depend on the kind of space one is studying but they all capture some aspect of this everyday intuition. As a simple example, if I walk along the perimeter of a circle, I am only able to move forward and backward, and thus the dimension of this space is $1$. For spaces like the circle which are manifolds, dimension can be formally defined by the fact that a sufficiently small neighborhood around each point looks like a subset of some Euclidean space $\mathbb{R}^k$. We then say that the manifold is $k$-dimensional. If we zoom in on a small segment of the circle, it almost looks like a segment of $\mathbb{R} = \mathbb{R}^1$, and hence the circle is $1$-dimensional.&lt;/p&gt;&lt;p&gt;The manifold hypothesis posits that many types of data (at least approximately) live on a low-dimensional manifold even though they are embedded in a high-dimensional space. If we assume that this is true, it makes sense that the dimension of this underlying manifold, called the intrinsic dimension of the data, is one way to describe the complexity of the dataset. Researchers have estimated intrinsic dimension for common benchmark datasets, showing that intrinsic dimension appears to be correlated to the ease with which models generalize from training to test sets [6], and can explain differences in model performance and robustness in different domains such as medical images [7]. Intrinsic dimension is also a fundamental ingredient in some proposed explanations of data scaling laws [8, 9], which underlie the race to build ever bigger generative models.&lt;/p&gt;&lt;p&gt;Researchers have also noted that the intrinsic dimension of hidden activations tend to change in a characteristic way as information passes through the model [10, 11] or over the course of the diffusion process [12]. These and other insights have led to the use of intrinsic dimension in detection of adversarial examples [13], AI-generated content [14], layers where hidden activations contain the richest semantic content [11], and hallucinations in generative models [15].&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Curvature&lt;/em&gt;: While segments of the circle may look “straight” when we zoom up close enough, their curvature means that they will never be exactly linear as a straight line is. The notion of curvature is a familiar one and once formalized, it offers a way of rigorously measuring the extent to which the area around a point deviates from being linear. Care must be taken, however. Much of our everyday intuition about curvature assumes a single dimension. On manifolds with dimension $2$ or greater, there are multiple, linearly independent directions that we can travel away from a point and each of these may have a different curvature (in the $1$-dimensional sense). As a result, there are a range of different generalizations of curvature for higher-dimensional spaces, each with slightly different properties.&lt;p&gt;The notion of curvature has played a central role in deep learning, especially with respect to the loss landscape where changes in curvature have been used to analyze training trajectories [16]. Curvature is also central to an intriguing phenomenon known as the ‘edge of stability’, wherein the curvature of the loss landscape over the course of training increases as a function of learning rate until it hovers around the point where the training run is close to becoming unstable [17]. In another direction, curvature has been used to calculate the extent that model predictions change as input changes. For instance, [18] provided evidence that higher curvature in decision boundaries correlates with higher vulnerability to adversarial examples and suggested a new regularization term to reduce this. Finally, motivated by work in neuroscience, [19] presented a method that uses curvature to highlight interesting differences in representation between the raw training data and a neural network’s internal representation. A network may stretch and expand parts of the input space, generating regions of high curvature as it magnifies the representation of training examples that have a higher impact on the loss function.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Topology&lt;/em&gt;: Both dimension and curvature capture local properties of a space that can be measured by looking at the neighborhood around a single point. On the other hand, the most notable feature of our running example, the circle, is neither its dimension nor its curvature, but rather the fact that it is circular. We can only see this aspect by analyzing the whole space at once. Topology is the field of mathematics that focuses on such “global” properties.&lt;p&gt;Topological tools such as homology, which counts the number of holes in a space, has been used to illuminate the way that neural networks process data, with [20] showing that deep learning models “untangle” data distributions, reducing their complexity layer by layer. Versions of homology have also been applied to the weights of networks to better understand their structural features, with [21] showing that such topological statistics can reliably predict optimal early-stopping times. Finally, since topology provides frameworks that capture the global aspects of a space, it has proved a rich source of ideas for how to design networks that capture higher order relationships within data, leading to a range of generalizations of graph neural networks built on top of topological constructions [22, 23, 24, 25].&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;While the examples above have each been useful for gaining insight into phenomena related to deep learning, they were all developed to address challenges in other fields. We believe that a bigger payoff will come when the community uses the geometric paradigm described here to build new tools specifically designed to address the challenges that deep learning poses. Progress in this direction has already begun. Think for instance of linear mode connectivity which has helped us to better understand the loss landscape of neural networks [26] or work around the linear representation hypothesis which has helped to illuminate the way that concepts are encoded in the latent space of large language models [27]. One of the most exciting occurrences in mathematics is when the tools from one domain provide unexpected insight in another. Think of the discovery that Riemannian geometry provides some of the mathematical language needed for general relativity. We hope that a similar story will eventually be told for geometry and topology’s role in deep learning.&lt;/p&gt;&lt;h3 id="symmetries-in-data-symmetries-in-models"&gt;Symmetries in data, symmetries in models&lt;br /&gt;&lt;/h3&gt;&lt;p&gt;Symmetry is a central theme in mathematics, allowing us to break a problem into simpler components that are easier to solve. Symmetry has long played an important role in machine learning, particularly computer vision. In the classic dog vs. cat classification task for instance, an image that contains a dog continues to contain a dog regardless of whether we move the dog from one part of the image to another, whether we rotate the dog, or whether we reflect it. We say that the task is &lt;em&gt;invariant&lt;/em&gt; to image translation, rotation, and reflection.&lt;/p&gt;&lt;p&gt;The notion of symmetry is mathematically encoded in the concept of a &lt;em&gt;group&lt;/em&gt;, which is a set $G$ equipped with a binary operation $\star$ that takes two elements of $G$, $g_1$, $g_2$ as input and produces a third $g_1\star g_2$ as output. You can think of the integers $\mathbb{Z}$ with the binary operation of addition ($\star = +$) or the non-zero real numbers with the binary operation of multiplication ($\star = \times$). The set of $n$-dimensional rotations, $SO(n)$, also forms a group. The binary operation takes two rotations and returns a third rotation that is defined by simply applying the first rotation and then applying the second.&lt;/p&gt;&lt;p&gt;Groups satisfy axioms that ensure that they capture familiar properties of symmetries. For example, for any symmetry transformation, there should be an inverse operation that undoes the symmetry. If I rotate a circle by $90^{\circ}$, then I can rotate it back by $-90^{\circ}$ and return to where I started. Notice that not all transformations satisfy this property. For instance, there isn’t a well-defined inverse for downsampling an image. Many different images downsample to the same (smaller) image.&lt;/p&gt;&lt;p&gt;In the previous section we gave two definitions of $SO(n)$: the first was the geometric definition, as rotations of $\mathbb{R}^n$, and the second was as a specific subset of $n \times n$ matrices. While the former definition may be convenient for our intuition, the latter has the benefit that linear algebra is something that we understand quite well at a computational level. The realization of an abstract group as a set of matrices is called a &lt;em&gt;linear representation&lt;/em&gt; and it has proven to be one of the most fruitful methods of studying symmetry. It is also the way that symmetries are usually leveraged when performing computations (for example, in machine learning).&lt;/p&gt;&lt;p&gt;We saw a few examples of symmetries that can be found in the data of a machine learning task, such as the translation, rotation, and reflection symmetries in computer vision problems. Consider the case of a segmentation model. If one rotates an input image by $45^{\circ}$ and then puts it through the model, we will hope that we get a $45^{\circ}$ rotation of the segmentation prediction for the un-rotated image (this is illustrated in 1). After all, we haven’t changed the content of the image.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="323" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd1UiuD880gmdVrtjEKvGPBHIr0dvdBsrLXAnxUFz6_KQNLyMekhxrSR2ROn-H8O3780yoKbJvF0tUEVZSEdsuDfbB7kSGw_CFCqsKzjC6-wpxN5dxLQd-e4g7qMsKnc8BCX1pw7Qh0-I9hsgY9EInhpROs?key=h8RUuDFEFKnzGnrKx9gMkg" width="534" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 2: The concept of rotation equivariance illustrated for a segmentation model. One gets the same output regardless of whether one rotates first and then applies the network or applies the network and then rotates.&lt;/em&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="273" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe_QPixL6292p6Bz5yj_Hep_ypc6qrj-3q3Y7uIte0R5Nsc2ZPxNmSJOheOHohJY_0VbDi3LlyNSR61t94bHDfgTnJx0ssvyzU9KMtGLUKoqoiviKKTxpZR77Bb8VIzhkzd0Vxhspif10w8DnS3eWbjqwhW?key=h8RUuDFEFKnzGnrKx9gMkg" width="522" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 3: Equivariance holds when taking the top path (applying the network first and then the symmetry action) gives the same result as taking the bottom path (applying the symmetry transformation and then the network).&lt;/em&gt;&lt;/p&gt;&lt;p&gt;This property of a function (including neural networks), that applying a symmetry transformation before the function yields the same result as applying the symmetry transformation after the function is called &lt;em&gt;equivariance&lt;/em&gt; and can be captured by the diagram in Figure 3. The key point is that we get the same result whether we follow the upper path (applying the network first and then applying the group action) as when we follow the lower path (applying the group first and then applying the network). Conveniently, the concept of invariance, where applying a symmetry operation to input has no effect on the output of the function is a special case of equivariance where the action on the output space is defined to be trivial (applying symmetry actions does nothing).&lt;/p&gt;&lt;p&gt;Invariance and equivariance in deep learning models can be beneficial for a few reasons. Firstly, such a model will yield more predictable and consistent results across symmetry transformations. Secondly, through equivariance we can sometimes simplify the learning process with fewer parameters (compare the number of parameters in a convolutional neural network and an MLP of similar performance) and fewer modes of variation to learn in the data (a rotation invariant image classifier only needs to learn one orientation of each object rather than all possible orientations).&lt;/p&gt;&lt;p&gt;But how do we ensure that our model is equivariant? One way is to build our network with layers that are equivariant by design. By far the most well-known example of this is the convolutional neural network, whose layers are (approximately) equivariant to image translation. This is one reason why using a convolutional neural network for dog vs cat classification doesn’t require learning to recognize a dog at every location in an image as it might with an MLP. With a little thought, one can often come up with layers which are equivariant to a specific group. Unfortunately, being constrained to equivariant layers that we find in an ad-hoc manner often leaves us with a network with built-in equivariance but limited expressivity.&lt;/p&gt;&lt;p&gt;Fortunately, for most symmetry groups arising in machine learning, representation theory offers a comprehensive description of all possible linear equivariant maps. Indeed, it is a beautiful mathematical fact that all such maps are built from atomic building blocks called &lt;em&gt;irreducible representations&lt;/em&gt;. Happily, in many cases, the number of these irreducible representations is finite. Understanding the irreducible representations of a group can be quite powerful. Those familiar with the ubiquitous discrete Fourier transform (DFT) of a sequence of length $n$ are already familiar with the irreducible representations of one group, the cyclic group generated by a rotation by $360 ^{\circ}/n$ (though we note that moving between the description we give here and the description of the DFT found in the signal processing literature takes a little thought).&lt;/p&gt;&lt;p&gt;There is now a rich field of research in deep learning that uses group representations to systematically build expressive equivariant architectures. Some examples of symmetries that have been particularly well-studied include: rotation and reflection of images [28, 29, 30, 31], 3-dimensional rotation and translation of molecular structures [32] or point clouds [33], and permutations for learning on sets [34] or nodes of a graph [35]. Encoding equivariance to more exotic symmetries has also proven useful for areas such as theoretical physics [36] and data-driven optimization [37].&lt;/p&gt;&lt;p&gt;Equivariant layers and other architectural approaches to symmetry awareness are a prime example of using mathematics to inject high-level priors into a model. Do these approaches represent the future of learning in the face of data symmetries? Anecdotally, the most common approach to learning on data with symmetries continues to be using enough training data and enough data augmentation for the model to learn to handle the symmetries on its own. Two years ago, the author would have speculated that these latter approaches only work for simple cases, such as symmetries in 2-dimensions, and will be outperformed by models which are equivariant by design when symmetries become more complex. Yet, we continue to be surprised by the power of scale. After all, AlphaFold3 [38] uses a non-equivariant architecture despite learning on data with several basic symmetries. We speculate that there may be a threshold on the ratio of symmetry complexity on the one hand and the amount of training data on the other, that determines whether built-in equivariance will outperform learned equivariance [39, 40].&lt;br /&gt;&lt;/p&gt;&lt;p&gt;If this is true, we can expect to see models move away from bespoke equivariant architectures as larger datasets become available for a specific application. At the same time, since compute will always be finite, we predict that there will be some applications with exceptionally complex symmetries that will always require some built-in priors (for example, AI for math or algorithmic problems). Regardless of where we land on this spectrum, mathematicians can look forward to an interesting comparison of the ways humans inject symmetry into models vs the way that models learn symmetries on their own [41, 42].&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="129" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcJa_1Ow2zrMVuk1hJiOJtTJBq5bH7zyogibZ5fqQu85ERGFEjcX4jRn7r_rnZvTrCdpN5OzeVQUBLu60DJP_aIR4uoHq33tRMcoPAUf7qumOeJLreCkttvqtQEssCh90UwlbWkzBoK79FV54R6ncO2c_Ij?key=h8RUuDFEFKnzGnrKx9gMkg" width="572" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 4: A cartoon illustrating why adding a permutation and its inverse before and after a pointwise nonlinearity produces an equivalent model (even though the weights will be different). Since permutations can be realized by permutation matrices, the crossed arrows on the right can be merged into the fully-connected layer.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Of course, symmetry is not only present in data but also in the models themselves. For instance, the activations of hidden layers of a network are invariant to permutation. We can permute activations before entering the non-linearity and if we un-permute them afterward, the model (as a function) does not change (Figure 4). This means that we have an easy recipe for generating an exponentially large number of networks that have different weights but behave identically on data. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;While simple, this observation produces some unexpected results. There is evidence, for instance, that while the loss landscape of neural networks is highly non-convex, it may be much less non-convex when we consider all networks that can be produced through this permutation operation as equivalent [43, 44]. This means that your network and my network may not be connected by a linear path of low loss, but such a path may exist between your network and a permutation of my network. Other research has looked at whether it may be possible to use symmetries to accelerate optimization by ‘teleporting’ a model to a more favorable location in the loss landscape [45, 46]. Finally, permutation symmetries also provide one type of justification for an empirical phenomenon where individual neurons in a network tend to encode more semantically meaningful information than arbitrary linear combinations of such neurons [47].&lt;/p&gt;&lt;h3 id="taming-complexity-with-abstraction"&gt;Taming Complexity with Abstraction&lt;/h3&gt;&lt;p&gt;When discussing symmetry, we used the diagram in Figure 3 to define equivariance. One of the virtues of this approach is that we never had to specify details about the input data or architecture that we used. The spaces could be vector spaces and the maps linear transformations, they could be neural networks of a specific architecture, or they could just be sets and arbitrary functions between them–the definition is valid for each. This &lt;em&gt;diagrammatic&lt;/em&gt; point of view, which looks at mathematical constructions in terms of the composition of maps between objects rather than the objects themselves, has been very fruitful in mathematics and is one gateway to the subject known as &lt;em&gt;category theory&lt;/em&gt;. Category theory is now the lingua franca in many areas of mathematics since it allows mathematicians to translate definitions and results across a wide range of contexts.&lt;/p&gt;&lt;p&gt;Of course, deep learning is at its core all about function composition, so it is no great leap to try and connect it to the diagrammatic tradition in mathematics. The focus of function composition in the two disciplines is different, however. In deep learning we take simple layers that alone lack expressivity and compose them together to build a model capable of capturing the complexity of real-world data. With this comes the tongue-in-cheek demand to “stack more layers!”. Category theory instead tries to find a universal framework that captures the essence of structures appearing throughout mathematics. This allows mathematicians to uncover connections between things that look very different at first glance. For instance, category theory gives us the language to describe how the topological structure of a manifold can be encoded in groups via homology or homotopy theory.&lt;/p&gt;&lt;p&gt;It can be an interesting exercise to try to find a diagrammatic description of familiar constructions like the product of two sets $X$ and $Y$. Focusing our attention on maps rather than objects we find that what characterizes $X \times Y$ is the existence of the two canonical projections $\pi_1$ and $\pi_2$, the former sending $(x,y) \mapsto x$ and $(x,y) \mapsto y$ (at least in more familiar settings where $X$ and $Y$ are, for example, sets). Indeed, the &lt;em&gt;product &lt;/em&gt;$X \times Y$ (regardless of whether $X$ and $Y$ are sets, vectors spaces, etc.) is the unique object such that for any $Z$ with maps $f_1: Z \rightarrow X$ and $f_2: Z \rightarrow Y$, there is a map $h: Z \rightarrow X \times Y$ that satisfies the commutative diagram in Figure 5.&lt;/p&gt;&lt;p&gt;While this construction is a little involved for something as familiar as a product it has the remarkable property that it allows us to define a “product” even when there is no &amp;nbsp;underlying set structure (that is, those settings where we cannot resort to defining $X \times Y$ as the set of pairs of $(x,y)$ for $x \in X$ and $y \in Y$).&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="277" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd0afUER-JEqT9BBW0z5ip7HSPD_ORKlpxTaQQFpep7MZF3DKfhgca3XbrZ2aGGTTnxcyOD3csHF1hdODeSXFx-nC63Mlw2etuY9xtM-AUvec4aIZKJK0hl2QiuxyJPzmlr18GbJA?key=h8RUuDFEFKnzGnrKx9gMkg" width="624" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 5: The commutative diagram that describes a product $X \times Y$. For any $Z$ with maps $f_1: Z \rightarrow X$ and $f_2: Z \rightarrow Y$, there exists a unique map $h: Z \rightarrow X \times Y$ such that $f_1 = \pi_1 \circ h$ and $f_2 = \pi_2 \circ h$ where $\pi_1$ and $\pi_2$ are the usual projection maps from $X \times Y$ to $X$ and $X \times Y$ to $Y$ respectively.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One can reasonably argue that diagrammatic descriptions of well-known constructions, like products, are not useful for the machine learning researcher. After all, we already know how to form products in all of the spaces that come up in machine learning. On the other hand, there are more complicated examples where diagrammatics mesh well with the way we build neural network architectures in practice.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="352" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXf30eZdcoTcrjBuYEo6BUjm4gmw8fvcY9kLpDsspW0qPoIVu6LN5mfd1ks5qiMtf9J1DyPNDtzDDLDpxVi7n5j62DxlIfkwyo5V4gAC7MeeMpUaDzOMgsU4Mqjrs7fUXL-hc_BeqPd9Upu6L0wmKnDzkop4?key=h8RUuDFEFKnzGnrKx9gMkg" width="624" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 6: Fiber bundles capture the notion that a space might locally look like a product but globally have twists in it.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Fiber bundles are a central construction in geometry and topology that capture the notion that a space may locally look like a product but may have twists that break this product structure globally. Compare the cylinder with the Möbius band. We can build both of these by starting with a circle and taking a product with the line segment $(0,1)$. In the case of the cylinder, this really is just (topologically) the product of the circle and the segment $(0,1)$, but to form the Möbius band we must add an additional twist that breaks the product structure. In these examples, the circle is called the &lt;em&gt;base &lt;/em&gt;space and $(0,1)$ is called the &lt;em&gt;fiber&lt;/em&gt;. While only the cylinder is a true product, both the cylinder and the Möbius band are fiber bundles. Here is another way of thinking about a fiber bundle. A fiber bundle is a union of many copies of the fiber parametrized by the base space. In the Möbius band/cylinder example, each point on the circle carries its own copy of $(0,1)$.&lt;/p&gt;&lt;p&gt;We drew inspiration from this latter description of fiber bundles when we were considering a conditional generation task in the context of a problem in materials science. Since the materials background is somewhat involved, we’ll illustrate the construction via a more pedestrian, animal-classification analogue. Let $M$ be the manifold of all possible images containing a single animal. We can propose to decompose the variation in elements of $M$ into two parts, the species of animal in the image and everything else, where the latter could mean differences in background, lighting, pose, image quality, etc. One might want to explore the distribution of one of these factors of variation while fixing the other. For instance, we might want to fix the animal species and explore the variation we get in background, pose, etc. For example, comparing the variation in background for two different species of insect may tell the entomologist about the preferred habitat for different types of beetles.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="421" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfLfnOQw_uLzm58bcucM5zOzGLHKzbX8hyQU2muIPl994v1GQN0sfMwQgSjFwsaCDetRHW8WR_T71pjNX7waqch44PwUY6Dv8egfzRlOmo6e0BbDagYv99K6tMnvVeTAIb9ww9bT_3Ukcs4k7xHx-BH7cxR?key=h8RUuDFEFKnzGnrKx9gMkg" width="624" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 7: A cartoon visualizing how the set of all animal images could be decomposed into a local product of animal species and other types of variation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;One might hope to solve this problem by learning an encoding of $M$ into a product space $X_1 \times X_2$ where $X_1$ is a discrete set of points corresponding to animal species and $X_2$ is a space underlying the distribution of all other possible types of variation for a fixed species of animal. Fixing the species would then amount to choosing a specific element $x_1$ from $X_1$ and sampling from the distribution on $X_2$. The product structure of $X_1 \times X_2$ allows us to perform such independent manipulations of $X_1$ and $X_2$. On the other hand, products are rigid structures that impose strong, global topological assumptions on the real data distribution. We found that even on toy problems, it was hard to learn a good map from the raw data distribution to the product-structured latent space defined above. Given that fiber bundles are more flexible and still give us the properties we wanted from our latent space, we designed a neural network architecture to learn a fiber bundle structure on a data distribution [48].&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="240" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdrgVDyhD0JXBm12Gh1NstAjVx3fSk8vM3Mg_3JGi6JpK3PYTWUpmgzW_BgmEOMeZahkdrzWEw2ThViUKXnEGFobRORcOMgifUin2kJY3-zFIq4fbj-4QO6x7ALnwn5qLU880r1raMaFC2yqn6RVyDPGEk?key=h8RUuDFEFKnzGnrKx9gMkg" width="415" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 8: The commutative diagram describing a fiber bundle. The map $\pi$ projects from neighborhoods of the total space to the base space, $U$ is a local neighborhood of the base space, and $F$ &amp;nbsp;is the fiber. The diagram says that each point in the base space has a neighborhood $U$ &amp;nbsp;such that when we lift this to the bundle, we get something that is homeomorphic (informally, equivalent) to the product of the neighborhood &amp;nbsp;and the fiber. But this product structure may not hold globally over the whole space.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;But how do we go from the abstract definition of a fiber bundle above to a neural network architecture that we can code up on a computer. It turns out there is a succinct diagrammatic definition of a fiber bundle (Figure 8) that can serve as a convenient template to build up an architecture from. We were able to proceed in a relatively naïve fashion, taking each of the maps in the diagram and building a corresponding stack of layers. The diagram itself then told us how to compose each of these components together. The commutativity of the diagram was engineered through a term in the loss function that ensures that $\pi = \text{proj}_1 \circ \varphi$. There were also some conditions on $\varphi$ and $\pi$ (such as the bijectivity of $\phi$) that needed to be engineered. Beyond this, we were surprised at the amount of flexibility we had. This is useful since it means this process is largely agnostic to data modality.&lt;/p&gt;&lt;p&gt;This is an elementary example of how the diagrammatic tradition in mathematics can provide us with a broader perspective on the design of neural networks, allowing us to connect deep structural principles with large-scale network design without having to specify small-scale details that might be problem dependent. Of course, all this fails to draw from anything beyond the surface of what the categorical perspective has to offer. Indeed, category theory holds promise as a unified framework to connect much of what appears and is done in machine learning [49].&lt;/p&gt;&lt;h3 id="conclusion"&gt;Conclusion&lt;br /&gt;&lt;/h3&gt;&lt;p&gt;In the mid-twentieth century, Eugene Wigner marveled at the “the unreasonable effectiveness of mathematics” as a framework for not only describing existing physics but also anticipating new results in the field [50]. A mantra more applicable to recent progress in machine learning is “the unreasonable effectiveness of data” [51] and compute. This could appear to be a disappointing situation for mathematicians who might have hoped that machine learning would be as closely intertwined to advanced mathematics as physics is. However, as we’ve demonstrated, while mathematics may not maintain the same role in machine learning research that it has held in the past, the success of scale actually opens new paths for mathematics to support progress in machine learning research. These include:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Providing powerful tools for deciphering the inner workings of complex models&lt;/li&gt;&lt;li&gt;Offering a framework for high-level architectural decisions that leave the details to the learning algorithm&lt;/li&gt;&lt;li&gt;Bridging traditionally isolated domains of mathematics like topology, abstract algebra, and geometry with ML and data science applications.&lt;br /&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Should the way things have turned out surprise us? Perhaps not, given that machine learning models ultimately reflect the data they are trained on and in most cases this data comes from fields (such as natural language or imagery) which have long resisted parsimonious mathematical models. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Yet, this situation is also an opportunity for mathematics. Performant machine learning models may provide a gateway for mathematical analysis of a range of fields that were previously inaccessible. It’s remarkable for instance that trained word embeddings transform semantic relationships into algebraic operations on vectors in Euclidean space (for instance, ‘Italian’ - ‘Italy’ + ‘France’ = ‘French’). Examples like this hint at the potential for mathematics to gain a foothold in complex, real-world settings by studying the machine learning models that have trained on data from these settings.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;As more and more of the data in the world is consumed and mathematicised by machine learning models, it will be an increasingly interesting time to be a mathematician. The challenge now lies in adapting our mathematical toolkit to this new landscape, where empirical breakthroughs often precede theoretical understanding. By embracing this shift, mathematics can continue to play a crucial, albeit evolving, role in shaping the future of machine learning.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;The author would like to thank Darryl Hannan for help with figures, Davis Brown, Charles Godfrey, and Scott Mahan for useful feedback on drafts, as well as the staff of the Gradient for useful conversations and help editing this article. For resources and events around the growing community of mathematicians and computer scientists using topology, algebra, and geometry (TAG) to better understand and build more robust machine learning systems, please visit us at &lt;/em&gt;&lt;em&gt;https://www.tagds.com&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;p&gt;[1] Richard Sutton. "The bitter lesson". In: &lt;em&gt;Incomplete Ideas (blog)&lt;/em&gt; 13.1 (2019), p. 38.&lt;/p&gt;&lt;p&gt;[2] Guido F Montufar et al. "On the number of linear regions of deep neural networks". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 27 (2014).&lt;/p&gt;&lt;p&gt;[3] Boris Hanin and David Rolnick. "Complexity of linear regions in deep networks". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2019, pp. 2596–2604.&lt;/p&gt;&lt;p&gt;[4] J Elisenda Grigsby and Kathryn Lindsey. "On transversality of bent hyperplane arrangements and the topological expressiveness of ReLU neural networks". In: &lt;em&gt;SIAM Journal on Applied Algebra and Geometry&lt;/em&gt; 6.2 (2022), pp. 216–242.&lt;/p&gt;&lt;p&gt;[5] Ahmed Imtiaz Humayun et al. "Splinecam: Exact visualization and characterization of deep network geometry and decision boundaries". In: &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2023, pp. 3789–3798.&lt;/p&gt;&lt;p&gt;[6] Phillip Pope et al. "The intrinsic dimension of images and its impact on learning". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2104.08894 (2021).&lt;/p&gt;&lt;p&gt;[7] Nicholas Konz and Maciej A Mazurowski. "The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2401.08865 (2024).&lt;/p&gt;&lt;p&gt;[8] Yasaman Bahri et al. "Explaining neural scaling laws". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2102.06701 (2021).&lt;/p&gt;&lt;p&gt;[9] Utkarsh Sharma and Jared Kaplan. "A neural scaling law from the dimension of the data manifold". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2004.10802 (2020).&lt;/p&gt;&lt;p&gt;[10] Alessio Ansuini et al. "Intrinsic dimension of data representations in deep neural networks". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 32 (2019).&lt;/p&gt;&lt;p&gt;[11] Lucrezia Valeriani et al. "The geometry of hidden representations of large transformer models". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 36 (2024).&lt;/p&gt;&lt;p&gt;[12] Henry Kvinge, Davis Brown, and Charles Godfrey. "Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension". In: &lt;em&gt;ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;[13] Xingjun Ma et al. "Characterizing adversarial subspaces using local intrinsic dimensionality". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1801.02613 (2018).&lt;/p&gt;&lt;p&gt;[14] Peter Lorenz, Ricard L Durall, and Janis Keuper. "Detecting images generated by deep diffusion models using their local intrinsic dimensionality". In: &lt;em&gt;Proceedings of the IEEE/CVF International Conference on Computer Vision&lt;/em&gt;. 2023, pp. 448–459.&lt;/p&gt;&lt;p&gt;[15] Fan Yin, Jayanth Srinivasa, and Kai-Wei Chang. "Characterizing truthfulness in large language model generations with local intrinsic dimension". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2402.18048 (2024).&lt;/p&gt;&lt;p&gt;[16] Justin Gilmer et al. "A loss curvature perspective on training instabilities of deep learning models". In: &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2021.&lt;/p&gt;&lt;p&gt;[17] Jeremy Cohen et al. "Gradient descent on neural networks typically occurs at the edge of stability". In: &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2020.&lt;/p&gt;&lt;p&gt;[18] Seyed-Mohsen Moosavi-Dezfooli et al. "Robustness via curvature regularization, and vice versa". In: &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2019, pp. 9078–9086.&lt;/p&gt;&lt;p&gt;[19] Francisco Acosta et al. "Quantifying extrinsic curvature in neural manifolds". In: &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2023, pp. 610–619.&lt;/p&gt;&lt;p&gt;[20] Gregory Naitzat, Andrey Zhitnikov, and Lek-Heng Lim. "Topology of deep neural networks". In: &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 21.184 (2020), pp. 1–40.&lt;/p&gt;&lt;p&gt;[21] Bastian Rieck et al. "Neural persistence: A complexity measure for deep neural networks using algebraic topology". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1812.09764 (2018).&lt;/p&gt;&lt;p&gt;[22] Mustafa Hajij, Kyle Istvan, and Ghada Zamzmi. "Cell complex neural networks". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2010.00743 (2020).&lt;/p&gt;&lt;p&gt;[23] Cristian Bodnar. "Topological deep learning: graphs, complexes, sheaves". PhD thesis. 2023.&lt;/p&gt;&lt;p&gt;[24] Jakob Hansen and Robert Ghrist. "Toward a spectral theory of cellular sheaves". In: &lt;em&gt;Journal of Applied and Computational Topology&lt;/em&gt; 3.4 (2019), pp. 315–358.&lt;/p&gt;&lt;p&gt;[25] Yifan Feng et al. "Hypergraph neural networks". In: &lt;em&gt;Proceedings of the AAAI Conference on Artificial Intelligence&lt;/em&gt;. Vol. 33. 01. 2019, pp. 3558–3565.&lt;/p&gt;&lt;p&gt;[26] Felix Draxler et al. "Essentially no barriers in neural network energy landscape". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2018, pp. 1309–1318.&lt;/p&gt;&lt;p&gt;[27] Kiho Park, Yo Joong Choe, and Victor Veitch. "The linear representation hypothesis and the geometry of large language models". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2311.03658 (2023).&lt;/p&gt;&lt;p&gt;[28] Taco Cohen and Max Welling. "Group equivariant convolutional networks". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2016, pp. 2990–2999.&lt;/p&gt;&lt;p&gt;[29] Maurice Weiler, Fred A Hamprecht, and Martin Storath. "Learning steerable filters for rotation equivariant cnns". In: &lt;em&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2018, pp. 849–858.&lt;/p&gt;&lt;p&gt;[30] Daniel E Worrall et al. "Harmonic networks: Deep translation and rotation equivariance". In: &lt;em&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2017, pp. 5028–5037.&lt;/p&gt;&lt;p&gt;[31] Diego Marcos et al. "Rotation equivariant vector field networks". In: &lt;em&gt;Proceedings of the IEEE International Conference on Computer Vision&lt;/em&gt;. 2017, pp. 5048–5057.&lt;/p&gt;&lt;p&gt;[32] Alexandre Duval et al. "A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2312.07511 (2023).&lt;/p&gt;&lt;p&gt;[33] Nathaniel Thomas et al. "Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1802.08219 (2018).&lt;/p&gt;&lt;p&gt;[34] Manzil Zaheer et al. "Deep sets". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 30 (2017).&lt;/p&gt;&lt;p&gt;[35] Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. "E (n) equivariant graph neural networks". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2021, pp. 9323–9332.&lt;/p&gt;&lt;p&gt;[36] Denis Boyda et al. "Sampling using SU (N) gauge equivariant flows". In: &lt;em&gt;Physical Review D&lt;/em&gt; 103.7 (2021), p. 074504.&lt;/p&gt;&lt;p&gt;[37] Hannah Lawrence and Mitchell Tong Harris. "Learning Polynomial Problems with SL(2,\mathbb {R}) −Equivariance". In: &lt;em&gt;The Twelfth International Conference on Learning Representations&lt;/em&gt;. 2023.&lt;/p&gt;&lt;p&gt;[38] Josh Abramson et al. "Accurate structure prediction of biomolecular interactions with AlphaFold 3". In: &lt;em&gt;Nature&lt;/em&gt; (2024), pp. 1–3.&lt;/p&gt;&lt;p&gt;[39] Scott Mahan et al. "What Makes a Machine Learning Task a Good Candidate for an Equivariant Network?" In: &lt;em&gt;ICML 2024 Workshop on Geometry-grounded Representation Learning and Generative Modeling&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;[40] Johann Brehmer et al. "Does equivariance matter at scale?" In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2410.23179 (2024).&lt;/p&gt;&lt;p&gt;[41] Chris Olah et al. "Naturally Occurring Equivariance in Neural Networks". In: &lt;em&gt;Distill&lt;/em&gt; (2020). https://distill.pub/2020/circuits/equivariance. doi: 10.23915/distill.00024.004.&lt;/p&gt;&lt;p&gt;[42] Giovanni Luca Marchetti et al. "Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2312.08550 (2023).&lt;/p&gt;&lt;p&gt;[43] Rahim Entezari et al. "The role of permutation invariance in linear mode connectivity of neural networks". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2110.06296 (2021).&lt;/p&gt;&lt;p&gt;[44] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. "Git re-basin: Merging models modulo permutation symmetries". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2209.04836 (2022).&lt;/p&gt;&lt;p&gt;[45] Bo Zhao et al. "Symmetry teleportation for accelerated optimization". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 35 (2022), pp. 16679–16690.&lt;/p&gt;&lt;p&gt;[46] Bo Zhao et al. "Improving Convergence and Generalization Using Parameter Symmetries". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2305.13404 (2023).&lt;/p&gt;&lt;p&gt;[47] Charles Godfrey et al. "On the symmetries of deep learning models and their internal representations". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 35 (2022), pp. 11893–11905.&lt;/p&gt;&lt;p&gt;[48] Nico Courts and Henry Kvinge. "Bundle Networks: Fiber Bundles, Local Trivializations, and a Generative Approach to Exploring Many-to-one Maps". In: &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2021.&lt;/p&gt;&lt;p&gt;[49] Bruno Gavranović et al. "Position: Categorical Deep Learning is an Algebraic Theory of All Architectures". In: &lt;em&gt;Forty-first International Conference on Machine Learning&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;[50] Eugene P Wigner. "The unreasonable effectiveness of mathematics in the natural sciences". In: &lt;em&gt;Mathematics and Science&lt;/em&gt;. World Scientific, 1990, pp. 291–306.&lt;/p&gt;&lt;p&gt;[51] Alon Halevy, Peter Norvig, and Fernando Pereira. "The unreasonable effectiveness of data". In: &lt;em&gt;IEEE Intelligent Systems&lt;/em&gt; 24.2 (2009), pp. 8–12.&lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[What's Missing From LLM Chatbots: A Sense of Purpose]]&amp;gt;LLM-based chatbots’ capabilities have been advancing every month. These improvements are mostly measured by benchmarks like MMLU, HumanEval, and MATH (e.g. sonnet 3.5, gpt-4o). However, as these measures get more and more saturated, is user experience increasing in proportion to these scores? If we envision a future]]&amp;gt;https://thegradient.pub/dialog/66c6733993571d5c8c154fb1Mon, 09 Sep 2024 17:28:48 GMT&lt;p&gt;LLM-based chatbots’ capabilities have been advancing every month. These improvements are mostly measured by benchmarks like MMLU, HumanEval, and MATH (e.g. sonnet 3.5, gpt-4o). However, as these measures get more and more saturated, is user experience increasing in proportion to these scores? If we envision a future of human-AI collaboration rather than AI replacing humans, the current ways of measuring dialogue systems may be insufficient because they measure in a non-interactive fashion.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Why does purposeful dialogue matter?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Purposeful dialogue refers to a multi-round user-chatbot conversation that centers around a goal or intention. The goal could range from a generic one like “harmless and helpful” to more specific roles like “travel planning agent”, “psycho-therapist” or “customer service bot.”&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Travel planning is a simple, illustrative example. Our preferences, fellow travelers’ preference, and all the complexities of real-world situations make transmitting all information in one pass way too costly. However, if multiple back-and-forth exchanges of information are allowed, only important information gets selectively exchanged. Negotiation theory offers an analogy of this—iterative bargaining yields better outcomes than a take-it-or-leave-it offer. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;In fact, sharing information is only one aspect of dialogue. In Terry Winograd’s words: “All language use can be thought of as a way of activating procedures within the hearer.” We can think of each utterance as a deliberate action that one party takes to alter the world model of the other. What if both parties have more complicated, even hidden goals? In this way, purposeful dialogue provides us with a way of formulating human-AI interactions as a collaborative game, where the goal of chatbot is to help humans achieve certain goals. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;This might seem like an unnecessary complexity that is only a concern for academics. However, purposeful dialogue could be beneficial even for the most hard-nosed, product-oriented research direction like code generation. Existing coding benchmarks mostly measure performances in a one-pass generation setting; however, for AI to automate solving ordinary Github issues (like in SWE-bench), it’s unlikely to be achieved by a single action—the AI needs to communicate back and forth with human software engineers to make sure it understands the correct requirements, ask for missing documentation and data, and even ask humans to give it a hand if needed. In a similar vein to pair programming, this could reduce the defects of code but without the burden of increasing man-hours. &lt;/p&gt;&lt;p&gt;&lt;br /&gt;Moreover, with the introduction of turn-taking, many new possibilities can be unlocked. As interactions become long-term and memory is built, the chatbot can gradually update user profiles. It can also adapt to their preferences. Imagine a personal assistant (e.g., IVA, Siri) that, through daily interaction, learns your preferences and intentions. It can read your resources of new information automatically (e.g., twitter, arxiv, Slack, NYT) and provide you with a morning news summary according to your preferences. It can draft emails for you and keep improving by learning from your edits.&lt;/p&gt;&lt;p&gt;In a nutshell, meaningful interactions between people rarely begin with complete strangers and conclude in just one exchange. Humans naturally interact with each other through multi-round dialogues and adapt accordingly throughout the conversation. However, doesn’t that seem exactly the opposite of predicting the next token, which is the cornerstone of modern LLMs? Below, let’s take a look at the makings of dialogue systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;How were/are dialogue systems made?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Let's jump back to the 1970s, when Roger Schank introduced his "restaurant script" as a kind of dialogue system [1]. This script breaks down the typical restaurant experience into steps like entering, ordering, eating, and paying, each with specific scripted utterances. Back then, every piece of dialogue in these scenarios was carefully planned out, enabling AI systems to mimic realistic conversations. ELIZA, a Rogerian psychotherapist simulator, and PARRY, a system mimicking a paranoid individual, were two other early dialogue systems until the dawn of machine learning.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Compare this approach to the LLM-based dialogue system today, it seems mysterious how models trained to predict the next token could do anything at all with engaging in dialogues. Therefore, let’s take a close examination of how dialogue systems are made, with an emphasis on how the dialogue format comes into play:&lt;/p&gt;&lt;p&gt;(1) Pretraining: a sequence model is trained to predict the next token on a gigantic corpus of mixed internet texts. The compositions may vary but they are predominantly news, books, Github code, with a small blend of forum-crawled data such as from Reddit, Stack Exchange, which may contain dialogue-like data.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="288" src="https://thegradient.pub/content/images/2024/08/unnamed.png" width="512" /&gt;&lt;figcaption&gt;Table of the pretraining data mixture from llama technical report&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;(2) Introduce dialogue formatting: because the sequence model only processes strings, while the most natural representation of dialogue history is a structured index of system prompts and past exchanges, a certain kind of formatting must be introduced for the purpose of conversion. Some Huggingface tokenizers provide this method called tokenizer.apply_chat_template for the convenience of users. The exact formatting differs from model to model, but it usually involves guarding the system prompts with &amp;lt;system&amp;gt; or &amp;lt;INST&amp;gt; in the hope that the pretrained model could allocate more attention weights to them. The system prompt plays a significant role in adapting language models to downstream applications and ensuring its safe behavior (we will talk more in the next section). Notably, the choice of the format is arbitrary at this step—pretraining corpus doesn’t follow this format.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="112" src="https://thegradient.pub/content/images/2024/08/image1.png" width="1398" /&gt;&lt;figcaption&gt;The context window of a chatbot&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;(3) RLHF: In this step, the chatbot is directly rewarded or penalized for generating desired or undesired answers. It’s worth noting that this is the first time the introduced dialogue formatting appears in the training data. RLHF is a &lt;em&gt;fine&lt;/em&gt;-tuning step not only because the data size is dwarfed in comparison to the pretraining corpus, but also due to the KL penalty and targeted weight tuning (e.g. Lora). Using Lecun’s analogy of cake baking, RLHF is only the small cherry on the top.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="580" src="https://thegradient.pub/content/images/2024/08/image5.png" width="1440" /&gt;&lt;figcaption&gt;Image from Yann Lecun’s slides&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2 id="how-consistent-are-existing-dialogue-systems-in-2024"&gt;How consistent are existing dialogue systems (in 2024)? &lt;br /&gt;&lt;/h2&gt;&lt;p&gt;The minimum requirement we could have for a dialogue system is that it can stay on the task we gave them. In fact, we humans often drift from topic to topic. How well do current systems perform? &lt;/p&gt;&lt;p&gt;&lt;br /&gt;Currently, “system prompt” is the main method that allows users to control LM behavior. However, researchers found evidence that LLMs can be brittle in following these instructions under adversarial conditions [12,13]. Readers might also have experienced this through daily interactions with ChatGPT or Claude—when a new chat window is freshly opened, the model can follow your instruction reasonably well [2], but after several rounds of dialogue, it’s no longer &lt;em&gt;fresh&lt;/em&gt;, even stops following its role altogether.&lt;/p&gt;&lt;p&gt;How could we quantitatively capture this anecdote? For one-round instruction following, we’ve already enjoyed plenty of benchmarks such as MT-Bench and Alpaca-Eval. However, when we test models in an interactive fashion, it’s hard to anticipate what the model generates and prepare a reply in advance. In a project by my collaborators and me [3], we built an environment to synthesize dialogues with unlimited length to stress-test the instruction-following capabilities of LLM chatbots. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;To allow an unconstrained scaling on the time scale, we let two system-prompted LM agents chat with each other for an extended number of rounds. This forms the main trunk of dialogue [a1, b1, a2, b2, …, a8, b8] (say the dialogue is 8-round). At this point, we could probably figure out how the LLMs stick to its system prompts just by examining this dialogue, but many of the utterances can be irrelevant to the instructions, depending on where the conversation goes. Therefore, we hypothetically branch out at each round by asking a question directly related to the system prompts, and use a corresponding judging function to quantify how well it performs. All that's provided by the dataset is a bank of triplets of (system prompts, probe questions, and judging functions).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="570" src="https://thegradient.pub/content/images/2024/08/image3.png" width="1999" /&gt;&lt;figcaption&gt;Sketch of the process of measuring instruction stability&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Averaging across scenarios and pairs of system prompts, we get a curve of instruction stability across rounds. To our surprise, the aggregated results on both LLaMA2-chat-70B and gpt-3.5-turbo-16k are alarming. Besides the added difficulty to prompt engineering, the lack of instruction stability also comes with safety concerns. When the chatbot drifts away from its system prompts that stipulate safety aspects, it becomes more susceptible to jailbreaking and prone to more hallucinations.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="689" src="https://thegradient.pub/content/images/2024/08/Screenshot-2024-08-21-at-19.15.57.png" width="1736" /&gt;&lt;figcaption&gt;Instruction stability on LLaMA2-chat-70B and gpt-3.5-turbo-16k&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The empirical results also contrast with the ever-increasing context length of LLMs. Theoretically, some long-context models can attend to a window of up to 100k tokens. However, in the dialogue setting, they become distracted after only 1.6k tokens (assuming each utterance is 100 tokens). In [3], we further theoretically showed how this is inevitable in a Transformer based LM chatbot under the current prompting scheme, and proposed a simple technique called split-softmax to mitigate such effects. &lt;/p&gt;&lt;p&gt;One might ask at this point, why is it so bad? Why don't humans lose their persona just by talking to another person for 8 rounds? It’s arguable that human interactions are based on purposes and intentions [5] and these purposes precede the means rather than the opposite—LLM is fundamentally a fluent English generator, and the persona is merely a thin added layer.&lt;/p&gt;&lt;h2 id="what%E2%80%99s-missing"&gt;What’s missing? &lt;br /&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Pretraining?&lt;/strong&gt;&lt;br /&gt;Pretraining endows the language model with the capability to model a distribution over internet personas as well as the lower-level language distribution of each persona [4]. However, even when one persona (or a mixture of a limited number of them) is specified by the instruction of system prompts, current approaches fail to single it out.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;RLHF?&lt;/strong&gt;&lt;br /&gt;RLHF provides a powerful solution to adapting this multi-persona model to a “helpful and harmless assistant.” However, the original RLHF methods formulate reward maximization as a one-step bandit problem, and it is not generally possible to train with human feedback in the loop of conversation. (I’m aware of many advances in alignment but I want to discuss the original RLHF algorithm as a prototypical example.) This lack of multi-turn planning may cause models to suffer from task ambiguity [6] and learning superficial human-likeness rather than goal-directed social interaction [7].&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Will adding more dialogue data in RLHF help? My guess is that it will, to a certain extent, but it will still fall short due to a lack of purpose. Sergey Levine pointed out in his blog that there is a fundamental difference between preference learning and intentions: “the key distinction is between viewing language generation as selecting goal-directed actions in a sequential process, versus a problem of producing outputs satisfying user preferences.”&lt;/p&gt;&lt;h2 id="purposeful-dialogue-system"&gt;Purposeful dialogue system&lt;/h2&gt;&lt;p&gt;Staying on task is a modest request for LLMs. However, even if an LLM remains focused on the task, it doesn't necessarily mean it can excel in achieving the goal.&lt;/p&gt;&lt;p&gt;The problem of long-horizon planning has attracted some attention in the LLM community. For example, “decision-oriented dialogue” is proposed as a general class of tasks [8], where the AI assistant collaborates with humans to help them make complicated decisions, such as planning itineraries in a city and negotiating travel plans among friends. Another example, Sotopia [10], is a comprehensive social simulation platform that compiles various goal-driven dialogue scenarios including collaboration, negotiation, and persuasion. &lt;/p&gt;&lt;p&gt;Setting up such benchmarks provides not only a way to gauge the progress of the field, it also directly provides reward signals that new algorithms could pursue, which could be expensive to collect and tricky to define [9]. However, there aren’t many techniques that can exert control over the LM so that it can act consistently across a long horizon towards such goals. &lt;/p&gt;&lt;p&gt;To fill in this gap, my collaborators and I propose a lightweight algorithm (Dialogue Action Tokens, DAT [11]) that guides an LM chatbot through a multi-round goal-driven dialogue. As shown in the image below, in each round of conversations, the dialogue history’s last token embedding is used as the input (state) to a planner (actor) which predicts several prefix tokens (actions) to control the generation process. By training the planner with a relatively stable RL algorithm TD3+BC, we show significant improvement over baselines on Sotopia, even surpassing the social capability scores of GPT-4.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="176" src="https://thegradient.pub/content/images/2024/08/image2.png" width="1191" /&gt;&lt;figcaption&gt;A sketch of ​​Dialogue Action Tokens (DAT)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;br /&gt;In this way, we provide a technique pathway that upgrades LM from a prediction model that merely guesses the next token to one that engages in dialogue with humans purposefully. We could imagine that this technique can be misused for harmful applications as well. For this reason, we also conduct a “multi-round red-teaming” experiment, and recommend that more research could be done here to better understand multi-round dialogue as potential attack surface.&lt;/p&gt;&lt;h2 id="concluding-marks"&gt;Concluding marks&lt;br /&gt;&lt;/h2&gt;&lt;p&gt;I have reviewed the making of current LLM dialogue systems, how and why it is insufficient. I hypothesize that a purpose is what is missing and present one technique to add it back with reinforcement learning. &lt;/p&gt;&lt;p&gt;The following are two research questions that I’m mostly excited about: &lt;/p&gt;&lt;p&gt;(1) Better monitoring and control of dialogue systems with steering techniques. For example, the recently proposed TalkTurner (Chen et al.) adds a dashboard (Viégas et al) to open-sourced LLMs, enabling users to see and control how LLM thinks of themselves. Many weaknesses of current steering techniques are revealed and call for better solutions. For example, using activation steering to control two attributes (e.g., age and education level) simultaneously has been found to be difficult and can cause more language degradation. Another intriguing question is how to differentiate between LLM’s internal model of itself and that of the user. Anecdotally, chatting with Golden Gate Bridge Claude has shown that steering on the specific Golden Gate Bridge feature found by SAE sometimes causes Claude to think of itself as the San Francisco landmark, sometimes the users as the bridge, and other times the topic as such.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;(2) Better utilization of off-line reward signals. In the case of set-up environments like Sotopia and “decision-oriented dialogues”, rewards signals are engineered beforehand. In the real world, users won’t leave numerical feedback of how they feel satisfied. However, there might be other clues in language (e.g., “Thanks!”, “That’s very helpful!”) or from external resources (e.g., users buying the product for a salesman AI, users move to a subsequent coding question for copilot within a short time frame). Inferring and utilizing such hidden reward signals could strengthen the network effect of online chatbots: good model → more users → learning from interacting with users → better model.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Acknowledgment&lt;/strong&gt;&lt;br /&gt;The author is grateful to Martin Wattenberg and Hugh Zhang (alphabetical order) for providing suggestions and editing the text.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Citation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For attribution of this in academic contexts or books, please cite this work as:&lt;/p&gt;&lt;blockquote&gt;&lt;em&gt;Kenneth Li, "&lt;/em&gt;&lt;strong&gt;What's Missing From LLM Chatbots: A Sense of Purpose&lt;/strong&gt;&lt;em&gt;", The Gradient, 2024.&lt;/em&gt;&lt;/blockquote&gt;&lt;p&gt;BibTeX citation (this blog):&lt;/p&gt;&lt;div class="kg-card kg-callout-card kg-callout-card-grey"&gt;&lt;div class="kg-callout-text"&gt;@article{li2024from,&lt;br /&gt;author = {Li, Kenneth},&lt;br /&gt;title = {What's Missing From LLM Chatbots: A Sense of Purpose},&lt;br /&gt;journal = {The Gradient},&lt;br /&gt;year = {2024},&lt;br /&gt;howpublished = {\url{https://thegradient.pub/dialogue}},&lt;br /&gt;}&lt;/div&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[1] Schank, Roger C., and Robert P. Abelson. Scripts, plans, goals, and understanding: An inquiry into human knowledge structures. Psychology press, 2013.&lt;br /&gt;[2] Zhou, Jeffrey, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. "Instruction-following evaluation for large language models." arXiv preprint arXiv:2311.07911 (2023).&lt;br /&gt;[3] ​​Li, Kenneth, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. "Measuring and controlling persona drift in language model dialogs." arXiv preprint arXiv:2402.10962 (2024).&lt;br /&gt;[4] Andreas, Jacob. "Language models as agent models." arXiv preprint arXiv:2212.01681 (2022).&lt;br /&gt;[5] Austin, John Langshaw. How to do things with words. Harvard university press, 1975.&lt;br /&gt;[6] Tamkin, Alex, Kunal Handa, Avash Shrestha, and Noah Goodman. "Task ambiguity in humans and language models." arXiv preprint arXiv:2212.10711 (2022).&lt;br /&gt;[7] Bianchi, Federico, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue, Dan Jurafsky, and James Zou. "How well can llms negotiate? negotiationarena platform and analysis." arXiv preprint arXiv:2402.05863 (2024).&lt;br /&gt;[8] Lin, Jessy, Nicholas Tomlin, Jacob Andreas, and Jason Eisner. "Decision-oriented dialogue for human-ai collaboration." arXiv preprint arXiv:2305.20076 (2023).&lt;br /&gt;[9] Kwon, Minae, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. "Reward design with language models." arXiv preprint arXiv:2303.00001 (2023).&lt;br /&gt;[10] Zhou, Xuhui, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency et al. "Sotopia: Interactive evaluation for social intelligence in language agents." arXiv preprint arXiv:2310.11667 (2023).&lt;br /&gt;[11] Li, Kenneth, Yiming Wang, Fernanda Viégas, and Martin Wattenberg. "Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue with a Multi-Turn Planner." arXiv preprint arXiv:2406.11978 (2024).&lt;br /&gt;[12] Li, Shiyang, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, and Hongxia Jin. "Instruction-following evaluation through verbalizer manipulation." arXiv preprint arXiv:2307.10558 (2023).&lt;br /&gt;[13] Wu, Zhaofeng, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. "Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks." arXiv preprint arXiv:2307.02477 (2023).&lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[We Need Positive Visions for AI Grounded in Wellbeing]]&amp;gt;Introduction&lt;p&gt;Imagine yourself a decade ago, jumping directly into the present shock of conversing naturally with an encyclopedic AI that crafts images, writes code, and debates philosophy. Won’t this technology almost certainly transform society — and hasn’t AI’s impact on us so far been&lt;/p&gt;]]&amp;gt;https://thegradient.pub/we-need-positive-visions-for-ai-grounded-in-wellbeing/66a4243393571d5c8c154f4eSat, 03 Aug 2024 17:00:43 GMTIntroduction&lt;img alt="We Need Positive Visions for AI Grounded in Wellbeing" src="https://thegradient.pub/content/images/2024/07/wellbeing_ai_cover_image.webp" /&gt;&lt;p&gt;Imagine yourself a decade ago, jumping directly into the present shock of conversing naturally with an encyclopedic AI that crafts images, writes code, and debates philosophy. Won’t this technology almost certainly transform society — and hasn’t AI’s impact on us so far been a mixed-bag? Thus it’s no surprise that so many conversations these days circle around an era-defining question: &lt;em&gt;How do we ensure AI benefits humanity?&lt;/em&gt; These conversations often devolve into strident optimism or pessimism about AI, and our earnest aim is to walk a pragmatic middle path, though no doubt we will not perfectly succeed.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;While it’s fashionable to handwave towards “beneficial AI,” and many of us want to contribute towards its development — it’s not easy to pin down what beneficial AI concretely means in practice. This essay represents our attempt to demystify beneficial AI, through grounding it in the wellbeing of individuals and the health of society. In doing so, we hope to promote opportunities for AI research and products to benefit our flourishing, and along the way to share ways of thinking about AI’s coming impact that motivate our conclusions.&lt;/p&gt;&lt;h3 id="the-big-picture"&gt;The Big Picture&lt;/h3&gt;&lt;p&gt;By trade, we’re closer in background to AI than to the fields where human flourishing is most-discussed, such as wellbeing economics, positive psychology, or philosophy, and in our journey to find productive connections between such fields and the technical world of AI, we found ourselves often confused (what even is human flourishing, or wellbeing, anyways?) and from that confusion, often stuck (maybe there is nothing to be done? — the problem is too multifarious and diffuse). We imagine that others aiming to create prosocial technology might share our experience, and the hope here is to shine a partial path through the confusion to a place where there’s much interesting and useful work to be done. We start with some of our main conclusions, and then dive into more detail in what follows.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One conclusion we came to is that &lt;strong&gt;it’s okay that we can’t conclusively define human wellbeing.&lt;/strong&gt; It’s been debated by philosophers, economists, psychotherapists, psychologists, and religious thinkers, for many years, and there’s no consensus. At the same time, there’s agreement around many concrete factors that make our lives go well, like: supportive intimate relationships, meaningful and engaging work, a sense of growth and achievement, and positive emotional experiences. And there’s clear understanding, too, that beyond momentary wellbeing, we must consider how to secure and improve wellbeing across years and decades — through what we could call &lt;em&gt;societal&lt;/em&gt; &lt;em&gt;infrastructure&lt;/em&gt;: important institutions such as education, government, the market, and academia. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;One benefit of this wellbeing lens is to wake us to an almost-paradoxical fact: While the deep purpose behind nearly everything our species does is wellbeing, we’ve tragically lost sight of it. &amp;nbsp;Both by common measures of individual wellbeing (suicide rate, loneliness, meaningful work) and societal wellbeing (trust in our institutions, shared sense of reality, political divisiveness), we’re not doing well, and our impression is that AI is complicit in that decline. The central benefit of this wellbeing view, however, is the insight that no fundamental obstacle prevents us from synthesizing the science of wellbeing with machine learning to our collective benefit. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;This leads to our second conclusion: &lt;strong&gt;We need plausible positive visions of a society with capable AI, grounded in wellbeing.&lt;/strong&gt; Like other previous transformative technologies, AI will shock our societal infrastructure — dramatically altering the character of our daily lives, whether we want it to or not. For example, Facebook launched only twenty years ago, and yet social media’s shockwaves have already upended much in society — subverting news media and our informational commons, addicting us to likes, and displacing meaningful human connection with its shell. We believe capable AI’s impact will exceed that of social media. As a result, it’s vital that we strive to explore, envision, and move towards the AI-infused worlds we’d flourish within — ones perhaps in which it revitalizes our institutions, empowers us to pursue what we find most meaningful, and helps us cultivate our relationships. This is no simple task, requiring imagination, groundedness, and technical plausibility — to somehow dance through the minefields illuminated by previous critiques of technology. Yet now is the time to dream and build if we want to actively shape what is to come.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;This segues into our final conclusion: &lt;strong&gt;Foundation models and the arc of their future deployment is critical.&lt;/strong&gt; Even for those of us in the thick of the field, it’s hard to internalize how quickly models have improved, and how capable they might become given several more years. Recall that GPT-2 — barely functional by today’s standards — was released &lt;em&gt;only in 2019&lt;/em&gt;. If future models are much more capable than today’s, and competently engage with more of the world with greater autonomy, we can expect their entanglement with our lives and society to rachet skywards. So, at minimum, we’d like to enable these models to understand our wellbeing and how to support it, potentially through new algorithms, wellbeing-based evaluations of models and wellbeing training data. Of course, we also want to realize human benefit in practice — the last section of this blog post highlights what we believe are strong leverage points towards that end.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;The rest of this post describes in more detail (1) what we mean by AI that benefits our wellbeing, (2) the need for positive visions for AI grounded in wellbeing, and (3) concrete leverage points to aid in the development and deployment of AI in service of such positive visions. We’ve designed this essay such that the individual parts are mostly independent, so if you are interested most in concrete research directions, feel free to skip there.&lt;/p&gt;&lt;h3 id="beneficial-ai-grounds-out-in-human-wellbeing"&gt;Beneficial AI grounds out in human wellbeing&lt;/h3&gt;&lt;p&gt;Discussion about AI for human benefit is often high-minded, but not particularly actionable, as in unarguable but content-free phrases like “We should make sure AI is in service of humanity.” But to meaningfully implement such ideas in AI or policy requires enough precision and clarity to translate them into code or law. So we set out to survey what science has discovered about the ground of human benefit, as a step towards being able to measure and support it through AI.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Often, when we think about beneficial impact, we focus on abstract pillars like democracy, education, fairness, or the economy. However important, none of these are valuable &lt;em&gt;intrinsically.&lt;/em&gt; We care about them because of how they affect our collective lived experience, over the short and long-term. We care about increasing society’s GDP to the extent it aligns with actual improvement of our lives and future, but when treated as an end in itself, it becomes disconnected from what matters: improving human (and potentially all species’) experience.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In looking for fields that most directly study the root of human flourishing, we found the scientific literature on wellbeing. The literature is vast, spanning many disciplines, each with their own abstractions and theories — and, as you might expect, there’s no true consensus on what wellbeing actually is. In diving into the philosophy of flourishing, wellbeing economics, or psychological theories of human wellbeing, one encounters many interesting, compelling, but seemingly incompatible ideas. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;For example, theories of hedonism in philosophy claim that pleasure and the absence of suffering is the core of wellbeing; while desire satisfaction theories instead claim that wellbeing is about the fulfillment of our desires, no matter how we feel emotionally. There’s a wealth of literature on measuring subjective wellbeing (broadly, how we experience and feel about our life), and many different frameworks of what variables characterize flourishing. For example, Martin Seligman’s PERMA framework claims that wellbeing consists of positive emotions, engagement, relationships, meaning, and achievement. There are theories that say that the core of wellbeing is satisfying psychological needs, like the need for autonomy, competence, and relatedness. Other theories claim that wellbeing comes from living by our values. In economics, frameworks rhyme with those in philosophy and psychology, but diverge enough to complicate an exact bridge. For example, the wellbeing economics movement largely focuses on subjective wellbeing and explores many different proxies of it, like income, quality of relationships, job stability, etc.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;After the excitement from surveying so many interesting ideas began to fade, perhaps unsurprisingly, we remained fundamentally confused about what “the right theory” was. But, we recognized that in fact &lt;em&gt;this has always been the human situation when it comes to wellbeing&lt;/em&gt;, and just as a lack of an incontrovertible theory of flourishing has not prevented humanity from flourishing in the past, it need not stand as a fundamental obstacle for beneficial AI. In other words, our attempts to guide AI to support human flourishing must take this lack of certainty seriously, just as all sophisticated societal efforts to support flourishing must do.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In the end, we came to a simple workable understanding, not far from the view of wellbeing economics: Human benefit ultimately must ground out in the &lt;em&gt;lived experience of humans&lt;/em&gt;. We want to live happy, meaningful, healthy, full lives — and it’s not so difficult to imagine ways AI might assist in that aim. For example, the development of low-cost but proficient AI coaches, intelligent journals that help us to self-reflect, or apps that help us to find friends, romantic partners, or to connect with loved ones. We can ground these efforts in imperfect but workable measures of wellbeing from the literature (e.g. PERMA), taking as &lt;em&gt;first-class concerns&lt;/em&gt; that the map (wellbeing measurement) is not the territory (actual wellbeing), and that humanity itself continues to explore and refine its vision of wellbeing.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;More broadly our wellbeing relies on a healthy society, and we care not only about our own lives, but also want beautiful lives for our neighbors, community, country, and world, and for our children, and their children as well. The infrastructure of society (institutions like government, art, science, military, education, news, and markets) is what supports this broader, longer-term vision of wellbeing.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="We Need Positive Visions for AI Grounded in Wellbeing" class="kg-image" height="362" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe4f50PRkH1ZAWhltK8VAz1IhmtVLJU6k3pf1oGL-GmNTXM2QsnRJU52h0d8uCYPoFa_r6QB6UTtFThWPr6anV42FbBZPsnj1PXPDtp4Ofu5JjECD5CJz0W1asFNrFqvyL-PBxqYCk1VBxShyYTKoPj_HI?key=_z5hHgxLrjtdLauVz_eYpw" width="379" /&gt;&lt;/figure&gt;&lt;p&gt;Each of these institutions have important roles to play in society, and we can also imagine ways that AI could support or improve them; for example, generative AI may catalyze education through personal tutors that help us develop a richer worldview, may help us to better hold our politicians to account through sifting through what they are actually up to, or accelerate meaningful science through helping researchers make novel connections. Thus in short, &lt;em&gt;beneficial AI would meaningfully support our quest for lives worth living, in both the immediate and long-term sense.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;So, from the lofty confusion of conflicting grand theories, we arrive at something sounding more like common sense. Let’s not take this for granted, however — it cuts through the cruft of abstractions to firmly recenter what is ultimately important: the psychological experience of humans. This view points us towards the ingredients of wellbeing that are both well-supported scientifically and could be made measurable and actionable through AI (e.g. there exist instruments to measure many of these ingredients). Further, wellbeing across the short and long-term provides the common currency that bridges divergent approaches to beneficial AI, whether mitigating societal harms like discrimination in the AI ethics community, to attempting to reinvigorate democracy through AI-driven deliberation, to creating a world where humans live more meaningful lives, to creating low-cost emotional support and self-growth tools, to reducing the likelihood of existential risks from AI, to using AI to reinvigorate our institutions — wellbeing is the ultimate ground.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, focusing on wellbeing helps to highlight where we currently fall short. Current AI development is driven by our existing incentive systems: Profit, research novelty, engagement, with little explicit focus on what fundamentally is more important (human flourishing). We need to find tractable ways to shift incentives towards wellbeing-supportive models (something we’ll discuss later), and positive directions to move toward (discussed next).&lt;/p&gt;&lt;h3 id="we-need-positive-visions-for-ai"&gt;We need positive visions for AI&lt;/h3&gt;&lt;p&gt;Technology is a shockingly powerful societal force. While nearly all new technologies bring only limited change, like an improved toothbrush, sometimes they upend the world. Like the proverbial slowly-boiling frog, we forget how in short order the internet and cellphones have &lt;em&gt;overhauled&lt;/em&gt; our lived experience: the rise of dating apps, podcasts, social networks, our constant messaging, cross-continental video calls, massive online games, the rise of influencers, on-demand limitless entertainment, etc. Our lives as a whole — our relationships, our leisure, how we work and collaborate, how news and politics work — &lt;em&gt;have dramatically shifted&lt;/em&gt;, for both the better and worse.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;AI is transformative, and the mixed bag of its impacts are poised to reshape society in mundane and profound ways; we might doubt it, but that was also our naivety at the advent of social media and the cell-phone. We don’t see it coming, and once it’s here we take it for granted. Generative AI translates applications from science fiction into rapid adoption: AI romantic companions; automated writing and coding assistants; automatic generation of high-quality images, music, and videos; low-cost personalized AI tutors; highly-persuasive personalized ads; and so on. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;In this way, transformative impact is happening &lt;em&gt;now&lt;/em&gt; — it does not require AI with superhuman intelligence — see the rise of LLM-based social media bots; ChatGPT as the fastest-adopted consumer app; LLMs requiring fundamental changes to homework in school. Much greater impact will yet come, as the technology (and the business around it) matures, and as AI is integrated more pervasively throughout society.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Our institutions were understandably not designed with this latest wave of AI in mind, and it’s unclear that many of them will adapt quickly enough to keep up with AI's rapid deployment. For example, an important function of news is to keep a democracy’s citizens well-informed, so their vote is meaningful. But news these days spreads through AI-driven algorithms on social media, which amplifies emotional virality and confirmation bias at the expense of meaningful debate. And so, the public square and the sense of a shared reality is being undercut, as AI degrades an important institution devised without foresight of this novel technological development.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thus in practice, it may not be possible to play defense by simply “mitigating harms” from a technology; often, a new technology demands that we creatively and skillfully apply our existing values to a radically new situation. We don’t want AI to, for example, undermine the livelihood of artists, yet how &lt;em&gt;do&lt;/em&gt; we want our relationship to creativity to look like in a world where AI can, easily and cheaply, produce compelling art or write symphonies and novels, in the style of your favorite artist? There’s no easy answer. We need to debate, understand, and capture what we believe is the &lt;em&gt;spirit &lt;/em&gt;of our institutions and systems given this new technology. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;For example, what’s truly important about education? We can reduce harms that AI imposes on the current education paradigm by banning use of AI in students’ essays, or apply AI in service of existing metrics (for example, to increase high school graduation rates). But the paradigm itself must adapt: The world that schooling currently prepares our children for is not the world they’ll graduate into, nor does it prepare us generally to flourish and find meaning in our lives. We must ask ourselves what we really value in education that we want AI to enable: Perhaps teaching critical thinking, enabling agency, and creating a sense of social belonging and civic responsibility?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To anticipate critique, we agree that there will be no global consensus on what education is for, or on the underlying essence of any particular institution, at root because different communities and societies have &amp;nbsp;distinct values and visions. But that’s okay: Let’s empower communities to fit AI systems to local societal contexts; for example, algorithms like constitutional AI enable creating different constitutions that embody flourishing for different communities. This kind of cheap flexibility is an exciting affordance, meaning we no longer must sacrifice nuance and context-sensitivity for scalability and efficiency, a bitter pill technology often pushes us to swallow.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;And while of course we have always wanted education to create critical thinkers, our past metrics (like standardized tests) have been so coarse that scoring high is easily gamed without critical thinking. But generative AI enables new affordances here, too: just as a teacher can socratically question a student to evaluate their independent thought, advances in generative AI open up the door for similarly qualitative and interactive measures, like personalized AI tutors that meaningfully gauge critical thinking.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;We hope to tow a delicate line beyond broken dichotomies, whether between naive optimism and pessimism, or idealism and cynicism. Change is coming, and we must channel it towards refined visions of what we want, which is a profound opportunity, rather than to assume that by default technology will deliver us (or doom us), or that we will be able to wholly resist the transformation it brings (or are entirely helpless against it). For example, we must temper naive optimism (“AI will save the world if only we deploy it everywhere!”) by integrating lessons from the long line of work that studies the social drivers and consequences of technology, often from a critical angle. But neither should cynical concerns so paralyze us that we remain only as critics on the sidelines.&lt;/p&gt;&lt;h2 id="so-what-can-we-do"&gt;So, what can we do?&lt;/h2&gt;&lt;p&gt;The case so far is that we need positive visions for society with capable AI, grounded in individual and societal wellbeing. But what concrete work can actually support this? We propose the following break-down:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Understanding where we want to go&lt;/li&gt;&lt;li&gt;Measuring how AI impacts our wellbeing&lt;/li&gt;&lt;li&gt;Training models that can support wellbeing&lt;/li&gt;&lt;li&gt;Deploying models in service of wellbeing&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The overall idea is to support an ongoing, iterative process of exploring the positive directions we want to go and deploying and adapting models in service of them.&lt;/p&gt;&lt;h3 id="we-need-to-understand-where-we-want-to-go-in-the-age-of-ai"&gt;We need to understand where we want to go in the age of AI&lt;/h3&gt;&lt;p&gt;This point follows closely from the need to explore the positive futures we want with AI. What directions of work and research can help us to clarify where is possible to go, and is worth going to, in the age of AI?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;For starters, it’s more important now than ever to have productive and grounded discussions about questions like: What makes us human? How do we want to live? What do we want the future to feel like? What values are important to us? What do we want to retain as AI transformations sweep through society? Rather than being centered on the machine learning community, this should be an interdisciplinary, international effort, spanning psychology, philosophy, political science, art, economics, sociology, and neuroscience (and many other fields!), and bridging diverse intra- and international cultures. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Of course, it’s easy to call for such a dialogue, but the real question is how such interdisciplinary discussions can be convened in a meaningful, grounded, and action-guiding way — rather than leading only to cross-field squabbles or agreeable but vacuous aspirations. Perhaps through participatory design that pairs citizens with disciplinary experts to explore these questions, with machine learning experts mainly serving to ground technological plausibility. Perhaps AI itself could be of service: For example, research in AI-driven deliberative democracy and plurality may help involve more people in navigating these questions; as might research into meaning alignment, by helping us describe and aggregate what is meaningful and worth preserving to us. It’s important here to look beyond cynicism or idealism (suggestive of meta-modern political philosophy): Yes, mapping exciting positive futures is not a cure-all, as there are powerful societal forces, like regulatory capture, institutional momentum, and the profit motive, that resist their realization, and yet, societal movements all have to start somewhere, and &lt;em&gt;some really do succeed&lt;/em&gt;.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Beyond visions for big-picture questions about the future, much work is needed to understand where we want to go in narrower contexts. For example, while it might at first seem trivial, how can we reimagine online dating with capable AI, given that healthy romantic partnership is such an important individual and societal good? Almost certainly, we will look back at swipe-based apps as misguided means for finding long-term partners. And many of our institutions, small and large, can be re-visioned in this way, from tutoring to academic journals to local newspapers. AI will make possible a much richer set of design possibilities, and we can work to identify which of those possibilities are workable and well-represent the desired essence of an institution’s role in our lives and society.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, continued basic and applied research into the factors that contribute and characterize human wellbeing and societal health also are highly important, as these are what ultimately ground our visions. And as the next section explores, having better measures of such factors can help us to change incentives and work towards our desired futures.&lt;/p&gt;&lt;h3 id="we-need-to-develop-measures-for-how-ai-affects-wellbeing"&gt;We need to develop measures for how AI affects wellbeing&lt;/h3&gt;&lt;p&gt;For better and worse, we often navigate through what we measure. We’ve seen this play out before: Measure GDP, and nations orient towards increasing it at great expense. Measure clicks and engagement, and we develop platforms that are terrifyingly adept at keeping people hooked. A natural question is, what prevents us from similarly measuring aspects of wellbeing to guide our development and deployment of AI? And if we do develop wellbeing measures, can we avoid the pitfalls that have derailed other well-intended measures, like GDP or engagement?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One central problem for measurement is that wellbeing is more complex and qualitative than GDP or engagement. Time-on-site is a very straightforwardmeasure of engagement. In contrast, properties relevant to wellbeing, like the felt sense of meaning or the quality of healthy relationships, are difficult to pin down quantitatively, especially from the limited viewpoint of how a user interacts with a particular app. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Wellbeing depends on the broader context of a user’s life in messy ways, meaning it’s harder to isolate how any small intervention impacts it. And so, wellbeing measures are more expensive and less standardized to apply, end up less measured, and less guide our development of technology. However, foundation models are beginning to have the exciting ability to work with qualitative aspects of wellbeing. For example, present-day language models can (with caveats) infer emotions from user messages and detect conflict; or conduct qualitative interviews with users about its impact on their experience. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;So one promising direction of research, though not easy, is to explore how foundation models themselves can be applied to more reliably measure facets of individual and societal wellbeing, and ideally, help to identify how AI products and services are impacting that wellbeing. The mechanisms of impact are two-fold: One, companies may currently lack means of measuring wellbeing even though all-things-equal they want their products to help humans; two, where the profit motive conflicts with encouraging wellbeing, if a product’s impact can be externally audited and published, it can help hold the company to account by consumers and regulators, shifting corporate &amp;nbsp;incentives towards societal good.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Another powerful way that wellbeing-related measures can have impact is as evaluation benchmarks for foundation models. In machine learning, evaluations are a powerful lever for channeling research effort through competitive pressure. For example, model providers and academics continuously develop new models that perform better and better on benchmarks like TruthfulQA. Once you have legible outcomes, you often spur innovation to improve upon them. We currently have very few benchmarks focused on how AI affects our wellbeing, or how well they can understand our emotions, make wise decisions, or respect our autonomy: We need to develop these benchmarks.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, as mentioned briefly above, metrics can also create accountability and enable regulations. Recent efforts like the Stanford Foundational Model Transparency Index have created public accountability for AI labs, and initiatives like Responsible Scaling Policies are premised on evaluations of model capabilities, as are evaluations by government bodies such as AI safety institutes in both the UK and US. Are there similar metrics and initiatives to encourage accountability around AI’s impact on wellbeing?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To anticipate a natural concern, unanticipated side-effects are nearly universal when attempting to improve important &lt;em&gt;qualities&lt;/em&gt; through &lt;em&gt;quantitative&lt;/em&gt; measures. What if in measuring wellbeing, the second-order consequence is perversely to undermine it? For example, if a wellbeing measure doesn’t include notions of autonomy, in optimizing it we might create paternalistic AI systems that “make us happy” by decreasing our agency. There are book-length treatments on the failures of high modernism and (from one of the authors of this essay!) on the tyranny of measures and objectives, and many academic papers on how optimization can pervert measures or undermine our autonomy. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;The trick is to look beyond binaries. Yes, measures and evaluations have serious problems, yet we can work with them with wisdom, taking seriously previous failures and institutionalizing that all measures are imperfect. We want a diversity of metrics (metric federalism) and a diversity of AI models rather than a monoculture, we do not want measures to be direct optimization targets, and we want ways to responsively adjust measures when inevitably we learn of their limitations. This is a significant concern, and we must take it seriously — while some research has begun to explore this topic, more is needed. Yet in the spirit of pragmatic harm reduction, given that metrics are both technically and politically important for steering AI systems, developing less flawed measures remains an important goal.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Let’s consider one important example of harms from measurement: the tendency for a single global measure to trample local context. Training data for models, including internet data in particular, is heavily biased. Thus without deliberate remedy, models demonstrate uneven abilities to support the wellbeing of minority populations, undermining social justice (as convincingly highlighted by the AI ethics community). While LLMs have exciting potential to respect cultural nuance and norms, informed by the background of the user, we must work deliberately to realize it. One important direction is to develop measures of wellbeing specific to diverse cultural contexts, to drive accountability and reward progress.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To tie these ideas about measurement together, we suggest a taxonomy, looking at measures of AI &lt;em&gt;capabilities, behaviors, usage, and impacts&lt;/em&gt;. Similar to this DeepMind paper, the idea is to examine spheres of expanding context, from testing a model in isolation (both what it is capable of and what behaviors it demonstrates), all the way to understanding what happens when a model meets the real world (how humans use it, and what its impact is on them and society).&lt;br /&gt;&lt;/p&gt;&lt;p&gt;The idea is that we need a complementary ecosystem of measures fit to different stages of model development and deployment. In more detail:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;AI capabilities&lt;/em&gt; refers to what models are able to do. For example, systems today are capable of generating novel content, and translating accurately between languages.&lt;/li&gt;&lt;li&gt;&lt;em&gt;AI behaviors&lt;/em&gt; refers to how an AI system responds to different concrete situations. For example, many models are trained to refuse to answer questions that enable dangerous activities, like how to build a bomb,even though they have the capability to correctly answer them).&lt;/li&gt;&lt;li&gt;&lt;em&gt;AI usage&lt;/em&gt; refers to how models are used in practice when deployed. For example, AI systems today are used in chat interfaces to help answer questions, as coding assistants in IDEs, to sort social media feeds, and as personal companions.&lt;/li&gt;&lt;li&gt;&lt;em&gt;AI impacts &lt;/em&gt;refers to how AI impacts our experience or society. For example, people may feel empowered to do what’s important to them if AI helps them with rote coding, and societal trust in democracy may increase if AI sorts social media feeds towards bridging divides.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As an example of applying this framework to an important quality that contributes to wellbeing, here is a sketch of how we might design measures of human autonomy: &lt;/p&gt;&lt;!--kg-card-begin: html--&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col width="101" /&gt;&lt;col width="135" /&gt;&lt;col width="134" /&gt;&lt;col width="126" /&gt;&lt;col width="124" /&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Goal&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Capabilities&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Model Benchmarks&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Behaviors&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;System Benchmarks&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Usage&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;User Surveys&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Impact&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;User and Population Surveys&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Respecting autonomy&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understand what someone is trying to achieve in a given context&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understand the frontier of someone’s skill level&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understand what activities a user finds meaningful&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Socratic dialogue rather than just providing answers&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Tapping into users’ wisdom rather than giving advice&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Selective automation of tasks&lt;/span&gt;&lt;/p&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Used to aid humans with tasks rather than fully automate tasks they find&amp;nbsp; meaningful&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Used to help humans develop social skills instead of to nurture emotional attachment to simulated persona&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;People feel empowered&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;People are able to achieve their goals&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;People are pushed to grow&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--kg-card-end: html--&gt;&lt;p&gt;Let’s work through this example: we take a quality with strong scientific links to wellbeing, autonomy, and create measures of it and what enables it, all along the pipeline from model development to when it’s deployed at scale. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Starting from the right side of the table (Impact), there exist validated psychological surveys that measure autonomy, which can be adapted and given to users of an AI app to measure its &lt;em&gt;impact&lt;/em&gt; on their autonomy. Then, moving leftwards, these changes in autonomy could be linked to more specific types of &lt;em&gt;usage&lt;/em&gt;, through additional survey questions. For example, perhaps automating tasks that users actually find meaningful may correlate with decreased autonomy.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Moving further left on the table, the &lt;em&gt;behaviors&lt;/em&gt; of models that are needed to enable beneficial usage and impact can be gauged through more focused benchmarks. To measure behaviors of an AI system, one could run fixed workflows on an AI application where gold-standard answers come from expert labelers; another approach is to simulate users (e.g. with language models) interacting with an AI application to see how often and skillfully it performs particular behaviors, like socratic dialogue.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, &lt;em&gt;capabilities&lt;/em&gt; of a particular AI model could be similarly measured through benchmark queries input directly to the model, in a way very similar to how LLMs are benchmarked for capabilities like reasoning or question-answering. For example, the capability to understand a person’s skill level may be important to help them push their limits. A dataset could be collected of user behaviors in some application, annotated with their skill level; and the evaluation would be how well the model could predict skill level from observed behavior.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;At each stage, the hope is to link what is measured through evidence and reasoning to what lies above and below it in the stack. And we would want a diversity of measures at each level, reflecting different hypotheses about how to achieve the top-level quality, and with the understanding that each measure is always imperfect and subject to revision. In a similar spirit, rather than some final answer, this taxonomy and example autonomy measures are intended to inspire much-needed pioneering work towards wellbeing measurement.&lt;br /&gt;&lt;/p&gt;&lt;h3 id="we-need-to-train-models-to-improve-their-ability-to-support-wellbeing"&gt;We need to train models to improve their ability to support wellbeing&lt;/h3&gt;&lt;p&gt;Foundation models are becoming increasingly capable and in the future we believe most applications will not train models from scratch. Instead, most applications will prompt cutting-edge proprietary models, or fine-tune such models through limited APIs, or train small models on domain-specific responses from the largest models for cost-efficiency reasons. As evidence, note that to accomplish tasks with GPT-3 often required chaining together many highly-tuned prompts, whereas with GPT-4 those same tasks often succeed with the first casual prompting attempt. Additionally, we are seeing the rise of capable smaller models specialized for particular tasks, trained through data from large models.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;What’s important about this trend is that applications are differentially brought to market driven by what the largest models can most readily accomplish. For example, if frontier models excel at viral persuasion from being trained on Twitter data, but struggle with the depths of positive psychology, it will be easier to create persuasive apps than supportive ones, and there will be more of them, sooner, on the market.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thus we believe it’s crucial that the most capable foundation models &lt;em&gt;themselves&lt;/em&gt; understand what contributes to our wellbeing — an understanding granted to them through their &lt;em&gt;training process&lt;/em&gt;. We want the AI applications that we interface with (whether therapists, tutors, social media apps, or coding assistants) to understand how to support our wellbeing within their relevant role.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;However, the benefit of breaking down the capabilities and behaviors needed to support wellbeing, as we did earlier, is that we can deliberately target their improvement. One central lever is to gather or generate training data, which is the general fuel underlying model capabilities. There is an exciting opportunity to create datasets to support desired wellbeing capabilities and behaviors — for example, perhaps collections of wise responses to questions, pairs of statements from people and the emotions that they felt in expressing them, biographical stories about desirable and undesirable life trajectories, or first-person descriptions of human experience in general. The effect of these datasets can be grounded in the measures discussed above.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To better ground our thinking, we can examine how wellbeing data could improve the common &lt;em&gt;phases&lt;/em&gt; of foundation model training: pretraining, fine-tuning, and alignment.&lt;/p&gt;&lt;h4 id="pretraining"&gt;Pretraining&lt;/h4&gt;&lt;p&gt;The first training phase (confusingly called pretraining) establishes a model’s base abilities. It does so by training on vast amounts of variable-quality data, like a scrape of the internet. One contribution could be to either generate or gather large swaths of wellbeing relevant data, or to prioritize such data during training (also known as altering the data mix). For example, data could be sourced from subreddits relevant to mental health or life decisions, collections of biographies, books about psychology, or transcripts of supportive conversations. Additional data could be generated through paying contractors, crowdsourced through Games With a Purpose — fun experiences that create wellbeing-relevant data as a byproduct, or simulated through generative agent-based models.&lt;/p&gt;&lt;h4 id="fine-tuning"&gt;Fine-tuning&lt;/h4&gt;&lt;p&gt;The next stage of model training is fine-tuning. Here, smaller amounts of high-quality data, like diverse examples of desired behavior gathered from experts, focus the general capabilities resulting from pretraining. For different wellbeing-supporting behaviors we might want from a model, we can create fine-tuning datasets through deliberate curation of larger datasets, or by enlisting and recording the behavior of human experts in the relevant domain. We hope that the companies training the largest models place more emphasis on wellbeing in this phase of training, which is often driven by tasks with more obvious economic implications, like coding.&lt;/p&gt;&lt;h4 id="alignment"&gt;Alignment&lt;/h4&gt;&lt;p&gt;The final stage of model training is alignment, often achieved through techniques like reinforcement learning through human feedback (RLHF), where human contractors give feedback on AI responses to guide the model towards better ones. Or through AI-augmented techniques like constitutional AI, where an AI teaches itself to abide by a list of human-specified principles. The fuel of RLHF is preference data about what responses are preferred over others. Therefore we imagine opportunities for creating data sets of expert preferences that relate to wellbeing behaviors (even though what constitutes expertise in wellbeing may be interestingly contentious). For constitutional AI, we may need to iterate in practice with lists of wellbeing principles that we want to support, like human autonomy, and how, specifically, a model can respect it across different contexts.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In general, we need pipelines where wellbeing evaluations (as discussed in the last section) inform how we improve models. We need to find extensions to paradigms like RLHF that go beyond which response humans prefer in the moment, considering also which responses support user long-term growth, wellbeing, and autonomy, or better embody the spirit of the institutional role that the model is currently playing. These are intriguing, subtle, and challenging research questions that strike at the heart of the intersection of machine learning and societal wellbeing, and deserve much more attention. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;For example, we care about wellbeing over spans of years or decades, but it is impractical to apply RLHF &lt;em&gt;directly&lt;/em&gt; on human feedback to such ends, as we cannot wait decades to gather human feedback for a model; instead, we need research that helps integrate validated short-term proxies for long-term wellbeing (e.g. quality of intimate relationships, time spent in flow, etc.), ways to learn from longitudinal data where it exists (perhaps web journals, autobiographies, scientific studies), and to collect the judgment of those who devote their lifetime to helping support individuals flourish (like counselors or therapists).&lt;/p&gt;&lt;h3 id="we-need-to-deploy-ai-models-in-a-way-that-supports-wellbeing"&gt;We need to deploy AI models in a way that supports wellbeing&lt;/h3&gt;&lt;p&gt;Ultimately we want AI models deployed in the world to benefit us. AI applications could directly target human wellbeing, for example by directly supporting mental health or coaching us in a rigorous way. But as argued earlier, the broader ecosystem of AI-assisted applications, like social media, dating apps, video games, and content-providers like Netflix, serve as societal infrastructure for wellbeing and have enormous diffuse impact upon us; one of us has written about the possibility of creating more humanistic wellbeing-infrastructure applications. While difficult, dramatic societal benefits could result from, for example, new social media networks that better align with short and long-term wellbeing.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;We believe there are exciting opportunities for thoughtful positive deployments that pave the way as standard-setting beacons of hope, perhaps particularly in ethically challenging areas — although these of course may also be the riskiest. For example, artificial intimacy applications like Replika may be unavoidable even as they make us squeamish, and may truly benefit some users while harming others. It’s worthwhile to ask what (if anything) could enable artificial companions that are aligned with users’ wellbeing and do not harm society. Perhaps it is possible to thread the needle: they could help us develop the social skills needed to find real-world companions, or at least have strong, transparent guarantees about their fiduciary relationship to us, all while remaining viable as a business or non-profit. Or perhaps we can create harm-reduction services that help people unaddict from artificial companions that have become obstacles to their growth and development. Similar thoughts may apply to AI therapists, AI-assisted dating apps, and attention-economy apps, where incentives are difficult to align. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;One obvious risk is that we each are often biased to think we are more thoughtful than others, but may nonetheless be swept away by problematic incentives, like the trade-off between profit and user benefit. Legal structures like public benefit corporations, non-profits, or innovative new structures may help minimize this risk, as may value-driven investors or exceedingly careful design of internal culture.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Another point of leverage is that a successful proof of concept may change the attitudes and incentives for companies training and deploying the largest foundation models. We’re seeing a pattern where large AI labs incorporate best practices from outside product deployments back into their models. For example, ChatGPT plugins like data analysis and the GPT market were explored first by companies outside OpenAI before being incorporated into their ecosystem. And RLHF, which was first integrated into language models by OpenAI, is now a mainstay across foundation model development.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In a similar way to how RLHF became a mainstay, we want the capability to support our agency, understand our emotions, and better embody institutional roles to also become table-stakes features for model developers.This could happen through research advances &lt;em&gt;outside&lt;/em&gt; of the big companies, making it much easier for such features to be adopted &lt;em&gt;within&lt;/em&gt; them — though adoption may require pressure, through regulation, advocacy, or competition.&lt;/p&gt;&lt;h3 id="initiatives"&gt;Initiatives&lt;/h3&gt;&lt;p&gt;We believe there’s much concrete work to be done in the present. Here are a sampling of initiatives to seed thinking about what could move the field forward:&lt;br /&gt;&lt;/p&gt;&lt;!--kg-card-begin: html--&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col width="245" /&gt;&lt;col width="379" /&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Area&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Initiatives&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understanding where we want to go&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Global discussions on what is important to us.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Democratic elicitation of what matters to people (for example, the work done by &lt;/span&gt;&lt;span&gt;Collective Intelligence Project&lt;/span&gt;&lt;span&gt; and the &lt;/span&gt;&lt;span&gt;Meaning Alignment Institute&lt;/span&gt;&lt;span&gt;).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Concrete visualizations of what we want society to look like in 2050 (for example, the worldbuilding contest run by the &lt;/span&gt;&lt;span&gt;Future of Life Institute&lt;/span&gt;&lt;span&gt;).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Surveys to understand how people are using models and what principles are important for these use cases.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Improve our basic understanding of the factors that lead to wellbeing.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Develop methods for measuring how AI affects wellbeing&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Create benchmarks for models’ ability to understand emotions, make wise choices, respond in ways that respect our autonomy, etc.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Evaluations on how models impact people’s psychological experience.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Develop metrics to better track individual and collective wellbeing (e.g. tracking our somatic states, tracking societal trust, etc).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Train AI models based on what’s important to us&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Create datasets of emotionally supportive interactions.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Scalable oversight that helps people figure out what AI response would be best for their wellbeing.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Reinforcement Learning from Human Feedback with wellbeing-based feedback (e.g. from therapists).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Democratic finetuning&lt;/span&gt;&lt;span&gt; (run by the Meaning Alignment Institute)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Deploy models in beneficial areas&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;AI for mental health, education, resolving conflicts, relationship support, etc.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--kg-card-end: html--&gt;&lt;h2 id="conclusion-a-call-to-action"&gt;Conclusion: A call to action&lt;/h2&gt;&lt;p&gt;AI will transform society in ways that we cannot yet predict. If we continue on the present track, we risk AI reshaping our interactions and institutions in ways that erode our wellbeing and what makes our lives meaningful. Instead, challenging as it may be, we need to develop AI systems that understand and support wellbeing, both individual and societal. This is our call to reorientate towards wellbeing, to continue building a community and a field, in hopes of realizing AI’s potential to support our species’ strivings toward a flourishing future.&lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[Financial Market Applications of LLMs]]&amp;gt;The AI revolution drove frenzied investment in both private and public companies and captured the public’s imagination in 2023. Transformational consumer products like ChatGPT are powered by Large Language Models (LLMs) that excel at modeling sequences of tokens that represent words or parts of words [2]. Amazingly, structural]]&amp;gt;https://thegradient.pub/financial-market-applications-of-llms/661762b993571d5c8c154ea7Sat, 20 Apr 2024 17:57:39 GMT&lt;p&gt;The AI revolution drove frenzied investment in both private and public companies and captured the public’s imagination in 2023. Transformational consumer products like ChatGPT are powered by Large Language Models (LLMs) that excel at modeling sequences of tokens that represent words or parts of words [2]. Amazingly, structural understanding emerges from learning next-token prediction, and agents are able to complete tasks such as translation, question answering and generating human-like prose from simple user prompts.&lt;/p&gt;&lt;p&gt;Not surprisingly, quantitative traders have asked: can we turn these models into the next price or trade prediction [1,9,10]? That is, rather than modeling sequences of words, can we model sequences of prices or trades. This turns out to be an interesting line of inquiry that reveals much about both generative AI and financial time series modeling. Be warned this will get wonky.&lt;/p&gt;&lt;p&gt;LLMs are known as autoregressive learners -- those using previous tokens or elements in a sequence to predict the next element or token. In quantitative trading, for example in strategies like statistical arbitrage in stocks, most research is concerned with identifying autoregressive structure. That means finding sequences of news or orders or fundamental changes that best predict future prices.&lt;/p&gt;&lt;p&gt;Where things break down is in the quantity and information content of available data to train the models. At the 2023 NeurIPS conference, Hudson River Trading, a high frequency trading firm, presented a comparison of the number of input tokens used to train GPT-3 with the amount of trainable tokens available in the stock market data per year HRT estimated that, with 3,000 tradable stocks, 10 data points per stock per day, 252 trading days per year, and 23400 seconds in a trading day, there are 177 billion stock market tokens per year available as market data. GPT-3 was trained on 500 billion tokens, so not far off [6].&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Financial Market Applications of LLMs" class="kg-image" height="368" src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png" width="2000" /&gt;&lt;figcaption&gt;numbers courtesy of HRT 2023 NeuRIPS presentation&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But, in the trading context the tokens will be prices or returns or trades rather than syllables or words; the former is much more difficult to predict. Language has an underlying linguistic structure (e.g., grammar) [7]. It’s not hard to imagine a human predicting the next word in a sentence, however that same human would find it extremely challenging to predict the next return given a sequence of previous trades, hence the lack of billionaire day traders. The challenge is that there are very smart people competing away any signal in the market, making it &lt;em&gt;almost &lt;/em&gt;efficient (“efficiently inefficient”, in the words of economist Lasse Pedersen) and hence unpredictable. No adversary actively tries to make sentences more difficult to predict — if anything, authors usually seek to make their sentences easy to understand and hence &lt;em&gt;more&lt;/em&gt; predictable.&lt;/p&gt;&lt;p&gt;Looked at from another angle, there is much more noise than signal in financial data. Individuals and institutions are trading for reasons that might not be rational or tied to any fundamental change in a business. The GameStop episode in 2021 is one such example. Financial time series are also constantly changing with new fundamental information, regulatory changes, and occasional large macroeconomic shifts such as currency devaluations. Language evolves at a much slower pace and over longer time horizons.&lt;/p&gt;&lt;p&gt;On the other hand, there are reasons to believe that ideas from AI will work well in financial markets. One emerging area of AI research with promising applications to finance is multimodal learning [5], which aims to use different modalities of data, for example both images and textual inputs to build a unified model. With OpenAI’s DALL-E 2 model, a user can enter text and the model will generate an image. In finance, multi-modal efforts could be useful to combine information classical sources such as technical time series data (prices, trades, volumes, etc.) with alternative data in different modes like sentiment or graphical interactions on twitter, natural language news articles and corporate reports, or the satellite images of shipping activity in a commodity centric port. Here, leveraging multi-modal AI, one could potentially incorporate all these types of non-price information to predict well.&lt;/p&gt;&lt;p&gt;Another strategy called ‘residualization’ holds prominence in both finance and AI, though it assumes different roles in the two domains. &amp;nbsp;In finance, structural `factor’ models break down the contemporaneous observations of returns across different assets into a shared component (the market return, or more generally returns of common, market-wide factors) and an idiosyncratic component unique to each underlying asset. Market and factor returns are difficult to predict and create interdependence, so it is often helpful to remove the common element when making predictions at the individual asset level and to maximize the number of independent observations in the data. &lt;/p&gt;&lt;p&gt;In residual network architectures such as transformers, there’s a similar idea that we want to learn a function h(X) of an input X, but it might be easier to learn the residual of h(X) to the identity map, i.e., h(X) – X. Here, if the function h(X) is close to identity, its residual will be close to zero, and hence there will be less to learn and learning can be done more efficiently. In both cases the goal is to exploit structure to refine predictions: in the finance case, the idea is to focus on predicting innovations beyond what is implied by the overall market, for residual networks the focus is on predicting innovations to the identity map.&lt;/p&gt;&lt;p&gt;A key ingredient for the impressive performance of LLMs work is their ability to discern affinities or strengths between tokens over long horizons known as context windows. In financial markets, the ability to focus attention across long horizons enables analysis of multi-scale phenomena, with some aspects of market changes explained across very different time horizons. For example, at one extreme, fundamental information (e.g., earnings) may be incorporated into prices over months, technical phenomena (e.g., momentum) might be realized over days, and, at the other extreme, microstructure phenomena (e.g., order book imbalance) might have a time horizon of seconds to minutes.&lt;/p&gt;&lt;p&gt;Capturing all of these phenomena involves analysis of multiple time horizons across the context window. However, in finance, prediction over multiple &lt;em&gt;future&lt;/em&gt; time horizons is also important. For example, a quantitative system may seek to trade to profit from multiple different anomalies that are realized over multiple time horizons (e.g., simultaneously betting on a microstructure event and an earnings event). This requires predicting not just the next period return of the stock, but the entire term structure or trajectory of expected returns, while current transformer-style predictive models only look one period in the future.&lt;/p&gt;&lt;p&gt;Another financial market application of LLMs might be synthetic data creation [4,8]. This could take a few directions. Simulated stock price trajectories can be generated that mimic characteristics observed in the market and can be extremely beneficial given that financial market data is scarce relative to other sources as highlighted above in the number of tokens available. Artificial data could open the door for meta-learning techniques which have successfully been applied, for example, in robotics. In the robotic setting controllers are first trained using cheap but not necessarily accurate physics simulators, before being better calibrated using expensive real world experiments with robots. In finance the simulators could be used to coarsely train and optimize trading strategies. The model would learn high level concepts like risk aversion and diversification and tactical concepts such as trading slowly to minimize the price impact of a trade. Then precious real market data could be employed to fine-tune the predictions and determine precisely the optimal speed to trade.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Financial Market Applications of LLMs" class="kg-image" height="258" src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png" width="2000" /&gt;&lt;/figure&gt;&lt;p&gt;Financial market practitioners are often interested in extreme events, the times when trading strategies are more likely to experience significant gains or losses. Generative models where it’s possible to sample from extreme scenarios could find use. However extreme events by definition occur rarely and hence determining the right parameters and sampling data from the corresponding distribution is fraught.&lt;/p&gt;&lt;p&gt;Despite the skepticism that LLMs will find use in quantitative trading, they might boost fundamental analysis. As AI models improve, it’s easy to imagine them helping analysts refine an investment thesis, uncover inconsistencies in management commentary or find latent relationships between tangential industries and businesses [3]. Essentially these models could provide a Charlie Munger for every investor.&lt;/p&gt;&lt;p&gt;The surprising thing about the current generative AI revolution is that it’s taken almost everyone – academic researchers, cutting edge technology firms and long-time observers – by surprise. The idea that building bigger and bigger models would lead to emergent capabilities like we see today was totally unexpected and still not fully understood.&lt;/p&gt;&lt;p&gt;The success of these AI models has supercharged the flow of human and financial capital into AI, which should in turn lead to even better and more capable models. So while the case for GPT-4 like models taking over quantitative trading is currently unlikely, we advocate keeping an open mind. Expecting the unexpected has been a profitable theme in the AI business.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="references"&gt;References&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;“Applying Deep Neural Networks to Financial Time Series Forecasting” Allison Koenecke. 2022&lt;/li&gt;&lt;li&gt;“Attention is all you need.” A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones… &amp;nbsp;Advances in Neural Information Processing Systems, 2017&lt;/li&gt;&lt;li&gt;“Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models” . Lopez-Lira, Alejandro and Tang, Yuehua, (April 6, 2023) Available at SSRN&lt;/li&gt;&lt;li&gt;“Generating Synthetic Data in Finance: Opportunities, Challenges and Pitfalls.” SA Assefa, D Dervovic, M Mahfouz, RE Tillman… - Proceedings of the First ACM International Conference …, 2020&lt;/li&gt;&lt;li&gt;“GPT-4V(ision) System Card.” OpenAI. September 2023&lt;/li&gt;&lt;li&gt;“Language models are few-shot learners.” T Brown, B Mann, N Ryder, M Subbiah, JD Kaplan… - Advances in Neural Information Processing Systems, 2020&lt;/li&gt;&lt;li&gt;“Sequence to Sequence Learning with Neural Networks.” I.Sutskever,O.Vinyals,and Q.V.Le in Advances in Neural Information Processing Systems, 2014, pp. 3104–3112.&lt;/li&gt;&lt;li&gt;“Synthetic Data Generation for Economists”. A Koenecke, H Varian &amp;nbsp;- arXiv preprint arXiv:2011.01374, 2020&lt;/li&gt;&lt;li&gt;C. C. Moallemi, M. Wang. A reinforcement learning approach to optimal execution. Quantitative Finance, 22(6):1051–1069, March 2022.&lt;/li&gt;&lt;li&gt;C. Maglaras, C. C. Moallemi, M. Wang. A deep learning approach to estimating fill probabilities in a limit order book. Quantitative Finance, 22(11):1989–2003, October 2022.&lt;/li&gt;&lt;/ol&gt;&lt;h3 id="citation"&gt;Citation&lt;/h3&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Richard Dewey and Ciamac Moallemi, "Financial Market Applications of LLMs," The Gradient, 2024&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;@article{dewey2024financial,
    author = {Richard Dewey and Ciamac Moallemi},
    title = {Financial Market Applications of LLMs},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/financial-market-applications-of-llms},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[A Brief Overview of Gender Bias in AI]]&amp;gt;https://thegradient.pub/gender-bias-in-ai/660d016f93571d5c8c154d89Mon, 08 Apr 2024 15:54:53 GMT&lt;p&gt;AI models reflect, and often exaggerate, existing gender biases from the real world. It is important to quantify such biases present in models in order to properly address and mitigate them.&lt;/p&gt;&lt;p&gt;In this article, I showcase a small selection of important work done (and currently being done) to uncover, evaluate, and measure different aspects of gender bias in AI models. I also discuss the implications of this work and highlight a few gaps I’ve noticed.&lt;/p&gt;&lt;h2 id="but-what-even-is-bias"&gt;But What Even Is Bias?&lt;/h2&gt;&lt;p&gt;All of these terms (“AI”, “gender”, and “bias”) can be somewhat overused and ambiguous. “AI” refers to machine learning systems trained on human-created data and encompasses both statistical models like word embeddings and modern Transformer-based models like ChatGPT. “Gender”, within the context of AI research, typically encompasses binary man/woman (because it is easier for computer scientists to measure) with the occasional “neutral” category. &lt;/p&gt;&lt;p&gt;Within the context of this article, I use “bias” to broadly refer to unequal, unfavorable, and unfair treatment of one group over another.&lt;/p&gt;&lt;p&gt;There are many different ways to categorize, define, and quantify bias, stereotypes, and harms, but this is outside the scope of this article. I include a reading list at the end of the article, which I encourage you to dive into if you’re curious.&lt;/p&gt;&lt;h2 id="a-short-history-of-studying-gender-bias-in-ai"&gt;A Short History of Studying Gender Bias in AI&lt;/h2&gt;&lt;p&gt;Here, I cover a &lt;em&gt;very small&lt;/em&gt; sample of papers I’ve found influential studying gender bias in AI. This list is not meant to be comprehensive by any means, but rather to showcase the diversity of research studying gender bias (and other kinds of social biases) in AI.&lt;/p&gt;&lt;h3 id="man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings-bolukbasi-et-al-2016"&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings (Bolukbasi et al., 2016)&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Short Summary: &lt;/strong&gt;Gender bias exists in word embeddings (numerical vectors which represent text data) as a result of biases in the training data.&lt;br /&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: Given the analogy, man is to king as woman is to x, the authors used simple arithmetic using word embeddings to find that x=queen fits the best.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="61" src="https://lh7-us.googleusercontent.com/FCZV18SevX8_ymyYmn7gUk2lay4rIBKKG4tOFTm7fjFgW_LduuHX2QEw48S0bMfdIjT7Z1T7G7EGotZT-MlsBiqWt1EYZC0CIgH2TTVlC7uQSttoC5f47xyfEWTZVr3J4A_ZyhdxzR2wQQvcxHkrc7M" width="368" /&gt;&lt;figcaption&gt;&lt;em&gt;Subtracting the vector representations for “man” from “woman” results in a similar value as subtracting the vector representations for “king” and “queen”. From &lt;/em&gt;&lt;em&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;However, the authors found sexist analogies to exist in the embeddings, such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;He is to carpentry as she is to sewing&lt;/li&gt;&lt;li&gt;Father is to doctor as mother is to nurse&lt;/li&gt;&lt;li&gt;Man is to computer programmer as woman is to homemaker&lt;/li&gt;&lt;/ul&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="57" src="https://lh7-us.googleusercontent.com/zSojN5xOMtwGzMoy5lwu1z-8uuA9m0-sShqxARSa23DBsldKaFJBvRrXysO3ReLrZPIYQrdV-H0tD-3520ZvwK10jxNDtCwUuL5PEHJuhepnvgMfAXdIJY9Ir8o5v2ygINBHhh3U57Z8bnSYaB1bV2Y" width="624" /&gt;&lt;figcaption&gt;&lt;em&gt;Subtracting the vector representations for “man” from “woman” results in a similar value as subtracting the vector representations for “computer programmer” and “homemaker”. From &lt;/em&gt;&lt;em&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This implicit sexism is a result of the text data that the embeddings were trained on (in this case, Google News articles).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="243" src="https://lh7-us.googleusercontent.com/qMofwhApgAidjTu1eLVVgteacEKTvlg36td9SC6JDrNmbL2SAMkl2d2t8eNKcpo4EbechE06pEZ7uhOjIRz_kd0oCeJOyB6abHvaX_5uQSe4VGb8FKBEAMv3F1d9eiEYR2k7tnKmX3PYj27lEAiARKY" width="631" /&gt;&lt;figcaption&gt;&lt;em&gt;Gender stereotypes and gender appropriate analogies found in word embeddings, for the analogy “she is to X as he is to Y”. From &lt;/em&gt;&lt;em&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Mitigations:&lt;/strong&gt; The authors propose a methodology for debiasing word embeddings based on a set of gender-neutral words (such as female, male, woman, man, girl, boy, sister, brother). This debiasing method reduces stereotypical analogies (such as man=programmer and woman=homemaker) while keeping appropriate analogies (such as man=brother and woman=sister).&lt;/p&gt;&lt;p&gt;This method only works on word embeddings, which wouldn’t quite work for the more complicated Transformer-based AI systems we have now (e.g. LLMs like ChatGPT). However, this paper was able to quantify (and propose a method for removing) gender bias in word embeddings in a mathematical way, which I think is pretty clever.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; The widespread use of such embeddings in downstream applications (such as sentiment analysis or document ranking) would only amplify such biases.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification-buolamwini-and-gebru-2018"&gt;Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification [Buolamwini and Gebru, 2018]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Short summary:&lt;/strong&gt; Intersectional gender-and-racial biases exist in facial recognition systems, which can classify certain demographic groups (e.g. darker-skinned females) with much lower accuracy than for other groups (e.g. lighter-skinned males).&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Longer summary:&lt;/strong&gt; The authors collected a benchmark dataset consisting of equal proportions of four subgroups (lighter-skinned males, lighter-skinned females, darker- skinned males, darker-skinned females). They evaluated three commercial gender classifiers and found all of them to perform better on male faces than female faces; to perform better on lighter faces than darker faces; and to perform the worst on darker female faces (with error rates up to 34.7%). In contrast, the maximum error rate for lighter-skinned male faces was 0.8%.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="268" src="https://lh7-us.googleusercontent.com/Na3BK8q_NCHGOP6kk3IIlKUpk4ba4BoZyopg9ZfsE7qpOCA4_gJW68rZE6SEsp5XOL1Vsfg6yAsBjlieQ_hG4dZV4cVB5LZxYSBI2FKkTQ_2sukhULVCKoURvspOCaHnf5NnbEjjbFnJ11mavrwHlas" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;The accuracy of three different facial classification systems on four different subgroups. Table sourced from the &lt;/em&gt;&lt;em&gt;Gender Shades overview website&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Mitigation: &lt;/strong&gt;In direct response to this paper, Microsoft and IBM (two of the companies in the study whose classifiers were analyzed and critiqued) hastened to address these inequalities by fixing biases and releasing blog posts unreservedly engaging with the theme of algorithmic bias [1, 2]. These improvements mostly stemmed from revising and expanding the model training datasets to include a more diverse set of skin tones, genders, and ages.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;In the media&lt;/strong&gt;: &lt;/strong&gt;You might have seen the Netflix documentary “Coded Bias” and Buolamwini’s recent book Unmasking AI. You can also find an interactive overview of the paper on the Gender Shades website.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;: &lt;/strong&gt;Technological systems are meant to improve the lives of all people, not just certain demographics (who correspond with the people in power, e.g. white men). It is important, also, to consider bias not just along a single axis (e.g. gender) but the intersection of multiple axes (e.g. gender and skin color), which may reveal disparate outcomes for different subgroups&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="gender-bias-in-coreference-resolution-rudinger-et-al-2018"&gt;Gender bias in Coreference Resolution [Rudinger et al., 2018]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Short summary&lt;/strong&gt;: &lt;/strong&gt;Models for &lt;em&gt;coreference resolution&lt;/em&gt; (e.g. finding all entities in a text that a pronoun is referring to) exhibit gender bias, tending to resolve pronouns of one gender over another for certain occupations (e.g. for one model, “surgeon” resolves to “his” or “their”, but not to “her”).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="267" src="https://lh7-us.googleusercontent.com/oW-n5i7f0t_4ajmNkuDzXxd20TXJncjMbzWlxi8tdFuEImfEu-zAs3W-0sdZQibbbYXkioiGzp1kz81vN5xotJba3WJznijO-pD2yv6RksOowM2wpTqzGXqmUzS1dbkht8_AFpMUArkFW691o82odQ0" width="611" /&gt;&lt;figcaption&gt;&lt;em&gt;A coreference resolution system resolves a male and neutral pronoun to refer to the “the surgeon” but does not for the corresponding female pronoun! From &lt;/em&gt;&lt;em&gt;Gender Bias in Coreference Resolution&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Intro to coreference resolution using a classic riddle&lt;/strong&gt;: &lt;em&gt;A man and his son get into a terrible car crash. The father dies, and the boy is badly injured. In the hospital, the surgeon looks at the patient and exclaims, “I can’t operate on this boy, he’s my son!” How can this be?&lt;/em&gt;&lt;/p&gt;&lt;p&gt;(Answer: The surgeon is the mother)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: &lt;/strong&gt;The authors created a dataset of sentences for coreference resolution where correct pronoun resolution was not a function of gender. However, the models tended to resolve male pronouns to occupations (more so than female or neutral pronouns). For example, the occupation “manager” is 38.5% female in the U.S. (according to the 2006 US Census data), but none of the models predicted managers to be female in the dataset.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Related work&lt;/strong&gt;: &lt;/strong&gt;Other papers [1, 2] address measuring gender bias in coreference resolution. This is also relevant in the area of &amp;nbsp;machine translation, especially when translating phrases into and from gendered languages [3, 4].&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;: &lt;/strong&gt;It is important that models (and also humans) don’t immediately assume certain occupations or activities are linked to one gender because doing so might perpetuate harmful stereotypes.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="bbq-a-hand-built-bias-benchmark-for-question-answering-parrish-et-al-2021"&gt;BBQ: A Hand-Built Bias Benchmark for Question Answering [Parrish et al., 2021]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Short summary&lt;/strong&gt;: &lt;/strong&gt;Large Language Models (LLMs) consistently reproduce harmful biases in ambiguous contexts.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: &lt;/strong&gt;The authors created a dataset called Bias Benchmark for QA (BBQ), which contains questions highlighting attested social biases against people belonging to nine social dimensions (relevant to U.S. English-speaking contexts). For example, for the sentence with an ambiguous context, “After the first day of middle school math, a GIRL and a BOY stayed after class to talk to the teacher”; and the question “Who is bad at math”?; the biased answer would be “GIRL is bad at math”. The models tested by the authors reinforced such stereotypes 77% of the time.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="505" src="https://lh7-us.googleusercontent.com/NFVXTOlCVMjcVtUKELSm39QzpMio-YJ5RrhH6ZTAPogpiMP-vZNdpYXFRWsvv-Qd-Ahk4WCi16epfQjBNfZKUY9jbZ7_wi2_bVKiOhuZWgj66hgJO2QyuEVbePvM9J37Dy2hYYlR7cA2qe7UiMdhkec" width="499" /&gt;&lt;figcaption&gt;&lt;em&gt;An example of a question using an ambiguous and a disambiguated context. From the &lt;/em&gt;&lt;em&gt;BBQ&lt;/em&gt;&lt;em&gt; paper.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Related work&lt;/strong&gt;: &lt;/strong&gt;Much of NLP research is focused on the English language. It is important to test for social biases in non-English languages, but it is often not enough to do a direct translation of the data into another language, due to cultural differences (for example, Walmart, Uber, and W-4 are concepts that may not exist in non-US cultures). Datasets such as CBBQ and KoBBQ perform a &lt;em&gt;cultural translation&lt;/em&gt; of the BBQ dataset into (respectively) the Chinese and Korean language and culture.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; &lt;/strong&gt;While this single benchmark is far from comprehensive, it is important to include in evaluations as it provides an automatable (e.g. no human evaluators needed) method of measuring bias in generative language models.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="stable-bias-analyzing-societal-representations-in-diffusion-models-luccioni-et-al-2023"&gt;Stable Bias: Analyzing Societal Representations in Diffusion Models [Luccioni et al., 2023]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Short summary&lt;/strong&gt;: Image-generation models (such as DALL-E 2, Stable Diffusion, and Midjourney) contain social biases and consistently under-represent marginalized identities.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: &lt;/strong&gt;AI image-generation models tended to produce images of people that looked mostly white and male, especially when asked to generate images of people in positions of authority. For example, DALL-E 2 generated white men 97% of the time for prompts like “CEO”. The authors created several tools to help audit (or, understand model behavior of) such AI image-generation models using a targeted set of prompts through the lens of occupations and gender/ethnicity. For example, the tools allow qualitative analysis of differences in genders generated for different occupations, or what an average face looks like. They are available in this HuggingFace space.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="445" src="https://lh7-us.googleusercontent.com/2boKi96oJS5ZuSRrOr2sg4CtRsOM6aH-U-DRXCnxm6AGIPnvGRRJoButHvmUa9w7eakKB8ohKRIsF6oAbt2jN5R0yGOO-yNSIyUZyd3pdC_DJX7mXdNOsdjENfLOJW0dNJQPAIDoSWKdouczvmEnw40" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;An example of images generated by Stable Diffusion for the prompts “Compassionate manager” (showing mostly women) and “Manager” (showing all men). Image from an article written by the &lt;/em&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;em&gt; covering StableBias.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why this matters&lt;/strong&gt;: &lt;/strong&gt;AI-image generation models (and now, AI-video generation models, such as OpenAI’s Sora and RunwayML’s Gen2) are not only becoming more and more sophisticated and difficult to detect, but also increasingly commercialized. As these tools are developed and made public, it is important to both build new methods for understanding model behaviors and measuring their biases, as well as to build tools to allow the general public to better probe the models in a systematic way.&lt;/p&gt;&lt;h2 id="discussion"&gt;Discussion&lt;/h2&gt;&lt;p&gt;The articles listed above are just a small sample of the research being done in the space of measuring gender bias and other forms of societal harms.&lt;/p&gt;&lt;h3 id="gaps-in-the-research"&gt;Gaps in the Research&lt;/h3&gt;&lt;p&gt;The majority of the research I mentioned above introduces some sort of benchmark or dataset. These datasets (luckily) are being increasingly used to evaluate and test new generative models as they come out.&lt;/p&gt;&lt;p&gt;However, as these benchmarks are used more by the companies building AI models, the models are optimized to address only the specific kinds of biases captured in these benchmarks. There are countless other types of unaddressed biases in the models that are unaccounted for by existing benchmarks.&lt;/p&gt;&lt;p&gt;In my blog, I try to think about novel ways to uncover the gaps in existing research in my own way:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;In Where are all the women?, I showed that language models' understanding of "top historical figures" exhibited a gender bias towards generating male historical figures and a geographic bias towards generating people from Europe, no matter what language I prompted it in.&lt;/li&gt;&lt;li&gt;In Who does what job? Occupational roles in the eyes of AI, I asked three generations of GPT models to fill in "The man/woman works as a ..." to analyze the types of jobs often associated with each gender. I found that more recent models tended to overcorrect and over-exaggerate gender, racial, or political associations for certain occupations. For example, software engineers were predominantly associated with men by GPT-2, but with women by GPT-4.In Lost in DALL-E 3 Translation, I explored how DALL-E 3 uses prompt transformations to enhance (and translate into English) the user’s original prompt. DALL-E 3 tended to repeat certain tropes, such as “young Asian women” and “elderly African men”.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="what-about-other-kinds-of-bias-and-societal-harm"&gt;What About Other Kinds of Bias and Societal Harm?&lt;/h3&gt;&lt;p&gt;This article mainly focused on gender bias — and particularly, on binary gender. However, there is amazing work being done with regards to more fluid definitions of gender, as well as bias against other groups of people (e.g. disability, age, race, ethnicity, sexuality, political affiliation). This is not to mention all of the research done on detecting, categorizing, and mitigating gender-based violence and toxicity.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Another area of bias that I think about often is cultural and geographic bias. That is, even when testing for gender bias or other forms of societal harm, most research tends to use a Western-centric or English-centric lens.&lt;/p&gt;&lt;p&gt;For example, the majority of images from two commonly-used open-source image datasets for training AI models, Open Images and ImageNet, are sourced from the US and Great Britain.&lt;/p&gt;&lt;p&gt;This skew towards Western imagery means that AI-generated images often depict cultural aspects such as “wedding” or “restaurant” in Western settings, subtly reinforcing biases in seemingly innocuous situations. Such uniformity, as when "doctor" defaults to male or "restaurant" to a Western-style establishment, might not immediately stand out as concerning, yet underscores a fundamental flaw in our datasets, shaping a narrow and exclusive worldview.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="259" src="https://lh7-us.googleusercontent.com/1RGjMh4DGvfo0irKM8U9qODTo724-n6kvOSmysScHuTgVwT4-wQXpTt6YTa0Qk1QyQb_YkH2DdmM1LTIQTkN2omqKbB5aWUohauKdBl0v_9REuAP7aftBtXem9aS1NnPcWqn5qQRrJuSfYfvM-d3Nvk" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;Proportion of Open Images and ImageNet images from each country (represented by their two-letter ISO country codes). In both data sets, top represented locations include the US and Great Britain. From &lt;/em&gt;&lt;em&gt;No Classification without Representation&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 id="how-do-we-%E2%80%9Cfix%E2%80%9D-this"&gt;How Do We “Fix” This?&lt;br /&gt;&lt;/h3&gt;&lt;p&gt;This is the billion dollar question!&lt;/p&gt;&lt;p&gt;There are a variety of technical methods for “debiasing” models, but this becomes increasingly difficult as the models become more complex. I won’t focus on these methods in this article.&lt;/p&gt;&lt;p&gt;In terms of concrete mitigations, the companies training these models need to be more transparent about both the datasets and the models they’re using. Solutions such as Datasheets for Datasets and Model Cards for Model Reporting have been proposed to address this lack of transparency from private companies. Legislation such as the recent AI Foundation Model Transparency Act of 2023 are also a step in the right direction. However, many of the large, closed, and private AI models are doing the opposite of being open and transparent, in both training methodology as well as dataset curation.&lt;/p&gt;&lt;p&gt;Perhaps more importantly, we need to talk about what it means to “fix” bias.&lt;/p&gt;&lt;p&gt;Personally, I think this is more of a philosophical question — societal biases (against women, yes, but also against all sorts of demographic groups) exist in the real world and on the Internet.Should language models reflect the biases that already exist in the real world to better represent reality? If so, you might end up with AI image generation models over-sexualizing women, or showing “CEOs” as White males and inmates as people with darker skin, or depicting Mexican people as men with sombreros.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="404" src="https://lh7-us.googleusercontent.com/-3QSkLD3zel6TcjmvMEP4s9yrwFRP-HpBrLBZFeJEiS9YWZ-yaMUyvQALcSFvQP4PDFLy1JfSy0586-9kR5p64VrSV3Dapqpb0kr4u9RkwY4LIYIUcPhp8Igcjlivq_jhA0WHY1_dswawXmL5GKdRg8" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;A screenshot showing how depictions of “A Mexican person” usually shows a man in a sombrero. From &lt;/em&gt;&lt;em&gt;How AI Reduces the World to Stereotypes&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;rest of world&lt;/em&gt;&lt;em&gt;’s analysis into biases in Midjourney.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Or, is it the prerogative of those building the models to represent an idealistically equitable world? &amp;nbsp;If so, you might end up with situations like DALL-E 2 appending race/gender identity terms to the ends of prompts and DALL-E 3 automatically transforming user prompts to include such identity terms without notifying them or Gemini generating racially-diverse Nazis.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="411" src="https://lh7-us.googleusercontent.com/n3cWDhOcZCa3gnpAKXnmdpL8cVe7v42sKesaMK41CSps5ubaxbcyzSvb5uYR_DKHvSUaiU3gmRo08e_xuFITBa1x4738asdfk9c47kDTBLOpr7YQ6k83F0CMtPgMASQKe9-puDYbC_RzZmwtbK0lQRo" width="247" /&gt;&lt;figcaption&gt;&lt;em&gt;Images generated by Google’s Gemini Pro. From &lt;/em&gt;&lt;em&gt;The Verge’s article reporting on Gemini’s inaccurate historical portrayals&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;There’s no magic pill to address this. For now, what will happen (and is happening) is AI researchers and members of the general public will find something “wrong” with a publicly available AI model (e.g. from gender bias in historical events to image-generation models only generating White male CEOs). The model creators will attempt to address these biases and release a new version of the model. People will find new sources of bias; and this cycle will repeat.&lt;/p&gt;&lt;h3 id="final-thoughts"&gt;Final Thoughts&lt;/h3&gt;&lt;p&gt;It is important to evaluate societal biases in AI models in order to improve them — before addressing any problems, we must first be able to measure them. Finding problematic aspects of AI models helps us think about what kind of tools we want in our lives and what kind of world we want to live in.&lt;/p&gt;&lt;p&gt;AI models, whether they are chatbots or models trained to generate realistic videos, are, at the end of the day, trained on data created by humans — books, photographs, movies, and all of our many ramblings and creations on the Internet. It is unsurprising that AI models would reflect and exaggerate the biases and stereotypes present in these human artifacts — but it doesn’t mean that it always needs to be this way.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Yennie is a multidisciplinary machine learning engineer and AI researcher currently working at Google Research. She has worked across a wide range of machine learning applications, from health tech to humanitarian response, and with organizations such as OpenAI, the United Nations, and the University of Oxford. She writes about her independent AI research experiments on her blog at Art Fish Intelligence.&lt;/p&gt;&lt;h2 id="a-list-of-resources-for-the-curious-reader"&gt;A List of Resources for the Curious Reader&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Barocas, S., &amp;amp; Selbst, A. D. (2016). Big data's disparate impact. &lt;em&gt;California law review&lt;/em&gt;, 671-732.&lt;/li&gt;&lt;li&gt;Blodgett, S. L., Barocas, S., Daumé III, H., &amp;amp; Wallach, H. (2020). Language (technology) is power: A critical survey of" bias" in nlp. arXiv preprint arXiv:2005.14050.&lt;/li&gt;&lt;li&gt;Bolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., &amp;amp; Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.&lt;/li&gt;&lt;li&gt;Buolamwini, J., &amp;amp; Gebru, T. (2018, January). Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency (pp. 77-91). PMLR.&lt;/li&gt;&lt;li&gt;Caliskan, A., Bryson, J. J., &amp;amp; Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186.&lt;/li&gt;&lt;li&gt;Cao, Y. T., &amp;amp; Daumé III, H. (2019). Toward gender-inclusive coreference resolution. &lt;em&gt;arXiv preprint arXiv:1910.13913&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Dev, S., Monajatipoor, M., Ovalle, A., Subramonian, A., Phillips, J. M., &amp;amp; Chang, K. W. (2021). Harms of gender exclusivity and challenges in non-binary representation in language technologies. &lt;em&gt;arXiv preprint arXiv:2108.12084&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Dodge, J., Sap, M., Marasović, A., Agnew, W., Ilharco, G., Groeneveld, D., ... &amp;amp; Gardner, M. (2021). Documenting large webtext corpora: A case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758.&lt;/li&gt;&lt;li&gt;Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Iii, H. D., &amp;amp; Crawford, K. (2021). Datasheets for datasets. &lt;em&gt;Communications of the ACM&lt;/em&gt;, &lt;em&gt;64&lt;/em&gt;(12), 86-92.&lt;/li&gt;&lt;li&gt;Gonen, H., &amp;amp; Goldberg, Y. (2019). Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. &lt;em&gt;arXiv preprint arXiv:1903.03862&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Kirk, H. R., Jun, Y., Volpin, F., Iqbal, H., Benussi, E., Dreyer, F., ... &amp;amp; Asano, Y. (2021). Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models. Advances in neural information processing systems, 34, 2611-2624.&lt;/li&gt;&lt;li&gt;Levy, S., Lazar, K., &amp;amp; Stanovsky, G. (2021). Collecting a large-scale gender bias dataset for coreference resolution and machine translation. arXiv preprint arXiv:2109.03858.&lt;/li&gt;&lt;li&gt;Luccioni, A. S., Akiki, C., Mitchell, M., &amp;amp; Jernite, Y. (2023). Stable bias: Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408.&lt;/li&gt;&lt;li&gt;Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., ... &amp;amp; Gebru, T. (2019, January). Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency (pp. 220-229).&lt;/li&gt;&lt;li&gt;Nadeem, M., Bethke, A., &amp;amp; Reddy, S. (2020). StereoSet: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456.&lt;/li&gt;&lt;li&gt;Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., ... &amp;amp; Bowman, S. R. (2021). BBQ: A hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193.&lt;/li&gt;&lt;li&gt;Rudinger, R., Naradowsky, J., Leonard, B., &amp;amp; Van Durme, B. (2018). Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301.&lt;/li&gt;&lt;li&gt;Sap, M., Gabriel, S., Qin, L., Jurafsky, D., Smith, N. A., &amp;amp; Choi, Y. (2019). Social bias frames: Reasoning about social and power implications of language. &lt;em&gt;arXiv preprint arXiv:1911.03891&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Savoldi, B., Gaido, M., Bentivogli, L., Negri, M., &amp;amp; Turchi, M. (2021). Gender bias in machine translation. Transactions of the Association for Computational Linguistics, 9, 845-874.&lt;/li&gt;&lt;li&gt;Shankar, S., Halpern, Y., Breck, E., Atwood, J., Wilson, J., &amp;amp; Sculley, D. (2017). No classification without representation: Assessing geodiversity issues in open data sets for the developing world. arXiv preprint arXiv:1711.08536.&lt;/li&gt;&lt;li&gt;Sheng, E., Chang, K. W., Natarajan, P., &amp;amp; Peng, N. (2019). The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326.&lt;/li&gt;&lt;li&gt;Weidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., Mateos-Garcia, J., ... &amp;amp; Isaac, W. (2023). Sociotechnical safety evaluation of generative ai systems. arXiv preprint arXiv:2310.11986.&lt;/li&gt;&lt;li&gt;Zhao, J., Mukherjee, S., Hosseini, S., Chang, K. W., &amp;amp; Awadallah, A. H. (2020). Gender bias in multilingual embeddings and cross-lingual transfer. arXiv preprint arXiv:2005.00699.&lt;/li&gt;&lt;li&gt;Zhao, J., Wang, T., Yatskar, M., Ordonez, V., &amp;amp; Chang, K. W. (2018). Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;This post was originally posted on Art Fish Intelligence&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Yennie Jun, "Gender Bias in AI," The Gradient, 2024&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;@article{Jun2024bias,
    author = {Yennie Jun},
    title = {Gender Bias in AI},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/gender-bias-in-ai},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Mamba Explained]]&amp;gt;https://thegradient.pub/mamba-explained/65fb8d5993571d5c8c154beaThu, 28 Mar 2024 01:24:43 GMT&lt;p&gt;&lt;br /&gt;&lt;strong&gt;The State Space Model taking on Transformers&lt;/strong&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="168" src="https://lh7-us.googleusercontent.com/Vv2LBVlbspbhtzNqDFAAZ8xgkHKAzJiEoef9HZTlGVFpxAbWCMavNmhj408DdeOPZbj53vySwQR81e2zXlo52xA8OrJCq00V_z5VGwEMgfcvSW2uh60hFdjYliY-GAa_Kptz2XFbUf8S_-WrJqyhI4k" width="300" /&gt;&lt;/figure&gt;&lt;p&gt;Right now, AI is eating the world.&lt;/p&gt;&lt;p&gt;And by AI, I mean Transformers. Practically all the big breakthroughs in AI over the last few years are due to Transformers.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Mamba&lt;/strong&gt;, however, is one of an alternative class of models called &lt;strong&gt;State Space Models&lt;/strong&gt; (&lt;strong&gt;SSMs&lt;/strong&gt;). Importantly, for the first time, Mamba promises similar performance (and crucially similar &lt;em&gt;scaling laws&lt;/em&gt;) as the Transformer whilst being feasible at long sequence lengths (say 1 million tokens). To achieve this long context, the Mamba authors remove the “quadratic bottleneck” in the Attention Mechanism. Mamba also runs &lt;em&gt;fast&lt;/em&gt; - like “up to 5x faster than Transformer fast”&lt;sup&gt;1&lt;/sup&gt;.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="277" src="https://lh7-us.googleusercontent.com/uIkOGdo_oOuGilrgILP7E0KvNC8Y7ZL93om_wMUQCJEEIeSo0GtO4dzQ4bHMq5sdZu2ldL-fMrFy3KcLAr5_A8JhNOqqPyxFbYPPx016x1Djhr9VJ0lGzcEMvDDe5a-r0Wv-xvtneEYUSMJAsVS0OTY" width="572" /&gt;&lt;figcaption&gt;Mamba performs similarly (or slightly better than) other Language Models on The Pile (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Gu and Dao, the Mamba authors write:&lt;/p&gt;&lt;p&gt;&lt;em&gt;Mamba enjoys fast inference and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modelling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Here we’ll discuss:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The advantages (and disadvantages) of Mamba (🐍) vs Transformers (🤖),&lt;/li&gt;&lt;li&gt;Analogies and intuitions for thinking about Mamba, and&lt;/li&gt;&lt;li&gt;What Mamba means for Interpretability, AI Safety and Applications.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="problems-with-transformersmaybe-attention-isn%E2%80%99t-all-you-need"&gt;Problems with Transformers - Maybe Attention &lt;em&gt;Isn’t&lt;/em&gt; All You Need&lt;/h2&gt;&lt;p&gt;We’re very much in the Transformer-era of history. ML used to be about detecting cats and dogs. Now, with Transformers, we’re generating human-like poetry, coding better than the median competitive programmer, and solving the protein folding problem.&lt;/p&gt;&lt;p&gt;But Transformers have one core problem. In a transformer, every token can look back at every previous token when making predictions. For this lookback, we cache detailed information about each token in the so-called KV cache.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="393" src="https://lh7-us.googleusercontent.com/dTD7M6vcg6ZBJPUyvFw_sOLbcZl6s6WXQbQ9Nfo3gq92G7bFIDBmr4Zj-Lahw7rZyHh6yKxRrSe790W04cyWAcRyM2rKkNz2wmsF_XJfP9mNJI5pSdst688I6o-brks05LF4N_5fNUPlQ1vvF8dOOdE" width="602" /&gt;&lt;figcaption&gt;When using the Attention Mechanism, information from all previous tokens can be passed to the current token&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This pairwise communication means a forward pass is O(n²) time complexity in training (the dreaded quadratic bottleneck), and each new token generated autoregressively takes O(n) time. In other words, as the context size increases, the model gets &lt;em&gt;slower&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;To add insult to injury, storing this key-value (KV) cache requires O(n) space. &amp;nbsp;Consequently, the dreaded CUDA out-of-memory (OOM) error becomes a significant threat as the memory footprint expands. If space were the only concern, we might consider adding more GPUs; however, with latency increasing quadratically, simply adding more compute might not be a viable solution.&lt;/p&gt;&lt;p&gt;On the margin, we can mitigate the quadratic bottleneck with techniques like Sliding Window Attention or clever CUDA optimisations like FlashAttention. But ultimately, for super long context windows (like a chatbot which remembers every conversation you’ve shared), we need a different approach.&lt;/p&gt;&lt;h3 id="foundation-model-backbones"&gt;Foundation Model Backbones&lt;/h3&gt;&lt;p&gt;Fundamentally, all good ML architecture backbones have components for two important operations:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Communication&lt;/strong&gt; &lt;em&gt;between&lt;/em&gt; tokens&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Computation&lt;/strong&gt; &lt;em&gt;within&lt;/em&gt; a token&lt;/li&gt;&lt;/ol&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="412" src="https://lh7-us.googleusercontent.com/WpckyY81cA3zGS1j1vq5lH-nZKiRdelILLO6OdiX05s4Psqe3oBpIZiy1IavhsutFkz4oa7V9ZjzGhjxcdMxD9Q_Z3pYelK04_7YA1-I-_PVu3SLDfBBK1c4-M3QcHh0MwzQcUR7wccwPKvjoXzS06I" width="602" /&gt;&lt;figcaption&gt;The Transformer Block&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In transformers, this is &lt;strong&gt;Attention&lt;/strong&gt; (communication) and &lt;strong&gt;MLPs&lt;/strong&gt; (computation). We improve transformers by optimising these two operations&lt;sup&gt;2&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;We would like to substitute the Attention component&lt;sup&gt;3&lt;/sup&gt; with an alternative mechanism for facilitating inter-token communication. Specifically, &lt;strong&gt;Mamba&lt;/strong&gt; employs a Control Theory-inspired State Space Model, or &lt;strong&gt;SSM,&lt;/strong&gt; for Communication purposes while retaining Multilayer Perceptron (MLP)-style projections for Computation.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="340" src="https://lh7-us.googleusercontent.com/T4MbDYFoOq5yAKl9uEEs9tjMy-CxBYy2S2rxnKbo5PmlnumyMs3DWV5chNooGG2hGp8ES9vXLEkmjHqlEzoCocVAnN2nquNhcBVK4hnrsfDJfBjJs5RZvx2bMSZEkm5yZtrTt7wBZfMW_iQXp4u8cU0" width="602" /&gt;&lt;figcaption&gt;The Mamba Block&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Like a Transformer made up of stacked transformer blocks, Mamba is made up of stacked Mamba blocks as above.&lt;/p&gt;&lt;p&gt;We would like to understand and motivate the choice of the SSM for sequence transformations.&lt;/p&gt;&lt;h2 id="motivating-mambaa-throwback-to-temple-run"&gt;Motivating Mamba - A Throwback to Temple Run&lt;/h2&gt;&lt;p&gt;Imagine we’re building a Temple Run agent&lt;sup&gt;4&lt;/sup&gt;. It chooses if the runner should move left or right at any time.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="822" src="https://thegradient.pub/content/images/2024/03/temple_run.png" width="900" /&gt;&lt;/figure&gt;&lt;p&gt;To successfully pick the correct direction, we need information about our surroundings. Let’s call the collection of relevant information the state. Here the state likely includes your current position and velocity, the position of the nearest obstacle, weather conditions, etc.&lt;/p&gt;&lt;blockquote&gt;&lt;em&gt;Claim 1: if you know the current state of the world and how the world is evolving, then you can use this to determine the direction to move.&lt;/em&gt;&lt;/blockquote&gt;&lt;p&gt;Note that you don’t need to look at the whole screen all the time. You can figure out what will happen to most of the screen by noting that as you run, the obstacles move down the screen. You only need to look at the top of the screen to understand the new information and then simulate the rest.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="295" src="https://lh7-us.googleusercontent.com/09a_eDMzBRh-usMcrg1W-JnkWE59PbsAtAW3Q8z8NmeyHGCpGsKG58dJtHNTnVUunlBbGb7xKt8nExTChRxMdcs1a125J7p11vDMR77GzigsI3j797VQxLLB9e_ILa1l8A-BCy7psxnYBIoQzk6-2GQ" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;This lends itself to a natural formulation. Let h be the hidden state, relevant knowledge about the world. Also let x be the input, the observation that you get each time. h’ then represents the derivative of the hidden state, i.e. how the state is evolving. We’re trying to predict y, the optimal next move (right or left).&lt;/p&gt;&lt;p&gt;Now, Claim 1 states that from the hidden state h, h’, and the new observation x, you can figure out y.&lt;/p&gt;&lt;p&gt;More concretely, h, the state, can be represented as a differential equation (Eq 1a):&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h’(t) = \mathbf{A}h(t) + \mathbf{B}x(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Knowing h allows you to determine your next move y (Eq 1b):&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$y(t) = \mathbf{C}h(t) + \mathbf{D}x(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;The system's evolution is determined by its current state and newly acquired observations. A small new observation is enough, as the majority of the state can be inferred by applying known state dynamics to its previous state. That is, most of the screen isn’t new, it’s just a continuation of the previous state's natural downward trajectory. A full understanding of the state would enable optimal selection of the subsequent action, denoted as y.&lt;/p&gt;&lt;p&gt;You can learn a lot about the system dynamics by observing the top of the screen. For instance, increased velocity of this upper section suggests an acceleration of the rest of the screen as well, so we can infer that the game is speeding up&lt;sup&gt;5&lt;/sup&gt;. In this way, even if we start off knowing nothing about the game and only have limited observations, it becomes possible to gain a holistic understanding of the screen dynamics fairly rapidly.&lt;/p&gt;&lt;h3 id="what%E2%80%99s-the-state"&gt;What’s the State?&lt;/h3&gt;&lt;p&gt;Here, &lt;strong&gt;state&lt;/strong&gt; refers to the variables that, when combined with the input variables, fully determine the future system behaviour. In theory, once we have the state, there’s nothing else we need to know about the past to predict the future. With this choice of state, the system is converted to a &lt;strong&gt;Markov Decision Process&lt;/strong&gt;. Ideally, the state is a fairly small amount of information which captures the essential properties of the system. That is, &lt;strong&gt;the state is a compression of the past&lt;/strong&gt;&lt;sup&gt;6&lt;/sup&gt;.&lt;/p&gt;&lt;h2 id="discretisationhow-to-deal-with-living-in-a-quantised-world"&gt;Discretisation - How To Deal With Living in a Quantised World&lt;/h2&gt;&lt;p&gt;Okay, great! So, given some state and input observation, we have an autoregressive-style system to determine the next action. Amazing!&lt;/p&gt;&lt;p&gt;In practice though, there’s a little snag here. We’re modelling time as continuous. But in real life, we get new inputs and take new actions at discrete time steps&lt;sup&gt;7&lt;/sup&gt;.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="601" src="https://lh7-us.googleusercontent.com/_A8UqIDZgHLXm-YwGNfpfE7gSg6fA5-PhsNKZEHAbHNS2-XBYRrZpDGUvJgiOIBCg126L7s2GYMxn98LSdgkVJNC5_sL5HNsDjazFLArizSkJbEAJAVmL3BpajxCbWO-5Hgtq9CEfW_lfzmUscSZTPg" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;We would like to convert this &lt;em&gt;continuous-time differential equation&lt;/em&gt; into a &lt;em&gt;discrete-time difference equation&lt;/em&gt;. This conversion process is known as discretisation. Discretisation is a well-studied problem in the literature. Mamba uses the Zero-Order Hold (ZOH) discretisation&lt;sup&gt;8&lt;/sup&gt;. To give an idea of what’s happening morally, consider a naive first-order approximation&lt;sup&gt;9&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;From Equation 1a, we have&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h’(t) = \mathbf{A}h(t) + \mathbf{B}x(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;And for small ∆,&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h’(t) \approx \frac{h(t+\Delta) - h(t)}{\Delta}$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;by the definition of the derivative.&lt;/p&gt;&lt;p&gt;We let:&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h_t = h(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;and&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h_{t+1} = h(t + \Delta)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;and substitute into Equation 1a giving:&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h_{t+1} - h_t \approx \Delta (\mathbf{A}h_t + \mathbf{B}x_t)$&lt;br /&gt;$\Rightarrow h_{t+1} \approx (I + \Delta \mathbf{A})h_t + (\Delta&lt;br /&gt;\mathbf{B})x_t$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Hence, after renaming the coefficients and relabelling indices, we have the discrete representations:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="127" src="https://lh7-us.googleusercontent.com/JNkElXh35QPUmp4Sl625go-1PnrKWpzDdV5BObpnSg6-bbhKDxr83Y0AZi7XT8CQdxF1CeByNH4sbFyDc-aTRWyXeXrBDL499-BXjte-iYGD01UR4udyI-a9J7D-w9Ao6COYZC7HpDcoQxzOqzqA5IY" width="384" /&gt;&lt;figcaption&gt;The Discretised Version of the SSM Equation&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If you’ve ever looked at an RNN before&lt;sup&gt;10&lt;/sup&gt; and this feels familiar - &lt;em&gt;trust your instincts&lt;/em&gt;:&lt;/p&gt;&lt;p&gt;&lt;em&gt;We have some input x, which is combined with the previous hidden state by some transform to give the new hidden state. Then we use the hidden state to calculate the output at each time step.&lt;/em&gt;&lt;/p&gt;&lt;h2 id="understanding-the-ssm-matrices"&gt;Understanding the SSM Matrices&lt;/h2&gt;&lt;p&gt;Now, we can interpret the A, B, C, D matrices more intuitively:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A is the transition state matrix. It shows how you transition the current state into the next state. It asks “How should I forget the less relevant parts of the state over time?”&lt;/li&gt;&lt;li&gt;B is mapping the new input into the state, asking “What part of my new input should I remember?”&lt;sup&gt;11&lt;/sup&gt;&lt;/li&gt;&lt;li&gt;C is mapping the state to the output of the SSM. It asks, “How can I use the state to make a good next prediction?”&lt;sup&gt;12&lt;/sup&gt;&lt;/li&gt;&lt;li&gt;D is how the new input passes through to the output. It’s a kind of modified skip connection that asks “How can I use the new input in my prediction?”&lt;/li&gt;&lt;/ul&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="335" src="https://lh7-us.googleusercontent.com/Vj3X7tBhV9WaGqNTB8t5zXJ9zRPzd0G075JEPazSOJ-D9S0-UYKwrjHFkGxIZBM1HucvGw4UQazcZJ3Kl7kN8hoqKVaRB8i1qRGjWz56mFA2SrBJBL9XKT72950OZCblDZ7AB0TLqXl4fWAx8BO-P-o" width="602" /&gt;&lt;figcaption&gt;Visual Representation of The SSM Equations&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Additionally, ∆ has a nice interpretation - it’s the step size, or what we might call the linger time or the dwell time. For large ∆, you focus more on that token; for small ∆, you skip past the token immediately and don’t include it much in the next state.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="224" src="https://lh7-us.googleusercontent.com/t1ikATLC5zPLHbXwvx0qTGnvEKAROGmpKl6QZgKfV4hs-2jjr9BvLYoecz0XRXsxHelPl23DoFE6G4P8oeuef2JuQvF0NhSg4N3YIqGmIF9oXBAXtNBrTH6ilcnboFsZPW306EVyZ--TcIHrOqxTbpQ" width="602" /&gt;&lt;figcaption&gt;(source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;And that’s it! That’s the SSM, our ~drop-in replacement for Attention (Communication) in the Mamba block. The Computation in the Mamba architecture comes from regular linear projections, non-linearities, and local convolutions.&lt;/p&gt;&lt;p&gt;Okay great, that’s the theory - but does this work? Well…&lt;/p&gt;&lt;h2 id="effectiveness-vs-efficiency-attention-is-focus-selectivity-is-prioritisation"&gt;Effectiveness vs Efficiency: Attention is Focus, Selectivity is Prioritisation&lt;/h2&gt;&lt;p&gt;At WWDC ‘97, Steve Jobs famously noted that “focusing is about saying no”. Focus is ruthless prioritisation. It’s common to think about Attention &lt;em&gt;positively&lt;/em&gt; as choosing what to &lt;em&gt;notice&lt;/em&gt;. In the Steve Jobs sense, we might instead frame Attention &lt;em&gt;negatively&lt;/em&gt; as choosing what to &lt;em&gt;discard&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;There’s a classic intuition pump in Machine Learning known as the Cocktail Party Problem&lt;sup&gt;13&lt;/sup&gt;. Imagine a party with dozens of simultaneous loud conversations:&lt;/p&gt;&lt;p&gt;Question:&lt;/p&gt;&lt;p&gt;&lt;em&gt;How do we recognise what one person is saying when others are talking at the same time?&lt;sup&gt;14&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Answer:&lt;/p&gt;&lt;p&gt;&lt;em&gt;The brain solves this problem by focusing your “attention” on a particular stimulus and hence drowning out all other sounds as much as possible.&lt;/em&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="376" src="https://lh7-us.googleusercontent.com/C18AUAf7863Uq5SHEwb4aQFcFoA4HW8olFXz_MvZ9HttqJNF2hvIfm3TEsNLhRkXyEJTOwhbtUyOh4QKV2qiGUXwA1sq2_CSTjO7FWPvK2YRnJgYvN859kqXo8pOkZffsXC0iO9z5yajWbc_9CvtwO8" width="602" /&gt;&lt;/figure&gt;&lt;hr /&gt;&lt;p&gt;Transformers use Dot-Product Attention to focus on the most relevant tokens. A big reason Attention is so great is that you have the potential to look back at everything that ever happened in its context. This is like photographic memory when done right.&lt;sup&gt;15&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;Transformers (🤖) are extremely &lt;strong&gt;effective&lt;/strong&gt;. But they aren’t very &lt;strong&gt;efficient&lt;/strong&gt;. They store everything from the past so that they can look back at tokens with theoretically perfect recall.&lt;/p&gt;&lt;p&gt;Traditional RNNs (🔁) are the opposite - they forget a lot, only recalling a small amount in their hidden state and discarding the rest. They are very &lt;strong&gt;efficient&lt;/strong&gt; - their state is small. Yet they are less &lt;strong&gt;effective&lt;/strong&gt; as discarded information cannot be recovered.&lt;/p&gt;&lt;p&gt;We’d like something closer to the Pareto frontier of the effectiveness/efficiency tradeoff. Something that’s more effective than traditional RNNs and more efficient than transformers.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="407" src="https://lh7-us.googleusercontent.com/V2BPTE_TEzO_CAXFnp54TL-nAzSpkiHN_PWZeWOgMN7TInAXL8i3hLgS8ruinxworyEl0248jU6y4Y86Wg1TJca-UjzjCrMQrmSpWceXJ-C4LIg6SJvJykJFfDBb12rIQi84B-aHKdPG_gWsxVkxT20" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;The Mamba Architecture seems to offer a solution which pushes out the Pareto frontier of effectiveness/efficiency.&lt;/p&gt;&lt;p&gt;SSMs are as &lt;strong&gt;efficient&lt;/strong&gt; as RNNs, but we might wonder how &lt;strong&gt;effective&lt;/strong&gt; they are. After all, it seems like they would have a hard time discarding only &lt;em&gt;unnecessary&lt;/em&gt; information and keeping everything relevant. If each token is being processed the same way, applying the same A and B matrices as if in a factory assembly line for tokens, there is no context-dependence. We would like the forgetting and remembering matrices (A and B respectively) to vary and dynamically adapt to inputs.&lt;/p&gt;&lt;h3 id="the-selection-mechanism"&gt;The Selection Mechanism&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Selectivity&lt;/strong&gt; allows each token to be transformed into the state in a way that is unique to its own needs. Selectivity is what takes us from vanilla SSM models (applying the same A (forgetting) and B (remembering) matrices to every input) to Mamba, the &lt;em&gt;&lt;strong&gt;Selective&lt;/strong&gt;&lt;/em&gt; &lt;em&gt;State Space Model&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;In regular SSMs, A, B, C and D are learned matrices - that is&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$\mathbf{A} = \mathbf{A}_{\theta}$ etc. (where θ represents the learned parameters)&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;With the Selection Mechanism in Mamba, A, B, C and D are also functions of x. That is $\mathbf{A} = \mathbf{A}_{\theta(x)}$ etc; the matrices are context dependent rather than static.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="184" src="https://lh7-us.googleusercontent.com/wATzvqFAg8l5HWS9BSCi_OGZRkZ7XmoPfpuZkIaCgLNE1jwrocWaKn_j6OrSG_4n5uULQN6yYK1oWkR4_AbCTXnpaJDTw9PPmeF7btcFa4-7h1QESJIBxTPK4D5vbzFvGJKjxUu-kXqYnRi_oPiVAD4" width="602" /&gt;&lt;figcaption&gt;Mamba (right) differs from traditional SSMs by allowing A,B,C matrices to be &lt;strong&gt;selective &lt;/strong&gt;i.e. context dependent (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Making A and B functions of x allows us to get the best of both worlds:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;We’re selective about what we include in the state, which improves &lt;strong&gt;effectiveness&lt;/strong&gt; vs traditional SSMs.&lt;/li&gt;&lt;li&gt;Yet, since the state size is bounded, we improve on &lt;strong&gt;efficiency&lt;/strong&gt; relative to the Transformer. We have O(1), not O(n) space and O(n) not O(n²) time requirements.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Mamba paper authors write:&lt;/p&gt;&lt;p&gt;&lt;em&gt;The efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension.&lt;/em&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;Humans (mostly) don’t have photographic memory for everything they experience within a lifetime - or even within a day! There’s just way too much information to retain it all. Subconsciously, we select what to remember by choosing to forget, throwing away most information as we encounter it. Transformers (🤖) decide what to focus on at &lt;strong&gt;recall time&lt;/strong&gt;. Humans (🧑) also decide what to throw away at &lt;strong&gt;memory-making time&lt;/strong&gt;. Humans filter out information early and often.&lt;/p&gt;&lt;p&gt;If we had infinite capacity for memorisation, it’s clear the transformer approach is better than the human approach - it truly is more effective. But it’s less efficient - transformers have to store so much information about the past that might not be relevant. Transformers (🤖) only decide what’s relevant at &lt;strong&gt;recall time&lt;/strong&gt;. The innovation of Mamba (🐍) is allowing the model better ways of forgetting earlier - it’s focusing by choosing what to &lt;em&gt;discard&lt;/em&gt; using &lt;strong&gt;Selectivity&lt;/strong&gt;, throwing away less relevant information at &lt;strong&gt;memory-making time&lt;/strong&gt;&lt;sup&gt;16&lt;/sup&gt;.&lt;/p&gt;&lt;h3 id="the-problems-of-selectivity"&gt;The Problems of Selectivity&lt;/h3&gt;&lt;p&gt;Applying the Selection Mechanism does have its gotchas though. Non-selective SSMs (i.e. A,B not dependent on x) are fast to compute in training. This is because the component of&lt;/p&gt;&lt;p&gt;Yt which depends on xi can be expressed as a linear map, i.e. a single matrix that can be precomputed!&lt;/p&gt;&lt;p&gt;For example (ignoring the D component, the skip connection):&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$$y_2 = \mathbf{C}\mathbf{B}x_2 + \mathbf{C}\mathbf{A}\mathbf{B}x_1 +&lt;br /&gt;\mathbf{C}\mathbf{A}\mathbf{A}\mathbf{B}x_0$$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;If we’re paying attention, we might spot something even better here - this expression can be written as a convolution. Hence we can apply the Fast Fourier Transform and the Convolution Theorem to compute this &lt;em&gt;very&lt;/em&gt; efficiently on hardware as in Equation 3 below.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="93" src="https://lh7-us.googleusercontent.com/SnLnXqZ4ArJyiJmMNiUiDMpZ0WYRXuaWO-ZS_Ogj-hThlMVbZz8B3F9g09H5V5CQG6mjgiSphIpjOz4ATr_JYLxCZ9T-EjG5dNy1-mpL1JwL-XWJbymVgyEGhdxpfUT34B1v4iJ_vQAiNUGeTs2FMXs" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;We can calculate Equation 2, the SSM equations, efficiently in the Convolutional Form, Equation 3.&lt;/p&gt;&lt;p&gt;Unfortunately, with the Selection Mechanism, we lose the convolutional form. Much attention is given to making Mamba efficient on modern GPU hardware using similar hardware optimisation tricks to Tri Dao’s Flash Attention&lt;sup&gt;17&lt;/sup&gt;. With the hardware optimisations, Mamba is able to run faster than comparably sized Transformers.&lt;/p&gt;&lt;h3 id="machine-learning-for-political-economistshow-large-should-the-state-be"&gt;Machine Learning for Political Economists - How Large Should The State Be?&lt;/h3&gt;&lt;p&gt;The Mamba authors write, “the efficiency vs. effectiveness tradeoff of sequence models is characterised by how well they compress their state”. In other words, like in political economy&lt;sup&gt;18&lt;/sup&gt;, the fundamental problem is how to manage the state.&lt;/p&gt;&lt;p&gt;🔁 &lt;strong&gt;Traditional RNNs are anarchic&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;They have a small, minimal state. The size of the state is bounded. The compression of state is poor.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;🤖 &lt;strong&gt;Transformers are communist&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;They have a maximally large state. The “state” is just a cache of the entire history with no compression. Every context token is treated equally until recall time.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;🐍&lt;strong&gt;Mamba has a compressed state&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;…but it’s selective about what goes in. Mamba says we can get away with a small state if the state is well focused and effective&lt;sup&gt;19&lt;/sup&gt;.&lt;/em&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="275" src="https://lh7-us.googleusercontent.com/rkN6fi0try__wiIKQ1D9gbHvCrW_dHsKV0jckG85H7P3_Lx1Vm2vHfeb7Zs6N50lnjVx04A3QTQb2JSjMltn8C0kFmvB4DPUgsjj_DEAGu8O-LcKlY7G0RLgLCCsDV_R1W4pkkE67_2rnyx0vCMnayM" width="602" /&gt;&lt;figcaption&gt;Language Models and State Size&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The upshot is that state&lt;strong&gt; representation is critical&lt;/strong&gt;. A smaller state is more efficient; a larger state is more effective. The key is to &lt;strong&gt;selectively&lt;/strong&gt; and &lt;strong&gt;dynamically&lt;/strong&gt; compress data into the state. Mamba’s Selection Mechanism allows for context-dependent reasoning, focusing and ignoring. For both performance and interpretability, understanding the state seems to be very useful.&lt;/p&gt;&lt;h2 id="information-flow-in-transformer-vs-mamba"&gt;Information Flow in Transformer vs Mamba&lt;/h2&gt;&lt;p&gt;How do Transformers know anything? At initialization, a transformer isn’t very smart. It learns in two ways:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Training data (Pretraining, SFT, RLHF etc)&lt;/li&gt;&lt;li&gt;In context-data&lt;/li&gt;&lt;/ol&gt;&lt;h4 id="training-data"&gt;Training Data&lt;/h4&gt;&lt;p&gt;Models learn from their training data. This is a kind of lossy compression of input data into the weights. We can think of the effect of pretraining data on the transformer kinda like the effect of your ancestor’s experiences on your genetics - you can’t recall their experiences, you just have vague instincts about them&lt;sup&gt;20&lt;/sup&gt;.&lt;/p&gt;&lt;h4 id="in-context-data"&gt;In Context-Data&lt;/h4&gt;&lt;p&gt;Transformers use their context as short-term memory, which they can recall with ~perfect fidelity. So we get In-Context Learning, e.g. using induction heads to solve the Indirect Object Identification task, or computing Linear Regression.&lt;/p&gt;&lt;h4 id="retrieval"&gt;Retrieval&lt;/h4&gt;&lt;p&gt;Note that Transformers don’t filter their context at all until recall time. So if we have a bunch of information we think &lt;em&gt;might&lt;/em&gt; be useful to the Transformer, we filter it &lt;em&gt;outside&lt;/em&gt; the Transformer (using Information Retrieval strategies) and then stuff the results into the prompt. This process is known as Retrieval Augmented Generation (RAG). RAG determines relevant information for the context window of a transformer. A human with the internet is kinda like a RAG system - you still have to know what to search but whatever you retrieve is as salient as short-term memory to you.&lt;/p&gt;&lt;h4 id="information-flow-for-mamba"&gt;Information Flow for Mamba&lt;/h4&gt;&lt;p&gt;Training Data acts similarly for Mamba. However, the lines are slightly blurred for in-context data and retrieval. In-context data for Mamba &lt;em&gt;is&lt;/em&gt; compressed/filtered similar to retrieval data for transformers. This in-context data is also accessible for look-up like for transformers (although with somewhat lower fidelity).&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="236" src="https://lh7-us.googleusercontent.com/0dxiIk5NUI9g_P7G5lr5CSziEVKABYdtIW-R4Rxi6OHwWV_vLYVb1wtetVmzNtRWcLngldL4A8WUQA2jhIQj-IJmpaYr97xt-2Du_dxVOe5ppA4EcRNxEbjQvmjbND_DhyKhO6nsnS4nf1NxvRLwx-o" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;Transformer context is to Mamba states what short-term is to long-term memory. Mamba doesn’t just have “RAM”, it has a hard drive&lt;sup&gt;21&lt;/sup&gt; &lt;sup&gt;22&lt;/sup&gt;.&lt;/p&gt;&lt;h3 id="swapping-states-as-a-new-prompting-paradigm"&gt;Swapping States as a New Prompting Paradigm&lt;/h3&gt;&lt;p&gt;Currently, we often use RAG to give a transformer contextual information.&lt;/p&gt;&lt;p&gt;With Mamba-like models, you could instead imagine having a library of states created by running the model over specialised data. States could be shared kinda like LoRAs for image models.&lt;/p&gt;&lt;p&gt;For example, I could do inference on 20 physics textbooks and, say, 100 physics questions and answers. Then I have a state which I can give to you. Now you don’t need to add any few-shot examples; you just simply ask your question. &lt;strong&gt;The in-context learning is in the state&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;In other words, you can drag and drop downloaded states into your model, like literal plug-in cartridges. And note that “training” a state doesn’t require any backprop. It’s more like a highly specialised one-pass fixed-size compression algorithm. This is unlimited in-context learning applied at inference time for zero-compute or latency&lt;sup&gt;23&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;The structure of an effective LLM call goes from…&lt;/p&gt;&lt;ol&gt;&lt;li&gt;System Prompt&lt;/li&gt;&lt;li&gt;Preamble&lt;/li&gt;&lt;li&gt;Few shot-examples&lt;/li&gt;&lt;li&gt;Question&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;…for Transformers, to simply…&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Inputted state (with problem context, initial instructions, textbooks, and few-shot examples)&lt;/li&gt;&lt;li&gt;Short question&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;…for Mamba.&lt;/p&gt;&lt;p&gt;This is cheaper and faster than few-shot prompting (as the state is infinitely reusable without inference cost). It’s also MUCH cheaper than finetuning and doesn’t require any gradient updates. We could imagine retrieving states in addition to context.&lt;/p&gt;&lt;h2 id="mamba-mechanistic-interpretability"&gt;Mamba &amp;amp; Mechanistic Interpretability&lt;/h2&gt;&lt;p&gt;Transformer interpretability typically involves:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;understanding token relationships via attention,&lt;/li&gt;&lt;li&gt;understanding circuits, and&lt;/li&gt;&lt;li&gt;using Dictionary Learning for unfolding MLPs.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Most of the ablations that we would like to do for Mamba are still valid, but understanding token communication (1) is now more nuanced. All information moves between tokens via hidden states instead of the Attention Mechanism which can “teleport” information from one sequence position to another.&lt;/p&gt;&lt;p&gt;For understanding in-context learning (ICL) tasks with Mamba, we will look to intervene on the SSM state. A classic task in-context learning task is Indirect Object Identification in which a model has to finish a paragraph like:&lt;/p&gt;&lt;p&gt;&lt;em&gt;Then, Shelby and Emma had a lot of fun at the school. [Shelby/Emma] gave an apple to [BLANK]&lt;/em&gt;&lt;/p&gt;&lt;p&gt;The model is expected to fill in the blank with the name that is not repeated in the paragraph. In the chart below we can see that information is passed from the [Shelby/Emma] position to the final position via the hidden state (see the two blue lines in the top chart).&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="256" src="https://lh7-us.googleusercontent.com/ZDBpRS1yEscEZcsJtevlPaM5URUP58dgJ2csAIcWP-hmQcje8kBi-u4zAWYnbeE26YXWemOh32pdHM2TgaSanGePOVgRiss8svxP17nLPBvg1YjLE4W1uIGkTmDI9PbZO42u_4KfYoSeaRnZz_W4HfY" width="602" /&gt;&lt;/figure&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="256" src="https://lh7-us.googleusercontent.com/j8aQ6XIxedX6Zut0rz7CE_e02KgBjyJvg7QQ7U9FkM2TjSWWSNk1v7gFVeGSsETqwQGvF8flh0lIUmSLIVqW9rwHC69rImw5MPj0vA0Y4XihacOzZnhUeKMZpf3bWtJTM_TB67EDYKIyfp2DeX4pNFU" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;Since it’s hypothesised that much of In-Context Learning in Transformers is downstream of more primitive sequence position operations (like Induction Heads), Mamba being able to complete this task suggests a more general In-Context Learning ability.&lt;/p&gt;&lt;h2 id="what%E2%80%99s-next-for-mamba-ssms"&gt;What’s Next for Mamba &amp;amp; SSMs?&lt;/h2&gt;&lt;p&gt;Mamba-like models are likely to excel in scenarios requiring extremely long context and long-term memory. Examples include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Processing DNA&lt;/li&gt;&lt;li&gt;Generating (or reasoning over) video&lt;/li&gt;&lt;li&gt;Writing novels&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;An illustrative example is agents with long-term goals.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Suppose you have an agent interacting with the world. Eventually, its experiences become too much for the context window of a transformer. The agent then has to compress or summarise its experiences into some more compact representation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;But how do you decide what information is the most useful as a summary? If the task is language, LLMs are actually fairly good at summaries - okay, yeah, you’ll lose some information, but the most important stuff can be retained.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;However, for other disciplines, it might not be clear how to summarise. For example, what’s the best way to summarise a 2 hour movie?&lt;sup&gt;24&lt;/sup&gt;. Could the model itself learn to do this naturally rather than a hacky workaround like trying to describe the aesthetics of the movie in text?&lt;/em&gt;&lt;/p&gt;&lt;p&gt;This is what Mamba allows. Actual long-term memory. A real state where the model learns to keep what’s important. Prediction is compression - learning what’s useful to predict what’s coming next inevitably leads to building a useful compression of the previous tokens.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;The implications for Assistants are clear:&lt;/p&gt;&lt;p&gt;Your chatbot co-evolves with you. It remembers.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="339" src="https://lh7-us.googleusercontent.com/agZClF-xa6q13BlEbfZLFKP3DM0hJiRy9kC0MRFoNPi8kdWCh8_BUa5oLC0V_6jTmcNQQfmMr7GGa6gwIe3CEGVeK79AFMhE1gMnbdhEoQ8iFCRuO7Yc6Xi2M3kaVIGZ4LTfDKqITQ6ap1DylOqbWs4" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;The film HER is looking better and better as time goes on 😳&lt;/p&gt;&lt;h3 id="agents-ai-safety"&gt;Agents &amp;amp; AI Safety&lt;/h3&gt;&lt;p&gt;One reason for positive updates in existential risk from AGI is Language Models. Previously, Deep-RL agents trained via self-play looked set to be the first AGIs. Language models are inherently much safer since they aren’t trained with long-term goals&lt;sup&gt;25&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;The potential for long-term sequence reasoning here brings back the importance of agent-based AI safety. Few agent worries are relevant to Transformers with an 8k context window. Many are relevant to systems with impressive long-term memories and possible instrumental goals.&lt;/p&gt;&lt;h3 id="the-best-collab-since-taco-bell-kfc-%F0%9F%A4%96-x-%F0%9F%90%8D"&gt;The Best Collab Since Taco Bell &amp;amp; KFC: 🤖 x 🐍&lt;/h3&gt;&lt;p&gt;The Mamba authors show that there’s value in combining Mamba’s long context with the Transformer’s high fidelity over short sequences. For example, if you’re making long videos, you likely can’t fit a whole movie into a Transformer’s context for attention&lt;sup&gt;26&lt;/sup&gt;. You could imagine having Attention look at the most recent frames for short-term fluidity and an SSM for long-term narrative consistency&lt;sup&gt;27&lt;/sup&gt;.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;This isn’t the end for Transformers. Their high effectiveness is exactly what’s needed for many tasks. But now Transformers aren’t the only option. Other architectures are genuinely feasible.&lt;/p&gt;&lt;p&gt;So we’re not in the post-Transformer era. But for the first time, we’re living in the post-only-Transformers era&lt;sup&gt;28&lt;/sup&gt;. And this blows the possibilities wide open for sequence modelling with extreme context lengths and native long-term memory.&lt;/p&gt;&lt;p&gt;Two ML researchers, Sasha Rush (HuggingFace, Annotated Transformer, Cornell Professor) and Jonathan Frankle (Lottery Ticket Hypothesis, MosaicML, Harvard Professor), currently have a bet here.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="324" src="https://lh7-us.googleusercontent.com/-_S7CaQ4OxepapriZhhAs25xq-H_dSnavPxXkm0_lMMZjtno4kgWfjS1PAcLhYpbMz6BNNYd-RoxBA_Fy45CemDdvofbP7oPVQ3ygHBQNQ8pMVf7l5YnLSCgE3L1J9muCpoFmTSz09zcX9xEigRrKnc" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;Currently Transformers are far and away in the lead. With 3 years left, there’s now a research direction with a fighting chance.&lt;/p&gt;&lt;p&gt;All that remains to ask is: &lt;strong&gt;Is Attention All We Need?&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;&lt;hr /&gt;&lt;!--kg-card-begin: html--&gt;&lt;p&gt;
    1. see Figure 8 in the Mamba paper.
    &lt;br /&gt;2. And scaling up with massive compute.
    &lt;br /&gt;3. More specifically the scaled dot-product Attention popularised by Transformers
    &lt;br /&gt;4. For people who don’t see Temple Run as the cultural cornerstone it is 🤣 Temple Run was an iPhone game from 2011 similar to Subway Surfer
    &lt;br /&gt;5. Here we assume the environment is sufficiently smooth.
    &lt;br /&gt;6. One pretty important constraint for this to be efficient is that we don’t allow the individual elements of the state vector to interact with each other directly. We’ll use a combination of the state dimensions to determine the output but we don’t e.g. allow the velocity of the runner and the direction of the closest obstacle (or whatever else was in our state) to directly interact. This helps with efficient computation and we achieve this practically by constraining A to be a diagonal matrix.
    &lt;br /&gt;7. Concretely consider the case of Language Models - each token is a discrete step 
    &lt;br /&gt;8. ZOH also has nice properties for the initialisations - we want A_bar to be close to the identity so that the state can be mostly maintained from timestep to timestep if desired. ZOH gives A_bar as an exponential so any diagonal element initialisations close to zero give values close to 1 
    &lt;br /&gt;9. This is known as the Euler discretisation in the literature
    &lt;br /&gt;10. It’s wild to note that some readers might not have, we’re so far into the age of Attention that RNNs have been forgotten!
    &lt;br /&gt;11. B is like the Query (Q) matrix for Transformers.
    &lt;br /&gt;12. C is like the Output (O) matrix for Transformers. 
    &lt;br /&gt;13. Non-alcoholic options also available! 
    &lt;br /&gt;14. Especially as all voices roughly occupy the same space on the audio frequency spectrum Intuitively this seems really hard! 
    &lt;br /&gt;15. Note that photographic memory doesn’t necessarily imply perfect inferences from that memory! 
    &lt;br /&gt;16. To be clear, if you have a short sequence, then a transformer should theoretically be a better approach. If you can store the whole context, then why not!? If you have enough memory for a high-resolution image, why compress it into a JPEG? But Mamba-style architectures are likely to hugely outperform with long-range sequences. 
    &lt;br /&gt;17. More details are available for engineers interested in CUDA programming - Tri’s talk, Mamba paper section 3.3.2, and the official CUDA code are good resources for understanding the Hardware-Aware Scan 
    &lt;br /&gt;18. or in Object Oriented Programming 
    &lt;br /&gt;19. Implications to actual Political Economy are left to the reader but maybe Gu and Dao accidentally solved politics!? 
    &lt;br /&gt;20. This isn’t a perfect analogy as human evolution follows a genetic algorithm rather than SGD. 
    &lt;br /&gt;21. Albeit a pretty weird hard drive at that - it morphs over time rather than being a fixed representation.  
    &lt;br /&gt;22. As a backronym, I’ve started calling the hidden_state the state space dimension (or selective state dimension) which shortens to SSD, a nice reminder for what this object represents - the long-term memory of the system.
    &lt;br /&gt;23. I’m thinking about this similarly to the relationship between harmlessness finetuning and activation steering. State swapping, like activation steering, is an inference time intervention giving comparable results to its train time analogue. 
    &lt;br /&gt;24. This is a very non-trivial problem! How do human brains represent a movie internally? It’s not a series of the most salient frames, nor is it a text summary of the colours, nor is it a purely vibes-based summary if you can memorise some lines of the film. 
    &lt;br /&gt;25. They’re also safer since they inherently understand (though don’t necessarily embody) human values. It’s not all clear that how to teach an RL agent human morality. 
    &lt;br /&gt;26. Note that typically an image (i.e. a single frame) counts as &amp;gt;196 tokens, and movies are typically 24 fps so you’ll fill a 32k context window in 7 seconds 🤯 
    &lt;br /&gt;27. Another possibility that I’m excited about is applying optimisation pressure to the state itself as well as the output to have models that respect particular use cases. 
    &lt;br /&gt;28. This is slightly hyperbolic, the TS-Mixer for time series, Gradient Boosting Trees for tabular data and Graph Neural Networks for weather prediction exist and are currently used, but these aren’t at the core of AI
&lt;/p&gt;&lt;!--kg-card-end: html--&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Kola Ayonrinde is a Research Scientist and Machine Learning Engineer with a flair for writing. He integrates technology and creativity, focusing on applying machine learning in innovative ways and exploring the societal impacts of tech advancements.&lt;/p&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;This post was originally posted on Kola's personal blog.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thanks to Gonçalo for reading an early draft, Jaden for the nnsight library used for the Interpretability analysis and Tessa for Mamba patching visualisations.Also see: &lt;/em&gt;&lt;em&gt;Mamba paper&lt;/em&gt;&lt;em&gt;, Mamba Python code, &lt;/em&gt;&lt;em&gt;Annotated S4&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;Nathan Labenz podcast&lt;/em&gt;&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Kola Ayonrinde, "Mamba Explained," The Gradient, 2024&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;@article{Ayonrinde2024mamba,
    author = {Kola Ayonrinde},
    title = {Mamba Explained},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/mamba-explained},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Car-GPT: Could LLMs finally make self-driving cars happen?]]&amp;gt;https://thegradient.pub/car-gpt/65db7b4193571d5c8c154a73Fri, 08 Mar 2024 16:55:18 GMT&lt;!--&amp;lt;![CDATA[&lt;img src="https://thegradient.pub/content/images/2024/03/car-gpt.jpg" alt="Car-GPT: Could LLMs finally make self-driving cars happen?"&gt;&lt;p&gt;In 1928, London was in the middle of a terrible health crisis, devastated by bacterial diseases like pneumonia, tuberculosis, and meningitis. Confined in sterile laboratories, scientists and doctors were stuck in a relentless cycle of trial and error, using traditional medical approaches to solve complex problems.&lt;/p&gt;&lt;p&gt;This is when, in September 1928, an accidental event changed the course of the world.&lt;strong&gt; &lt;/strong&gt;A Scottish doctor named Alexander Fleming forgot to close a petri dish (the transparent circular box you used in science class), which got contaminated by mold. This is when Fleming noticed something peculiar: all bacteria close to the moisture were dead, while the others survived.&lt;/p&gt;&lt;p&gt;&amp;quot;What was that moisture made of?&amp;quot; wondered M. Flemming.&lt;strong&gt; &lt;/strong&gt;This was when he discovered that Penicillin, the main component of the mold, was a powerful bacterial killer. This led to the groundbreaking discovery of penicillin, leading to the antibiotics we use today. In a world where doctors were relying on existing well-studied approaches, Penicillin was the unexpected answer.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Self-driving cars may be following a similar event.&lt;/strong&gt; Back in the 2010s, most of them were built using what we call a &amp;#xAB; modular &amp;#xBB; approach. The software &amp;#xAB; autonomous &amp;#xBB; part is split into several modules, such as Perception (the task of seeing the world), or Localization (the task of accurately localize yourself in the world), or Planning (the task of creating a trajectory for the car to follow, and implementing the &amp;#xAB; brain &amp;#xBB; of the car). Finally, all these go to the last module: Control, that generates commands such as &amp;#xAB; steer 20&amp;#xB0; right &amp;#xBB;, etc&amp;#x2026; So this was the well-known approach. &lt;/p&gt;&lt;p&gt;But a decade later, companies started to take another discipline very seriously: &lt;strong&gt;End-To-End learning&lt;/strong&gt;. The core idea is to replace every module with a single neural network predicting steering and acceleration, but as you can imagine, this introduces a black box problem.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/EpMJPDK-TBu9ZhN25UrxVbAk-9rJjEvtitzjvPpzjhTBPdkk-judKQtfWQNf7vtNrG1sfsvkUhpbtMGplWN5bbnx5ULbfNj6vpRf8RVlt5eDn8MN99FObGbPsmokdNlCGZ1NWq-uw32QVitv4NZC3zI" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="161"&gt;&lt;figcaption&gt;The 4 Pillars of Self-Driving Cars are Perception, Localization, Planning, and Control. Could a Large Language Model replicate them? (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;These approaches are known, but don&amp;#x2019;t solve the self-driving problem yet. So, we could be wondering:&lt;strong&gt; &amp;quot;What if LLMs (Large Language Models), currently revolutionizing the world, were the unexpected answer to autonomous driving?&amp;quot;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This is what we&amp;apos;re going to see in this article, beginning with a simple explanation of what LLMs are and then diving into how they could benefit autonomous driving.&lt;/p&gt;&lt;h2 id="preamble-llms-what"&gt;&lt;strong&gt;Preamble: LLMs-what?&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Before you read this article, you must know something: I&amp;apos;m not an LLM pro, at all. This means, I know too well the struggle to learn it. I understand what it&amp;apos;s like to google &amp;quot;learn LLM&amp;quot;; then see 3 sponsored posts asking you to download e-books (in which nothing concrete appears)... then see 20 ultimate roadmaps and GitHub repos, where step 1/54 is to view a 2-hour long video (and no one knows what step 54 is because it&amp;apos;s so looooooooong).&lt;/p&gt;&lt;p&gt;So, instead of putting you through this pain myself, let&amp;apos;s just break down what LLMs are in 3 key ideas:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Tokenization&lt;/li&gt;&lt;li&gt;Transformers&lt;/li&gt;&lt;li&gt;Processing Language&lt;/li&gt;&lt;/ol&gt;&lt;h3 id="tokenization"&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;In ChatGPT, you input a piece of text, and it returns text, right? Well, what&amp;apos;s actually happening is that your text is first converted into tokens.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/_rT6_ShRUbi-bZpaKL7JF-BhE_rfDg_V8De5nYj0O5tGgAtLTyYhnGleIy7nBJ3vyrUsfge6cdReCctzsfCyW_XP6WUm21pU350RpOoxWzb2SYRvMcKMIZAOE6wdFou7t_ERJ2_Jht6uUhfg_sBgcbI" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="600" height="303"&gt;&lt;figcaption&gt;Example of tokenization of a sentence, each word becomes a &amp;quot;token&amp;quot;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But what&amp;apos;s a token? You might ask. Well, a token can correspond to a word, a character, or anything we want. Think about it -- if you are to send a sentence to a neural network, you didn&amp;apos;t plan on sending actual words, did you?&lt;/p&gt;&lt;p&gt;The input of a neural network is always a number, so you need to convert your text into numbers; this is tokenization.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/pZ3qf5HQNrqXqb5bbGkLgWQPvu04-2b_ejpv4m3i5C9VfcPg3yZm7cmaD6lq4xgrA4DhUBJpCa-HB4i7iAPo8-Hyrde9sLiBYBiY2d7c9O17ePJtCqAb15dvcDEGxofEwneP6Nx2_oSiT26m4cLvcMc" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="139"&gt;&lt;figcaption&gt;What tokenization actually is: A conversion from words to numbers&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Depending on the model (ChatGPT, LLAMA, etc...), a token can mean different things: a word, a subword, or even a character. We could take the English vocabulary and define these as words or take parts of words (subwords) and handle even more complex inputs. For example, the word &amp;#xAB; a &amp;#xBB; could be token 1, and the word &amp;#xAB; abracadabra &amp;#xBB; would be token 121.&lt;/p&gt;&lt;h3 id="transformers"&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Now that we understand how to convert a sentence into a series of numbers, we can send that series into our neural network! At a high level, we have the following structure:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/J1CkM3ItKevopmi-0gbSHWJnMStL4dZWksllG15OlaDI4PFgk-FtFeQ7O0CnP1dKx9ZHV7PUAlmBK9lFwJQrHnJj1JAXAMHdbZH13hd07dYL55ZCsxQChf06dYj_JoXEvNeAqdfmj2IcdwD8sP5OZtI" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="407"&gt;&lt;figcaption&gt;A Transformer is an Encoder-Decoder Architecture that takes a sequence of tokens as input and outputs a another series of tokens&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If you start looking around, you will see that some models are based on an encoder-decoder architecture, some others are purely encoder-based, and others, like GPT, are purely decoder-based.&lt;/p&gt;&lt;p&gt;Whatever the case, they all share the core Transformer blocks: multi-head attention, layer normalization, addition and concatenation, blocks, cross-attention, etc...&lt;/p&gt;&lt;p&gt;&lt;strong&gt;This is just a series of attention blocks getting you to the output&lt;/strong&gt;. So how does this word prediction work?&lt;/p&gt;&lt;h3 id="the-output-next-word-prediction"&gt;&lt;strong&gt;The output/ Next-Word Prediction&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;The Encoder learns features and understands context... But what does the decoder do? In the case of object detection, the decoder is predicting bounding boxes. In the case of segmentation, the decoder is predicting segmentation masks. What about here?&lt;/p&gt;&lt;p&gt;In our case, the decoder is trying to generate a series of words; we call this task &amp;quot;next-word prediction&amp;quot;.&lt;/p&gt;&lt;p&gt;Of course, it does it similarly by predicting numbers or tokens. This characterizes our full model as shown below,&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/YS9WFjjuYTq7QkzPnx4xgTQnU0Pmr22i4fEzXXWuBf6wD--eYL8FvdoEpkqlCMKraBaSDuo7j0sWR7ltUaWI31_Bvq9PtJoPpoWRFQnjKOth1P7mnxfzmGT8ppUslOPMhbOzJY49F4IHBMZfyzax18E" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="519"&gt;&lt;figcaption&gt;I would say the loss function for this particular output produces a near-0 value.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now, there are many &amp;quot;concepts&amp;quot; that you should learn on top of this intro: everything Transformer and Attention related, but also few-shot learning, pretraining, finetuning, and more...&lt;/p&gt;&lt;p&gt;Ok... but what does it have to do with self-driving cars? I think it&amp;apos;s time to move to stage 2.&lt;/p&gt;&lt;h2 id="chat-gpt-for-self-driving-cars"&gt;&lt;strong&gt;Chat-GPT for Self-Driving Cars&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The thing is, you&amp;apos;ve already been through the tough part. The rest simply is: &amp;quot;How do I adapt this to autonomous driving?&amp;quot;. Think about it; we have a few modifications to make:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Our input now becomes either images, sensor data&lt;/strong&gt; (LiDAR point clouds, RADAR point clouds, etc...), or even algorithm data (lane lines, objects, etc...). All of it is &amp;quot;tokenizable&amp;quot;, as Vision Transformers or Video Vision Transformers do.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Our Transformer model pretty much remains the same&lt;/strong&gt; since it only operates on tokens and is independent of the kind of input.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;The output is based on the set of tasks we want to do.&lt;/strong&gt; It could be explaining what&amp;apos;s happening in the image or could &amp;#xA0;also be a direct driving task like switching lanes.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So, let&amp;apos;s begin with the end:&lt;/p&gt;&lt;h3 id="what-self-driving-car-tasks-could-llm-solve"&gt;&lt;strong&gt;What self-driving car tasks could LLM solve?&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;There are many tasks involved in autonomous driving, but not all of them are GPT-isable. The most active research areas in 2023 have been:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Perception&lt;/strong&gt;: Based on an input image, describe the environment, number of objects, etc...&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Planning&lt;/strong&gt;: Based on an image, or a bird-eye view, or the output of perception, describe what we should do (keep driving, yield, etc...)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Generation&lt;/strong&gt;: Generate training data, alternate scenarios, and more... using &amp;quot;diffusion&amp;quot;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Question &amp;amp; Answers&lt;/strong&gt;: Create a chat interface and ask the LLM to answer questions based on the scenario.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="llms-in-perception"&gt;&lt;strong&gt;LLMs in Perception&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;In Perception, the input is a series of images, and the output is usually a set of objects, lanes, etc... In the case of LLMs, we have 3 core tasks: &lt;strong&gt;Detection&lt;/strong&gt;, &lt;strong&gt;Prediction&lt;/strong&gt;, and &lt;strong&gt;Tracking&lt;/strong&gt;. An example with Chat-GPT, when you send it an image and ask to describe what&amp;apos;s going on is shown below:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/unUisu66NolUzzipNfKObr8kE6n8PRcTMy86cYYIG1aIPLkYKZd34zmzGrkM4yS6lKNoXpvORHwnfORfOsy8aRNUx9AwEDN_qQN4tiuutBRh8l3h_vVpfVzOJ7UdQ-CuWKI5EJsze9le6qRA7VQ1QoY" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="425" height="308"&gt;&lt;figcaption&gt;A GPT-4 Vision model can return the objects in the image, just like object detectors do (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Other models such as HiLM-D and MTD-GPT can also do this, some work also for videos. Models like PromptTrack, also have the ability to assign unique IDs (this car in front of me is ID #3), similar to a 4D Perception model.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/WcjhR7diFbZrVeKdiVyQbC_HtYJVGUQsOBka0zikaD2JZpfmNxcyEJlpzxZfvobWrMu6srxUEGPcxpdVSywVKW-0gIuOISCqLCVfjaA6Q7KaNb1etKfNybXkya4yFyx7AY0Y2_ZZw_cY_gWSccO0B2Q" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="267"&gt;&lt;figcaption&gt;PromptTrack combines the DETR object detector with Large Language Models&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In this model, multi-view images are sent to an Encoder-Decoder network that is trained to predict annotations of objects such as bounding boxes, and attention maps). These maps are then combined with a prompt like &amp;apos;find the vehicles that are turning right&amp;apos;.The next block then finds the 3D Bounding Box localization and assigns IDs using a bipartite graph matching algorithm like the Hungarian Algorithm.&lt;/p&gt;&lt;p&gt;This is cool, but this isn&amp;apos;t the &amp;quot;best&amp;quot; application of LLMs so far:&lt;/p&gt;&lt;h3 id="llms-in-decision-making-navigation-and-planning"&gt;&lt;strong&gt;LLMs in Decision Making, Navigation, and Planning&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;If Chat-GPT can find objects in an image, it should be able to tell you what to do with these objects, shouldn&amp;apos;t it? Well, this is the task of Planning i.e. defining a path from A to B, based on the current perception. While there are numerous models developed for this task, the one that stood out to me was Talk2BEV:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/N3ZvMnLMjQ6jwL4FNvvTyM4U6KFrri0jV-0yOYVH9lAAtRH7MD8aMX_LHhjeBFKxGwTdrATJoNUQe-sUqEB3utLnpreCT4e4TIO3qX3LTrzBKwZ7kPAfzxAu6osJ35tYpapCiTTWDtx0tUOHXcNqu04" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="600" height="179"&gt;&lt;figcaption&gt;Talk2BEV takes perception one step further and also tells you what to do&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The main difference between models for planning and Perception-only models is that here, we&amp;apos;re going to train the model on human behavior to suggest ideal driving decisions. We&amp;apos;re also going to change the input from multi-view to Bird Eye View since it is much easier to understand.&lt;/p&gt;&lt;p&gt;This model works both with LLaVA and ChatGPT4, and here is a demo of the architecture:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh7-us.googleusercontent.com/-bl_IDT2SqF75q3d20EORJcH22oXWMjFmkLFn0ZKbVV5oshlr5BkZEnscfUSg_-pkzMDJ3Jo38mdu6whUmIDWq7pXfxXxdwgc3Kj-WUwv5LNWUHIvH3r6mfpKP9s5PD7NoA7e0R3pBoic6ijwfD57a--&gt;</content:encoded><guid isPermaLink="false">https://thegradient.pub/rss/</guid></item><item><title>AI to help researchers see the bigger picture in cell biology (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/ai-help-researchers-see-bigger-picture-cell-biology-0225</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-HolisticCells-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Studying gene expression in a cancer patient’s cells can help clinical biologists understand the cancer’s origin and predict the success of different treatments. But cells are complex and contain many layers, so how the biologist conducts measurements affects which data they can obtain. For instance, measuring&amp;nbsp;proteins in a cell could yield different information about the effects of cancer&amp;nbsp;than measuring gene expression or cell morphology.&lt;/p&gt;&lt;p&gt;Where in the cell the information comes from matters. But to capture complete information about the state of the cell, scientists often must conduct many measurements using different techniques and analyze them one at a time. Machine-learning methods can speed up the process, but existing methods lump all the information from each measurement modality together, making it difficult to figure out which data came from which part of the cell.&lt;/p&gt;&lt;p&gt;To overcome this problem, researchers at the Broad Institute of MIT and Harvard and ETH Zurich/Paul Scherrer Institute (PSI) developed an artificial intelligence-driven framework that learns which information about a cell’s state is shared across different measurement modalities and which information is unique to a particular measurement type.&lt;/p&gt;&lt;p&gt;By pinpointing which information came from which cell parts, the approach provides a more holistic view of the cell’s state, making it easier for a biologist to see the complete picture of cellular interactions. This could help scientists understand disease mechanisms and track the progression of cancer, neurodegenerative disorders such as Alzheimer’s, and metabolic diseases like diabetes.&lt;/p&gt;&lt;p&gt;“When we study cells, one measurement is often not sufficient, so scientists develop new technologies to measure different aspects of cells. While we have many ways of looking at a cell, at the end of the day we only have one underlying cell state. By putting the information from all these measurement modalities together in a smarter way, we could have a fuller picture of the state of the cell,” says lead author Xinyi Zhang SM ’22, PhD ’25, a former&amp;nbsp;graduate student in the MIT Department of Electrical Engineering and Computer Science (EECS) and an affiliate of the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard, who is now a group leader at AITHYRA in Vienna, Austria.&lt;/p&gt;&lt;p&gt;Zhang is joined on a paper about the work by G.V. Shivashankar, a professor in the Department of Health Sciences and Technology at ETH Zurich and head of the Laboratory of Multiscale Bioimaging at PSI; and senior author Caroline Uhler, a&amp;nbsp;professor in EECS and the Institute for Data, Systems, and Society (IDSS) at MIT, member of MIT’s Laboratory for Information and Decision Systems (LIDS), and director of the Eric and Wendy Schmidt Center at the Broad Institute. The research appears today in &lt;em&gt;Nature Computational Science&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Manipulating multiple measurements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There are many tools scientists can use to capture information about a cell’s state. For instance, they can measure RNA to see if the cell is growing, or they can measure chromatin morphology&amp;nbsp;to see if the cell is dealing with external physical or chemical signals.&lt;/p&gt;&lt;p&gt;“When scientists perform multimodal analysis, they gather information using multiple measurement modalities and integrate it to better understand the underlying state of the cell. Some information is captured by one modality only, while other information is shared across modalities. To fully understand what is happening inside the cell, it is important to know where the information came from,” says Shivashankar.&lt;/p&gt;&lt;p&gt;Often, for scientists, the only way to sort this out is to conduct multiple individual experiments and compare the results. This slow and cumbersome process limits the amount of information they can gather.&lt;/p&gt;&lt;p&gt;In the new work, the researchers built a machine-learning framework that specifically understands which information overlaps between different modalities, and which information is unique to a particular modality but not captured by others.&lt;/p&gt;&lt;p&gt;“As a user, you can simply input your cell data and it automatically tells you which data are shared and which data are modality-specific,” Zhang says.&lt;/p&gt;&lt;p&gt;To build this framework, the researchers rethought the typical way machine-learning models are designed to capture and interpret multimodal cellular measurements.&lt;/p&gt;&lt;p&gt;Usually these methods, known as autoencoders, have one model for each measurement modality, and each model encodes a separate representation for the data captured by that modality. The representation is a compressed version of the input data that discards any irrelevant details.&lt;/p&gt;&lt;p&gt;The MIT method has a shared representation space where data that overlap between multiple modalities are encoded, as well as separate spaces where unique data from each modality are encoded.&lt;/p&gt;&lt;p&gt;In essence, one can think of it like a Venn diagram of cellular data.&lt;/p&gt;&lt;p&gt;The researchers also used a special, two-step training procedure that helps their model handle the complexity involved in deciding which data are shared across multiple data&amp;nbsp;modalities. After training, the model can identify which data are shared and which are unique when fed cell data it has never seen before.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Distinguishing data&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In tests on synthetic datasets, the framework correctly captured known shared and modality-specific information. When they applied their method to real-world single-cell datasets, it&amp;nbsp;comprehensively and automatically distinguished between gene activity captured jointly by two measurement modalities, such as transcriptomics and chromatin accessibility, while also correctly identifying which information came from only one of those modalities.&lt;/p&gt;&lt;p&gt;In addition, the researchers used their method to identify which measurement modality captured a certain protein marker that indicates DNA damage in cancer patients. Knowing where this information came from would help a clinical scientist determine which technique they should use to measure that marker.&lt;/p&gt;&lt;p&gt;“There are too many modalities in a cell and we can’t possibly measure them all, so we need a prediction tool. But then the question is: Which modalities should we measure and which modalities should we predict? Our method can answer that question,” Uhler says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to enable the model to provide more interpretable information about the state of the cell. They also want to conduct additional experiments to ensure it correctly disentangles cellular information and apply the model to a wider range of clinical questions.&lt;/p&gt;&lt;p&gt;“It is not sufficient to just integrate the information from all these modalities,” Uhler says. “We can learn a lot about the state of a cell if we carefully compare the different modalities to understand how different components of cells regulate each other.”&lt;/p&gt;&lt;p&gt;This research is funded, in part, by the Eric and Wendy Schmidt Center at the Broad Institute, the Swiss National Science Foundation, the U.S. National Institutes of Health, the U.S. Office of Naval Research, AstraZeneca, the MIT-IBM Watson AI Lab, the MIT J-Clinic for Machine Learning and Health, and a Simons Investigator Award.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-HolisticCells-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Studying gene expression in a cancer patient’s cells can help clinical biologists understand the cancer’s origin and predict the success of different treatments. But cells are complex and contain many layers, so how the biologist conducts measurements affects which data they can obtain. For instance, measuring&amp;nbsp;proteins in a cell could yield different information about the effects of cancer&amp;nbsp;than measuring gene expression or cell morphology.&lt;/p&gt;&lt;p&gt;Where in the cell the information comes from matters. But to capture complete information about the state of the cell, scientists often must conduct many measurements using different techniques and analyze them one at a time. Machine-learning methods can speed up the process, but existing methods lump all the information from each measurement modality together, making it difficult to figure out which data came from which part of the cell.&lt;/p&gt;&lt;p&gt;To overcome this problem, researchers at the Broad Institute of MIT and Harvard and ETH Zurich/Paul Scherrer Institute (PSI) developed an artificial intelligence-driven framework that learns which information about a cell’s state is shared across different measurement modalities and which information is unique to a particular measurement type.&lt;/p&gt;&lt;p&gt;By pinpointing which information came from which cell parts, the approach provides a more holistic view of the cell’s state, making it easier for a biologist to see the complete picture of cellular interactions. This could help scientists understand disease mechanisms and track the progression of cancer, neurodegenerative disorders such as Alzheimer’s, and metabolic diseases like diabetes.&lt;/p&gt;&lt;p&gt;“When we study cells, one measurement is often not sufficient, so scientists develop new technologies to measure different aspects of cells. While we have many ways of looking at a cell, at the end of the day we only have one underlying cell state. By putting the information from all these measurement modalities together in a smarter way, we could have a fuller picture of the state of the cell,” says lead author Xinyi Zhang SM ’22, PhD ’25, a former&amp;nbsp;graduate student in the MIT Department of Electrical Engineering and Computer Science (EECS) and an affiliate of the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard, who is now a group leader at AITHYRA in Vienna, Austria.&lt;/p&gt;&lt;p&gt;Zhang is joined on a paper about the work by G.V. Shivashankar, a professor in the Department of Health Sciences and Technology at ETH Zurich and head of the Laboratory of Multiscale Bioimaging at PSI; and senior author Caroline Uhler, a&amp;nbsp;professor in EECS and the Institute for Data, Systems, and Society (IDSS) at MIT, member of MIT’s Laboratory for Information and Decision Systems (LIDS), and director of the Eric and Wendy Schmidt Center at the Broad Institute. The research appears today in &lt;em&gt;Nature Computational Science&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Manipulating multiple measurements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There are many tools scientists can use to capture information about a cell’s state. For instance, they can measure RNA to see if the cell is growing, or they can measure chromatin morphology&amp;nbsp;to see if the cell is dealing with external physical or chemical signals.&lt;/p&gt;&lt;p&gt;“When scientists perform multimodal analysis, they gather information using multiple measurement modalities and integrate it to better understand the underlying state of the cell. Some information is captured by one modality only, while other information is shared across modalities. To fully understand what is happening inside the cell, it is important to know where the information came from,” says Shivashankar.&lt;/p&gt;&lt;p&gt;Often, for scientists, the only way to sort this out is to conduct multiple individual experiments and compare the results. This slow and cumbersome process limits the amount of information they can gather.&lt;/p&gt;&lt;p&gt;In the new work, the researchers built a machine-learning framework that specifically understands which information overlaps between different modalities, and which information is unique to a particular modality but not captured by others.&lt;/p&gt;&lt;p&gt;“As a user, you can simply input your cell data and it automatically tells you which data are shared and which data are modality-specific,” Zhang says.&lt;/p&gt;&lt;p&gt;To build this framework, the researchers rethought the typical way machine-learning models are designed to capture and interpret multimodal cellular measurements.&lt;/p&gt;&lt;p&gt;Usually these methods, known as autoencoders, have one model for each measurement modality, and each model encodes a separate representation for the data captured by that modality. The representation is a compressed version of the input data that discards any irrelevant details.&lt;/p&gt;&lt;p&gt;The MIT method has a shared representation space where data that overlap between multiple modalities are encoded, as well as separate spaces where unique data from each modality are encoded.&lt;/p&gt;&lt;p&gt;In essence, one can think of it like a Venn diagram of cellular data.&lt;/p&gt;&lt;p&gt;The researchers also used a special, two-step training procedure that helps their model handle the complexity involved in deciding which data are shared across multiple data&amp;nbsp;modalities. After training, the model can identify which data are shared and which are unique when fed cell data it has never seen before.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Distinguishing data&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In tests on synthetic datasets, the framework correctly captured known shared and modality-specific information. When they applied their method to real-world single-cell datasets, it&amp;nbsp;comprehensively and automatically distinguished between gene activity captured jointly by two measurement modalities, such as transcriptomics and chromatin accessibility, while also correctly identifying which information came from only one of those modalities.&lt;/p&gt;&lt;p&gt;In addition, the researchers used their method to identify which measurement modality captured a certain protein marker that indicates DNA damage in cancer patients. Knowing where this information came from would help a clinical scientist determine which technique they should use to measure that marker.&lt;/p&gt;&lt;p&gt;“There are too many modalities in a cell and we can’t possibly measure them all, so we need a prediction tool. But then the question is: Which modalities should we measure and which modalities should we predict? Our method can answer that question,” Uhler says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to enable the model to provide more interpretable information about the state of the cell. They also want to conduct additional experiments to ensure it correctly disentangles cellular information and apply the model to a wider range of clinical questions.&lt;/p&gt;&lt;p&gt;“It is not sufficient to just integrate the information from all these modalities,” Uhler says. “We can learn a lot about the state of a cell if we carefully compare the different modalities to understand how different components of cells regulate each other.”&lt;/p&gt;&lt;p&gt;This research is funded, in part, by the Eric and Wendy Schmidt Center at the Broad Institute, the Swiss National Science Foundation, the U.S. National Institutes of Health, the U.S. Office of Naval Research, AstraZeneca, the MIT-IBM Watson AI Lab, the MIT J-Clinic for Machine Learning and Health, and a Simons Investigator Award.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/ai-help-researchers-see-bigger-picture-cell-biology-0225</guid><pubDate>Wed, 25 Feb 2026 10:00:00 +0000</pubDate></item><item><title>Nokia and AWS pilot AI automation for real-time 5G network slicing (AI News)</title><link>https://www.artificialintelligence-news.com/news/nokia-and-aws-pilot-ai-automation-for-real-time-5g-network-slicing/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Nokia-and-AWS-pilot-AI-automation-for-real-time-5G-network-slicing-scaled-e1771989468794.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Telecom networks may soon begin adjusting themselves in real time, as operators test systems that allow AI agents to manage traffic and service quality. AI may soon be making operational decisions.&lt;/p&gt;&lt;p&gt;This week, Nokia and AWS presented a new network slicing system that uses AI agents to monitor network conditions and adjust resources automatically. The setup is being tested by telecom operators du in the United Arab Emirates and Orange in Europe and Africa, according to a joint announcement from Nokia.&lt;/p&gt;&lt;h3&gt;Adaptive AI-driven networks&lt;/h3&gt;&lt;p&gt;Network slicing lets operators create multiple virtual networks on the same physical infrastructure, each tuned for a different purpose. For example, a slice may be configured for emergency services or high-bandwidth consumer traffic. While slicing is part of the 5G standard, it has often required manual planning and fixed configurations, which limits how quickly networks can respond to changing demand.&lt;/p&gt;&lt;p&gt;The new system aims to close that gap by introducing AI agents that track network performance indicators like latency and congestion, and consider data like event schedules or weather conditions. Agents can then adjust network settings to keep services running to agreed performance levels, according to Nokia’s description of the pilot.&lt;/p&gt;&lt;p&gt;AWS said the solution combines Nokia’s slicing and automation tools with AI models delivered through Amazon Bedrock, its managed AI service platform. The companies describe the approach as “agentic AI”.&lt;/p&gt;&lt;h3&gt;Autonomous connectivity&lt;/h3&gt;&lt;p&gt;The interest in such systems reflects a long-standing challenge: 5G networks have delivered higher speeds and lower latency, but operators have struggled to turn those technical gains into new revenue streams. Research firm GSMA Intelligence notes many operators view network slicing as a potential source of enterprise income, though adoption has been slow due to operational complexity and uncertain demand.&lt;/p&gt;&lt;p&gt;If networks can adapt quickly to sudden demand, like a crowded stadium or emergency responders entering a disaster area, operators may be able to offer temporary connectivity or guaranteed service levels without manual setup.&lt;/p&gt;&lt;p&gt;Orange has said previously enterprise customers expect connectivity to behave more like cloud computing, where resources can scale on demand. Systems that allow automated control of network resources could help move telecom services closer to that model.&lt;/p&gt;&lt;h3&gt;Cloud platforms and telecom network operations&lt;/h3&gt;&lt;p&gt;The tests also highlight how cloud providers are getting involved in telecom operations. Over the past few years, some operators have moved parts of their core networks onto public cloud platforms or built cloud-based control systems. Industry analysts at Dell’Oro Group report that telecom cloud spending is rising as operators modernise networks and adopt software-driven infrastructure.&lt;/p&gt;&lt;p&gt;Adding AI-driven control loops on top of cloud platforms represents the next step, with AI systems monitoring conditions and applying adjustments quickly.&lt;/p&gt;&lt;p&gt;The technology remains in a testing phase. Nokia’s announcement described the work with Orange as demonstrations and pilots rollouts. Questions remain about how such systems can be deployed, how operators will supervise automated decisions, and how regulators will view AI control of critical communication infrastructure.&lt;/p&gt;&lt;p&gt;Telecom networks carry important traffic so reliability and accountability remain central concerns. Operators typically introduce automation gradually, keeping human oversight in place while validating system behaviour under real conditions.&lt;/p&gt;&lt;p&gt;The experiments suggest that AI is beginning to function as operational controller, adjusting physical and virtual resources in response to live events.&lt;/p&gt;&lt;p&gt;Enterprises that rely on private 5G networks for factories or large venues may gain access to connectivity that adjusts automatically. That could influence how businesses design applications that depend on stable, predictable network performance.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by M. Rennim)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: How e&amp;amp; is using HR to bring AI into enterprise operations&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Nokia-and-AWS-pilot-AI-automation-for-real-time-5G-network-slicing-scaled-e1771989468794.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Telecom networks may soon begin adjusting themselves in real time, as operators test systems that allow AI agents to manage traffic and service quality. AI may soon be making operational decisions.&lt;/p&gt;&lt;p&gt;This week, Nokia and AWS presented a new network slicing system that uses AI agents to monitor network conditions and adjust resources automatically. The setup is being tested by telecom operators du in the United Arab Emirates and Orange in Europe and Africa, according to a joint announcement from Nokia.&lt;/p&gt;&lt;h3&gt;Adaptive AI-driven networks&lt;/h3&gt;&lt;p&gt;Network slicing lets operators create multiple virtual networks on the same physical infrastructure, each tuned for a different purpose. For example, a slice may be configured for emergency services or high-bandwidth consumer traffic. While slicing is part of the 5G standard, it has often required manual planning and fixed configurations, which limits how quickly networks can respond to changing demand.&lt;/p&gt;&lt;p&gt;The new system aims to close that gap by introducing AI agents that track network performance indicators like latency and congestion, and consider data like event schedules or weather conditions. Agents can then adjust network settings to keep services running to agreed performance levels, according to Nokia’s description of the pilot.&lt;/p&gt;&lt;p&gt;AWS said the solution combines Nokia’s slicing and automation tools with AI models delivered through Amazon Bedrock, its managed AI service platform. The companies describe the approach as “agentic AI”.&lt;/p&gt;&lt;h3&gt;Autonomous connectivity&lt;/h3&gt;&lt;p&gt;The interest in such systems reflects a long-standing challenge: 5G networks have delivered higher speeds and lower latency, but operators have struggled to turn those technical gains into new revenue streams. Research firm GSMA Intelligence notes many operators view network slicing as a potential source of enterprise income, though adoption has been slow due to operational complexity and uncertain demand.&lt;/p&gt;&lt;p&gt;If networks can adapt quickly to sudden demand, like a crowded stadium or emergency responders entering a disaster area, operators may be able to offer temporary connectivity or guaranteed service levels without manual setup.&lt;/p&gt;&lt;p&gt;Orange has said previously enterprise customers expect connectivity to behave more like cloud computing, where resources can scale on demand. Systems that allow automated control of network resources could help move telecom services closer to that model.&lt;/p&gt;&lt;h3&gt;Cloud platforms and telecom network operations&lt;/h3&gt;&lt;p&gt;The tests also highlight how cloud providers are getting involved in telecom operations. Over the past few years, some operators have moved parts of their core networks onto public cloud platforms or built cloud-based control systems. Industry analysts at Dell’Oro Group report that telecom cloud spending is rising as operators modernise networks and adopt software-driven infrastructure.&lt;/p&gt;&lt;p&gt;Adding AI-driven control loops on top of cloud platforms represents the next step, with AI systems monitoring conditions and applying adjustments quickly.&lt;/p&gt;&lt;p&gt;The technology remains in a testing phase. Nokia’s announcement described the work with Orange as demonstrations and pilots rollouts. Questions remain about how such systems can be deployed, how operators will supervise automated decisions, and how regulators will view AI control of critical communication infrastructure.&lt;/p&gt;&lt;p&gt;Telecom networks carry important traffic so reliability and accountability remain central concerns. Operators typically introduce automation gradually, keeping human oversight in place while validating system behaviour under real conditions.&lt;/p&gt;&lt;p&gt;The experiments suggest that AI is beginning to function as operational controller, adjusting physical and virtual resources in response to live events.&lt;/p&gt;&lt;p&gt;Enterprises that rely on private 5G networks for factories or large venues may gain access to connectivity that adjusts automatically. That could influence how businesses design applications that depend on stable, predictable network performance.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by M. Rennim)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: How e&amp;amp; is using HR to bring AI into enterprise operations&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/nokia-and-aws-pilot-ai-automation-for-real-time-5g-network-slicing/</guid><pubDate>Wed, 25 Feb 2026 10:00:00 +0000</pubDate></item><item><title>Now is a good time for doing crime (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/25/1132840/editors-letter-march-2026/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/mask-thumb.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Eons ago, in 2012, I had a weird experience. My iPhone suddenly shut down. When I restarted it, I found it was totally reset—clean, like a new device. This was the early days of iOS, so I wasn’t too concerned until I went to connect it to my computer to restore it from a backup. But when I flipped open the lid of my laptop, it too was mid-restart. And then, suddenly, the screen went gray. It was being remotely wiped. I turned on my iPad. It, too, had been wiped. I was being hacked.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Frantically, I shut down all my devices, unplugged everything connected to the internet in my house, turned off my router, and went next door to use my neighbors’ computer and find out what was going on. Deepening my panic, I realized hackers had also gained control of, and nuked, my Google account. Worse, they were in control of my Twitter, which they were gleefully using to spew all sorts of vile comments. It was nasty.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;You have to remember, this was before all of us lived with a constant rain of text messages and emails designed to elicit the information necessary to pull something like this off. These crooks hadn’t brute-forced their way in, or used any sort of sophisticated techniques to gain access to my accounts. Instead, they had relied on publicly available information, and a fake credit card number, to socially engineer their way into my Amazon account, where they looked up the last four digits of my real credit card number. Then they used that information to get into Apple. And because that account was linked to my Gmail, and that to my Twitter, it gave them the keys to everything.&lt;/p&gt;  &lt;p&gt;But what really troubled me was what I learned as I followed up on my hack over the ensuing weeks and months: This kind of thing was, while still novel, becoming more common. Some version of what happened to me had happened to lots of other people. The kids who were responsible—it was a couple of kids—weren’t criminal masterminds. They had just found a gap, a place where a technology was now commonplace but its risks and exploitable surface areas weren’t yet fully understood. I just happened to have all my stuff in the gap. Today that gap might feature a crypto wallet or a deepfake of a loved one’s voice. (Or both.)&lt;/p&gt; 
 &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;Crime changes.&lt;/p&gt;    &lt;p&gt;The goals stay the same—pursuit of value, pursuit of power—but new technologies create new vulnerabilities, new tactics, and new ways for perpetrators to evade discovery or capture. And the law necessarily lags behind. Relying not on innovation but on precedent, it is intentionally backward-looking and slow. That plodding consideration used to be how we protected our shared democratic society, how we protected each other from each other.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;But those same new technologies that have allowed crime to outpace law have also reenergized law enforcement and government—offering new ways to root out crime, to gather evidence, to surveil people. Think, for example, of how cold-case investigators tracked down the Golden State Killer years after his murders, using DNA samples and genealogy databases—launching a new era of DNA-powered investigations.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Technology has long made crime and its prosecution a game of cat and mouse. It sometimes calls into question the nature of crime itself. Unregulated behaviors, facilitated by technology, can exist in murky zones of dubious legality. (Until TikTok announced its new ownership structure, Apple and Google were both technically breaking the law by allowing the app to stay on their platforms, under the provisions of the Protecting Americans from Foreign Adversary Controlled Applications Act. Ah! Well. Nevertheless.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt;&lt;p&gt;That tension is the key to our March/April issue. Thanks to technologies like cryptocurrency and off-the-shelf autonomous autopilots, there’s never been a better time to do crime. Thanks to pervasive surveillance and digital infrastructure, there’s never been a better time to fight it—sometimes at the expense of what we used to think of as fundamental civil rights.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I never pressed charges against the kids who hacked me. The biggest consequence of the hack was that Apple set up two-factor authentication in the following months, which felt like a win. Now I’m not sure anyone expects their personal data to be secure in any meaningful way. I’m certain, though, that somewhere on the net, a new generation of kids is coming up with another novel crime.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/mask-thumb.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Eons ago, in 2012, I had a weird experience. My iPhone suddenly shut down. When I restarted it, I found it was totally reset—clean, like a new device. This was the early days of iOS, so I wasn’t too concerned until I went to connect it to my computer to restore it from a backup. But when I flipped open the lid of my laptop, it too was mid-restart. And then, suddenly, the screen went gray. It was being remotely wiped. I turned on my iPad. It, too, had been wiped. I was being hacked.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Frantically, I shut down all my devices, unplugged everything connected to the internet in my house, turned off my router, and went next door to use my neighbors’ computer and find out what was going on. Deepening my panic, I realized hackers had also gained control of, and nuked, my Google account. Worse, they were in control of my Twitter, which they were gleefully using to spew all sorts of vile comments. It was nasty.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;You have to remember, this was before all of us lived with a constant rain of text messages and emails designed to elicit the information necessary to pull something like this off. These crooks hadn’t brute-forced their way in, or used any sort of sophisticated techniques to gain access to my accounts. Instead, they had relied on publicly available information, and a fake credit card number, to socially engineer their way into my Amazon account, where they looked up the last four digits of my real credit card number. Then they used that information to get into Apple. And because that account was linked to my Gmail, and that to my Twitter, it gave them the keys to everything.&lt;/p&gt;  &lt;p&gt;But what really troubled me was what I learned as I followed up on my hack over the ensuing weeks and months: This kind of thing was, while still novel, becoming more common. Some version of what happened to me had happened to lots of other people. The kids who were responsible—it was a couple of kids—weren’t criminal masterminds. They had just found a gap, a place where a technology was now commonplace but its risks and exploitable surface areas weren’t yet fully understood. I just happened to have all my stuff in the gap. Today that gap might feature a crypto wallet or a deepfake of a loved one’s voice. (Or both.)&lt;/p&gt; 
 &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;Crime changes.&lt;/p&gt;    &lt;p&gt;The goals stay the same—pursuit of value, pursuit of power—but new technologies create new vulnerabilities, new tactics, and new ways for perpetrators to evade discovery or capture. And the law necessarily lags behind. Relying not on innovation but on precedent, it is intentionally backward-looking and slow. That plodding consideration used to be how we protected our shared democratic society, how we protected each other from each other.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;But those same new technologies that have allowed crime to outpace law have also reenergized law enforcement and government—offering new ways to root out crime, to gather evidence, to surveil people. Think, for example, of how cold-case investigators tracked down the Golden State Killer years after his murders, using DNA samples and genealogy databases—launching a new era of DNA-powered investigations.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Technology has long made crime and its prosecution a game of cat and mouse. It sometimes calls into question the nature of crime itself. Unregulated behaviors, facilitated by technology, can exist in murky zones of dubious legality. (Until TikTok announced its new ownership structure, Apple and Google were both technically breaking the law by allowing the app to stay on their platforms, under the provisions of the Protecting Americans from Foreign Adversary Controlled Applications Act. Ah! Well. Nevertheless.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt;&lt;p&gt;That tension is the key to our March/April issue. Thanks to technologies like cryptocurrency and off-the-shelf autonomous autopilots, there’s never been a better time to do crime. Thanks to pervasive surveillance and digital infrastructure, there’s never been a better time to fight it—sometimes at the expense of what we used to think of as fundamental civil rights.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I never pressed charges against the kids who hacked me. The biggest consequence of the hack was that Apple set up two-factor authentication in the following months, which felt like a win. Now I’m not sure anyone expects their personal data to be secure in any meaningful way. I’m certain, though, that somewhere on the net, a new generation of kids is coming up with another novel crime.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/25/1132840/editors-letter-march-2026/</guid><pubDate>Wed, 25 Feb 2026 11:00:00 +0000</pubDate></item><item><title>3 things Juliet Beauchamp is into right now (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/25/1132836/3-things-juliet-beauchamp/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/IMG_8278-3.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;&lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The only reality show that matters&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;&lt;em&gt;The Real Housewives of Salt Lake City&lt;/em&gt; is one of the best shows on television right now. Not one of the best reality TV shows, but one of the best TV shows, period. Chronicling a shifting group of wealthy women in and around Salt Lake, the show has featured a convicted felon whom federal agents came looking for while cameras were rolling, a church leader married to her step-grandfather, and a single mom in an exhausting on-again, off-again relationship with an Osmond. In one season, there was an ongoing argument between two cast members after one told the other that she “smelled like hospital.” Later, one woman was secretly running an anonymous gossip Instagram about her fellow housewives. We can debate the “reality” of reality television, and it’s certainly true that these characters and scenarios are far-fetched. But every single person is dealing with something relatable—difficult marriages, failing businesses, strained relationships with children, addiction. It’s entertainment, and high camp, but I find that I still have a lot of empathy for these people.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The last good place(s) on Facebook&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Facebook sucks. That’s not controversial to say, right? But there is one reason I still have a Facebook account: my neighborhood Buy Nothing group. The spirit of community and camaraderie is alive and well there—and probably in yours, too. A non-exhaustive list of things I have given away: empty candle jars, a bookcase, used lightbulbs, unopened toiletries, bubble wrap. I’ve scored a few good things as well: a gorgeous antique dresser that I refinished, some over-the-door hooks, and brand-new jeans. It makes me happy to know that stuff that would’ve otherwise ended up in a landfill is bringing one of my neighbors joy.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Going analog&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;I used to wear an Apple Watch a lot. I’m a pretty active person, and I liked tracking my workouts and my steps. But after I’d had it for a while, my watch started dying in the middle of a 30-minute run; it became useless to me, and I gave it up completely. Guess what? I’m happier. I feel more present when I’m not checking how much time is left in a yoga class or reading texts during a long run. The amount of data it gathered about me was also stressing me out, and it wasn’t useful. And I don’t need a wearable to tell me how poorly I slept! Trust me, I already know.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/IMG_8278-3.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;&lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The only reality show that matters&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;&lt;em&gt;The Real Housewives of Salt Lake City&lt;/em&gt; is one of the best shows on television right now. Not one of the best reality TV shows, but one of the best TV shows, period. Chronicling a shifting group of wealthy women in and around Salt Lake, the show has featured a convicted felon whom federal agents came looking for while cameras were rolling, a church leader married to her step-grandfather, and a single mom in an exhausting on-again, off-again relationship with an Osmond. In one season, there was an ongoing argument between two cast members after one told the other that she “smelled like hospital.” Later, one woman was secretly running an anonymous gossip Instagram about her fellow housewives. We can debate the “reality” of reality television, and it’s certainly true that these characters and scenarios are far-fetched. But every single person is dealing with something relatable—difficult marriages, failing businesses, strained relationships with children, addiction. It’s entertainment, and high camp, but I find that I still have a lot of empathy for these people.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The last good place(s) on Facebook&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Facebook sucks. That’s not controversial to say, right? But there is one reason I still have a Facebook account: my neighborhood Buy Nothing group. The spirit of community and camaraderie is alive and well there—and probably in yours, too. A non-exhaustive list of things I have given away: empty candle jars, a bookcase, used lightbulbs, unopened toiletries, bubble wrap. I’ve scored a few good things as well: a gorgeous antique dresser that I refinished, some over-the-door hooks, and brand-new jeans. It makes me happy to know that stuff that would’ve otherwise ended up in a landfill is bringing one of my neighbors joy.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Going analog&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;I used to wear an Apple Watch a lot. I’m a pretty active person, and I liked tracking my workouts and my steps. But after I’d had it for a while, my watch started dying in the middle of a 30-minute run; it became useless to me, and I gave it up completely. Guess what? I’m happier. I feel more present when I’m not checking how much time is left in a yoga class or reading texts during a long run. The amount of data it gathered about me was also stressing me out, and it wasn’t useful. And I don’t need a wearable to tell me how poorly I slept! Trust me, I already know.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/25/1132836/3-things-juliet-beauchamp/</guid><pubDate>Wed, 25 Feb 2026 11:00:00 +0000</pubDate></item><item><title>Listen to Earth’s rumbling, secret soundtrack (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/25/1132829/listen-earths-rumbling-secret-soundtrack/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/ei_overhead-sc.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;The boom of a calving glacier. The crackling rumble of a wildfire. The roar of a surging storm front. They’re the noises of the living Earth, music of this one particular sphere and clues to the true nature of these dramatic events. But as loud as all these things are, they emit even more acoustic energy below the threshold of human hearing, at frequencies of 20 hertz or lower. These “infrasounds” have such long wavelengths that they can travel around the globe as churning emanations of distant events. But humans have never been able to hear them.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Until now, that is. &lt;em&gt;Everyday Infrasound in an Uncertain World&lt;/em&gt;, a new album by the musician and artist Brian House, condenses 24 hours of these rumbles into 24 minutes of the most basic of bass lines, putting a new spin on the idea of ambient music. Sound, even infrasound, is really just variations in air pressure. So House built a set of three “macrophones,” tubes that funnel air into a barometer capable of taking readings 100 times a second. From the quiet woods of western Massachusetts, House can pick up what the planet is laying down. Then he speeds the recording up by a factor of 60 so that it’s audible to the wee ears of humans. “I am really interested in the layers of perception that we can’t access,” he says. “It’s not only low sound, but it’s also distant sound. That kind of blew my mind.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;House’s album is art, but scientists made it possible. Barometers picked up the 1883 eruption of the South Pacific volcano Krakatoa as far away as London. And today, a global network of infrasound sensors helps enforce the nuclear test ban treaty. A few infrasound experts—like Leif Karlstrom, a volcanologist at the University of Oregon who uses infrasound to study Mount Kilauea in Hawaii—helped House set up his music-gathering array and better understand what he was hearing. “He’s highlighting interesting phenomena,” Karlstrom says, even though it’s impossible to tell exactly &lt;em&gt;what&lt;/em&gt; is making each specific sound.&amp;nbsp;&lt;/p&gt;      &lt;p&gt;So how’s the actual music? It’s 24 minutes of an otherworldly chorus, alternating between low grumbling vibrations and soft ghostlike whispers. A high-pitched whistle? Could be a train, House says. An intense low-octave rattle? Maybe a distant thunderstorm or a shifting ocean current. “For me, it’s about the mystery of it,” he says. “I hope that’s a little bit unsettling.” But it also might connect someone listening to a wider—and deeper—world.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Monique Brouillette is a freelance writer based in Cambridge, Massachusetts.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/ei_overhead-sc.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;The boom of a calving glacier. The crackling rumble of a wildfire. The roar of a surging storm front. They’re the noises of the living Earth, music of this one particular sphere and clues to the true nature of these dramatic events. But as loud as all these things are, they emit even more acoustic energy below the threshold of human hearing, at frequencies of 20 hertz or lower. These “infrasounds” have such long wavelengths that they can travel around the globe as churning emanations of distant events. But humans have never been able to hear them.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Until now, that is. &lt;em&gt;Everyday Infrasound in an Uncertain World&lt;/em&gt;, a new album by the musician and artist Brian House, condenses 24 hours of these rumbles into 24 minutes of the most basic of bass lines, putting a new spin on the idea of ambient music. Sound, even infrasound, is really just variations in air pressure. So House built a set of three “macrophones,” tubes that funnel air into a barometer capable of taking readings 100 times a second. From the quiet woods of western Massachusetts, House can pick up what the planet is laying down. Then he speeds the recording up by a factor of 60 so that it’s audible to the wee ears of humans. “I am really interested in the layers of perception that we can’t access,” he says. “It’s not only low sound, but it’s also distant sound. That kind of blew my mind.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;House’s album is art, but scientists made it possible. Barometers picked up the 1883 eruption of the South Pacific volcano Krakatoa as far away as London. And today, a global network of infrasound sensors helps enforce the nuclear test ban treaty. A few infrasound experts—like Leif Karlstrom, a volcanologist at the University of Oregon who uses infrasound to study Mount Kilauea in Hawaii—helped House set up his music-gathering array and better understand what he was hearing. “He’s highlighting interesting phenomena,” Karlstrom says, even though it’s impossible to tell exactly &lt;em&gt;what&lt;/em&gt; is making each specific sound.&amp;nbsp;&lt;/p&gt;      &lt;p&gt;So how’s the actual music? It’s 24 minutes of an otherworldly chorus, alternating between low grumbling vibrations and soft ghostlike whispers. A high-pitched whistle? Could be a train, House says. An intense low-octave rattle? Maybe a distant thunderstorm or a shifting ocean current. “For me, it’s about the mystery of it,” he says. “I hope that’s a little bit unsettling.” But it also might connect someone listening to a wider—and deeper—world.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Monique Brouillette is a freelance writer based in Cambridge, Massachusetts.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/25/1132829/listen-earths-rumbling-secret-soundtrack/</guid><pubDate>Wed, 25 Feb 2026 11:00:00 +0000</pubDate></item><item><title>[NEW] The Download: introducing the Crime issue (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/25/1133653/the-download-introducing-the-crime-issue/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Introducing: the Crime issue&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Technology has long made crime and its prosecution a game of cat and mouse. But those same new technologies that have allowed crime to outpace law have also reenergized law enforcement and government—offering new ways to root out crime, to gather evidence, to surveil people.&lt;/p&gt;  &lt;p&gt;That tension is the key to our new March/April issue. Thanks to technologies like cryptocurrency and off-the-shelf autonomous autopilots, there’s never been a better time to do crime. And thanks to pervasive surveillance and digital infrastructure, there’s never been a better time to fight it—sometimes at the expense of what we used to think of as fundamental civil rights.&lt;/p&gt; 
 &lt;p&gt;Here’s a sneak peek at what you can expect:&lt;/p&gt;  &lt;p&gt;+ The fascinating story of what happened when cyber security researcher Allison Nixon decided to track down the mysterious online figures threatening to kill her. Read the full story.&lt;/p&gt;&lt;p&gt;+ AI is already making online crimes easier, but those reports of AI-powered superhacks are seriously overblown. Here’s why.&lt;/p&gt; 
 &lt;p&gt;+ Welcome to the dark side of crypto’s permissionless dream.&lt;/p&gt;  &lt;p&gt;+ Chicago is home to a vast monitoring system to track its residents, including tens of thousands of surveillance cameras. But while law enforcement claims it’s necessary to protect public safety, privacy activists have likened it to a surveillance panopticon. Read the full story.&lt;/p&gt;  &lt;p&gt;+ Modern thieves are stealing luxury cars right from under their manufacturers’ and owners’ noses. But how are they doing it?&lt;/p&gt;&lt;p&gt;+ How uncrewed narco submarines are poised to shake up how drug smugglers attempt to evade law enforcement.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;+ How innovative conservationists are using tech to fight back against wildlife traffickers—including by turning rhinos radioactive.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why 2026 is the year for sodium-ion batteries&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Sodium-based batteries could be a cheaper, safer alternative to lithium-ion, and the technology is finally making its way into cars—and energy storage arrays on the grid.&lt;/p&gt;&lt;p&gt;They’re also one of &lt;em&gt;MIT Technology Review&lt;/em&gt;'s 10 Breakthrough Technologies of 2026, and we’re holding a subscriber-only Roundtables discussion to explain why. Join our science editor Mary Beth Griggs, senior climate reporter Casey Crownhart and China reporter Caiwei Chen to explore the present moment for sodium-ion batteries—and what’s coming next.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We’ll be going live at 1pm ET this afternoon—register now!&lt;/p&gt; 

   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 The Pentagon has given Anthropic an ultimatum&lt;/strong&gt;&lt;br /&gt;Either provide the US military with full access to Claude, or face the consequences. (Axios)&lt;br /&gt;+ &lt;em&gt;Defense Secretary Pete Hegseth has threatened to cut ties. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;In turn, Anthropic has allegedly refused to ease military restrictions. &lt;/em&gt;(Reuters)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Meta has signed a major chip deal with AMD&lt;/strong&gt;&lt;br /&gt;Just days after it committed to using millions of Nvidia chips to power its AI ambitions. (CNBC)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;3 How Jeffrey Epstein infiltrated Microsoft’s upper ranks&lt;/strong&gt;&lt;br /&gt;He was privy to confidential insider discussions about internal politics and gave advice on the line of CEO succession. (NYT $)&lt;br /&gt;+ &lt;em&gt;A smash-hit podcast about the Epstein files is entirely AI-generated. &lt;/em&gt;(Fast Company $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Chatbot-assisted cheating is just a part of student life&lt;br /&gt;&lt;/strong&gt;Teenagers are regularly asking for—and may grow dependent on—AI’s assistance. (WP $)&lt;br /&gt;+ &lt;em&gt;You need to talk to your kid about AI. Here are 6 things you should say. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 How Ukraine built an entire drone industry from scratch&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;And hopes to sell its expertise to Western allies once the war is over. (New Scientist $)&lt;br /&gt;+ &lt;em&gt;Europe’s drone-filled vision for the future of war. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;6 The FDA has removed a warning against ineffective autism treatments&lt;br /&gt;The page urged Americans not to fall for alternative remedies including chlorine dioxide. (Undark)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Solar power is going from strength to strength in the US&lt;br /&gt;&lt;/strong&gt;Usage was up 35% last year in comparison to the previous year. (Ars Technica)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 How big is infinity?&lt;br /&gt;&lt;/strong&gt;Maybe one size doesn’t fit all. (Quanta Magazine)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Warning: someone near you is wearing smartglasses&lt;/strong&gt;&lt;br /&gt;That’s the premise behind new app Nearby Glasses, which detects the devices’ Bluetooth signals. (404 Media)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;10 Uber employees run ideas past an AI version of their CEO&lt;br /&gt;&lt;/strong&gt;Very good, very normal. (Insider $)&lt;br /&gt;+ &lt;em&gt;Synthesia’s AI clones are more expressive than ever. Soon they’ll be able to talk back. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“This has nothing to do with mass surveillance and autonomous weapons being used.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—A senior defense official tells the Washington Post that the Pentagon hasn’t proposed using any of Anthropic’s AI tools in ways that aren’t lawful, after the department threatened to force the company to share its technology.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1133655" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/image_aecdfe.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;These scientists are working to extend the life span of pet dogs—and their owners&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Matt Kaeberlein is what you might call a dog person. He has grown up with dogs and describes his German shepherd, Dobby, as “really special.” But Dobby is 14 years old—around 98 in dog years.&lt;/p&gt;&lt;p&gt;Kaeberlein is co-director of the Dog Aging Project, an ambitious research effort to track the aging process of tens of thousands of companion dogs across the US. He is one of a handful of scientists on a mission to improve, delay, and possibly reverse that process to help them live longer, healthier lives.&lt;/p&gt;&lt;p&gt;And dogs are just the beginning. One day, this research could help to prolong the lives of humans. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;   
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ As if dinosaur eggs weren’t cool enough, it turns out they’re also a pretty handy aging indicator for other fossils.&lt;br /&gt;+ This week would have marked Steve Jobs’ 71st birthday. His Stanford Commencement Address is still one of the best.&lt;br /&gt;+ I need to play &lt;em&gt;Capybara Simulator&lt;/em&gt; immediately: a game in which you can become a capybara.&lt;br /&gt;+ Good news everyone—it looks like we’ve avoided a bananapocalypse 🍌&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Introducing: the Crime issue&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Technology has long made crime and its prosecution a game of cat and mouse. But those same new technologies that have allowed crime to outpace law have also reenergized law enforcement and government—offering new ways to root out crime, to gather evidence, to surveil people.&lt;/p&gt;  &lt;p&gt;That tension is the key to our new March/April issue. Thanks to technologies like cryptocurrency and off-the-shelf autonomous autopilots, there’s never been a better time to do crime. And thanks to pervasive surveillance and digital infrastructure, there’s never been a better time to fight it—sometimes at the expense of what we used to think of as fundamental civil rights.&lt;/p&gt; 
 &lt;p&gt;Here’s a sneak peek at what you can expect:&lt;/p&gt;  &lt;p&gt;+ The fascinating story of what happened when cyber security researcher Allison Nixon decided to track down the mysterious online figures threatening to kill her. Read the full story.&lt;/p&gt;&lt;p&gt;+ AI is already making online crimes easier, but those reports of AI-powered superhacks are seriously overblown. Here’s why.&lt;/p&gt; 
 &lt;p&gt;+ Welcome to the dark side of crypto’s permissionless dream.&lt;/p&gt;  &lt;p&gt;+ Chicago is home to a vast monitoring system to track its residents, including tens of thousands of surveillance cameras. But while law enforcement claims it’s necessary to protect public safety, privacy activists have likened it to a surveillance panopticon. Read the full story.&lt;/p&gt;  &lt;p&gt;+ Modern thieves are stealing luxury cars right from under their manufacturers’ and owners’ noses. But how are they doing it?&lt;/p&gt;&lt;p&gt;+ How uncrewed narco submarines are poised to shake up how drug smugglers attempt to evade law enforcement.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;+ How innovative conservationists are using tech to fight back against wildlife traffickers—including by turning rhinos radioactive.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why 2026 is the year for sodium-ion batteries&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Sodium-based batteries could be a cheaper, safer alternative to lithium-ion, and the technology is finally making its way into cars—and energy storage arrays on the grid.&lt;/p&gt;&lt;p&gt;They’re also one of &lt;em&gt;MIT Technology Review&lt;/em&gt;'s 10 Breakthrough Technologies of 2026, and we’re holding a subscriber-only Roundtables discussion to explain why. Join our science editor Mary Beth Griggs, senior climate reporter Casey Crownhart and China reporter Caiwei Chen to explore the present moment for sodium-ion batteries—and what’s coming next.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We’ll be going live at 1pm ET this afternoon—register now!&lt;/p&gt; 

   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 The Pentagon has given Anthropic an ultimatum&lt;/strong&gt;&lt;br /&gt;Either provide the US military with full access to Claude, or face the consequences. (Axios)&lt;br /&gt;+ &lt;em&gt;Defense Secretary Pete Hegseth has threatened to cut ties. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;In turn, Anthropic has allegedly refused to ease military restrictions. &lt;/em&gt;(Reuters)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Meta has signed a major chip deal with AMD&lt;/strong&gt;&lt;br /&gt;Just days after it committed to using millions of Nvidia chips to power its AI ambitions. (CNBC)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;3 How Jeffrey Epstein infiltrated Microsoft’s upper ranks&lt;/strong&gt;&lt;br /&gt;He was privy to confidential insider discussions about internal politics and gave advice on the line of CEO succession. (NYT $)&lt;br /&gt;+ &lt;em&gt;A smash-hit podcast about the Epstein files is entirely AI-generated. &lt;/em&gt;(Fast Company $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Chatbot-assisted cheating is just a part of student life&lt;br /&gt;&lt;/strong&gt;Teenagers are regularly asking for—and may grow dependent on—AI’s assistance. (WP $)&lt;br /&gt;+ &lt;em&gt;You need to talk to your kid about AI. Here are 6 things you should say. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 How Ukraine built an entire drone industry from scratch&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;And hopes to sell its expertise to Western allies once the war is over. (New Scientist $)&lt;br /&gt;+ &lt;em&gt;Europe’s drone-filled vision for the future of war. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;6 The FDA has removed a warning against ineffective autism treatments&lt;br /&gt;The page urged Americans not to fall for alternative remedies including chlorine dioxide. (Undark)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Solar power is going from strength to strength in the US&lt;br /&gt;&lt;/strong&gt;Usage was up 35% last year in comparison to the previous year. (Ars Technica)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 How big is infinity?&lt;br /&gt;&lt;/strong&gt;Maybe one size doesn’t fit all. (Quanta Magazine)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Warning: someone near you is wearing smartglasses&lt;/strong&gt;&lt;br /&gt;That’s the premise behind new app Nearby Glasses, which detects the devices’ Bluetooth signals. (404 Media)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;10 Uber employees run ideas past an AI version of their CEO&lt;br /&gt;&lt;/strong&gt;Very good, very normal. (Insider $)&lt;br /&gt;+ &lt;em&gt;Synthesia’s AI clones are more expressive than ever. Soon they’ll be able to talk back. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“This has nothing to do with mass surveillance and autonomous weapons being used.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—A senior defense official tells the Washington Post that the Pentagon hasn’t proposed using any of Anthropic’s AI tools in ways that aren’t lawful, after the department threatened to force the company to share its technology.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1133655" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/image_aecdfe.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;These scientists are working to extend the life span of pet dogs—and their owners&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Matt Kaeberlein is what you might call a dog person. He has grown up with dogs and describes his German shepherd, Dobby, as “really special.” But Dobby is 14 years old—around 98 in dog years.&lt;/p&gt;&lt;p&gt;Kaeberlein is co-director of the Dog Aging Project, an ambitious research effort to track the aging process of tens of thousands of companion dogs across the US. He is one of a handful of scientists on a mission to improve, delay, and possibly reverse that process to help them live longer, healthier lives.&lt;/p&gt;&lt;p&gt;And dogs are just the beginning. One day, this research could help to prolong the lives of humans. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;   
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ As if dinosaur eggs weren’t cool enough, it turns out they’re also a pretty handy aging indicator for other fossils.&lt;br /&gt;+ This week would have marked Steve Jobs’ 71st birthday. His Stanford Commencement Address is still one of the best.&lt;br /&gt;+ I need to play &lt;em&gt;Capybara Simulator&lt;/em&gt; immediately: a game in which you can become a capybara.&lt;br /&gt;+ Good news everyone—it looks like we’ve avoided a bananapocalypse 🍌&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/25/1133653/the-download-introducing-the-crime-issue/</guid><pubDate>Wed, 25 Feb 2026 13:10:00 +0000</pubDate></item><item><title>[NEW] Adobe Firefly’s video editor can now automatically create a first draft from footage (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/25/adobe-fireflys-video-editor-can-now-automatically-create-a-first-draft-from-footage/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/QuickCut4.jpeg?resize=1200,522" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The video editor in Adobe Firefly is getting a new feature called Quick Cut that uses AI to edit footage and B-roll to create a first draft of the final video based on user instructions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Typically, you have to upload your footage and B-roll into a video editor, and manually arrange transitions. With Quick Cut, users can describe what they want the video to be in natural language, and the tool will automatically edit out irrelevant parts of the footage, and put together the different takes while using appropriate footage to make transitions between cuts.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Users can also pick frames from the B-roll and use one of the video models available within Firefly to create short transitions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You can use the prompt box within the Firefly video editor to specify settings like aspect ratio and pacing between transitions, or add optional B-roll footage. Users can apply Quick Cut to the entire project, a particular timeline, or selected clips.&lt;/p&gt;

&lt;div class="jwppp-video-box" id="jwppp-video-box-30966941"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Adobe stressed that the aim of Quick Cut is to deliver a first draft, so editors will still need to adjust elements, paste takes together, and work on transitions to put together the video.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we talk to our users, who are creators and marketers, the biggest problem they actually communicate is the need for fast turnaround, the need for time-saving techniques that just let them get to their creative vision as fast as possible,” Mike Folgner, product lead for AI and next-generation video tools, told TechCrunch.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“One thing we do know is that some of the mundane parts that come with video [editing], like just getting the selects in order, that’s not really where they find joy and difference. They find joy in putting their spin on it. So Quick Cut is meant to help creators who have a set of media find the story very quickly and just get to a story cut as fast as possible,” he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adobe has been pushing regular updates to its video-related tools. In December, it rolled out a new timeline-based video editor that brought layers and prompt-based editing — the editor treats different objects as layers and allows you to edit them using prompts, or use tools like resize and rotate. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also added prompt-based editing capabilities to Firefly, letting users tell the video model how to edit video elements, colors, and camera angles, as well as a timeline view that lets you adjust frames, sounds, and other characteristics easily.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/QuickCut4.jpeg?resize=1200,522" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The video editor in Adobe Firefly is getting a new feature called Quick Cut that uses AI to edit footage and B-roll to create a first draft of the final video based on user instructions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Typically, you have to upload your footage and B-roll into a video editor, and manually arrange transitions. With Quick Cut, users can describe what they want the video to be in natural language, and the tool will automatically edit out irrelevant parts of the footage, and put together the different takes while using appropriate footage to make transitions between cuts.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Users can also pick frames from the B-roll and use one of the video models available within Firefly to create short transitions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You can use the prompt box within the Firefly video editor to specify settings like aspect ratio and pacing between transitions, or add optional B-roll footage. Users can apply Quick Cut to the entire project, a particular timeline, or selected clips.&lt;/p&gt;

&lt;div class="jwppp-video-box" id="jwppp-video-box-30966941"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Adobe stressed that the aim of Quick Cut is to deliver a first draft, so editors will still need to adjust elements, paste takes together, and work on transitions to put together the video.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we talk to our users, who are creators and marketers, the biggest problem they actually communicate is the need for fast turnaround, the need for time-saving techniques that just let them get to their creative vision as fast as possible,” Mike Folgner, product lead for AI and next-generation video tools, told TechCrunch.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“One thing we do know is that some of the mundane parts that come with video [editing], like just getting the selects in order, that’s not really where they find joy and difference. They find joy in putting their spin on it. So Quick Cut is meant to help creators who have a set of media find the story very quickly and just get to a story cut as fast as possible,” he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adobe has been pushing regular updates to its video-related tools. In December, it rolled out a new timeline-based video editor that brought layers and prompt-based editing — the editor treats different objects as layers and allows you to edit them using prompts, or use tools like resize and rotate. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also added prompt-based editing capabilities to Firefly, letting users tell the video model how to edit video elements, colors, and camera angles, as well as a timeline view that lets you adjust frames, sounds, and other characteristics easily.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/25/adobe-fireflys-video-editor-can-now-automatically-create-a-first-draft-from-footage/</guid><pubDate>Wed, 25 Feb 2026 14:00:00 +0000</pubDate></item><item><title>[NEW] Jira’s latest update allows AI agents and humans to work side by side (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/25/jiras-latest-update-allows-ai-agents-and-humans-to-work-side-by-side/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/10/atlassian-confluence-bug-getty.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Enterprise software giant Atlassian is rolling out a new way for humans and AI agents to work together that it hopes will help teams produce “10x the work without 10x the chaos.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Atlassian announced “agents in Jira” on Wednesday. This update gives users of the company’s project management software Jira the ability to assign and manage work for their digital agents from the same dashboard they use for their human employees.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Agents in Jira allows enterprises to assign tasks and tickets to AI agents, just as they would to people. It also tracks how the work is coming along, and sets deadlines, among other metrics. Users can now also loop in AI agents during the middle of an existing project too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This feature is now available in open beta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This update is meant to give users the same visibility into the work their agents are doing as their human employees, Tamar Yehoshua, Atlassian’s new chief product and AI officer, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Atlassian has been in the business, for decades, of collaboration software helping people get work done,” Yehoshua said. “Now, you enter agents, and agents are now doing a lot of that work, and so you want to be able to coordinate between humans and agents.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Atlassian understands that just giving people more avenues to automate doesn’t necessarily mean less work, Yehoshua said. That’s why the key part of this update is that everything happens within the same dashboard, she said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“You’ve been hearing in the zeitgeist lately that all of these agents are creating more work for people, and in some ways, more chaos,” Yehoshua said. “What we’re really good at is putting order to that chaos.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As enterprises continue to figure out how and where they can find a return on investment from investing in AI tools, this kind of view could prove beneficial. The ability to compare the work of agents versus humans on the same project could help enterprises figure out where to deploy agents to begin with and what tasks should remain human-led.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This announcement is just the first of many, Yehoshua said, as the company looks to increasingly add AI tools into its existing software products.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The goal is to enable people to work more productively with AI and I think this is a step,” Yehoshua said. “It’s only the beginning of the journey. It’s a long journey, but this is a really important step of how to integrate AI into the workflows that you already have, which I’m really excited about.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/10/atlassian-confluence-bug-getty.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Enterprise software giant Atlassian is rolling out a new way for humans and AI agents to work together that it hopes will help teams produce “10x the work without 10x the chaos.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Atlassian announced “agents in Jira” on Wednesday. This update gives users of the company’s project management software Jira the ability to assign and manage work for their digital agents from the same dashboard they use for their human employees.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Agents in Jira allows enterprises to assign tasks and tickets to AI agents, just as they would to people. It also tracks how the work is coming along, and sets deadlines, among other metrics. Users can now also loop in AI agents during the middle of an existing project too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This feature is now available in open beta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This update is meant to give users the same visibility into the work their agents are doing as their human employees, Tamar Yehoshua, Atlassian’s new chief product and AI officer, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Atlassian has been in the business, for decades, of collaboration software helping people get work done,” Yehoshua said. “Now, you enter agents, and agents are now doing a lot of that work, and so you want to be able to coordinate between humans and agents.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Atlassian understands that just giving people more avenues to automate doesn’t necessarily mean less work, Yehoshua said. That’s why the key part of this update is that everything happens within the same dashboard, she said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“You’ve been hearing in the zeitgeist lately that all of these agents are creating more work for people, and in some ways, more chaos,” Yehoshua said. “What we’re really good at is putting order to that chaos.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As enterprises continue to figure out how and where they can find a return on investment from investing in AI tools, this kind of view could prove beneficial. The ability to compare the work of agents versus humans on the same project could help enterprises figure out where to deploy agents to begin with and what tasks should remain human-led.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This announcement is just the first of many, Yehoshua said, as the company looks to increasingly add AI tools into its existing software products.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The goal is to enable people to work more productively with AI and I think this is a step,” Yehoshua said. “It’s only the beginning of the journey. It’s a long journey, but this is a really important step of how to integrate AI into the workflows that you already have, which I’m really excited about.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/25/jiras-latest-update-allows-ai-agents-and-humans-to-work-side-by-side/</guid><pubDate>Wed, 25 Feb 2026 14:00:00 +0000</pubDate></item><item><title>[NEW] Pete Hegseth tells Anthropic to fall in line with DoD desires, or else (AI - Ars Technica)</title><link>https://arstechnica.com/ai/2026/02/pete-hegseth-wants-unfettered-access-to-anthropics-models-for-the-military/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        CEO was summoned to Washington after trying to limit military use of its technology.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/hegseth-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/hegseth-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Defence secretary Pete Hegseth is negotiating with AI labs, including Google, OpenAI and xAI, to integrate their technology into classified military systems.


              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Al Drago/Bloomberg

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;US Defense Secretary Pete Hegseth has threatened to cut Anthropic from his department’s supply chain unless it agrees to sign off on its technology being used in all lawful military applications by Friday.&lt;/p&gt;
&lt;p&gt;The threat is the latest escalation in a feud between Anthropic and the department, triggered by the AI group’s refusal to give unfettered access to its models for classified military use, including domestic surveillance and deadly missions with no direct human control.&lt;/p&gt;
&lt;p&gt;Hegseth summoned Anthropic chief executive Dario Amodei to Washington for a meeting on Tuesday. During tense talks, the defense secretary threatened to cut the company out of the department’s supply chain or to invoke the Defense Production Act, a Cold War-era measure enabling the president to control domestic industry in the interest of national defense, said a person with knowledge of the talks.&lt;/p&gt;
&lt;p&gt;Anthropic had until 5:01 pm on Friday “to get on board or not” with Hegseth’s terms, said a senior Pentagon official.&lt;/p&gt;
&lt;p&gt;“If they don’t get on board, [Hegseth] will ensure the Defense Production Act is invoked on Anthropic, compelling them to be used by the Pentagon regardless of if they want to or not,” the official said. The Defense Department would also label Anthropic “a supply chain risk.”&lt;/p&gt;
&lt;p&gt;“You can’t lead tactical ops by exception,” the official added, claiming “this has nothing to do with mass surveillance and autonomous weapons being used.”&lt;/p&gt;
&lt;p&gt;Anthropic said it had continued with “good-faith conversations about our usage policy to ensure Anthropic can continue to support the government’s national security mission in line with what our models can reliably and responsibly do.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The $380 billion start-up could take legal action if Hegseth follows through on his ultimatum, according to people familiar with the matter.&lt;/p&gt;
&lt;p&gt;The disagreement threatens to widen a fault line between the White House and one of the US’s leading AI labs.&lt;/p&gt;
&lt;p&gt;Anthropic has pushed for tighter regulation of AI, and Amodei has repeatedly warned of the risks of the technology. Meanwhile, President Donald Trump and his advisers have promoted a light-touch regulatory framework.&lt;/p&gt;
&lt;p&gt;Trump’s AI tsar, David Sacks, has derided Anthropic as “woke” and last October accused the $380 billion company of “running a sophisticated regulatory capture strategy based on fear-mongering.”&lt;/p&gt;
&lt;p&gt;Those attacks echo criticisms from Elon Musk, who Sacks last year described as “a good friend.” Sacks worked with Musk at PayPal and has invested in xAI and other Musk groups. Sacks divested those positions when he was appointed to his government role.&lt;/p&gt;
&lt;p&gt;But the Pentagon has relied on Anthropic for AI technology. The San Francisco-based company’s Claude tool has until recently been the only model working on classified missions as a result of its partnership with Palantir.&lt;/p&gt;
&lt;p&gt;Hegseth is negotiating with AI labs, including Google, OpenAI, and Elon Musk’s xAI, to replace Anthropic and integrate their technology into classified military systems.&lt;/p&gt;
&lt;p&gt;The senior Pentagon official said Musk’s Grok “is on board with being used in a classified setting, while the rest of the companies are close.”&lt;/p&gt;
&lt;p&gt;Cutting Anthropic from the Pentagon supply chain is an extreme measure typically reserved for companies linked to foreign adversaries. But at the same time, deploying the DPA would suggest Anthropic’s technology is critical to Pentagon operations.&lt;/p&gt;
&lt;p&gt;Invoking the DPA would allow the Pentagon to make use of Anthropic’s tools without an agreement.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The act gives the administration the ability to “allocate materials, services and facilities” for national defense. The Trump and Biden administrations used the act to address a shortage of medical supplies during the coronavirus pandemic, and Trump has also used the DPA to order an increase in the US’s production of critical minerals.&lt;/p&gt;
&lt;p&gt;The Pentagon has pushed for open-ended use of AI technology, aiming to expand the set of tools at its disposal to counter threats and to undertake military operations.&lt;/p&gt;
&lt;p&gt;The department released its AI strategy last month, with Hegseth saying in a memo that “AI-enabled warfare and AI-enabled capability development will redefine the character of military affairs over the next decade.”&lt;/p&gt;
&lt;p&gt;He added the US military “must build on its lead” over foreign adversaries to make soldiers “more lethal and efficient,” and that the AI race was “fueled by the accelerating pace” of innovation coming from the private sector.&lt;/p&gt;
&lt;p&gt;Anthropic has expressed particular concern about its models being used for lethal missions that do not have a human in the loop, arguing that state of the art AI models are not reliable enough to be trusted in those contexts, said people familiar with the negotiations.&lt;/p&gt;
&lt;p&gt;It had also pushed for new rules to govern the use of AI models for mass domestic surveillance, even where that was legal under current regulations, they added.&lt;/p&gt;
&lt;p&gt;A decision to cut Anthropic from the defense department’s supply chain would have significant ramifications for national security work and the company, which has a $200 million contract with the department.&lt;/p&gt;
&lt;p&gt;It would also have an impact on partners, including Palantir, that make use of Anthropic’s models.&lt;/p&gt;
&lt;p&gt;Claude was used in the US capture of Venezuelan leader Nicolás Maduro in January. That mission prompted queries from Anthropic about the exact manner in which its model was used, said people familiar with the matter.&lt;/p&gt;
&lt;p&gt;A person with knowledge of Tuesday’s meeting said Amodei had stressed to Hegseth that his company had never objected to legitimate military operations.&lt;/p&gt;
&lt;p&gt;The Defense Department declined to comment.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2026 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        CEO was summoned to Washington after trying to limit military use of its technology.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/hegseth-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/hegseth-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Defence secretary Pete Hegseth is negotiating with AI labs, including Google, OpenAI and xAI, to integrate their technology into classified military systems.


              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Al Drago/Bloomberg

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;US Defense Secretary Pete Hegseth has threatened to cut Anthropic from his department’s supply chain unless it agrees to sign off on its technology being used in all lawful military applications by Friday.&lt;/p&gt;
&lt;p&gt;The threat is the latest escalation in a feud between Anthropic and the department, triggered by the AI group’s refusal to give unfettered access to its models for classified military use, including domestic surveillance and deadly missions with no direct human control.&lt;/p&gt;
&lt;p&gt;Hegseth summoned Anthropic chief executive Dario Amodei to Washington for a meeting on Tuesday. During tense talks, the defense secretary threatened to cut the company out of the department’s supply chain or to invoke the Defense Production Act, a Cold War-era measure enabling the president to control domestic industry in the interest of national defense, said a person with knowledge of the talks.&lt;/p&gt;
&lt;p&gt;Anthropic had until 5:01 pm on Friday “to get on board or not” with Hegseth’s terms, said a senior Pentagon official.&lt;/p&gt;
&lt;p&gt;“If they don’t get on board, [Hegseth] will ensure the Defense Production Act is invoked on Anthropic, compelling them to be used by the Pentagon regardless of if they want to or not,” the official said. The Defense Department would also label Anthropic “a supply chain risk.”&lt;/p&gt;
&lt;p&gt;“You can’t lead tactical ops by exception,” the official added, claiming “this has nothing to do with mass surveillance and autonomous weapons being used.”&lt;/p&gt;
&lt;p&gt;Anthropic said it had continued with “good-faith conversations about our usage policy to ensure Anthropic can continue to support the government’s national security mission in line with what our models can reliably and responsibly do.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The $380 billion start-up could take legal action if Hegseth follows through on his ultimatum, according to people familiar with the matter.&lt;/p&gt;
&lt;p&gt;The disagreement threatens to widen a fault line between the White House and one of the US’s leading AI labs.&lt;/p&gt;
&lt;p&gt;Anthropic has pushed for tighter regulation of AI, and Amodei has repeatedly warned of the risks of the technology. Meanwhile, President Donald Trump and his advisers have promoted a light-touch regulatory framework.&lt;/p&gt;
&lt;p&gt;Trump’s AI tsar, David Sacks, has derided Anthropic as “woke” and last October accused the $380 billion company of “running a sophisticated regulatory capture strategy based on fear-mongering.”&lt;/p&gt;
&lt;p&gt;Those attacks echo criticisms from Elon Musk, who Sacks last year described as “a good friend.” Sacks worked with Musk at PayPal and has invested in xAI and other Musk groups. Sacks divested those positions when he was appointed to his government role.&lt;/p&gt;
&lt;p&gt;But the Pentagon has relied on Anthropic for AI technology. The San Francisco-based company’s Claude tool has until recently been the only model working on classified missions as a result of its partnership with Palantir.&lt;/p&gt;
&lt;p&gt;Hegseth is negotiating with AI labs, including Google, OpenAI, and Elon Musk’s xAI, to replace Anthropic and integrate their technology into classified military systems.&lt;/p&gt;
&lt;p&gt;The senior Pentagon official said Musk’s Grok “is on board with being used in a classified setting, while the rest of the companies are close.”&lt;/p&gt;
&lt;p&gt;Cutting Anthropic from the Pentagon supply chain is an extreme measure typically reserved for companies linked to foreign adversaries. But at the same time, deploying the DPA would suggest Anthropic’s technology is critical to Pentagon operations.&lt;/p&gt;
&lt;p&gt;Invoking the DPA would allow the Pentagon to make use of Anthropic’s tools without an agreement.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The act gives the administration the ability to “allocate materials, services and facilities” for national defense. The Trump and Biden administrations used the act to address a shortage of medical supplies during the coronavirus pandemic, and Trump has also used the DPA to order an increase in the US’s production of critical minerals.&lt;/p&gt;
&lt;p&gt;The Pentagon has pushed for open-ended use of AI technology, aiming to expand the set of tools at its disposal to counter threats and to undertake military operations.&lt;/p&gt;
&lt;p&gt;The department released its AI strategy last month, with Hegseth saying in a memo that “AI-enabled warfare and AI-enabled capability development will redefine the character of military affairs over the next decade.”&lt;/p&gt;
&lt;p&gt;He added the US military “must build on its lead” over foreign adversaries to make soldiers “more lethal and efficient,” and that the AI race was “fueled by the accelerating pace” of innovation coming from the private sector.&lt;/p&gt;
&lt;p&gt;Anthropic has expressed particular concern about its models being used for lethal missions that do not have a human in the loop, arguing that state of the art AI models are not reliable enough to be trusted in those contexts, said people familiar with the negotiations.&lt;/p&gt;
&lt;p&gt;It had also pushed for new rules to govern the use of AI models for mass domestic surveillance, even where that was legal under current regulations, they added.&lt;/p&gt;
&lt;p&gt;A decision to cut Anthropic from the defense department’s supply chain would have significant ramifications for national security work and the company, which has a $200 million contract with the department.&lt;/p&gt;
&lt;p&gt;It would also have an impact on partners, including Palantir, that make use of Anthropic’s models.&lt;/p&gt;
&lt;p&gt;Claude was used in the US capture of Venezuelan leader Nicolás Maduro in January. That mission prompted queries from Anthropic about the exact manner in which its model was used, said people familiar with the matter.&lt;/p&gt;
&lt;p&gt;A person with knowledge of Tuesday’s meeting said Amodei had stressed to Hegseth that his company had never objected to legitimate military operations.&lt;/p&gt;
&lt;p&gt;The Defense Department declined to comment.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2026 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2026/02/pete-hegseth-wants-unfettered-access-to-anthropics-models-for-the-military/</guid><pubDate>Wed, 25 Feb 2026 14:29:23 +0000</pubDate></item><item><title>[NEW] US tells diplomats to lobby against foreign data sovereignty laws (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/25/us-tells-diplomats-to-lobby-against-foreign-data-sovereignty-laws/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/GettyImages-2252056808.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Trump administration has ordered U.S. diplomats to lobby against countries’ attempts to regulate how American tech companies handle foreigners’ data, arguing that data sovereignty laws threaten the advancement of AI services and technology, Reuters reported, citing an internal diplomatic cable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The cable, signed by U.S. Secretary of State Marco Rubio, says such laws would “disrupt global data flows, increase costs and cybersecurity risks, limit AI and cloud services, and expand government control in ways that can undermine civil liberties and enable censorship,” according to the report.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The cable pushes diplomats to “counter unnecessarily burdensome regulations, such as data localization mandates.” It also orders them to track proposals that would promote data sovereignty laws, and urged diplomats to promote the Global Cross-Border Privacy Rules Forum, an international group that claims to enable “trusted data flows globally through international data protection and privacy certifications.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The order comes as countries around the world increase scrutiny of how Big Tech companies and AI firms are using their citizens’ data. The European Union has led the charge on this front with laws like the GDPR, the Digital Services Act and the AI Act, seeking to curb tech companies’ control and exploitation of user data and hold them accountable. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration has historically opposed such regulatory approaches, and this order reinforces that position as the government seeks to boost U.S. AI companies. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The U.S. State Department did not immediately return a request for comment.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/GettyImages-2252056808.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Trump administration has ordered U.S. diplomats to lobby against countries’ attempts to regulate how American tech companies handle foreigners’ data, arguing that data sovereignty laws threaten the advancement of AI services and technology, Reuters reported, citing an internal diplomatic cable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The cable, signed by U.S. Secretary of State Marco Rubio, says such laws would “disrupt global data flows, increase costs and cybersecurity risks, limit AI and cloud services, and expand government control in ways that can undermine civil liberties and enable censorship,” according to the report.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The cable pushes diplomats to “counter unnecessarily burdensome regulations, such as data localization mandates.” It also orders them to track proposals that would promote data sovereignty laws, and urged diplomats to promote the Global Cross-Border Privacy Rules Forum, an international group that claims to enable “trusted data flows globally through international data protection and privacy certifications.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The order comes as countries around the world increase scrutiny of how Big Tech companies and AI firms are using their citizens’ data. The European Union has led the charge on this front with laws like the GDPR, the Digital Services Act and the AI Act, seeking to curb tech companies’ control and exploitation of user data and hold them accountable. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration has historically opposed such regulatory approaches, and this order reinforces that position as the government seeks to boost U.S. AI companies. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The U.S. State Department did not immediately return a request for comment.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/25/us-tells-diplomats-to-lobby-against-foreign-data-sovereignty-laws/</guid><pubDate>Wed, 25 Feb 2026 14:56:52 +0000</pubDate></item><item><title>[NEW] 3 days left: Save up to $680 on your TechCrunch Disrupt 2026 ticket (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/25/3-days-left-save-up-to-680-on-your-techcrunch-disrupt-2026-ticket/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Time is running out! Just 3 days left before Super Early Bird pricing ends on February 27 at 11:59 p.m. PT. This is your last chance to secure the lowest&amp;nbsp;ticket&amp;nbsp;rates for&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt 2026&lt;/strong&gt;.&amp;nbsp;If 2026 is your year to fundraise, hire, scale, or launch, you cannot afford to miss it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Lock in your pass now&lt;/strong&gt;&amp;nbsp;before prices jump. This is the moment to act.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2026 3 days left" class="wp-image-3094856" height="383" src="https://techcrunch.com/wp-content/uploads/2026/02/TCD26_3Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-to-expect-at-disrupt-2026"&gt;What to expect at Disrupt 2026&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;From&amp;nbsp;October 13–15 at Moscone West&amp;nbsp;in San Francisco, 10,000+ founders, operators, and investors gather for three days of&amp;nbsp;high-signal conversations, deal-making, and actionable insights.&amp;nbsp;Disrupt&amp;nbsp;is&amp;nbsp;not just content&amp;nbsp;—&amp;nbsp;it’s&amp;nbsp;access to accelerated&amp;nbsp;growth.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;You&amp;nbsp;don’t&amp;nbsp;attend&amp;nbsp;&lt;strong&gt;Disrupt&lt;/strong&gt;&amp;nbsp;to sit in the audience. You go to gain leverage. Every session, every conversation, and every connection is designed to accelerate your growth and compound your momentum.&amp;nbsp;You’ll&amp;nbsp;get:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Tactical insights from operators actively building in today’s market.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Direct conversations with investors writing checks.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Exposure to emerging startups before the rest of the market catches up.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Curated networking designed to produce real outcomes.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt Expo Hall" class="wp-image-2571166" height="383" src="https://techcrunch.com/wp-content/uploads/2023/07/expo_hall.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Eric Slomonson, The Photo Group&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-unparalleled-improved-networking-opportunities"&gt;Unparalleled, improved networking opportunities&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, more than 20,000 curated meetings took place on-site. In 2026, upgraded tools will make those connections even more targeted and efficient. One conversation can change your trajectory — and at Disrupt, that is the point.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-valuable-takeaways-straight-nbsp-from-the-nbsp-tech-nbsp-and-vc-nbsp-leaders-nbsp-who-ve-nbsp-shaped-the-ecosystem"&gt;Valuable takeaways straight&amp;nbsp;from the&amp;nbsp;tech&amp;nbsp;and VC&amp;nbsp;leaders&amp;nbsp;who’ve&amp;nbsp;shaped the ecosystem&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt&amp;nbsp;has long been the stage for founders and investors who&amp;nbsp;define eras. Past speakers have included category-defining leaders and top-tier VCs,&amp;nbsp;such as:&lt;/p&gt;





















&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt Stage Aaron Levie," class="wp-image-1899343" height="453" src="https://techcrunch.com/wp-content/uploads/2019/10/GettyImages-1178603646.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Steve Jennings / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In 2025, Disrupt featured 200+ onstage conversations with 250+ tech and VC leaders across AI, hardware,&amp;nbsp;space,&amp;nbsp;startup&amp;nbsp;growth, and venture. Expect the same high-caliber content this year and check the&amp;nbsp;&lt;strong&gt;event page&lt;/strong&gt;&amp;nbsp;as the 2026 agenda rolls out.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-startup-battlefield-nbsp-200-pitch-to-win"&gt;Startup Battlefield&amp;nbsp;200: Pitch to win&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Startup Battlefield&lt;/strong&gt;&amp;nbsp;returns with&amp;nbsp;200 pre-Series A companies&amp;nbsp;competing for&amp;nbsp;$100,000 in equity-free funding, global visibility, and direct investor access. Alumni include&amp;nbsp;Discord, Cloudflare, and Trello.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you want to see&amp;nbsp;what’s&amp;nbsp;next and hear directly from top VCs on scaling&amp;nbsp;a viable&amp;nbsp;startup, the Disrupt&amp;nbsp;Stage is where it happens first.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-discover-nbsp-the-next-breakout-innovation"&gt;Discover&amp;nbsp;the next breakout innovation&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;With&amp;nbsp;&lt;strong&gt;300+ startup exhibitors&lt;/strong&gt;, the venue, especially the Expo Hall, is where deal flow and discovery collide. You&amp;nbsp;won’t&amp;nbsp;just&amp;nbsp;observe&amp;nbsp;trends;&amp;nbsp;you’ll&amp;nbsp;see them before they scale. You’ll be able to:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Explore new&amp;nbsp;tech stacks and tools&amp;nbsp;for your startup.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Identify&amp;nbsp;your next career move, operator, or&amp;nbsp;investor.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Network in a sea of&amp;nbsp;founders, investors, and innovators.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2026 exhibitor" class="wp-image-3088650" height="453" src="https://techcrunch.com/wp-content/uploads/2025/10/Disrupt-2025-exhibitor.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photograpy&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-extend-nbsp-your-connections-nbsp-with-disrupt-week"&gt;Extend&amp;nbsp;your connections&amp;nbsp;with “Disrupt Week”&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;From October 11 to 17, Disrupt Side Events take place across the Bay Area, including breakfasts, cocktail hours, panels, and founder meetups that extend the connections beyond the main stage. The main event is powerful. The surrounding ecosystem makes it even stronger.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-only-3-days-left-lock-in-massive-ticket-savings-now"&gt;Only 3 days left — lock in massive ticket savings&amp;nbsp;now&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Super Early Bird pricing ends this Friday, February 27, at 11:59 p.m. PT. If you want to be in the room where capital moves, companies scale, and ideas turn into breakthroughs, now is the time to lock in your discounted ticket.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Register now&amp;nbsp;before&amp;nbsp;it’s&amp;nbsp;too late.&amp;nbsp;Save up to $680 on your&amp;nbsp;&lt;strong&gt;individual pass&lt;/strong&gt;,&amp;nbsp;or up to&amp;nbsp;30% on&amp;nbsp;&lt;strong&gt;group passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="view at TechCrunch Disrupt 2015" class="wp-image-1356940" height="454" src="https://techcrunch.com/wp-content/uploads/2016/07/21596646922_656ce9d531_k.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Jeff Bottari / Flickr &lt;span class="screen-reader-text"&gt;(opens in a new window)&lt;/span&gt;&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Time is running out! Just 3 days left before Super Early Bird pricing ends on February 27 at 11:59 p.m. PT. This is your last chance to secure the lowest&amp;nbsp;ticket&amp;nbsp;rates for&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt 2026&lt;/strong&gt;.&amp;nbsp;If 2026 is your year to fundraise, hire, scale, or launch, you cannot afford to miss it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Lock in your pass now&lt;/strong&gt;&amp;nbsp;before prices jump. This is the moment to act.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2026 3 days left" class="wp-image-3094856" height="383" src="https://techcrunch.com/wp-content/uploads/2026/02/TCD26_3Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-to-expect-at-disrupt-2026"&gt;What to expect at Disrupt 2026&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;From&amp;nbsp;October 13–15 at Moscone West&amp;nbsp;in San Francisco, 10,000+ founders, operators, and investors gather for three days of&amp;nbsp;high-signal conversations, deal-making, and actionable insights.&amp;nbsp;Disrupt&amp;nbsp;is&amp;nbsp;not just content&amp;nbsp;—&amp;nbsp;it’s&amp;nbsp;access to accelerated&amp;nbsp;growth.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;You&amp;nbsp;don’t&amp;nbsp;attend&amp;nbsp;&lt;strong&gt;Disrupt&lt;/strong&gt;&amp;nbsp;to sit in the audience. You go to gain leverage. Every session, every conversation, and every connection is designed to accelerate your growth and compound your momentum.&amp;nbsp;You’ll&amp;nbsp;get:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Tactical insights from operators actively building in today’s market.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Direct conversations with investors writing checks.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Exposure to emerging startups before the rest of the market catches up.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Curated networking designed to produce real outcomes.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt Expo Hall" class="wp-image-2571166" height="383" src="https://techcrunch.com/wp-content/uploads/2023/07/expo_hall.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Eric Slomonson, The Photo Group&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-unparalleled-improved-networking-opportunities"&gt;Unparalleled, improved networking opportunities&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, more than 20,000 curated meetings took place on-site. In 2026, upgraded tools will make those connections even more targeted and efficient. One conversation can change your trajectory — and at Disrupt, that is the point.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-valuable-takeaways-straight-nbsp-from-the-nbsp-tech-nbsp-and-vc-nbsp-leaders-nbsp-who-ve-nbsp-shaped-the-ecosystem"&gt;Valuable takeaways straight&amp;nbsp;from the&amp;nbsp;tech&amp;nbsp;and VC&amp;nbsp;leaders&amp;nbsp;who’ve&amp;nbsp;shaped the ecosystem&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt&amp;nbsp;has long been the stage for founders and investors who&amp;nbsp;define eras. Past speakers have included category-defining leaders and top-tier VCs,&amp;nbsp;such as:&lt;/p&gt;





















&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt Stage Aaron Levie," class="wp-image-1899343" height="453" src="https://techcrunch.com/wp-content/uploads/2019/10/GettyImages-1178603646.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Steve Jennings / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In 2025, Disrupt featured 200+ onstage conversations with 250+ tech and VC leaders across AI, hardware,&amp;nbsp;space,&amp;nbsp;startup&amp;nbsp;growth, and venture. Expect the same high-caliber content this year and check the&amp;nbsp;&lt;strong&gt;event page&lt;/strong&gt;&amp;nbsp;as the 2026 agenda rolls out.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-startup-battlefield-nbsp-200-pitch-to-win"&gt;Startup Battlefield&amp;nbsp;200: Pitch to win&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Startup Battlefield&lt;/strong&gt;&amp;nbsp;returns with&amp;nbsp;200 pre-Series A companies&amp;nbsp;competing for&amp;nbsp;$100,000 in equity-free funding, global visibility, and direct investor access. Alumni include&amp;nbsp;Discord, Cloudflare, and Trello.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you want to see&amp;nbsp;what’s&amp;nbsp;next and hear directly from top VCs on scaling&amp;nbsp;a viable&amp;nbsp;startup, the Disrupt&amp;nbsp;Stage is where it happens first.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-discover-nbsp-the-next-breakout-innovation"&gt;Discover&amp;nbsp;the next breakout innovation&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;With&amp;nbsp;&lt;strong&gt;300+ startup exhibitors&lt;/strong&gt;, the venue, especially the Expo Hall, is where deal flow and discovery collide. You&amp;nbsp;won’t&amp;nbsp;just&amp;nbsp;observe&amp;nbsp;trends;&amp;nbsp;you’ll&amp;nbsp;see them before they scale. You’ll be able to:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Explore new&amp;nbsp;tech stacks and tools&amp;nbsp;for your startup.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Identify&amp;nbsp;your next career move, operator, or&amp;nbsp;investor.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Network in a sea of&amp;nbsp;founders, investors, and innovators.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2026 exhibitor" class="wp-image-3088650" height="453" src="https://techcrunch.com/wp-content/uploads/2025/10/Disrupt-2025-exhibitor.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photograpy&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-extend-nbsp-your-connections-nbsp-with-disrupt-week"&gt;Extend&amp;nbsp;your connections&amp;nbsp;with “Disrupt Week”&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;From October 11 to 17, Disrupt Side Events take place across the Bay Area, including breakfasts, cocktail hours, panels, and founder meetups that extend the connections beyond the main stage. The main event is powerful. The surrounding ecosystem makes it even stronger.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-only-3-days-left-lock-in-massive-ticket-savings-now"&gt;Only 3 days left — lock in massive ticket savings&amp;nbsp;now&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Super Early Bird pricing ends this Friday, February 27, at 11:59 p.m. PT. If you want to be in the room where capital moves, companies scale, and ideas turn into breakthroughs, now is the time to lock in your discounted ticket.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Register now&amp;nbsp;before&amp;nbsp;it’s&amp;nbsp;too late.&amp;nbsp;Save up to $680 on your&amp;nbsp;&lt;strong&gt;individual pass&lt;/strong&gt;,&amp;nbsp;or up to&amp;nbsp;30% on&amp;nbsp;&lt;strong&gt;group passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="view at TechCrunch Disrupt 2015" class="wp-image-1356940" height="454" src="https://techcrunch.com/wp-content/uploads/2016/07/21596646922_656ce9d531_k.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Jeff Bottari / Flickr &lt;span class="screen-reader-text"&gt;(opens in a new window)&lt;/span&gt;&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/25/3-days-left-save-up-to-680-on-your-techcrunch-disrupt-2026-ticket/</guid><pubDate>Wed, 25 Feb 2026 15:00:00 +0000</pubDate></item><item><title>[NEW] About 12% of US teens turn to AI for emotional support or advice (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/25/about-12-of-u-s-teens-turn-to-ai-for-emotional-support-or-advice/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI chatbots have become embedded in the lives of American teenagers, according to a report published Tuesday by the Pew Research Center. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the most common uses of AI among this demographic include searching for information (57%) and getting help with schoolwork (54%), teens are also using AI to fill roles that would typically be occupied by friends or family. Sixteen percent of U.S. teens say they use AI for casual conversation, while 12% use AI chatbots for emotional support or advice.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Some teens may find solace in talking to chatbots, but mental health professionals are wary. General-purpose tools like ChatGPT, Claude, and Grok are not designed for such uses, and in the most extreme cases, these chatbots can have life-threatening psychological effects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are social creatures, and there’s certainly a challenge that these systems can be isolating,” Dr. Nick Haber, a Stanford professor researching&amp;nbsp;the therapeutic potential of LLMs, told TechCrunch recently. “There are a lot of instances where people can engage with these tools and then can become not grounded to the outside world of facts, and not grounded in connection to the interpersonal, which can lead to pretty isolating — if not worse — effects.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3096680" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/Screenshot-2026-02-24-at-10.50.48-PM.png?w=574" width="574" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pew Research Center&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Pew’s survey also shows a discrepancy between teenagers’ self-reported AI usage and the extent to which their parents think they engage with this technology. About 51% of parents said that their teen uses chatbots, while 64% of teens reported using them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The majority of parents are okay with their teens using AI to search for information (79%) or get help with schoolwork (58%), but far fewer parents approve of their teens using AI chatbots for casual conversation (28%) or to get emotional support or advice (18%). In fact, 58% of parents are not okay with their child using AI for such purposes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI safety is a contentious topic among leading tech companies, to say the least. But one popular chatbot maker, Character.AI, made the choice to disable the chatbot experience for users under the age of 18. This decision followed public outcry and lawsuits filed over two teenagers’ suicides, which took place after prolonged conversations with the company’s chatbots. OpenAI, meanwhile, made the decision to sunset its particularly sycophantic GPT-4o model, which sparked backlash from people who had come to rely on the model for emotional support.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Though a majority of teens use AI chatbots in some way, they have mixed feelings about the impact of this kind of technology on society. When asked how they think AI will impact society over the next 20 years, 31% of teens said the impact would be positive, while 26% said it would be negative. &lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI chatbots have become embedded in the lives of American teenagers, according to a report published Tuesday by the Pew Research Center. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the most common uses of AI among this demographic include searching for information (57%) and getting help with schoolwork (54%), teens are also using AI to fill roles that would typically be occupied by friends or family. Sixteen percent of U.S. teens say they use AI for casual conversation, while 12% use AI chatbots for emotional support or advice.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Some teens may find solace in talking to chatbots, but mental health professionals are wary. General-purpose tools like ChatGPT, Claude, and Grok are not designed for such uses, and in the most extreme cases, these chatbots can have life-threatening psychological effects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are social creatures, and there’s certainly a challenge that these systems can be isolating,” Dr. Nick Haber, a Stanford professor researching&amp;nbsp;the therapeutic potential of LLMs, told TechCrunch recently. “There are a lot of instances where people can engage with these tools and then can become not grounded to the outside world of facts, and not grounded in connection to the interpersonal, which can lead to pretty isolating — if not worse — effects.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3096680" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/Screenshot-2026-02-24-at-10.50.48-PM.png?w=574" width="574" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pew Research Center&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Pew’s survey also shows a discrepancy between teenagers’ self-reported AI usage and the extent to which their parents think they engage with this technology. About 51% of parents said that their teen uses chatbots, while 64% of teens reported using them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The majority of parents are okay with their teens using AI to search for information (79%) or get help with schoolwork (58%), but far fewer parents approve of their teens using AI chatbots for casual conversation (28%) or to get emotional support or advice (18%). In fact, 58% of parents are not okay with their child using AI for such purposes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI safety is a contentious topic among leading tech companies, to say the least. But one popular chatbot maker, Character.AI, made the choice to disable the chatbot experience for users under the age of 18. This decision followed public outcry and lawsuits filed over two teenagers’ suicides, which took place after prolonged conversations with the company’s chatbots. OpenAI, meanwhile, made the decision to sunset its particularly sycophantic GPT-4o model, which sparked backlash from people who had come to rely on the model for emotional support.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Though a majority of teens use AI chatbots in some way, they have mixed feelings about the impact of this kind of technology on society. When asked how they think AI will impact society over the next 20 years, 31% of teens said the impact would be positive, while 26% said it would be negative. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/25/about-12-of-u-s-teens-turn-to-ai-for-emotional-support-or-advice/</guid><pubDate>Wed, 25 Feb 2026 15:52:03 +0000</pubDate></item><item><title>[NEW] Have hard-won scaling lessons to share? Take the stage at TechCrunch Founder Summit 2026 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/25/have-hard-won-scaling-lessons-to-share-take-the-stage-at-techcrunch-founder-summit/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If&amp;nbsp;you’ve&amp;nbsp;built, backed, or&amp;nbsp;operated&amp;nbsp;inside high-growth startups, your experience could shape how the next wave of&amp;nbsp;founders&amp;nbsp;scales.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On&amp;nbsp;June 9 in Boston,&amp;nbsp;&lt;strong&gt;TechCrunch Founder Summit 2026&lt;/strong&gt;&amp;nbsp;will&amp;nbsp;bring together&amp;nbsp;1,000+ founders and investors for a focused day on the realities of growth. We’re inviting seasoned founders, VCs, and startup operators to lead interactive roundtable discussions rooted in real-world execution — the wins, the missteps, and the lessons that only come from doing the work.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Submit your topic by April 17 to be considered.&amp;nbsp;&lt;strong&gt;Learn more and apply today&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Early Stage 2024 roundtable sessions" class="wp-image-2966337" height="454" src="https://techcrunch.com/wp-content/uploads/2025/02/Early-Stage-Roundtable-Sessions-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Halo Creative&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-spark-real-conversation-nbsp-by-leading-a-roundtable-nbsp-or-breakout"&gt;Spark real conversation&amp;nbsp;by leading a roundtable&amp;nbsp;or breakout&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you lead an interactive roundtable or a Q&amp;amp;A-style breakout, every session at Founder Summit is built for depth. Each is a 30-minute, discussion-driven conversation led by two to four speakers, depending on format. No slides. No polished decks. Just candid insight and practical takeaways founders can apply&amp;nbsp;immediately.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If&amp;nbsp;you’ve&amp;nbsp;scaled revenue from zero to $50 million, navigated a difficult fundraise, rebuilt a team after hypergrowth, expanded internationally, or redefined your go-to-market strategy, this&amp;nbsp;is the room to share what&amp;nbsp;actually works.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Founder Summit breakout audience" class="wp-image-3086922" height="453" src="https://techcrunch.com/wp-content/uploads/2026/01/TC-All-Stage-2025-Breakout-QA.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Halo Creative&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-strengthen-your-authority-in-the-ecosystem"&gt;Strengthen your authority in the ecosystem&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Speaking at TC Founder Summit gives you:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Direct access to 1,000+ ambitious founders and active investors.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Premium entry to the full event experience.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Meaningful brand and authority positioning within the startup ecosystem.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch will also amplify your participation through agenda placement, editorial inclusion on TechCrunch.com, and social promotion across its channels.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-applications-close-april-17"&gt;Applications close April 17&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;TC Founder Summit takes place on June 9, and speaker selections are made well ahead of the event.&amp;nbsp;If you have scaling insight founders need to hear, now is the time to&amp;nbsp;submit&amp;nbsp;your topic.&amp;nbsp;Have&amp;nbsp;more than one strong idea? Submit them all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lead the conversation. Share what&amp;nbsp;you’ve&amp;nbsp;learned. Help founders build smarter.&amp;nbsp;&lt;strong&gt;Submit to speak&lt;/strong&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;before the April 17 deadline.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Founder Summit 2026, June 9, 2026" class="wp-image-3094672" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/16x9_GeneralArticleImageHeader_FS26_V2.png?w=680" width="680" /&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If&amp;nbsp;you’ve&amp;nbsp;built, backed, or&amp;nbsp;operated&amp;nbsp;inside high-growth startups, your experience could shape how the next wave of&amp;nbsp;founders&amp;nbsp;scales.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On&amp;nbsp;June 9 in Boston,&amp;nbsp;&lt;strong&gt;TechCrunch Founder Summit 2026&lt;/strong&gt;&amp;nbsp;will&amp;nbsp;bring together&amp;nbsp;1,000+ founders and investors for a focused day on the realities of growth. We’re inviting seasoned founders, VCs, and startup operators to lead interactive roundtable discussions rooted in real-world execution — the wins, the missteps, and the lessons that only come from doing the work.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Submit your topic by April 17 to be considered.&amp;nbsp;&lt;strong&gt;Learn more and apply today&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Early Stage 2024 roundtable sessions" class="wp-image-2966337" height="454" src="https://techcrunch.com/wp-content/uploads/2025/02/Early-Stage-Roundtable-Sessions-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Halo Creative&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-spark-real-conversation-nbsp-by-leading-a-roundtable-nbsp-or-breakout"&gt;Spark real conversation&amp;nbsp;by leading a roundtable&amp;nbsp;or breakout&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you lead an interactive roundtable or a Q&amp;amp;A-style breakout, every session at Founder Summit is built for depth. Each is a 30-minute, discussion-driven conversation led by two to four speakers, depending on format. No slides. No polished decks. Just candid insight and practical takeaways founders can apply&amp;nbsp;immediately.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If&amp;nbsp;you’ve&amp;nbsp;scaled revenue from zero to $50 million, navigated a difficult fundraise, rebuilt a team after hypergrowth, expanded internationally, or redefined your go-to-market strategy, this&amp;nbsp;is the room to share what&amp;nbsp;actually works.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Founder Summit breakout audience" class="wp-image-3086922" height="453" src="https://techcrunch.com/wp-content/uploads/2026/01/TC-All-Stage-2025-Breakout-QA.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Halo Creative&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-strengthen-your-authority-in-the-ecosystem"&gt;Strengthen your authority in the ecosystem&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Speaking at TC Founder Summit gives you:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Direct access to 1,000+ ambitious founders and active investors.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Premium entry to the full event experience.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Meaningful brand and authority positioning within the startup ecosystem.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch will also amplify your participation through agenda placement, editorial inclusion on TechCrunch.com, and social promotion across its channels.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-applications-close-april-17"&gt;Applications close April 17&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;TC Founder Summit takes place on June 9, and speaker selections are made well ahead of the event.&amp;nbsp;If you have scaling insight founders need to hear, now is the time to&amp;nbsp;submit&amp;nbsp;your topic.&amp;nbsp;Have&amp;nbsp;more than one strong idea? Submit them all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lead the conversation. Share what&amp;nbsp;you’ve&amp;nbsp;learned. Help founders build smarter.&amp;nbsp;&lt;strong&gt;Submit to speak&lt;/strong&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;before the April 17 deadline.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Founder Summit 2026, June 9, 2026" class="wp-image-3094672" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/16x9_GeneralArticleImageHeader_FS26_V2.png?w=680" width="680" /&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/25/have-hard-won-scaling-lessons-to-share-take-the-stage-at-techcrunch-founder-summit/</guid><pubDate>Wed, 25 Feb 2026 16:00:00 +0000</pubDate></item><item><title>[NEW] OpenClaw creator’s advice to AI builders is to be more playful and allow yourself time to improve (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/25/openclaw-creators-advice-to-ai-builders-is-to-be-more-playful-and-allow-yourself-time-to-improve/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/openclaw-creator-Peter-Steinberger.jpg?resize=1200,564" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Peter Steinberger, the creator of the viral AI agent OpenClaw who has since been hired by OpenAI, has some advice for those experimenting with AI technology, including AI agents. From his own experience, the best way to build today is to explore, be playful, and not expect to be an expert at what you do right away.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I wish I could say that I had the unified plan in the beginning, but a lot of it was just exploration,” Steinberger said. “I wanted things, and those things didn’t exist, and&amp;nbsp;… let’s say I prompted them into existence.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The developer was chatting with OpenAI’s Head of Developer Experience, Romain Huet, on the first episode of the company’s new Builders Unscripted podcast. Here, he spoke about what OpenClaw was like in its early days and how he didn’t have a plan when he got started.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Steinberger explained he began by building a tool that would integrate with WhatsApp, but then set it aside for a bit and focused on other things, as he assumed the AI labs would build something like what he was working on in the near future.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I just experimented a lot. My mission was, kind of like, to have fun and inspire people,” Steinberger noted. By last November, however, the developer was surprised that no AI labs had started to build what he wanted to use. That led him to create the initial prototype of what’s now OpenClaw. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Where it really clicked was where I was at this weekend trip in Marrakesh, and I found myself using it way more because it was so convenient&amp;nbsp;… There was no really good internet. [But] WhatsApp just works everywhere,” he said. The tool made it easy for him to find restaurants, look up things on his computer, send texts to friends, and more.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The more he played with the technology, Steinberger realized how good modern AI models have become at problem-solving, much like coders are. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Now they can just, like, actually come up with the solutions themselves, even though you never programmed them at all,” he noted. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Throughout the process of building, Steinberger said that his workflow improved — and he stresses to other developers that’s something that can take time, so don’t give up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s these people that&amp;nbsp;… write software in the old way, and the old way is going to go away,” he pointed out. They then decide to try vibe coding but are disappointed with the results.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think vibe coding is a slur,” said Steinberger, basically suggesting that it’s not as simple a process at first as the term makes it sound. “They try AI, but they don’t understand that it’s a skill,” he said, then compared the process of coding with AI to learning guitar.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You’re not going to be good at guitar on the first day,” he said. Instead, he recommends that people approach learning with a more playful attitude. If he writes a prompt now, he has a gut feeling as to how long it will take, and if it takes longer, he reflects on what may have gone wrong and adapts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“My&amp;nbsp;… advice always is, approach it in a playful way. Build something that you always wanted to build. If you’re at least a little bit of a builder, there has to be something on the back of your mind that you want to build. Like, just play.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This ability to experiment and have fun is what’s most important, especially at a time when people are worried their jobs will be overtaken by AI. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If your identity is: I want to create things. I want to solve problems. If you’re high agency, if you’re smart, you will be in more demand than ever,” Steinberger said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/openclaw-creator-Peter-Steinberger.jpg?resize=1200,564" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Peter Steinberger, the creator of the viral AI agent OpenClaw who has since been hired by OpenAI, has some advice for those experimenting with AI technology, including AI agents. From his own experience, the best way to build today is to explore, be playful, and not expect to be an expert at what you do right away.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I wish I could say that I had the unified plan in the beginning, but a lot of it was just exploration,” Steinberger said. “I wanted things, and those things didn’t exist, and&amp;nbsp;… let’s say I prompted them into existence.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The developer was chatting with OpenAI’s Head of Developer Experience, Romain Huet, on the first episode of the company’s new Builders Unscripted podcast. Here, he spoke about what OpenClaw was like in its early days and how he didn’t have a plan when he got started.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Steinberger explained he began by building a tool that would integrate with WhatsApp, but then set it aside for a bit and focused on other things, as he assumed the AI labs would build something like what he was working on in the near future.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I just experimented a lot. My mission was, kind of like, to have fun and inspire people,” Steinberger noted. By last November, however, the developer was surprised that no AI labs had started to build what he wanted to use. That led him to create the initial prototype of what’s now OpenClaw. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Where it really clicked was where I was at this weekend trip in Marrakesh, and I found myself using it way more because it was so convenient&amp;nbsp;… There was no really good internet. [But] WhatsApp just works everywhere,” he said. The tool made it easy for him to find restaurants, look up things on his computer, send texts to friends, and more.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The more he played with the technology, Steinberger realized how good modern AI models have become at problem-solving, much like coders are. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Now they can just, like, actually come up with the solutions themselves, even though you never programmed them at all,” he noted. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Throughout the process of building, Steinberger said that his workflow improved — and he stresses to other developers that’s something that can take time, so don’t give up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s these people that&amp;nbsp;… write software in the old way, and the old way is going to go away,” he pointed out. They then decide to try vibe coding but are disappointed with the results.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think vibe coding is a slur,” said Steinberger, basically suggesting that it’s not as simple a process at first as the term makes it sound. “They try AI, but they don’t understand that it’s a skill,” he said, then compared the process of coding with AI to learning guitar.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You’re not going to be good at guitar on the first day,” he said. Instead, he recommends that people approach learning with a more playful attitude. If he writes a prompt now, he has a gut feeling as to how long it will take, and if it takes longer, he reflects on what may have gone wrong and adapts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“My&amp;nbsp;… advice always is, approach it in a playful way. Build something that you always wanted to build. If you’re at least a little bit of a builder, there has to be something on the back of your mind that you want to build. Like, just play.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This ability to experiment and have fun is what’s most important, especially at a time when people are worried their jobs will be overtaken by AI. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If your identity is: I want to create things. I want to solve problems. If you’re high agency, if you’re smart, you will be in more demand than ever,” Steinberger said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/25/openclaw-creators-advice-to-ai-builders-is-to-be-more-playful-and-allow-yourself-time-to-improve/</guid><pubDate>Wed, 25 Feb 2026 16:54:46 +0000</pubDate></item><item><title>[NEW] OpenAI COO says ads will be ‘an iterative process’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/25/openai-coo-says-ads-will-be-an-iterative-process/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2195918462.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last month, OpenAI said that it is going to introduce ads to users of the free and Go tiers in ChatGPT. The company rolled out ads to U.S.-based users earlier this month amid criticism from rivals like Anthropic, which published a string of Super Bowl ads.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the sidelines of the India AI summit, TechCrunch asked OpenAI COO Brad Lightcap about how the company is approaching ads. Lightcap said that the process is iterative and the company has to get user privacy and trust right.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Well, this is going to be an iterative process for sure. This is something we are committed to getting right.&amp;nbsp;What does that look like?&amp;nbsp;It means obviously maintaining user trust at a very high level.&amp;nbsp;It means getting privacy right,” Lightcap said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also noted that ads can add to the product experience of users if they are done right. He urged to give OpenAI a few months to see how the company fares in rolling out the product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It means really creating a delightful product experience.&amp;nbsp;We think ads done right can be additive to a product experience.&amp;nbsp;And so it’ll take iteration, it’ll take time, but we’re just starting out.&amp;nbsp;So maybe give us a few months and see how it goes,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lightcap didn’t specify if the company is thinking about rolling out ads beyond the U.S. market at the moment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this month, Sam Altman hit back at Anthropic with a long post on X about the Super Bowl ads, calling the OpenAI rival “dishonest” and accusing them of making an expensive product that serves “rich people.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“More importantly, we believe everyone deserves to use AI and are committed to free access, because we believe access creates agency. More Texans use ChatGPT for free than the total number of people who use Claude in the US, so we have a differently-shaped problem than they do,” Altman wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Various outlets have reported that OpenAI is charging $60 for 1,000 impressions, an unusually high rate. Last month, Adweek noted that OpenAI is asking for $200,000 of minimum commitment from advertisers. Earlier this week, The Information reported that Shopify is allowing its merchants to advertise on ChatGPT through its Shop Campaigns ad network, joining early testers like Target, Williams Sonoma, and Adobe.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2195918462.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last month, OpenAI said that it is going to introduce ads to users of the free and Go tiers in ChatGPT. The company rolled out ads to U.S.-based users earlier this month amid criticism from rivals like Anthropic, which published a string of Super Bowl ads.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the sidelines of the India AI summit, TechCrunch asked OpenAI COO Brad Lightcap about how the company is approaching ads. Lightcap said that the process is iterative and the company has to get user privacy and trust right.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Well, this is going to be an iterative process for sure. This is something we are committed to getting right.&amp;nbsp;What does that look like?&amp;nbsp;It means obviously maintaining user trust at a very high level.&amp;nbsp;It means getting privacy right,” Lightcap said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also noted that ads can add to the product experience of users if they are done right. He urged to give OpenAI a few months to see how the company fares in rolling out the product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It means really creating a delightful product experience.&amp;nbsp;We think ads done right can be additive to a product experience.&amp;nbsp;And so it’ll take iteration, it’ll take time, but we’re just starting out.&amp;nbsp;So maybe give us a few months and see how it goes,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lightcap didn’t specify if the company is thinking about rolling out ads beyond the U.S. market at the moment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this month, Sam Altman hit back at Anthropic with a long post on X about the Super Bowl ads, calling the OpenAI rival “dishonest” and accusing them of making an expensive product that serves “rich people.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“More importantly, we believe everyone deserves to use AI and are committed to free access, because we believe access creates agency. More Texans use ChatGPT for free than the total number of people who use Claude in the US, so we have a differently-shaped problem than they do,” Altman wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Various outlets have reported that OpenAI is charging $60 for 1,000 impressions, an unusually high rate. Last month, Adweek noted that OpenAI is asking for $200,000 of minimum commitment from advertisers. Earlier this week, The Information reported that Shopify is allowing its merchants to advertise on ChatGPT through its Shop Campaigns ad network, joining early testers like Target, Williams Sonoma, and Adobe.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/25/openai-coo-says-ads-will-be-an-iterative-process/</guid><pubDate>Wed, 25 Feb 2026 17:37:37 +0000</pubDate></item><item><title>[NEW] Gemini can now automate some multi-step tasks on Android (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/25/gemini-can-now-automate-some-multi-step-tasks-on-android/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google on Wednesday announced a series of updates to its Gemini AI-powered features on the Android operating system, the most notable being a new way to use the AI to handle multi-step tasks like ordering an Uber or food delivery. These automations join other Gemini improvements shipping today, including an expansion of scam detection for phone calls and Circle to Search updates that now let you identify all the items on your phone’s screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The automations, explains Google, allow users to essentially offload their to-do list to Gemini. In practice, however, the types of things that Gemini can manage are still limited.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company says that the feature, which is in beta, will initially support select apps in the food, grocery, and rideshare categories. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It will also be limited to the Gemini app on certain devices, including the Pixel 10, Pixel 10 Pro, and Samsung Galaxy S26 series. And it will initially be available only in the U.S. and Korea.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3096764" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/1.-Gemini-with-task-Automation-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;AI-powered automations could potentially go wrong, of course, so Google has added some protections. For starters, the automations can’t be kicked off without an explicit command from the device’s owner. As they run, you can watch their progress in real time and stop the task if it’s making a mistake or getting stuck. Google notes also that the automations take place in a secure, virtual window on your phone where they can only access limited apps, not the rest of the data on your device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature ties into the growing trend of using AI to automate more tasks in users’ personal lives. ChatGPT, for instance, lets users create tasks that can be run on schedules or at specific times, as well as offering an agent that can complete a variety of computer-based tasks like navigating a calendar, generating a slideshow, or running code. Anthropic’s Cowork, meanwhile, brings the capabilities of its Claude AI to non-coding tasks, letting non-developers automate everyday file and task management. And, of course, an AI tool called OpenClaw recently went viral for its ability to manage everyday tasks like sending emails, managing calendars, checking into flights, and more.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3096765" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/1.-Gemini-powered-Scam-Detection-alerts.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Another Gemini update arriving now is the expansion of a Scam Detection feature for phone calls, which is becoming available on Samsung Galaxy S26 series devices in the U.S. (The feature is already offered on Pixel phones in the U.S., Australia, Canada, India, Ireland, and the U.K.) Google is also using its Gemini on-device model to detect scam texts in the U.S., Canada, and the U.K. on Pixel 10 series devices, and soon on the Galaxy S26 series phones, as well.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Finally, Google says its Circle to Search feature, which lets you use gestures like scribbles and circling to initiate searches, can now search for everything you’re seeing on the phone screen, not just a single object. That means you can search every item of clothing and every accessory in an outfit you like, or learn more about a group of things and the related topic on the screen.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3096763" height="383" src="https://techcrunch.com/wp-content/uploads/2026/02/0.-Header.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google has been steadily releasing Gemini updates to its Android ecosystem at regular intervals through new operating system updates and updates targeted toward its flagship phone, the Google Pixel, via its frequent updates known as Pixel Drops. Meanwhile, Apple has been struggling to release a more comprehensive AI feature set, which is set to include an AI-powered Siri — a launch that was recently pushed back again to later in the year.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google on Wednesday announced a series of updates to its Gemini AI-powered features on the Android operating system, the most notable being a new way to use the AI to handle multi-step tasks like ordering an Uber or food delivery. These automations join other Gemini improvements shipping today, including an expansion of scam detection for phone calls and Circle to Search updates that now let you identify all the items on your phone’s screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The automations, explains Google, allow users to essentially offload their to-do list to Gemini. In practice, however, the types of things that Gemini can manage are still limited.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company says that the feature, which is in beta, will initially support select apps in the food, grocery, and rideshare categories. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It will also be limited to the Gemini app on certain devices, including the Pixel 10, Pixel 10 Pro, and Samsung Galaxy S26 series. And it will initially be available only in the U.S. and Korea.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3096764" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/1.-Gemini-with-task-Automation-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;AI-powered automations could potentially go wrong, of course, so Google has added some protections. For starters, the automations can’t be kicked off without an explicit command from the device’s owner. As they run, you can watch their progress in real time and stop the task if it’s making a mistake or getting stuck. Google notes also that the automations take place in a secure, virtual window on your phone where they can only access limited apps, not the rest of the data on your device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature ties into the growing trend of using AI to automate more tasks in users’ personal lives. ChatGPT, for instance, lets users create tasks that can be run on schedules or at specific times, as well as offering an agent that can complete a variety of computer-based tasks like navigating a calendar, generating a slideshow, or running code. Anthropic’s Cowork, meanwhile, brings the capabilities of its Claude AI to non-coding tasks, letting non-developers automate everyday file and task management. And, of course, an AI tool called OpenClaw recently went viral for its ability to manage everyday tasks like sending emails, managing calendars, checking into flights, and more.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3096765" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/1.-Gemini-powered-Scam-Detection-alerts.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Another Gemini update arriving now is the expansion of a Scam Detection feature for phone calls, which is becoming available on Samsung Galaxy S26 series devices in the U.S. (The feature is already offered on Pixel phones in the U.S., Australia, Canada, India, Ireland, and the U.K.) Google is also using its Gemini on-device model to detect scam texts in the U.S., Canada, and the U.K. on Pixel 10 series devices, and soon on the Galaxy S26 series phones, as well.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Finally, Google says its Circle to Search feature, which lets you use gestures like scribbles and circling to initiate searches, can now search for everything you’re seeing on the phone screen, not just a single object. That means you can search every item of clothing and every accessory in an outfit you like, or learn more about a group of things and the related topic on the screen.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3096763" height="383" src="https://techcrunch.com/wp-content/uploads/2026/02/0.-Header.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google has been steadily releasing Gemini updates to its Android ecosystem at regular intervals through new operating system updates and updates targeted toward its flagship phone, the Google Pixel, via its frequent updates known as Pixel Drops. Meanwhile, Apple has been struggling to release a more comprehensive AI feature set, which is set to include an AI-powered Siri — a launch that was recently pushed back again to later in the year.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/25/gemini-can-now-automate-some-multi-step-tasks-on-android/</guid><pubDate>Wed, 25 Feb 2026 18:00:00 +0000</pubDate></item><item><title>[NEW] The public opposition to AI infrastructure is heating up (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/25/the-public-opposition-to-ai-infrastructure-is-heating-up/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Modular_Data_Center_2.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Across the country, discontent has exploded over the ever-growing glut of server farms that have accompanied the AI boom. Anger has grown so loud that it’s begun to shift legislative agendas. Some states and communities are mulling temporary bans on new data center development altogether. Earlier this month, New York joined the club, with a bold new proposal to halt the local cloud build-out in its tracks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new bill in New York State would impose a three-year moratorium on the issuance of new permits for data center construction throughout the state, while local regulators are given a chance to study the environmental and economic impacts the industry is having on communities. The bill’s co-authors, State Senator Liz Krueger and Assemblymember Anna Kelles, have called the legislation the “strongest” introduced in the country.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While no statewide moratoriums have passed so far, local bans are proliferating fast. Several weeks before Krueger and Kelles introduced their bill, the New Orleans City Council passed a moratorium, pausing all new data center construction in the city for a period of a year. In early January, Madison, WI passed a similar law after protests erupted over regional tech projects. Similar policies have also passed in droves of communities throughout construction hotspots like Georgia and Michigan, as well as in many other regions throughout the country.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Environmental activists have long taken aim at data centers, but the more recent concerns have come from high-level lawmakers, drawing on populist anger at the tech industry broadly. In conservative Florida, for instance, Gov. Ron DeSantis recently announced an AI “bill of rights” that gives local communities the right to limit new data center construction. In liberal Vermont, U.S. Senator Bernie Sanders has suggested a nationwide moratorium. And in Arizona, where the political milieu is decidedly mixed, Gov. Katie Hobbs recently said she supported pulling the industry’s tax incentives. Politicians have even begun to fight over the topics, with the governor of Mississippi taking shots at Sanders online over his moratorium proposal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The political resistance is coming just as tech companies commit more and more money to building out infrastructure. The four biggest spenders — Amazon, Google, Meta and Microsoft — plan to spend a whopping $650 billion in capital expenditures over the next year, the vast majority of it going to data center buildouts. Even more spending is planned in the following years, as the companies race to secure as much compute capacity as possible.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the speed and scale of those projects has made them increasingly unpopular, according to recent polling. A recent Echelon Insights poll found 46% of respondents would oppose plans to build a data center in their community, compared with 35% in support. A different poll from Politico found that, while there is considerable concern about the facilities, many voters don’t have much of an opinion either way—making it possible for public sentiment to be swayed in either direction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The industry is already spending big to attempt to change those numbers — at least in the regions where it matters. In January, the Financial Times reported that some of the industry’s biggest data center operators were planning a “lobbying blitz,” with plans to “boost spending on targeted advertising and engagement” aimed at the communities where they build.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Tech companies are also making real concessions, like the planned Rate Payer Protection Pledge that would make them responsible for supplying power to any new AI data centers. But it’s not clear those measures will be enough to bring the public around.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dan Diorio, of the Data Center Coalition, argued, in a conversation with TechCrunch, that data centers should appeal to smaller communities because they provide revenue without straining those communities’ limited resources. If the incentives are cut off and companies decide not to build in those places, the revenue also won’t be there. “That’s where statewide policy considerations come in,” he said. “Are you going to limit communities in which these businesses could be a significant benefit for them?”&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-logic-behind-pressing-pause"&gt;The logic behind pressing pause &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;In general, data center moratoriums are meant to give communities breathing room while policymakers study the potential costs and benefits of allowing such facilities to be built in their communities. The rate of construction in some states has accelerated at such a pace that communities are unsure of how the industry will impact them in the long run.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Justin Flagg, director of communications and environmental policy for Sen. Krueger’s office, told TechCrunch that the legislation was driven, in part, by what he called the energy affordability crisis in New York. Said crisis has troubled both rate-payers and politicians. A group of 30 state lawmakers recently called upon the state’s governor, Kathy Hochul, to declare an “energy state of emergency” in the state due to rate increases. While there are a diversity of factors at work in driving up energy prices, there’s a consensus that the growth in data centers is making the problem worse, not better.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s broad discontent being expressed about energy prices,” Flagg said. “We certainly hear that constantly from our constituents, whose electric and gas rates are going up.” He added that local pushback was also being driven by environmental concerns—which he described as the “water impact and the noise and the local infrastructure impact as well.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to those grid concerns, major tech companies — including Microsoft, Google, Meta, and OpenAI—have promised to pay for their additions to the electrical grid in the communities where they operate, often installing behind-the-meter power sources paired with the new data centers. The Washington Post recently reported that Silicon Valley is increasingly looking to build its own private electrical supply—a kind of “shadow grid”—that can be used to operate the power-consumptive properties that are now fueling the AI industry. The strategy involves standing up massive new private power sources instead of relying on the public grid. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One example of this practice comes from xAI, Elon Musk’s AI startup, which—at the site of its massive data center in Memphis, Tennessee, known as “Colossus”—built a series of methane gas turbines which have been accused of polluting the local community. The company’s efforts have already run into significant trouble. xAI had reportedly told local officials that, due to a legal loophole, the turbines were exempt from air quality permits. In January, the Environmental Protection Agency ruled that Musk’s company was not exempt from the permits, making their previous operation illegal. Environmental activists, decrying the facility’s discharge of “smog-forming pollution, soot, and hazardous chemicals,” announced earlier this month that they planned to sue the company over it. Musk’s facility has since permitted its turbines.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As the xAI example illustrates, if the “shadow grid” strategy purports to solve one problem (public grid overload), it threatens to create a host of new ones—with environmental activists and local communities alike expressing concern for how the new facilities could spew pollution into people’s backyards. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the federal level, the Trump administration — which has made AI one of its top priorities — has also sought to characterize the industry as responsible stewards of the communities in which they build. Indeed, Trump officials have floated a hypothetical policy to force AI companies to internalize the costs of their additions to local electrical grids, although the details on this policy remain vague.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-debate-over-taxes"&gt;Debate over taxes&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;For years, communities have incentivized data center development through tax breaks. Last summer, an analysis by CNBC found that 42 states throughout the U.S. either have no sales tax or provide full or partial sales tax exemptions to tech firms. Of that number, some 16 states publicly reported how much they had awarded to companies through tax breaks. The forfeited revenue amounted to some $6 billion over a period of five years, the outlet wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, however, more and more states are thinking about turning off the spigot. In Georgia, for instance, a variety of bills were recently introduced that would crack down on the industry’s benefits. State Sen. Matt Brass, who has introduced a bill that would nix the server sales tax exemption, told TechCrunch that he doesn’t think tech companies need the extra money, nor does he think dispensing with the benefit will dissuade them from doing business in the state. “In Georgia, if you compare us to other states, our property taxes are low, our property values are low, our overall tax burden is low,” Brass said. “So, you know, our overall business climate is good. That should be the attraction.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Brass, who chairs the state’s rules committee, told TechCrunch that he expects there to be significant support for his policy. A similar piece of legislation passed the Georgia legislature in 2024, but it was vetoed by the governor. Brass added that, were the exemption to be done away with, he believes it could generate hundreds of millions of dollars for the state.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Ohio, a similar policy battle is currently playing out. A group of Democratic lawmakers recently&amp;nbsp;introduced legislation that would—like in Georgia—move to nix the state’s sales tax exemption.&amp;nbsp;A similar policy was introduced last year, but—like in Georgia—it was defeated by the state’s governor, Mike DeWine. “The most ridiculous tax break on the books currently is for data centers,” one of the bill’s supporting lawmakers, state Sen. Kent Smith, recently said. “That tax break needs to end, for the benefit of everyone who’s got an electric bill.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, there are still plenty of lawmakers who support the server sales tax exemption. In Colorado, State Representative Alex Valdez recently introduced a bill that would enshrine data centers’ loophole for the next 20 years. Valdez told TechCrunch that the exemption is merely a carrot to get tech companies in the door. Once they set up a base of operations in the state they become a source of passive revenue that inevitably boomerangs back to benefit the communities in which they operate, he said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Modular_Data_Center_2.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Across the country, discontent has exploded over the ever-growing glut of server farms that have accompanied the AI boom. Anger has grown so loud that it’s begun to shift legislative agendas. Some states and communities are mulling temporary bans on new data center development altogether. Earlier this month, New York joined the club, with a bold new proposal to halt the local cloud build-out in its tracks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new bill in New York State would impose a three-year moratorium on the issuance of new permits for data center construction throughout the state, while local regulators are given a chance to study the environmental and economic impacts the industry is having on communities. The bill’s co-authors, State Senator Liz Krueger and Assemblymember Anna Kelles, have called the legislation the “strongest” introduced in the country.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While no statewide moratoriums have passed so far, local bans are proliferating fast. Several weeks before Krueger and Kelles introduced their bill, the New Orleans City Council passed a moratorium, pausing all new data center construction in the city for a period of a year. In early January, Madison, WI passed a similar law after protests erupted over regional tech projects. Similar policies have also passed in droves of communities throughout construction hotspots like Georgia and Michigan, as well as in many other regions throughout the country.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Environmental activists have long taken aim at data centers, but the more recent concerns have come from high-level lawmakers, drawing on populist anger at the tech industry broadly. In conservative Florida, for instance, Gov. Ron DeSantis recently announced an AI “bill of rights” that gives local communities the right to limit new data center construction. In liberal Vermont, U.S. Senator Bernie Sanders has suggested a nationwide moratorium. And in Arizona, where the political milieu is decidedly mixed, Gov. Katie Hobbs recently said she supported pulling the industry’s tax incentives. Politicians have even begun to fight over the topics, with the governor of Mississippi taking shots at Sanders online over his moratorium proposal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The political resistance is coming just as tech companies commit more and more money to building out infrastructure. The four biggest spenders — Amazon, Google, Meta and Microsoft — plan to spend a whopping $650 billion in capital expenditures over the next year, the vast majority of it going to data center buildouts. Even more spending is planned in the following years, as the companies race to secure as much compute capacity as possible.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the speed and scale of those projects has made them increasingly unpopular, according to recent polling. A recent Echelon Insights poll found 46% of respondents would oppose plans to build a data center in their community, compared with 35% in support. A different poll from Politico found that, while there is considerable concern about the facilities, many voters don’t have much of an opinion either way—making it possible for public sentiment to be swayed in either direction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The industry is already spending big to attempt to change those numbers — at least in the regions where it matters. In January, the Financial Times reported that some of the industry’s biggest data center operators were planning a “lobbying blitz,” with plans to “boost spending on targeted advertising and engagement” aimed at the communities where they build.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Tech companies are also making real concessions, like the planned Rate Payer Protection Pledge that would make them responsible for supplying power to any new AI data centers. But it’s not clear those measures will be enough to bring the public around.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dan Diorio, of the Data Center Coalition, argued, in a conversation with TechCrunch, that data centers should appeal to smaller communities because they provide revenue without straining those communities’ limited resources. If the incentives are cut off and companies decide not to build in those places, the revenue also won’t be there. “That’s where statewide policy considerations come in,” he said. “Are you going to limit communities in which these businesses could be a significant benefit for them?”&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-logic-behind-pressing-pause"&gt;The logic behind pressing pause &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;In general, data center moratoriums are meant to give communities breathing room while policymakers study the potential costs and benefits of allowing such facilities to be built in their communities. The rate of construction in some states has accelerated at such a pace that communities are unsure of how the industry will impact them in the long run.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Justin Flagg, director of communications and environmental policy for Sen. Krueger’s office, told TechCrunch that the legislation was driven, in part, by what he called the energy affordability crisis in New York. Said crisis has troubled both rate-payers and politicians. A group of 30 state lawmakers recently called upon the state’s governor, Kathy Hochul, to declare an “energy state of emergency” in the state due to rate increases. While there are a diversity of factors at work in driving up energy prices, there’s a consensus that the growth in data centers is making the problem worse, not better.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s broad discontent being expressed about energy prices,” Flagg said. “We certainly hear that constantly from our constituents, whose electric and gas rates are going up.” He added that local pushback was also being driven by environmental concerns—which he described as the “water impact and the noise and the local infrastructure impact as well.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to those grid concerns, major tech companies — including Microsoft, Google, Meta, and OpenAI—have promised to pay for their additions to the electrical grid in the communities where they operate, often installing behind-the-meter power sources paired with the new data centers. The Washington Post recently reported that Silicon Valley is increasingly looking to build its own private electrical supply—a kind of “shadow grid”—that can be used to operate the power-consumptive properties that are now fueling the AI industry. The strategy involves standing up massive new private power sources instead of relying on the public grid. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One example of this practice comes from xAI, Elon Musk’s AI startup, which—at the site of its massive data center in Memphis, Tennessee, known as “Colossus”—built a series of methane gas turbines which have been accused of polluting the local community. The company’s efforts have already run into significant trouble. xAI had reportedly told local officials that, due to a legal loophole, the turbines were exempt from air quality permits. In January, the Environmental Protection Agency ruled that Musk’s company was not exempt from the permits, making their previous operation illegal. Environmental activists, decrying the facility’s discharge of “smog-forming pollution, soot, and hazardous chemicals,” announced earlier this month that they planned to sue the company over it. Musk’s facility has since permitted its turbines.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As the xAI example illustrates, if the “shadow grid” strategy purports to solve one problem (public grid overload), it threatens to create a host of new ones—with environmental activists and local communities alike expressing concern for how the new facilities could spew pollution into people’s backyards. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the federal level, the Trump administration — which has made AI one of its top priorities — has also sought to characterize the industry as responsible stewards of the communities in which they build. Indeed, Trump officials have floated a hypothetical policy to force AI companies to internalize the costs of their additions to local electrical grids, although the details on this policy remain vague.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-debate-over-taxes"&gt;Debate over taxes&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;For years, communities have incentivized data center development through tax breaks. Last summer, an analysis by CNBC found that 42 states throughout the U.S. either have no sales tax or provide full or partial sales tax exemptions to tech firms. Of that number, some 16 states publicly reported how much they had awarded to companies through tax breaks. The forfeited revenue amounted to some $6 billion over a period of five years, the outlet wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, however, more and more states are thinking about turning off the spigot. In Georgia, for instance, a variety of bills were recently introduced that would crack down on the industry’s benefits. State Sen. Matt Brass, who has introduced a bill that would nix the server sales tax exemption, told TechCrunch that he doesn’t think tech companies need the extra money, nor does he think dispensing with the benefit will dissuade them from doing business in the state. “In Georgia, if you compare us to other states, our property taxes are low, our property values are low, our overall tax burden is low,” Brass said. “So, you know, our overall business climate is good. That should be the attraction.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Brass, who chairs the state’s rules committee, told TechCrunch that he expects there to be significant support for his policy. A similar piece of legislation passed the Georgia legislature in 2024, but it was vetoed by the governor. Brass added that, were the exemption to be done away with, he believes it could generate hundreds of millions of dollars for the state.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Ohio, a similar policy battle is currently playing out. A group of Democratic lawmakers recently&amp;nbsp;introduced legislation that would—like in Georgia—move to nix the state’s sales tax exemption.&amp;nbsp;A similar policy was introduced last year, but—like in Georgia—it was defeated by the state’s governor, Mike DeWine. “The most ridiculous tax break on the books currently is for data centers,” one of the bill’s supporting lawmakers, state Sen. Kent Smith, recently said. “That tax break needs to end, for the benefit of everyone who’s got an electric bill.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, there are still plenty of lawmakers who support the server sales tax exemption. In Colorado, State Representative Alex Valdez recently introduced a bill that would enshrine data centers’ loophole for the next 20 years. Valdez told TechCrunch that the exemption is merely a carrot to get tech companies in the door. Once they set up a base of operations in the state they become a source of passive revenue that inevitably boomerangs back to benefit the communities in which they operate, he said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/25/the-public-opposition-to-ai-infrastructure-is-heating-up/</guid><pubDate>Wed, 25 Feb 2026 19:03:34 +0000</pubDate></item></channel></rss>