<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 06 Feb 2026 02:23:45 +0000</lastBuildDate><item><title>Fundamental raises $255M Series A with a new take on big data analysis (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/fundamental-raises-255-million-series-a-with-a-new-take-on-big-data-analysis/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/CEO_3.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;An AI lab called Fundamental emerged from stealth on Thursday, offering a new foundation model to solve an old problem: how to draw insights from the huge quantities of structured data produced by enterprises. By combining the old systems of predictive AI with more contemporary tools, the company believes it can reshape how large enterprises analyze their data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“While LLMs have been great at working with unstructured data, like text, audio, video, and code, they don’t work well with structured data like tables,” CEO Jeremy Fraenkel told TechCrunch. “With our model Nexus, we have built the best foundation model to handle that type of data.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The idea has already drawn significant interest from investors. The company is emerging from stealth with $255 million in funding at a $1.2 billion valuation. The bulk of it comes from the recent $225 million Series A round led by Oak HC/FT, Valor Equity Partners, Battery Ventures, and Salesforce Ventures; Hetz Ventures also participated in the Series A, with angel funding from Perplexity CEO Aravind Srinivas, Brex co-founder Henrique Dubugras, and Datadog CEO Olivier Pomel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Called a large tabular model (LTM) rather than a large language model (LLM), Fundamental’s Nexus breaks from contemporary AI practices in a number of significant ways. The model is deterministic — that is, it will give the same answer every time it is asked a given question — and doesn’t rely on the transformer architecture that defines models from most contemporary AI labs. Fundamental calls it a foundation model because it goes through the normal steps of pre-training and fine-tuning, but the result is something profoundly different from what a client would get when partnering with OpenAI or Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those differences are important because Fundamental is chasing a use case where contemporary AI models often falter. Because Transformer-based AI models can only process data that’s within their context window, they often have trouble reasoning over extremely large datasets — analyzing a spreadsheet with billions of rows, for instance. But that kind of enormous structured dataset is common within large enterprises, creating a significant opportunity for models that can handle the scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Fraenkel sees it, that’s a huge opportunity for Fundamental. Using Nexus, the company can bring contemporary techniques to big data analysis, offering something more powerful and flexible than the algorithms that are currently in use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can now have one model across all of your use cases, so you can now expand massively the number of use cases that you tackle,” he told TechCrunch. “And on each one of those use cases, you get better performance than what you would otherwise be able to do with an army of data scientists.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That promise has already brought in a number of high-profile contracts, including seven-figure contracts with Fortune 100 clients. The company has also entered into a strategic partnership with AWS that will allow AWS users to deploy Nexus directly from existing instances.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/CEO_3.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;An AI lab called Fundamental emerged from stealth on Thursday, offering a new foundation model to solve an old problem: how to draw insights from the huge quantities of structured data produced by enterprises. By combining the old systems of predictive AI with more contemporary tools, the company believes it can reshape how large enterprises analyze their data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“While LLMs have been great at working with unstructured data, like text, audio, video, and code, they don’t work well with structured data like tables,” CEO Jeremy Fraenkel told TechCrunch. “With our model Nexus, we have built the best foundation model to handle that type of data.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The idea has already drawn significant interest from investors. The company is emerging from stealth with $255 million in funding at a $1.2 billion valuation. The bulk of it comes from the recent $225 million Series A round led by Oak HC/FT, Valor Equity Partners, Battery Ventures, and Salesforce Ventures; Hetz Ventures also participated in the Series A, with angel funding from Perplexity CEO Aravind Srinivas, Brex co-founder Henrique Dubugras, and Datadog CEO Olivier Pomel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Called a large tabular model (LTM) rather than a large language model (LLM), Fundamental’s Nexus breaks from contemporary AI practices in a number of significant ways. The model is deterministic — that is, it will give the same answer every time it is asked a given question — and doesn’t rely on the transformer architecture that defines models from most contemporary AI labs. Fundamental calls it a foundation model because it goes through the normal steps of pre-training and fine-tuning, but the result is something profoundly different from what a client would get when partnering with OpenAI or Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those differences are important because Fundamental is chasing a use case where contemporary AI models often falter. Because Transformer-based AI models can only process data that’s within their context window, they often have trouble reasoning over extremely large datasets — analyzing a spreadsheet with billions of rows, for instance. But that kind of enormous structured dataset is common within large enterprises, creating a significant opportunity for models that can handle the scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Fraenkel sees it, that’s a huge opportunity for Fundamental. Using Nexus, the company can bring contemporary techniques to big data analysis, offering something more powerful and flexible than the algorithms that are currently in use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can now have one model across all of your use cases, so you can now expand massively the number of use cases that you tackle,” he told TechCrunch. “And on each one of those use cases, you get better performance than what you would otherwise be able to do with an army of data scientists.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That promise has already brought in a number of high-profile contracts, including seven-figure contracts with Fortune 100 clients. The company has also entered into a strategic partnership with AWS that will allow AWS users to deploy Nexus directly from existing instances.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/fundamental-raises-255-million-series-a-with-a-new-take-on-big-data-analysis/</guid><pubDate>Thu, 05 Feb 2026 15:00:02 +0000</pubDate></item><item><title>Consolidating systems for AI with iPaaS (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/05/1132200/consolidating-systems-for-ai-with-ipaas/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;SAP&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;For decades, enterprises reacted to shifting business pressures with stopgap technology solutions. To rein in rising infrastructure costs, they adopted cloud services that could scale on demand. When customers shifted their lives onto smartphones, companies rolled out mobile apps to keep pace. And when businesses began needing real-time visibility into factories and stockrooms, they layered on IoT systems to supply those insights.&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-1132242" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/MIT_SAP_V4-COVERJan302026.png?w=1555" width="1555" /&gt;&lt;/figure&gt;  &lt;p&gt;Each new plug-in or platform promised better, more efficient operations. And individually, many delivered. But as more and more solutions stacked up, IT teams had to string together a tangled web to connect them—less an IT ecosystem and more of a make-do collection of ad-hoc workarounds.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;That reality has led to bottlenecks and maintenance burdens, and the impact is showing up in performance. Today, fewer than half of CIOs (48%) say their current digital initiatives are meeting or exceeding business outcome targets. Another 2025 survey found that operations leaders point to integration complexity and data quality issues as top culprits for why investments haven’t delivered as expected.&lt;/p&gt;  &lt;p&gt;Achim Kraiss, chief product officer of SAP Integration Suite, elaborates on the wide-ranging problems inherent in patchwork IT: “A fragmented landscape makes it difficult to see and control end-to-end business processes,” he explains. “Monitoring, troubleshooting, and governance all suffer. Costs go up because of all the complex mappings and multi-application connectivity you have to maintain.”&lt;/p&gt; 
 &lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-1132246" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/MITTR-SAP-Socials_AchimKraissQuote.png" /&gt;&lt;/figure&gt;  &lt;p&gt;These challenges take on new significance as enterprises look to adopt AI. As AI becomes embedded in everyday workflows, systems are suddenly expected to move far larger volumes of data, at higher speeds, and with tighter coordination than yesterday’s architectures were built&lt;br /&gt;to sustain.&lt;/p&gt;  &lt;p&gt;As companies now prepare for an AI-powered future, whether that is generative AI, machine learning, or agentic AI, many are realizing that the way data moves through their business matters just as much as the insights it generates. As a result, organizations are moving away from scattered integration tools and toward consolidated, end-to-end platforms that restore order and streamline how systems interact.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Download the report.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;SAP&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;For decades, enterprises reacted to shifting business pressures with stopgap technology solutions. To rein in rising infrastructure costs, they adopted cloud services that could scale on demand. When customers shifted their lives onto smartphones, companies rolled out mobile apps to keep pace. And when businesses began needing real-time visibility into factories and stockrooms, they layered on IoT systems to supply those insights.&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-1132242" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/MIT_SAP_V4-COVERJan302026.png?w=1555" width="1555" /&gt;&lt;/figure&gt;  &lt;p&gt;Each new plug-in or platform promised better, more efficient operations. And individually, many delivered. But as more and more solutions stacked up, IT teams had to string together a tangled web to connect them—less an IT ecosystem and more of a make-do collection of ad-hoc workarounds.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;That reality has led to bottlenecks and maintenance burdens, and the impact is showing up in performance. Today, fewer than half of CIOs (48%) say their current digital initiatives are meeting or exceeding business outcome targets. Another 2025 survey found that operations leaders point to integration complexity and data quality issues as top culprits for why investments haven’t delivered as expected.&lt;/p&gt;  &lt;p&gt;Achim Kraiss, chief product officer of SAP Integration Suite, elaborates on the wide-ranging problems inherent in patchwork IT: “A fragmented landscape makes it difficult to see and control end-to-end business processes,” he explains. “Monitoring, troubleshooting, and governance all suffer. Costs go up because of all the complex mappings and multi-application connectivity you have to maintain.”&lt;/p&gt; 
 &lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-1132246" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/MITTR-SAP-Socials_AchimKraissQuote.png" /&gt;&lt;/figure&gt;  &lt;p&gt;These challenges take on new significance as enterprises look to adopt AI. As AI becomes embedded in everyday workflows, systems are suddenly expected to move far larger volumes of data, at higher speeds, and with tighter coordination than yesterday’s architectures were built&lt;br /&gt;to sustain.&lt;/p&gt;  &lt;p&gt;As companies now prepare for an AI-powered future, whether that is generative AI, machine learning, or agentic AI, many are realizing that the way data moves through their business matters just as much as the insights it generates. As a result, organizations are moving away from scattered integration tools and toward consolidated, end-to-end platforms that restore order and streamline how systems interact.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Download the report.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/05/1132200/consolidating-systems-for-ai-with-ipaas/</guid><pubDate>Thu, 05 Feb 2026 15:20:37 +0000</pubDate></item><item><title>AI Expo 2026 Day 2: Moving experimental pilots to AI production (AI News)</title><link>https://www.artificialintelligence-news.com/news/ai-expo-2026-day-2-moving-experimental-pilots-ai-production/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/20260205_1351241-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;The second day of the co-located AI &amp;amp; Big Data Expo and Digital Transformation Week in London showed a market in a clear transition.&lt;/p&gt;&lt;p&gt;Early excitement over generative models is fading. Enterprise leaders now face the friction of fitting these tools into current stacks. Day two sessions focused less on large language models and more on the infrastructure needed to run them: data lineage, observability, and compliance.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-data-maturity-determines-deployment-success"&gt;Data maturity determines deployment success&lt;/h3&gt;&lt;p&gt;AI reliability depends on data quality. DP Indetkar from Northern Trust warned against allowing AI to become a “B-movie robot.” This scenario occurs when algorithms fail because of poor inputs. Indetkar noted that analytics maturity must come before AI adoption. Automated decision-making amplifies errors rather than reducing them if the data strategy is unverified.&lt;/p&gt;&lt;p&gt;Eric Bobek of Just Eat supported this view. He explained how data and machine learning guide decisions at the global enterprise level. Investments in AI layers are wasted if the data foundation remains fragmented.&lt;/p&gt;&lt;p&gt;Mohsen Ghasempour from Kingfisher also noted the need to turn raw data into real-time actionable intelligence. Retail and logistics firms must cut the latency between data collection and insight generation to see a return.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-scaling-in-regulated-environments"&gt;Scaling in regulated environments&lt;/h3&gt;&lt;p&gt;The finance, healthcare, and legal sectors have near-zero tolerance for error. Pascal Hetzscholdt from Wiley addressed these sectors directly.&lt;/p&gt;&lt;p&gt;Hetzscholdt stated that responsible AI in science, finance, and law relies on accuracy, attribution, and integrity. Enterprise systems in these fields need audit trails. Reputational damage or regulatory fines make “black box” implementations impossible.&lt;/p&gt;&lt;p&gt;Konstantina Kapetanidi of Visa outlined the difficulties in building multilingual, tool-using, scalable generative AI applications. Models are becoming active agents that execute tasks rather than just generating text. Allowing a model to use tools – like querying a database – creates security vectors that need serious testing.&lt;/p&gt;&lt;p&gt;Parinita Kothari from Lloyds Banking Group detailed the requirements for deploying, scaling, monitoring, and maintaining AI systems. Kothari challenged the “deploy-and-forget” mentality. AI models need continuous oversight, similar to traditional software infrastructure.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-change-in-developer-workflows"&gt;The change in developer workflows&lt;/h3&gt;&lt;p&gt;Of course, AI is fundamentally changing how code is written. A panel with speakers from Valae, Charles River Labs, and Knight Frank examined how AI copilots reshape software creation. While these tools speed up code generation, they also force developers to focus more on review and architecture.&lt;/p&gt;&lt;p&gt;This change requires new skills. A panel with representatives from Microsoft, Lloyds, and Mastercard discussed the tools and mindsets needed for future AI developers. A gap exists between current workforce capabilities and the needs of an AI-augmented environment. Executives must plan training programmes that ensure developers sufficiently validate AI-generated code.&lt;/p&gt;&lt;p&gt;Dr Gurpinder Dhillon from Senzing and Alexis Ego from Retool presented low-code and no-code strategies. Ego described using AI with low-code platforms to make production-ready internal apps. This method aims to cut the backlog of internal tooling requests.&lt;/p&gt;&lt;p&gt;Dhillon argued that these strategies speed up development without dropping quality. For the C-suite, this suggests cheaper internal software delivery if governance protocols stay in place.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-workforce-capability-and-specific-utility"&gt;Workforce capability and specific utility&lt;/h3&gt;&lt;p&gt;The broader workforce is starting to work with “digital colleagues.” Austin Braham from EverWorker explained how agents reshape workforce models. This terminology implies a move from passive software to active participants. Business leaders must re-evaluate human-machine interaction protocols.&lt;/p&gt;&lt;p&gt;Paul Airey from Anthony Nolan gave an example of AI delivering literally life-changing value. He detailed how automation improves donor matching and transplant timelines for stem cell transplants. The utility of these technologies extends to life-saving logistics.&lt;/p&gt;&lt;p&gt;A recurring theme throughout the event is that effective applications often solve very specific and high-friction problems rather than attempting to be general-purpose solutions.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-managing-the-transition"&gt;Managing the transition&lt;/h3&gt;&lt;p&gt;The day two sessions from the co-located events show that enterprise focus has now moved to integration. The initial novelty is gone and has been replaced by demands for uptime, security, and compliance. Innovation heads should assess which projects have the data infrastructure to survive contact with the real world.&lt;/p&gt;&lt;p&gt;Organisations must prioritise the basic aspects of AI: cleaning data warehouses, establishing legal guardrails, and training staff to supervise automated agents. The difference between a successful deployment and a stalled pilot lies in these details.&lt;/p&gt;&lt;p&gt;Executives, for their part, should direct resources toward data engineering and governance frameworks. Without them, advanced models will fail to deliver value.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;AI Expo 2026 Day 1: Governance and data readiness enable the agentic enterprise&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/20260205_1351241-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;The second day of the co-located AI &amp;amp; Big Data Expo and Digital Transformation Week in London showed a market in a clear transition.&lt;/p&gt;&lt;p&gt;Early excitement over generative models is fading. Enterprise leaders now face the friction of fitting these tools into current stacks. Day two sessions focused less on large language models and more on the infrastructure needed to run them: data lineage, observability, and compliance.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-data-maturity-determines-deployment-success"&gt;Data maturity determines deployment success&lt;/h3&gt;&lt;p&gt;AI reliability depends on data quality. DP Indetkar from Northern Trust warned against allowing AI to become a “B-movie robot.” This scenario occurs when algorithms fail because of poor inputs. Indetkar noted that analytics maturity must come before AI adoption. Automated decision-making amplifies errors rather than reducing them if the data strategy is unverified.&lt;/p&gt;&lt;p&gt;Eric Bobek of Just Eat supported this view. He explained how data and machine learning guide decisions at the global enterprise level. Investments in AI layers are wasted if the data foundation remains fragmented.&lt;/p&gt;&lt;p&gt;Mohsen Ghasempour from Kingfisher also noted the need to turn raw data into real-time actionable intelligence. Retail and logistics firms must cut the latency between data collection and insight generation to see a return.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-scaling-in-regulated-environments"&gt;Scaling in regulated environments&lt;/h3&gt;&lt;p&gt;The finance, healthcare, and legal sectors have near-zero tolerance for error. Pascal Hetzscholdt from Wiley addressed these sectors directly.&lt;/p&gt;&lt;p&gt;Hetzscholdt stated that responsible AI in science, finance, and law relies on accuracy, attribution, and integrity. Enterprise systems in these fields need audit trails. Reputational damage or regulatory fines make “black box” implementations impossible.&lt;/p&gt;&lt;p&gt;Konstantina Kapetanidi of Visa outlined the difficulties in building multilingual, tool-using, scalable generative AI applications. Models are becoming active agents that execute tasks rather than just generating text. Allowing a model to use tools – like querying a database – creates security vectors that need serious testing.&lt;/p&gt;&lt;p&gt;Parinita Kothari from Lloyds Banking Group detailed the requirements for deploying, scaling, monitoring, and maintaining AI systems. Kothari challenged the “deploy-and-forget” mentality. AI models need continuous oversight, similar to traditional software infrastructure.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-change-in-developer-workflows"&gt;The change in developer workflows&lt;/h3&gt;&lt;p&gt;Of course, AI is fundamentally changing how code is written. A panel with speakers from Valae, Charles River Labs, and Knight Frank examined how AI copilots reshape software creation. While these tools speed up code generation, they also force developers to focus more on review and architecture.&lt;/p&gt;&lt;p&gt;This change requires new skills. A panel with representatives from Microsoft, Lloyds, and Mastercard discussed the tools and mindsets needed for future AI developers. A gap exists between current workforce capabilities and the needs of an AI-augmented environment. Executives must plan training programmes that ensure developers sufficiently validate AI-generated code.&lt;/p&gt;&lt;p&gt;Dr Gurpinder Dhillon from Senzing and Alexis Ego from Retool presented low-code and no-code strategies. Ego described using AI with low-code platforms to make production-ready internal apps. This method aims to cut the backlog of internal tooling requests.&lt;/p&gt;&lt;p&gt;Dhillon argued that these strategies speed up development without dropping quality. For the C-suite, this suggests cheaper internal software delivery if governance protocols stay in place.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-workforce-capability-and-specific-utility"&gt;Workforce capability and specific utility&lt;/h3&gt;&lt;p&gt;The broader workforce is starting to work with “digital colleagues.” Austin Braham from EverWorker explained how agents reshape workforce models. This terminology implies a move from passive software to active participants. Business leaders must re-evaluate human-machine interaction protocols.&lt;/p&gt;&lt;p&gt;Paul Airey from Anthony Nolan gave an example of AI delivering literally life-changing value. He detailed how automation improves donor matching and transplant timelines for stem cell transplants. The utility of these technologies extends to life-saving logistics.&lt;/p&gt;&lt;p&gt;A recurring theme throughout the event is that effective applications often solve very specific and high-friction problems rather than attempting to be general-purpose solutions.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-managing-the-transition"&gt;Managing the transition&lt;/h3&gt;&lt;p&gt;The day two sessions from the co-located events show that enterprise focus has now moved to integration. The initial novelty is gone and has been replaced by demands for uptime, security, and compliance. Innovation heads should assess which projects have the data infrastructure to survive contact with the real world.&lt;/p&gt;&lt;p&gt;Organisations must prioritise the basic aspects of AI: cleaning data warehouses, establishing legal guardrails, and training staff to supervise automated agents. The difference between a successful deployment and a stalled pilot lies in these details.&lt;/p&gt;&lt;p&gt;Executives, for their part, should direct resources toward data engineering and governance frameworks. Without them, advanced models will fail to deliver value.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;AI Expo 2026 Day 1: Governance and data readiness enable the agentic enterprise&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ai-expo-2026-day-2-moving-experimental-pilots-ai-production/</guid><pubDate>Thu, 05 Feb 2026 16:08:36 +0000</pubDate></item><item><title>Introducing SyGra Studio (Hugging Face - Blog)</title><link>https://huggingface.co/blog/ServiceNow-AI/sygra-studio</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://cdn-thumbnails.huggingface.co/social-thumbnails/blog/ServiceNow-AI/sygra-studio.png" /&gt;&lt;/div&gt;&lt;!-- HTML_TAG_START --&gt;
SyGra 2.0.0 introduces &lt;strong&gt;Studio&lt;/strong&gt;, an interactive environment that turns synthetic data generation into a transparent, visual craft. Instead of juggling YAML files and terminals, you compose flows directly on the canvas, preview datasets before committing, tune prompts with inline variable hints, and watch executions stream live—all from a single pane. Under the hood it’s the same platform, so everything you do visually generates the corresponding SyGra compatible graph config and task executor scripts.
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		What Studio lets you do
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Configure and validate models with guided forms (OpenAI, Azure OpenAI, Ollama, Vertex, Bedrock, vLLM, custom endpoints).&lt;/li&gt;
&lt;li&gt;Connect Hugging Face, file-system, or ServiceNow data sources and preview rows before execution.&lt;/li&gt;
&lt;li&gt;Configure nodes by selecting models, writing prompts (with auto-suggested variables), and defining outputs or structured schemas.&lt;/li&gt;
&lt;li&gt;Design downstream outputs using shared state variables and Pydantic-powered mappings.&lt;/li&gt;
&lt;li&gt;Execute flows end-to-end and review generated results instantly with node-level progress.&lt;/li&gt;
&lt;li&gt;Debug with inline logs, breakpoints, Monaco-backed code editors, and auto-saved drafts.&lt;/li&gt;
&lt;li&gt;Monitor per-run token cost, latency, and guardrail outcomes with execution history stored in &lt;code&gt;.executions/&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s walk through this experience step by step.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 1: Configure the data source
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Open Studio, click &lt;strong&gt;Create Flow&lt;/strong&gt;, and Start/End nodes appear automatically. Before adding anything else:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose a connector (Hugging Face, disk, or ServiceNow).&lt;/li&gt;
&lt;li&gt;Enter parameters like &lt;code&gt;repo_id&lt;/code&gt;, split, or file path, then click &lt;strong&gt;Preview&lt;/strong&gt; to fetch sample rows.&lt;/li&gt;
&lt;li&gt;Column names immediately become state variables (e.g., &lt;code&gt;{prompt}&lt;/code&gt;, &lt;code&gt;{genre}&lt;/code&gt;), so you know exactly what can be referenced inside prompts and processors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once validated, Studio keeps the configuration in sync and pipes those variables throughout the flow—no manual wiring or guesswork.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 2: Build the flow visually
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Drag the blocks you need from the palette. For a story-generation pipeline:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Drop an &lt;strong&gt;LLM node&lt;/strong&gt; named “Story Generator,” select a configured model (say, &lt;code&gt;gpt-4o-mini&lt;/code&gt;), write the prompt, and store the result in &lt;code&gt;story_body&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Add a second &lt;strong&gt;LLM node&lt;/strong&gt; named “Story Summarizer,” reference &lt;code&gt;{story_body}&lt;/code&gt; inside the prompt, and output to &lt;code&gt;story_summary&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Toggle structured outputs, attach tools, or add Lambda/Subgraph nodes if you need reusable logic or branching behavior.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Studio’s detail panel keeps everything in context—model parameters, prompt editor, tool configuration, pre/post-process code, and even multi-LLM settings if you want parallel generations. Typing &lt;code&gt;{&lt;/code&gt; inside a prompt surfaces every available state variable instantly.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 3: Review and run
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Open the &lt;strong&gt;Code Panel&lt;/strong&gt; to inspect the exact YAML/JSON Studio is generating. This is the same artifact written to &lt;code&gt;tasks/examples/&lt;/code&gt;, so what you see is what gets committed.&lt;/p&gt;
&lt;p&gt;When you’re ready to execute:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Click &lt;strong&gt;Run Workflow&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Choose record counts, batch sizes, retry behavior etc.&lt;/li&gt;
&lt;li&gt;Hit &lt;strong&gt;Run&lt;/strong&gt; and watch the Execution panel stream node status, token usage, latency, and cost in real time. Detailed logs provide observability and make debugging effortless. All executions are written to &lt;code&gt;.executions/runs/*.json&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After the run, download outputs, compare against prior executions, get metadata of latency and usage details.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		See it in action!
	&lt;/span&gt;
&lt;/h2&gt;
&lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/603c6bf03249b99991dbcbd0/VytOnFoygBxG0-ITGkcna.mp4"&gt;&lt;/video&gt;

&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Running Existing Workflows
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Run the Glaive Code Assistant workflow
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;SyGra Studio can also execute existing workflow in the &lt;code&gt;tasks&lt;/code&gt;. For example, in the &lt;code&gt;tasks/examples/glaive_code_assistant/&lt;/code&gt; workflow — it ingests the &lt;code&gt;glaiveai/glaive-code-assistant-v2&lt;/code&gt; dataset, drafts answers, critiques them, and loops until the critique returns “NO MORE FEEDBACK.”&lt;/p&gt;
&lt;p&gt;Inside Studio you’ll notice:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Canvas layout&lt;/strong&gt; – two LLM nodes (&lt;code&gt;generate_answer&lt;/code&gt; and &lt;code&gt;critique_answer&lt;/code&gt;) linked by a conditional edge that either routes back for more revisions or exits to &lt;strong&gt;END&lt;/strong&gt; when the critique is satisfied.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tunable inputs&lt;/strong&gt; – the Run modal lets you switch dataset splits, adjust batch sizes, cap records, or tweak temperatures without touching YAML.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Observable execution&lt;/strong&gt; – watch both nodes light up in sequence, inspect intermediate critiques, and monitor status in real time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generated outputs&lt;/strong&gt; – synthetic data is generated, ready for model training, evaluation pipelines or annotation tools.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Get started
	&lt;/span&gt;
&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;git &lt;span class="hljs-built_in"&gt;clone&lt;/span&gt; https://github.com/ServiceNow/SyGra.git
&lt;span class="hljs-built_in"&gt;cd&lt;/span&gt; SyGra &amp;amp;&amp;amp; make studio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SyGra Studio turns synthetic data workflows into a visual, user friendly experience. Configure once, build with confidence, run with full observability, generate the data without ever leaving the canvas.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://cdn-thumbnails.huggingface.co/social-thumbnails/blog/ServiceNow-AI/sygra-studio.png" /&gt;&lt;/div&gt;&lt;!-- HTML_TAG_START --&gt;
SyGra 2.0.0 introduces &lt;strong&gt;Studio&lt;/strong&gt;, an interactive environment that turns synthetic data generation into a transparent, visual craft. Instead of juggling YAML files and terminals, you compose flows directly on the canvas, preview datasets before committing, tune prompts with inline variable hints, and watch executions stream live—all from a single pane. Under the hood it’s the same platform, so everything you do visually generates the corresponding SyGra compatible graph config and task executor scripts.
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		What Studio lets you do
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Configure and validate models with guided forms (OpenAI, Azure OpenAI, Ollama, Vertex, Bedrock, vLLM, custom endpoints).&lt;/li&gt;
&lt;li&gt;Connect Hugging Face, file-system, or ServiceNow data sources and preview rows before execution.&lt;/li&gt;
&lt;li&gt;Configure nodes by selecting models, writing prompts (with auto-suggested variables), and defining outputs or structured schemas.&lt;/li&gt;
&lt;li&gt;Design downstream outputs using shared state variables and Pydantic-powered mappings.&lt;/li&gt;
&lt;li&gt;Execute flows end-to-end and review generated results instantly with node-level progress.&lt;/li&gt;
&lt;li&gt;Debug with inline logs, breakpoints, Monaco-backed code editors, and auto-saved drafts.&lt;/li&gt;
&lt;li&gt;Monitor per-run token cost, latency, and guardrail outcomes with execution history stored in &lt;code&gt;.executions/&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s walk through this experience step by step.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 1: Configure the data source
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Open Studio, click &lt;strong&gt;Create Flow&lt;/strong&gt;, and Start/End nodes appear automatically. Before adding anything else:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose a connector (Hugging Face, disk, or ServiceNow).&lt;/li&gt;
&lt;li&gt;Enter parameters like &lt;code&gt;repo_id&lt;/code&gt;, split, or file path, then click &lt;strong&gt;Preview&lt;/strong&gt; to fetch sample rows.&lt;/li&gt;
&lt;li&gt;Column names immediately become state variables (e.g., &lt;code&gt;{prompt}&lt;/code&gt;, &lt;code&gt;{genre}&lt;/code&gt;), so you know exactly what can be referenced inside prompts and processors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once validated, Studio keeps the configuration in sync and pipes those variables throughout the flow—no manual wiring or guesswork.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 2: Build the flow visually
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Drag the blocks you need from the palette. For a story-generation pipeline:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Drop an &lt;strong&gt;LLM node&lt;/strong&gt; named “Story Generator,” select a configured model (say, &lt;code&gt;gpt-4o-mini&lt;/code&gt;), write the prompt, and store the result in &lt;code&gt;story_body&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Add a second &lt;strong&gt;LLM node&lt;/strong&gt; named “Story Summarizer,” reference &lt;code&gt;{story_body}&lt;/code&gt; inside the prompt, and output to &lt;code&gt;story_summary&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Toggle structured outputs, attach tools, or add Lambda/Subgraph nodes if you need reusable logic or branching behavior.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Studio’s detail panel keeps everything in context—model parameters, prompt editor, tool configuration, pre/post-process code, and even multi-LLM settings if you want parallel generations. Typing &lt;code&gt;{&lt;/code&gt; inside a prompt surfaces every available state variable instantly.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 3: Review and run
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Open the &lt;strong&gt;Code Panel&lt;/strong&gt; to inspect the exact YAML/JSON Studio is generating. This is the same artifact written to &lt;code&gt;tasks/examples/&lt;/code&gt;, so what you see is what gets committed.&lt;/p&gt;
&lt;p&gt;When you’re ready to execute:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Click &lt;strong&gt;Run Workflow&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Choose record counts, batch sizes, retry behavior etc.&lt;/li&gt;
&lt;li&gt;Hit &lt;strong&gt;Run&lt;/strong&gt; and watch the Execution panel stream node status, token usage, latency, and cost in real time. Detailed logs provide observability and make debugging effortless. All executions are written to &lt;code&gt;.executions/runs/*.json&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After the run, download outputs, compare against prior executions, get metadata of latency and usage details.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		See it in action!
	&lt;/span&gt;
&lt;/h2&gt;
&lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/603c6bf03249b99991dbcbd0/VytOnFoygBxG0-ITGkcna.mp4"&gt;&lt;/video&gt;

&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Running Existing Workflows
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Run the Glaive Code Assistant workflow
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;SyGra Studio can also execute existing workflow in the &lt;code&gt;tasks&lt;/code&gt;. For example, in the &lt;code&gt;tasks/examples/glaive_code_assistant/&lt;/code&gt; workflow — it ingests the &lt;code&gt;glaiveai/glaive-code-assistant-v2&lt;/code&gt; dataset, drafts answers, critiques them, and loops until the critique returns “NO MORE FEEDBACK.”&lt;/p&gt;
&lt;p&gt;Inside Studio you’ll notice:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Canvas layout&lt;/strong&gt; – two LLM nodes (&lt;code&gt;generate_answer&lt;/code&gt; and &lt;code&gt;critique_answer&lt;/code&gt;) linked by a conditional edge that either routes back for more revisions or exits to &lt;strong&gt;END&lt;/strong&gt; when the critique is satisfied.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tunable inputs&lt;/strong&gt; – the Run modal lets you switch dataset splits, adjust batch sizes, cap records, or tweak temperatures without touching YAML.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Observable execution&lt;/strong&gt; – watch both nodes light up in sequence, inspect intermediate critiques, and monitor status in real time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generated outputs&lt;/strong&gt; – synthetic data is generated, ready for model training, evaluation pipelines or annotation tools.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Get started
	&lt;/span&gt;
&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;git &lt;span class="hljs-built_in"&gt;clone&lt;/span&gt; https://github.com/ServiceNow/SyGra.git
&lt;span class="hljs-built_in"&gt;cd&lt;/span&gt; SyGra &amp;amp;&amp;amp; make studio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SyGra Studio turns synthetic data workflows into a visual, user friendly experience. Configure once, build with confidence, run with full observability, generate the data without ever leaving the canvas.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/ServiceNow-AI/sygra-studio</guid><pubDate>Thu, 05 Feb 2026 16:52:28 +0000</pubDate></item><item><title>Rethinking imitation learning with Predictive Inverse Dynamics Models (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/rethinking-imitation-learning-with-predictive-inverse-dynamics-models/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Smart Replay - flowchart diagram showing the flow between Encoder, State Predictor, and Policy" class="wp-image-1161128" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay-BlogHeroFeature-1400x788_New.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;div class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section"&gt;
	
	&lt;div class="container"&gt;
		&lt;div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow"&gt;
			&lt;div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h2 class="wp-block-heading h3" id="at-a-glance"&gt;At a glance&lt;/h2&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Imitation learning becomes easier when an AI&amp;nbsp;agent&amp;nbsp;understands why an action is taken.&lt;/li&gt;



&lt;li&gt;Predictive Inverse Dynamics Models (PIDMs)&amp;nbsp;predict&amp;nbsp;plausible future states,&amp;nbsp;clarifying the direction of behavior during imitation&amp;nbsp;learning.&lt;/li&gt;



&lt;li&gt;Even imperfect predictions reduce ambiguity,&amp;nbsp;making&amp;nbsp;it clearer which action makes sense&amp;nbsp;in the moment.&lt;/li&gt;



&lt;li&gt;This makes PIDMs far more data‑efficient than traditional approaches.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;		&lt;/div&gt;
	&lt;/div&gt;

	&lt;/div&gt;



&lt;p&gt;Imitation&amp;nbsp;learning&amp;nbsp;teaches&amp;nbsp;AI agents by example: show the agent recordings of how people perform a task and let it&amp;nbsp;infer&amp;nbsp;what to do.&amp;nbsp;The&amp;nbsp;most common&amp;nbsp;approach,&amp;nbsp;Behavior Cloning&amp;nbsp;(BC),&amp;nbsp;frames this as a simple question: “Given the current state&amp;nbsp;of the environment, what action&amp;nbsp;would&amp;nbsp;an expert take?”&lt;/p&gt;



&lt;p&gt;In practice, this is done through supervised learning, where the states serve as inputs and expert actions as outputs. While simple in principle, BC often requires large demonstration datasets to account for the natural variability in human behavior, but collecting such datasets can be costly and difficult in real-world settings.&lt;/p&gt;



&lt;p&gt;Predictive Inverse Dynamics Models (PIDMs) offer a different take on imitation learning by changing how agents interpret human behavior. Instead of directly mapping states to actions, PIDMs break down the problem into two subproblems: predicting what should happen next and inferring an appropriate action to go from the current state to the predicted future state. While PIDMs often outperform BC, it has not been clear why they work so well, motivating a closer look at the mechanisms behind their performance.&lt;/p&gt;



&lt;p&gt;In the paper, “When does predictive inverse dynamics outperform behavior cloning?” we show how this two-stage approach enables PIDMs to learn effective policies from far fewer demonstrations than BC. By grounding the selection process in a plausible future, PIDMs provide a clearer basis for choosing an action&amp;nbsp;during inference. In practice, this can mean achieving comparable performance with as few as one-fifth the demonstrations required by BC, even when predictions are imperfect.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Figure 1. BC vs. PIDM architectures.&amp;nbsp;(Top) Behavior&amp;nbsp;Cloning learns&amp;nbsp;how to perform&amp;nbsp;a direct mapping from the current state to an action. (Bottom)&amp;nbsp;PIDMs add a state predictor that predicts future&amp;nbsp;states. They&amp;nbsp;then use an inverse dynamics model to predict the action&amp;nbsp;required&amp;nbsp;to move from the current state towards that future state. Both approaches share a common latent representation through a shared state encoder." class="wp-image-1161185" height="658" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/SmartReplay_FIG1.png" width="1009" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. BC vs. PIDM architectures.&amp;nbsp;(Top) Behavior&amp;nbsp;Cloning learns&amp;nbsp;how to perform&amp;nbsp;a direct mapping from the current state to an action. (Bottom)&amp;nbsp;PIDMs add a state predictor that predicts future&amp;nbsp;states. They&amp;nbsp;then use an inverse dynamics model to predict the action&amp;nbsp;required&amp;nbsp;to move from the current state towards that future state. Both approaches share a common latent representation through a shared state encoder.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="how-pidms-rethink-imitation"&gt;How PIDMs rethink imitation&lt;/h2&gt;



&lt;p&gt;PIDMs’ approach to imitation learning consists of two core elements: a model that forecasts plausible future states, and an inverse dynamics model (IDM) that predicts the action needed to move from the present state toward that future. Instead of asking, “What action would an expert take?” PIDMs effectively ask, “What would an expert try to achieve, and what action would lead to it?” This shift turns the information in the current observation (e.g., video frame) into a coherent sense of direction, reducing ambiguity about intent and making action prediction easier.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;video series&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;On Second Thought&lt;/h2&gt;
				
								&lt;p class="large" id="on-second-thought"&gt;A video series with Sinead Bovell built around the questions everyone’s asking about AI. With expert voices from across Microsoft, we break down the tension and promise of this rapidly changing technology, exploring what’s evolving and what’s possible.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="real-world-validation-in-a-3d-gameplay-environment"&gt;Real-world validation in a 3D gameplay environment&lt;/h2&gt;



&lt;p&gt;To&amp;nbsp;evaluate&amp;nbsp;PIDMs&amp;nbsp;under realistic conditions,&amp;nbsp;we trained&amp;nbsp;agents on human gameplay demonstrations in a visually rich video game. These conditions&amp;nbsp;include&amp;nbsp;operating&amp;nbsp;directly from raw video&amp;nbsp;input, interacting with&amp;nbsp;a complex 3D&amp;nbsp;environment in real time at 30 frames&amp;nbsp;per&amp;nbsp;second, and&amp;nbsp;handling&amp;nbsp;visual artifacts and unpredictable system delays.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The agents ran from beginning to end, taking video frames as input and continuously deciding which buttons to press and how to move the joysticks. Instead of relying on a hand-coded set of game variables and rules, the model worked directly from visual input, using past examples to predict what comes next and choosing actions that moved play in that direction.&lt;/p&gt;



&lt;p&gt;We ran all experiments on a cloud gaming platform, which introduced additional delays and visual distortions. Despite these challenges, the PIDM agents consistently matched human patterns of play and achieved high success rates across tasks, as shown in Video 1 below and Videos 2 and 3 in the appendix.&lt;/p&gt;



&lt;figure class="wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;figcaption class="wp-element-caption"&gt;Video 1. A player&amp;nbsp;(left)&amp;nbsp;and a PIDM agent&amp;nbsp;(right)&amp;nbsp;side by side playing the game&amp;nbsp;&lt;em&gt;Bleeding Edge&lt;/em&gt;.&amp;nbsp;Both&amp;nbsp;navigate the same trajectory,&amp;nbsp;jumping over obstacles and engaging&amp;nbsp;with&amp;nbsp;nonplayer&amp;nbsp;characters. Despite&amp;nbsp;network delays, the&amp;nbsp;agent closely matches the player’s timing and&amp;nbsp;movement&amp;nbsp;in real time.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="why-and-when-pidms-outperform-bc"&gt;Why and when PIDMs outperform BC&lt;/h2&gt;



&lt;p&gt;Of course, AI agents do not have access to future outcomes. They can only generate predictions based on available data, and those predictions are sometimes wrong. This creates a central trade‑off for PIDMs.&lt;/p&gt;



&lt;p&gt;On one hand, anticipating where the agent should be heading can clarify what action makes sense in the present. Knowing the intended direction helps narrow an otherwise ambiguous choice. On the other hand, inaccurate predictions can occasionally steer the model toward the wrong action.&lt;/p&gt;



&lt;p&gt;The key insight is that these effects are not symmetric. While prediction errors introduce some risk, reducing ambiguity in the present often matters more. Our theoretical analysis shows that even with imperfect predictions, PIDMs outperform BC as long as the prediction error remains modest. If future states were known perfectly, PIDMs would outperform BC outright.&lt;/p&gt;



&lt;p&gt;In practice, this means that clarifying intent often matters more than accurately predicting the future. That advantage is most evident in the situations where BC struggles: where human behavior varies and actions are driven by underlying goals rather than by what is immediately visible on the screen.&lt;/p&gt;



&lt;p&gt;BC requires many demonstrations because each example is noisy and open to multiple interpretations. PIDMs, by contrast, sharpen each demonstration by linking actions to the future states they aim to reach. As a result, PIDMs can learn effective action strategies from far fewer examples.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="evaluation"&gt;Evaluation&lt;/h2&gt;



&lt;p&gt;To test these ideas under realistic conditions, we designed a sequence of experiments that begins with a simple, interpretable 2D environment (Video 4 in the appendix) and culminates in a complex 3D video game. We trained both BC and PIDM on very small datasets, ranging from one to fifty demonstrations in the 2D environment and from five to thirty for the 3D video game. Across all tasks, PIDM reached high success rates with far fewer demonstrations than BC.&lt;/p&gt;



&lt;p&gt;In the 2D setting, BC needed two to five times more data to match PIDM’s performance (Figure 2). In the 3D game, BC needed 66% more data to achieve comparable results (Video 5 in the appendix).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2. Performance gains in the 2D environment. As the number of training demonstrations increases, PIDM consistently achieves higher success rates than BC across all four tasks. Curves show mean performance, with shading indicating variability across 20 experiments for reproducibility." class="wp-image-1161012" height="871" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay_blog_Fig2a-d.png" width="1166" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Performance gains in the 2D environment. As the number of training demonstrations increases, PIDM consistently achieves higher success rates than BC across all four tasks. Curves show mean performance, with shading indicating variability across 20 experiments for reproducibility. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="takeaway-intent-matters-in-imitation-learning"&gt;Takeaway: Intent matters in imitation learning&lt;/h2&gt;



&lt;p&gt;The main message of our investigation is simple: imitation becomes easier when intent is made explicit. Predicting a plausible future, even an imperfect one, helps resolve ambiguity about which action makes sense right now, much like driving more confidently in the fog when the driver already knows where the road is headed. PIDM shifts imitation learning from pure copying toward goal-oriented action.&lt;/p&gt;



&lt;p&gt;This approach has limits. If predictions of future states become too unreliable, they can mislead the model about the intended next move. In those cases, the added uncertainty can outweigh the benefit of reduced ambiguity, causing PIDM to underperform BC.&lt;/p&gt;



&lt;p&gt;But when predictions are reasonably accurate, reframing action prediction as “&lt;em&gt;How do I get there from here&lt;/em&gt;?” helps explain why learning from small, messy human datasets can be surprisingly effective. In settings where data is expensive and demonstrations are limited, that shift in perspective can make a meaningful difference.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="appendix-visualizations-and-results-videos"&gt;Appendix: Visualizations and results (videos)&lt;/h2&gt;



&lt;div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h3 class="wp-block-heading h4" id="a-player-a-naive-action-replay-baseline-and-a-pidm-agent-playing-bleeding-edge-1"&gt;A player, a naïve action-replay baseline, and a PIDM agent playing &lt;em&gt;Bleeding Edge&lt;/em&gt;&lt;/h3&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;figcaption class="wp-element-caption"&gt;Video&amp;nbsp;2. (Left)&amp;nbsp;The player completes the task under normal conditions. (Middle)&amp;nbsp;The baseline replays the recorded actions at their original timestamps, which initially appears to work. Because the game runs on a cloud gaming platform, however, random network delays quickly push the replay&amp;nbsp;out of sync, causing the trajectory to fail. (Right) Under the same conditions, the PIDM agent behaves differently. Instead of naively replaying actions, it continuously interprets visual input, predicts how the behavior is likely to unfold, and adapts its actions in real time. This allows it to correct delays, recover from deviations, and successfully reproduce the task in settings where naïve replay inevitably fails.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;



&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h3 class="wp-block-heading h4" id="a-player-and-a-pidm-agent-performing-a-complex-task-in-bleeding-edge"&gt;A player and a PIDM agent&amp;nbsp;performing a complex task in&amp;nbsp;&lt;em&gt;Bleeding Edge&lt;/em&gt;&lt;/h3&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;figcaption class="wp-element-caption"&gt;Video&amp;nbsp;3.&amp;nbsp;In this video, the task&amp;nbsp;exhibits&amp;nbsp;strong partial observability: correct behavior depends on whether a location is being visited for the first or second time. For example,&amp;nbsp;in the first encounter, the agent proceeds straight up the ramp; on the second, it turns right toward the bridge. Similarly, it may jump over a box on the first pass but walk around it on the second. The PIDM agent reproduces this trajectory reliably, using coarse future guidance to select actions in the correct direction.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;
&lt;/div&gt;



&lt;div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h3 class="wp-block-heading h4" id="visualization-of-the-2d-navigation-environment"&gt;Visualization of the 2D navigation environment&lt;/h3&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;figcaption class="wp-element-caption"&gt;Video&amp;nbsp;4.&amp;nbsp;These&amp;nbsp;videos show ten demonstrations for each of four tasks: Four Room, Zigzag, Maze, and Multiroom. In all cases, the setup is the same: the character (blue box) moves through the environment and must reach a sequence of goals (red squares).&amp;nbsp;The overlaid trajectories visualize the paths the player took; the models never see these paths. Instead, they observe only their character’s current location, the position of all goals, and whether each goal has already been reached. Because these demonstrations come from real players, no two paths are identical: players pause, take detours, or correct small mistakes along the way. That natural variability is exactly what the models must learn to handle.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;



&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h3 class="wp-block-heading h4" id="pidm-vs-bc-in-a-3d-environment"&gt;PIDM vs. BC in a 3D&amp;nbsp;environment&lt;/h3&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;figcaption class="wp-element-caption"&gt;Video&amp;nbsp;5. The PIDM agent achieves an 85% success rate with only fifteen demonstrations used in training. The BC agent struggles to stay on track and levels off around 60%.&amp;nbsp;The contrast illustrates how differently the two approaches perform when training data is limited.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Smart Replay - flowchart diagram showing the flow between Encoder, State Predictor, and Policy" class="wp-image-1161128" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay-BlogHeroFeature-1400x788_New.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;div class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section"&gt;
	
	&lt;div class="container"&gt;
		&lt;div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow"&gt;
			&lt;div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h2 class="wp-block-heading h3" id="at-a-glance"&gt;At a glance&lt;/h2&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Imitation learning becomes easier when an AI&amp;nbsp;agent&amp;nbsp;understands why an action is taken.&lt;/li&gt;



&lt;li&gt;Predictive Inverse Dynamics Models (PIDMs)&amp;nbsp;predict&amp;nbsp;plausible future states,&amp;nbsp;clarifying the direction of behavior during imitation&amp;nbsp;learning.&lt;/li&gt;



&lt;li&gt;Even imperfect predictions reduce ambiguity,&amp;nbsp;making&amp;nbsp;it clearer which action makes sense&amp;nbsp;in the moment.&lt;/li&gt;



&lt;li&gt;This makes PIDMs far more data‑efficient than traditional approaches.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;		&lt;/div&gt;
	&lt;/div&gt;

	&lt;/div&gt;



&lt;p&gt;Imitation&amp;nbsp;learning&amp;nbsp;teaches&amp;nbsp;AI agents by example: show the agent recordings of how people perform a task and let it&amp;nbsp;infer&amp;nbsp;what to do.&amp;nbsp;The&amp;nbsp;most common&amp;nbsp;approach,&amp;nbsp;Behavior Cloning&amp;nbsp;(BC),&amp;nbsp;frames this as a simple question: “Given the current state&amp;nbsp;of the environment, what action&amp;nbsp;would&amp;nbsp;an expert take?”&lt;/p&gt;



&lt;p&gt;In practice, this is done through supervised learning, where the states serve as inputs and expert actions as outputs. While simple in principle, BC often requires large demonstration datasets to account for the natural variability in human behavior, but collecting such datasets can be costly and difficult in real-world settings.&lt;/p&gt;



&lt;p&gt;Predictive Inverse Dynamics Models (PIDMs) offer a different take on imitation learning by changing how agents interpret human behavior. Instead of directly mapping states to actions, PIDMs break down the problem into two subproblems: predicting what should happen next and inferring an appropriate action to go from the current state to the predicted future state. While PIDMs often outperform BC, it has not been clear why they work so well, motivating a closer look at the mechanisms behind their performance.&lt;/p&gt;



&lt;p&gt;In the paper, “When does predictive inverse dynamics outperform behavior cloning?” we show how this two-stage approach enables PIDMs to learn effective policies from far fewer demonstrations than BC. By grounding the selection process in a plausible future, PIDMs provide a clearer basis for choosing an action&amp;nbsp;during inference. In practice, this can mean achieving comparable performance with as few as one-fifth the demonstrations required by BC, even when predictions are imperfect.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Figure 1. BC vs. PIDM architectures.&amp;nbsp;(Top) Behavior&amp;nbsp;Cloning learns&amp;nbsp;how to perform&amp;nbsp;a direct mapping from the current state to an action. (Bottom)&amp;nbsp;PIDMs add a state predictor that predicts future&amp;nbsp;states. They&amp;nbsp;then use an inverse dynamics model to predict the action&amp;nbsp;required&amp;nbsp;to move from the current state towards that future state. Both approaches share a common latent representation through a shared state encoder." class="wp-image-1161185" height="658" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/SmartReplay_FIG1.png" width="1009" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. BC vs. PIDM architectures.&amp;nbsp;(Top) Behavior&amp;nbsp;Cloning learns&amp;nbsp;how to perform&amp;nbsp;a direct mapping from the current state to an action. (Bottom)&amp;nbsp;PIDMs add a state predictor that predicts future&amp;nbsp;states. They&amp;nbsp;then use an inverse dynamics model to predict the action&amp;nbsp;required&amp;nbsp;to move from the current state towards that future state. Both approaches share a common latent representation through a shared state encoder.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="how-pidms-rethink-imitation"&gt;How PIDMs rethink imitation&lt;/h2&gt;



&lt;p&gt;PIDMs’ approach to imitation learning consists of two core elements: a model that forecasts plausible future states, and an inverse dynamics model (IDM) that predicts the action needed to move from the present state toward that future. Instead of asking, “What action would an expert take?” PIDMs effectively ask, “What would an expert try to achieve, and what action would lead to it?” This shift turns the information in the current observation (e.g., video frame) into a coherent sense of direction, reducing ambiguity about intent and making action prediction easier.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;video series&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;On Second Thought&lt;/h2&gt;
				
								&lt;p class="large" id="on-second-thought"&gt;A video series with Sinead Bovell built around the questions everyone’s asking about AI. With expert voices from across Microsoft, we break down the tension and promise of this rapidly changing technology, exploring what’s evolving and what’s possible.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="real-world-validation-in-a-3d-gameplay-environment"&gt;Real-world validation in a 3D gameplay environment&lt;/h2&gt;



&lt;p&gt;To&amp;nbsp;evaluate&amp;nbsp;PIDMs&amp;nbsp;under realistic conditions,&amp;nbsp;we trained&amp;nbsp;agents on human gameplay demonstrations in a visually rich video game. These conditions&amp;nbsp;include&amp;nbsp;operating&amp;nbsp;directly from raw video&amp;nbsp;input, interacting with&amp;nbsp;a complex 3D&amp;nbsp;environment in real time at 30 frames&amp;nbsp;per&amp;nbsp;second, and&amp;nbsp;handling&amp;nbsp;visual artifacts and unpredictable system delays.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The agents ran from beginning to end, taking video frames as input and continuously deciding which buttons to press and how to move the joysticks. Instead of relying on a hand-coded set of game variables and rules, the model worked directly from visual input, using past examples to predict what comes next and choosing actions that moved play in that direction.&lt;/p&gt;



&lt;p&gt;We ran all experiments on a cloud gaming platform, which introduced additional delays and visual distortions. Despite these challenges, the PIDM agents consistently matched human patterns of play and achieved high success rates across tasks, as shown in Video 1 below and Videos 2 and 3 in the appendix.&lt;/p&gt;



&lt;figure class="wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;figcaption class="wp-element-caption"&gt;Video 1. A player&amp;nbsp;(left)&amp;nbsp;and a PIDM agent&amp;nbsp;(right)&amp;nbsp;side by side playing the game&amp;nbsp;&lt;em&gt;Bleeding Edge&lt;/em&gt;.&amp;nbsp;Both&amp;nbsp;navigate the same trajectory,&amp;nbsp;jumping over obstacles and engaging&amp;nbsp;with&amp;nbsp;nonplayer&amp;nbsp;characters. Despite&amp;nbsp;network delays, the&amp;nbsp;agent closely matches the player’s timing and&amp;nbsp;movement&amp;nbsp;in real time.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="why-and-when-pidms-outperform-bc"&gt;Why and when PIDMs outperform BC&lt;/h2&gt;



&lt;p&gt;Of course, AI agents do not have access to future outcomes. They can only generate predictions based on available data, and those predictions are sometimes wrong. This creates a central trade‑off for PIDMs.&lt;/p&gt;



&lt;p&gt;On one hand, anticipating where the agent should be heading can clarify what action makes sense in the present. Knowing the intended direction helps narrow an otherwise ambiguous choice. On the other hand, inaccurate predictions can occasionally steer the model toward the wrong action.&lt;/p&gt;



&lt;p&gt;The key insight is that these effects are not symmetric. While prediction errors introduce some risk, reducing ambiguity in the present often matters more. Our theoretical analysis shows that even with imperfect predictions, PIDMs outperform BC as long as the prediction error remains modest. If future states were known perfectly, PIDMs would outperform BC outright.&lt;/p&gt;



&lt;p&gt;In practice, this means that clarifying intent often matters more than accurately predicting the future. That advantage is most evident in the situations where BC struggles: where human behavior varies and actions are driven by underlying goals rather than by what is immediately visible on the screen.&lt;/p&gt;



&lt;p&gt;BC requires many demonstrations because each example is noisy and open to multiple interpretations. PIDMs, by contrast, sharpen each demonstration by linking actions to the future states they aim to reach. As a result, PIDMs can learn effective action strategies from far fewer examples.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="evaluation"&gt;Evaluation&lt;/h2&gt;



&lt;p&gt;To test these ideas under realistic conditions, we designed a sequence of experiments that begins with a simple, interpretable 2D environment (Video 4 in the appendix) and culminates in a complex 3D video game. We trained both BC and PIDM on very small datasets, ranging from one to fifty demonstrations in the 2D environment and from five to thirty for the 3D video game. Across all tasks, PIDM reached high success rates with far fewer demonstrations than BC.&lt;/p&gt;



&lt;p&gt;In the 2D setting, BC needed two to five times more data to match PIDM’s performance (Figure 2). In the 3D game, BC needed 66% more data to achieve comparable results (Video 5 in the appendix).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2. Performance gains in the 2D environment. As the number of training demonstrations increases, PIDM consistently achieves higher success rates than BC across all four tasks. Curves show mean performance, with shading indicating variability across 20 experiments for reproducibility." class="wp-image-1161012" height="871" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay_blog_Fig2a-d.png" width="1166" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Performance gains in the 2D environment. As the number of training demonstrations increases, PIDM consistently achieves higher success rates than BC across all four tasks. Curves show mean performance, with shading indicating variability across 20 experiments for reproducibility. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="takeaway-intent-matters-in-imitation-learning"&gt;Takeaway: Intent matters in imitation learning&lt;/h2&gt;



&lt;p&gt;The main message of our investigation is simple: imitation becomes easier when intent is made explicit. Predicting a plausible future, even an imperfect one, helps resolve ambiguity about which action makes sense right now, much like driving more confidently in the fog when the driver already knows where the road is headed. PIDM shifts imitation learning from pure copying toward goal-oriented action.&lt;/p&gt;



&lt;p&gt;This approach has limits. If predictions of future states become too unreliable, they can mislead the model about the intended next move. In those cases, the added uncertainty can outweigh the benefit of reduced ambiguity, causing PIDM to underperform BC.&lt;/p&gt;



&lt;p&gt;But when predictions are reasonably accurate, reframing action prediction as “&lt;em&gt;How do I get there from here&lt;/em&gt;?” helps explain why learning from small, messy human datasets can be surprisingly effective. In settings where data is expensive and demonstrations are limited, that shift in perspective can make a meaningful difference.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="appendix-visualizations-and-results-videos"&gt;Appendix: Visualizations and results (videos)&lt;/h2&gt;



&lt;div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h3 class="wp-block-heading h4" id="a-player-a-naive-action-replay-baseline-and-a-pidm-agent-playing-bleeding-edge-1"&gt;A player, a naïve action-replay baseline, and a PIDM agent playing &lt;em&gt;Bleeding Edge&lt;/em&gt;&lt;/h3&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;figcaption class="wp-element-caption"&gt;Video&amp;nbsp;2. (Left)&amp;nbsp;The player completes the task under normal conditions. (Middle)&amp;nbsp;The baseline replays the recorded actions at their original timestamps, which initially appears to work. Because the game runs on a cloud gaming platform, however, random network delays quickly push the replay&amp;nbsp;out of sync, causing the trajectory to fail. (Right) Under the same conditions, the PIDM agent behaves differently. Instead of naively replaying actions, it continuously interprets visual input, predicts how the behavior is likely to unfold, and adapts its actions in real time. This allows it to correct delays, recover from deviations, and successfully reproduce the task in settings where naïve replay inevitably fails.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;



&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h3 class="wp-block-heading h4" id="a-player-and-a-pidm-agent-performing-a-complex-task-in-bleeding-edge"&gt;A player and a PIDM agent&amp;nbsp;performing a complex task in&amp;nbsp;&lt;em&gt;Bleeding Edge&lt;/em&gt;&lt;/h3&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;figcaption class="wp-element-caption"&gt;Video&amp;nbsp;3.&amp;nbsp;In this video, the task&amp;nbsp;exhibits&amp;nbsp;strong partial observability: correct behavior depends on whether a location is being visited for the first or second time. For example,&amp;nbsp;in the first encounter, the agent proceeds straight up the ramp; on the second, it turns right toward the bridge. Similarly, it may jump over a box on the first pass but walk around it on the second. The PIDM agent reproduces this trajectory reliably, using coarse future guidance to select actions in the correct direction.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;
&lt;/div&gt;



&lt;div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h3 class="wp-block-heading h4" id="visualization-of-the-2d-navigation-environment"&gt;Visualization of the 2D navigation environment&lt;/h3&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;figcaption class="wp-element-caption"&gt;Video&amp;nbsp;4.&amp;nbsp;These&amp;nbsp;videos show ten demonstrations for each of four tasks: Four Room, Zigzag, Maze, and Multiroom. In all cases, the setup is the same: the character (blue box) moves through the environment and must reach a sequence of goals (red squares).&amp;nbsp;The overlaid trajectories visualize the paths the player took; the models never see these paths. Instead, they observe only their character’s current location, the position of all goals, and whether each goal has already been reached. Because these demonstrations come from real players, no two paths are identical: players pause, take detours, or correct small mistakes along the way. That natural variability is exactly what the models must learn to handle.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;



&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h3 class="wp-block-heading h4" id="pidm-vs-bc-in-a-3d-environment"&gt;PIDM vs. BC in a 3D&amp;nbsp;environment&lt;/h3&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;figcaption class="wp-element-caption"&gt;Video&amp;nbsp;5. The PIDM agent achieves an 85% success rate with only fifteen demonstrations used in training. The BC agent struggles to stay on track and levels off around 60%.&amp;nbsp;The contrast illustrates how differently the two approaches perform when training data is limited.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/rethinking-imitation-learning-with-predictive-inverse-dynamics-models/</guid><pubDate>Thu, 05 Feb 2026 17:00:00 +0000</pubDate></item><item><title>Meta tests a stand-alone app for its AI-generated ‘Vibes’ videos (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/meta-tests-a-standalone-app-for-its-ai-generated-vibes-videos/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2194278734.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta is testing a stand-alone Vibes app, the company confirmed to TechCrunch on Thursday. Launched last September, Vibes lets you create and share short-form AI-generated videos and access a dedicated feed that displays AI videos from others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Think TikTok or Instagram Reels, but every video you come across is AI generated. Until now, the feed has lived in the Meta AI app. By making Vibes available outside of the Meta AI app, the company is positioning it as a more direct competitor to Sora, OpenAI’s AI-generated video and social app that launched shortly after Vibes.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Following the strong early traction of Vibes within Meta AI, we are testing a standalone app to build on that momentum,” Meta said in an emailed statement. “We’ve seen that users are increasingly leaning into the format to create, discover, and share AI-generated video with friends. This standalone app provides a dedicated home for that experience, offering people a more focused and immersive environment. We will look to expand the app further based on what we learn from the community.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The news was first reported by Platformer. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta says it doesn’t share specific numbers but claims Vibes has performed well, with Meta AI usage continuing to grow steadily since its launch, which it believes signals demand for a stand-alone app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant also notes that while users engage with content in Meta AI, a stand-alone app allows for a more focused experience for creation and engagement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vibes lets users generate a video from scratch or remix a video that they see on their feed. Before publishing, you can add new visuals, layer in music, and adjust styles. You can then post the video directly to the Vibes feed, DM it to others, or cross-post to Instagram and Facebook Stories and Reels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta says collaboration and sharing are on the rise, with many Vibes videos being messaged to friends, which the company says mirrors how people use Reels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth noting that Meta told TechCrunch last week that, in addition to testing new premium subscriptions across Facebook, Instagram, and WhatsApp, it’s going to explore subscriptions for AI features, including Vibes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although Vibes has been free since its launch, Meta plans to offer freemium access to Vibes video creation, with the option to subscribe to unlock additional video creation opportunities each month.&amp;nbsp;Meta plans to launch these test subscriptions in the coming months. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2194278734.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta is testing a stand-alone Vibes app, the company confirmed to TechCrunch on Thursday. Launched last September, Vibes lets you create and share short-form AI-generated videos and access a dedicated feed that displays AI videos from others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Think TikTok or Instagram Reels, but every video you come across is AI generated. Until now, the feed has lived in the Meta AI app. By making Vibes available outside of the Meta AI app, the company is positioning it as a more direct competitor to Sora, OpenAI’s AI-generated video and social app that launched shortly after Vibes.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Following the strong early traction of Vibes within Meta AI, we are testing a standalone app to build on that momentum,” Meta said in an emailed statement. “We’ve seen that users are increasingly leaning into the format to create, discover, and share AI-generated video with friends. This standalone app provides a dedicated home for that experience, offering people a more focused and immersive environment. We will look to expand the app further based on what we learn from the community.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The news was first reported by Platformer. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta says it doesn’t share specific numbers but claims Vibes has performed well, with Meta AI usage continuing to grow steadily since its launch, which it believes signals demand for a stand-alone app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant also notes that while users engage with content in Meta AI, a stand-alone app allows for a more focused experience for creation and engagement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vibes lets users generate a video from scratch or remix a video that they see on their feed. Before publishing, you can add new visuals, layer in music, and adjust styles. You can then post the video directly to the Vibes feed, DM it to others, or cross-post to Instagram and Facebook Stories and Reels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta says collaboration and sharing are on the rise, with many Vibes videos being messaged to friends, which the company says mirrors how people use Reels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth noting that Meta told TechCrunch last week that, in addition to testing new premium subscriptions across Facebook, Instagram, and WhatsApp, it’s going to explore subscriptions for AI features, including Vibes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although Vibes has been free since its launch, Meta plans to offer freemium access to Vibes video creation, with the option to subscribe to unlock additional video creation opportunities each month.&amp;nbsp;Meta plans to launch these test subscriptions in the coming months. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/meta-tests-a-standalone-app-for-its-ai-generated-vibes-videos/</guid><pubDate>Thu, 05 Feb 2026 17:19:01 +0000</pubDate></item><item><title>OpenAI is hoppin' mad about Anthropic's new Super Bowl TV ads (AI - Ars Technica)</title><link>https://arstechnica.com/information-technology/2026/02/openai-is-hoppin-mad-about-anthropics-new-super-bowl-tv-ads/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Sam Altman calls AI competitor “dishonest” and “authoritarian” in lengthy post on X.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A screenshot of one of the new Anthropic ads featuring the tagline, &amp;quot;Ads are coming to AI. But not to Claude.&amp;quot;" class="absolute inset-0 w-full h-full object-cover hidden" height="353" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/anthropic_ad_2-640x353.jpg" width="640" /&gt;
                  &lt;img alt="A screenshot of one of the new Anthropic ads featuring the tagline, &amp;quot;Ads are coming to AI. But not to Claude.&amp;quot;" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/anthropic_ad_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A screenshot of one of the new Anthropic ads featuring the tagline, "Ads are coming to AI. But not to Claude."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Anthropic

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday, OpenAI CEO Sam Altman and Chief Marketing Officer Kate Rouch complained on X after rival AI lab Anthropic released four commercials, two of which will run during the Super Bowl on Sunday, mocking the idea of including ads in AI chatbot conversations. Anthropic’s campaign seemingly touched a nerve at OpenAI just weeks after the ChatGPT maker began testing ads in a lower-cost tier of its chatbot.&lt;/p&gt;
&lt;p&gt;Altman called Anthropic’s ads “clearly dishonest,” accused the company of being “authoritarian,” and said it “serves an expensive product to rich people,” while Rouch wrote, “Real betrayal isn’t ads. It’s control.”&lt;/p&gt;
&lt;p&gt;Anthropic’s four commercials, part of a campaign called “A Time and a Place,” each open with a single word splashed across the screen: “Betrayal,” “Violation,” “Deception,” and “Treachery.” They depict scenarios where a person asks a human stand-in for an AI chatbot for personal advice, only to get blindsided by a product pitch.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Anthropic’s 2026 Super Bowl commercial.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;In one spot, a man asks a therapist-style chatbot (a woman sitting in a chair) how to communicate better with his mom. The bot offers a few suggestions, then pivots to promoting a fictional cougar-dating site called Golden Encounters.&lt;/p&gt;
&lt;p&gt;In another spot, a skinny man looking for fitness tips instead gets served an ad for height-boosting insoles. Each ad ends with the tagline: “Ads are coming to AI. But not to Claude.” Anthropic plans to air a 30-second version during Super Bowl LX, with a 60-second cut running in the pregame, according to CNBC.&lt;/p&gt;
&lt;p&gt;In the X posts, the OpenAI executives argue that these commercials are misleading because the planned ChatGPT ads will appear labeled at the bottom of conversational responses in banners and will not alter the chatbot’s answers.&lt;/p&gt;
&lt;p&gt;But there’s a slight twist: OpenAI’s own blog post about its ad plans states that the company will “test ads at the bottom of answers in ChatGPT when there’s a relevant sponsored product or service based on your current conversation,” meaning the ads will be conversation-specific.&lt;/p&gt;
&lt;p&gt;The financial backdrop explains some of the tension over ads in chatbots. As Ars previously reported, OpenAI struck more than $1.4 trillion in infrastructure deals in 2025 and expects to burn roughly $9 billion this year while generating about $13 billion in revenue. Only about 5 percent of ChatGPT’s 800 million weekly users pay for subscriptions. Anthropic is also not yet profitable, but it relies on enterprise contracts and paid subscriptions rather than advertising, and it has not taken on infrastructure commitments at the same scale as OpenAI.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Three OpenAI leaders weigh in&lt;/h2&gt;
&lt;p&gt;Competition between Anthropic and OpenAI is especially testy because several OpenAI employees left the company to found Anthropic in 2021. Currently, Anthropic’s Claude Code has pulled off something of a market upset, becoming a favorite among some software developers despite the company’s much smaller overall market share among chatbot users.&lt;/p&gt;
&lt;p&gt;Altman opened his lengthy post on X by granting that the ads were “funny” and that he “laughed.” But then the tone shifted. “I wonder why Anthropic would go for something so clearly dishonest,” he wrote. “We would obviously never run ads in the way Anthropic depicts them. We are not stupid and we know our users would reject that.”&lt;/p&gt;
&lt;p&gt;He went further: “I guess it’s on brand for Anthropic doublespeak to use a deceptive ad to critique theoretical deceptive ads that aren’t real, but a Super Bowl ad is not where I would expect it.”&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2139534 align-center"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="A screenshot of one of the new Anthropic ads featuring a woman as a stand-in for a chatbot." class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/anthropic_commercial_snapshot-1024x576.jpg" width="1024" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A screenshot of one of the new Anthropic ads featuring a woman as a stand-in for a chatbot.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Anthropic

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Altman framed the dispute as a fight over access. “More Texans use ChatGPT for free than total people use Claude in the US, so we have a differently shaped problem than they do,” he wrote. He then accused Anthropic of overreach: “Anthropic wants to control what people do with AI,” adding that Anthropic blocks “companies they don’t like from using their coding product (including us).” He closed with: “One authoritarian company won’t get us there on their own, to say nothing of the other obvious risks. It is a dark path.”&lt;/p&gt;
&lt;p&gt;OpenAI CMO Kate Rouch posted a response, calling the ads “funny” before pivoting. “Anthropic thinks powerful AI should be tightly controlled in small rooms in San Francisco and Davos,” she wrote. “That it’s too DANGEROUS for you.”&lt;/p&gt;
&lt;p&gt;Anthropic’s post declaring Claude ad-free does hedge a bit, however. “Should we need to revisit this approach, we’ll be transparent about our reasons for doing so,” Anthropic wrote.&lt;/p&gt;
&lt;p&gt;OpenAI President Greg Brockman pointed this out on X, asking Anthropic CEO Dario Amodei directly whether he would “commit to never selling Claude’s ‘users’ attention or data to advertisers,’” calling it a “genuine question” and noting that Anthropic’s blog post “makes it sound like you’re keeping the option open.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Sam Altman calls AI competitor “dishonest” and “authoritarian” in lengthy post on X.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A screenshot of one of the new Anthropic ads featuring the tagline, &amp;quot;Ads are coming to AI. But not to Claude.&amp;quot;" class="absolute inset-0 w-full h-full object-cover hidden" height="353" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/anthropic_ad_2-640x353.jpg" width="640" /&gt;
                  &lt;img alt="A screenshot of one of the new Anthropic ads featuring the tagline, &amp;quot;Ads are coming to AI. But not to Claude.&amp;quot;" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/anthropic_ad_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A screenshot of one of the new Anthropic ads featuring the tagline, "Ads are coming to AI. But not to Claude."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Anthropic

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday, OpenAI CEO Sam Altman and Chief Marketing Officer Kate Rouch complained on X after rival AI lab Anthropic released four commercials, two of which will run during the Super Bowl on Sunday, mocking the idea of including ads in AI chatbot conversations. Anthropic’s campaign seemingly touched a nerve at OpenAI just weeks after the ChatGPT maker began testing ads in a lower-cost tier of its chatbot.&lt;/p&gt;
&lt;p&gt;Altman called Anthropic’s ads “clearly dishonest,” accused the company of being “authoritarian,” and said it “serves an expensive product to rich people,” while Rouch wrote, “Real betrayal isn’t ads. It’s control.”&lt;/p&gt;
&lt;p&gt;Anthropic’s four commercials, part of a campaign called “A Time and a Place,” each open with a single word splashed across the screen: “Betrayal,” “Violation,” “Deception,” and “Treachery.” They depict scenarios where a person asks a human stand-in for an AI chatbot for personal advice, only to get blindsided by a product pitch.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Anthropic’s 2026 Super Bowl commercial.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;In one spot, a man asks a therapist-style chatbot (a woman sitting in a chair) how to communicate better with his mom. The bot offers a few suggestions, then pivots to promoting a fictional cougar-dating site called Golden Encounters.&lt;/p&gt;
&lt;p&gt;In another spot, a skinny man looking for fitness tips instead gets served an ad for height-boosting insoles. Each ad ends with the tagline: “Ads are coming to AI. But not to Claude.” Anthropic plans to air a 30-second version during Super Bowl LX, with a 60-second cut running in the pregame, according to CNBC.&lt;/p&gt;
&lt;p&gt;In the X posts, the OpenAI executives argue that these commercials are misleading because the planned ChatGPT ads will appear labeled at the bottom of conversational responses in banners and will not alter the chatbot’s answers.&lt;/p&gt;
&lt;p&gt;But there’s a slight twist: OpenAI’s own blog post about its ad plans states that the company will “test ads at the bottom of answers in ChatGPT when there’s a relevant sponsored product or service based on your current conversation,” meaning the ads will be conversation-specific.&lt;/p&gt;
&lt;p&gt;The financial backdrop explains some of the tension over ads in chatbots. As Ars previously reported, OpenAI struck more than $1.4 trillion in infrastructure deals in 2025 and expects to burn roughly $9 billion this year while generating about $13 billion in revenue. Only about 5 percent of ChatGPT’s 800 million weekly users pay for subscriptions. Anthropic is also not yet profitable, but it relies on enterprise contracts and paid subscriptions rather than advertising, and it has not taken on infrastructure commitments at the same scale as OpenAI.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Three OpenAI leaders weigh in&lt;/h2&gt;
&lt;p&gt;Competition between Anthropic and OpenAI is especially testy because several OpenAI employees left the company to found Anthropic in 2021. Currently, Anthropic’s Claude Code has pulled off something of a market upset, becoming a favorite among some software developers despite the company’s much smaller overall market share among chatbot users.&lt;/p&gt;
&lt;p&gt;Altman opened his lengthy post on X by granting that the ads were “funny” and that he “laughed.” But then the tone shifted. “I wonder why Anthropic would go for something so clearly dishonest,” he wrote. “We would obviously never run ads in the way Anthropic depicts them. We are not stupid and we know our users would reject that.”&lt;/p&gt;
&lt;p&gt;He went further: “I guess it’s on brand for Anthropic doublespeak to use a deceptive ad to critique theoretical deceptive ads that aren’t real, but a Super Bowl ad is not where I would expect it.”&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2139534 align-center"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="A screenshot of one of the new Anthropic ads featuring a woman as a stand-in for a chatbot." class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/anthropic_commercial_snapshot-1024x576.jpg" width="1024" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A screenshot of one of the new Anthropic ads featuring a woman as a stand-in for a chatbot.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Anthropic

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Altman framed the dispute as a fight over access. “More Texans use ChatGPT for free than total people use Claude in the US, so we have a differently shaped problem than they do,” he wrote. He then accused Anthropic of overreach: “Anthropic wants to control what people do with AI,” adding that Anthropic blocks “companies they don’t like from using their coding product (including us).” He closed with: “One authoritarian company won’t get us there on their own, to say nothing of the other obvious risks. It is a dark path.”&lt;/p&gt;
&lt;p&gt;OpenAI CMO Kate Rouch posted a response, calling the ads “funny” before pivoting. “Anthropic thinks powerful AI should be tightly controlled in small rooms in San Francisco and Davos,” she wrote. “That it’s too DANGEROUS for you.”&lt;/p&gt;
&lt;p&gt;Anthropic’s post declaring Claude ad-free does hedge a bit, however. “Should we need to revisit this approach, we’ll be transparent about our reasons for doing so,” Anthropic wrote.&lt;/p&gt;
&lt;p&gt;OpenAI President Greg Brockman pointed this out on X, asking Anthropic CEO Dario Amodei directly whether he would “commit to never selling Claude’s ‘users’ attention or data to advertisers,’” calling it a “genuine question” and noting that Anthropic’s blog post “makes it sound like you’re keeping the option open.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2026/02/openai-is-hoppin-mad-about-anthropics-new-super-bowl-tv-ads/</guid><pubDate>Thu, 05 Feb 2026 17:46:59 +0000</pubDate></item><item><title>Anthropic releases Opus 4.6 with new ‘agent teams’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/anthropic-releases-opus-4-6-with-new-agent-teams/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/Claude2_Blog_V1-1.webp?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Thursday, Anthropic released the latest version of Opus — its most advanced model and a particularly important model for Claude Code. Opus 4.5 was only released last November, and with 4.6, the company has sought to broaden its model’s capabilities and appeal, allowing for a greater variety of uses and customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perhaps the most notable addition to the newest version of Opus is the inclusion of what the company calls “agent teams” — teams of agents that can split larger tasks into segmented jobs.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Instead of one agent working through tasks sequentially, you can split the work across multiple agents — each owning its piece and coordinating directly with the others,” the company says. Scott White, Head of Product at Anthropic, compared the new feature to having a talented team of humans working for you, noting that the segmenting of agent responsibilities allows them “to coordinate in parallel [and work] faster.” The agent teams are currently available in a research preview for API users and subscribers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Opus 4.6 also comes with a longer context window — meaning that the program has a capacity to recall a greater amount of information per user session. The new model offers 1 million tokens of context, which is comparable to what the company’s Sonnet (versions 4 and 4.5) currently offers. Those context windows allow for work involving larger code bases and can also allow for the processing of larger documents, the company says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new version of Opus also integrates Claude directly into PowerPoint as an accessible side panel. This is a step up from PowerPoint’s previous integration with the chatbot. Previously, a user could tell Claude to create a PowerPoint deck, but the file would then have to be transferred to PowerPoint to edit the presentation, White said. Now the presentation can be crafted within PowerPoint, with direct help from Claude.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;White told TechCrunch that Opus has evolved from a model that was highly capable in one particular domain — software development — into a program that could be “really useful for a broader set” of knowledge workers. “We noticed a lot of people who are not professional software developers using Claude Code simply because it was a really amazing engine to do tasks,” he said. White added that the kinds of people the company has seen using it include not just software engineers, but product managers, financial analysts, and people from a variety of other industries.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/Claude2_Blog_V1-1.webp?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Thursday, Anthropic released the latest version of Opus — its most advanced model and a particularly important model for Claude Code. Opus 4.5 was only released last November, and with 4.6, the company has sought to broaden its model’s capabilities and appeal, allowing for a greater variety of uses and customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perhaps the most notable addition to the newest version of Opus is the inclusion of what the company calls “agent teams” — teams of agents that can split larger tasks into segmented jobs.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Instead of one agent working through tasks sequentially, you can split the work across multiple agents — each owning its piece and coordinating directly with the others,” the company says. Scott White, Head of Product at Anthropic, compared the new feature to having a talented team of humans working for you, noting that the segmenting of agent responsibilities allows them “to coordinate in parallel [and work] faster.” The agent teams are currently available in a research preview for API users and subscribers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Opus 4.6 also comes with a longer context window — meaning that the program has a capacity to recall a greater amount of information per user session. The new model offers 1 million tokens of context, which is comparable to what the company’s Sonnet (versions 4 and 4.5) currently offers. Those context windows allow for work involving larger code bases and can also allow for the processing of larger documents, the company says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new version of Opus also integrates Claude directly into PowerPoint as an accessible side panel. This is a step up from PowerPoint’s previous integration with the chatbot. Previously, a user could tell Claude to create a PowerPoint deck, but the file would then have to be transferred to PowerPoint to edit the presentation, White said. Now the presentation can be crafted within PowerPoint, with direct help from Claude.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;White told TechCrunch that Opus has evolved from a model that was highly capable in one particular domain — software development — into a program that could be “really useful for a broader set” of knowledge workers. “We noticed a lot of people who are not professional software developers using Claude Code simply because it was a really amazing engine to do tasks,” he said. White added that the kinds of people the company has seen using it include not just software engineers, but product managers, financial analysts, and people from a variety of other industries.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/anthropic-releases-opus-4-6-with-new-agent-teams/</guid><pubDate>Thu, 05 Feb 2026 17:51:13 +0000</pubDate></item><item><title>OpenAI launches a way for enterprises to build and manage AI agents (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/openai-launches-a-way-for-enterprises-to-build-and-manage-ai-agents/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2213399157.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI has launched a new product to help enterprises navigate the world of AI agents, focusing on agent management as critical infrastructure for enterprise AI adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, AI giant OpenAI announced the launch of OpenAI Frontier, an end-to-end platform designed for enterprises to build and manage AI agents. It’s an open platform, which means users can manage agents built outside of OpenAI too.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Frontier users can program AI agents to connect to external data and applications, which allows them to execute tasks far outside of the OpenAI platform. Users can also limit and manage what these agents have access to, and what they can do, of course.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI said Frontier was designed to work the same way companies manage human employees. Frontier offers an onboarding process for agents and a feedback loop that is meant to help them improve over time the same way a review might help an employee.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI touted enterprises, including HP, Oracle, State Farm, and Uber as customers, but Frontier is currently only available to a limited number of users with plans to roll out more generally in the coming months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company would not disclose pricing details in a press briefing earlier this week, according to reporting from The Verge. OpenAI declined to comment on pricing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Agent-management products become table stakes since AI agents rose to prominence in 2024. Salesforce has arguably the best-known such product, Agentforce, which the company launched in the fall of 2024. Others have quickly followed. LangChain is a notable player in the space that was founded in 2022 and has raised more than $150 million in venture capital. CrewAI is a smaller upstart that has raised more than $20 million in venture capital.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In December, global research and advisory firm Gartner released a report about this type of software and called agent management platforms both the “most valuable real estate in AI” and a necessary piece of infrastructure for enterprises to adopt AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s not surprising that OpenAI would release this platform in early 2026, as the company has made it clear that enterprise adoption is one of its main focus areas for this year. The company has also announced two notable enterprise deals this year with ServiceNow and Snowflake.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, if OpenAI wants to be a meaningful player in the enterprise space, offering a product like Frontier is a promising step.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2213399157.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI has launched a new product to help enterprises navigate the world of AI agents, focusing on agent management as critical infrastructure for enterprise AI adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, AI giant OpenAI announced the launch of OpenAI Frontier, an end-to-end platform designed for enterprises to build and manage AI agents. It’s an open platform, which means users can manage agents built outside of OpenAI too.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Frontier users can program AI agents to connect to external data and applications, which allows them to execute tasks far outside of the OpenAI platform. Users can also limit and manage what these agents have access to, and what they can do, of course.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI said Frontier was designed to work the same way companies manage human employees. Frontier offers an onboarding process for agents and a feedback loop that is meant to help them improve over time the same way a review might help an employee.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI touted enterprises, including HP, Oracle, State Farm, and Uber as customers, but Frontier is currently only available to a limited number of users with plans to roll out more generally in the coming months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company would not disclose pricing details in a press briefing earlier this week, according to reporting from The Verge. OpenAI declined to comment on pricing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Agent-management products become table stakes since AI agents rose to prominence in 2024. Salesforce has arguably the best-known such product, Agentforce, which the company launched in the fall of 2024. Others have quickly followed. LangChain is a notable player in the space that was founded in 2022 and has raised more than $150 million in venture capital. CrewAI is a smaller upstart that has raised more than $20 million in venture capital.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In December, global research and advisory firm Gartner released a report about this type of software and called agent management platforms both the “most valuable real estate in AI” and a necessary piece of infrastructure for enterprises to adopt AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s not surprising that OpenAI would release this platform in early 2026, as the company has made it clear that enterprise adoption is one of its main focus areas for this year. The company has also announced two notable enterprise deals this year with ServiceNow and Snowflake.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, if OpenAI wants to be a meaningful player in the enterprise space, offering a product like Frontier is a promising step.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/openai-launches-a-way-for-enterprises-to-build-and-manage-ai-agents/</guid><pubDate>Thu, 05 Feb 2026 18:09:50 +0000</pubDate></item><item><title>Elon Musk is getting serious about orbital data centers (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/elon-musk-is-getting-serious-about-orbital-data-centers/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Screen-Shot-2026-02-05-at-10.07.05-AM.jpg?resize=1200,924" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Friday, when SpaceX filed plans with the Federal Communications Commission (FCC) for a million-satellite data center network, you might have thought Elon Musk was having a bit of fun with us. But a week later, it is clear that he is dead serious.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most obvious step, of course, is the formal merger between SpaceX and xAI that went forward on Monday, officially drawing together Musk’s space and AI ventures in a way that makes a lot more sense if there’s some kind of joint infrastructure project planned.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But even beyond the merger, we’re starting to see the idea of orbital AI data clusters — essentially, networks of computers operating in space — cohere into an actual plan. On Wednesday, the FCC accepted the filing and set a schedule seeking public comment. It’s a pro forma step normally, but FCC chairman Brendan Carr took the unusual step of sharing the filing on X. Throughout his tenure as chairman, Carr has shown himself eager to help Trump’s friends and punish his enemies — so as long as Musk stays on Trump’s good side, the proposal is likely to sail through without issue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, Elon Musk has started to flesh out the argument for orbital data centers in public. On a new episode of Stripe co-founder Patrick Collison’s podcast “Cheeky Pint,” which also featured guest Dwarkesh Patel, Musk laid out the basic case for moving most of our AI computing power into space. Essentially, solar panels produce more power in space, so you can cut down on one of the main operating expenses for data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s harder to scale on the ground than it is to scale in space,” Musk said in the podcast. “Any given solar panel is going to give you about five times more power in space than on the ground, so it’s actually much cheaper to do in space.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Close listeners will note that there is a bit of a gap in the logic here! It’s true that solar panels produce more power in space, but since power isn’t the only cost in operating a data center and solar panels aren’t the only way to power a data center, it doesn’t follow that it’s cheaper to do the whole thing in orbit, as Patel noted in the podcast. Patel also raised concerns about servicing GPUs that fail during AI model training, but you’ll have to listen to the full episode for that.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, Musk was undeterred, marking 2028 as a tipping point year for orbital data centers. “You can mark my words, in 36 months but probably closer to 30 months, the most economically compelling place to put AI will be space,” Musk said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;He didn’t stop there. “Five years from now, my prediction is we will launch and be operating every year more AI in space than the cumulative total on Earth,” Musk continued.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For context, as of 2030, global data center capacity will be an estimated 200 GW, which is roughly a trillion dollars’ worth of infrastructure when you’re just putting it on the ground.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, SpaceX makes its money by launching things into orbit, so all this is pretty convenient for Musk — particularly now that SpaceX has an AI company attached to it. And with the new SpaceX-xAI conglomerate headed for an IPO in just a few months, you can expect to hear a lot more about orbital data centers in the months ahead. With tech companies still pouring hundreds of billions of dollars into data center spending each year, there’s a real chance that not all the money will remain earthbound.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Screen-Shot-2026-02-05-at-10.07.05-AM.jpg?resize=1200,924" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Friday, when SpaceX filed plans with the Federal Communications Commission (FCC) for a million-satellite data center network, you might have thought Elon Musk was having a bit of fun with us. But a week later, it is clear that he is dead serious.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most obvious step, of course, is the formal merger between SpaceX and xAI that went forward on Monday, officially drawing together Musk’s space and AI ventures in a way that makes a lot more sense if there’s some kind of joint infrastructure project planned.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But even beyond the merger, we’re starting to see the idea of orbital AI data clusters — essentially, networks of computers operating in space — cohere into an actual plan. On Wednesday, the FCC accepted the filing and set a schedule seeking public comment. It’s a pro forma step normally, but FCC chairman Brendan Carr took the unusual step of sharing the filing on X. Throughout his tenure as chairman, Carr has shown himself eager to help Trump’s friends and punish his enemies — so as long as Musk stays on Trump’s good side, the proposal is likely to sail through without issue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, Elon Musk has started to flesh out the argument for orbital data centers in public. On a new episode of Stripe co-founder Patrick Collison’s podcast “Cheeky Pint,” which also featured guest Dwarkesh Patel, Musk laid out the basic case for moving most of our AI computing power into space. Essentially, solar panels produce more power in space, so you can cut down on one of the main operating expenses for data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s harder to scale on the ground than it is to scale in space,” Musk said in the podcast. “Any given solar panel is going to give you about five times more power in space than on the ground, so it’s actually much cheaper to do in space.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Close listeners will note that there is a bit of a gap in the logic here! It’s true that solar panels produce more power in space, but since power isn’t the only cost in operating a data center and solar panels aren’t the only way to power a data center, it doesn’t follow that it’s cheaper to do the whole thing in orbit, as Patel noted in the podcast. Patel also raised concerns about servicing GPUs that fail during AI model training, but you’ll have to listen to the full episode for that.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, Musk was undeterred, marking 2028 as a tipping point year for orbital data centers. “You can mark my words, in 36 months but probably closer to 30 months, the most economically compelling place to put AI will be space,” Musk said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;He didn’t stop there. “Five years from now, my prediction is we will launch and be operating every year more AI in space than the cumulative total on Earth,” Musk continued.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For context, as of 2030, global data center capacity will be an estimated 200 GW, which is roughly a trillion dollars’ worth of infrastructure when you’re just putting it on the ground.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, SpaceX makes its money by launching things into orbit, so all this is pretty convenient for Musk — particularly now that SpaceX has an AI company attached to it. And with the new SpaceX-xAI conglomerate headed for an IPO in just a few months, you can expect to hear a lot more about orbital data centers in the months ahead. With tech companies still pouring hundreds of billions of dollars into data center spending each year, there’s a real chance that not all the money will remain earthbound.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/elon-musk-is-getting-serious-about-orbital-data-centers/</guid><pubDate>Thu, 05 Feb 2026 18:50:49 +0000</pubDate></item><item><title>[NEW] OpenAI launches new agentic coding model only minutes after Anthropic drops its own (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/openai-launches-new-agentic-coding-model-only-minutes-after-anthropic-drops-its-own/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/GettyImages-2224158119.jpg?resize=1200,837" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Monday, OpenAI launched Codex, an agentic coding tool marketed to software developers. Today, OpenAI also launched a new model designed to turbo-charge Codex: GPT-5.3 Codex.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says that the model transforms Codex&amp;nbsp;from an agent that can merely “write and review code” to one that can do “nearly anything developers and professionals do on a computer, expanding who can build software and how work gets done.” Having tested its new model against a number of performance benchmarks, OpenAI claims that it can create “highly functional complex games and apps from scratch over the course of days.” &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI says that GPT-5.3 Codex is also 25% faster than its previous model (GPT-5.2) and that it was the company’s first model that “was instrumental in creating itself,” meaning that the company’s staff used early versions of the program to debug itself and evaluate how it was performing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new model release notably follows hot on the heels of a new agentic coding model released by its competitor, Anthropic. Indeed, OpenAI and Anthropic had originally planned to release their two agentic coding tools at the exact same time: 10 a.m. PST. However, not long before the original release time, Anthropic moved its release date up by 15 minutes, slightly besting OpenAI in the race to publicize the models.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/GettyImages-2224158119.jpg?resize=1200,837" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Monday, OpenAI launched Codex, an agentic coding tool marketed to software developers. Today, OpenAI also launched a new model designed to turbo-charge Codex: GPT-5.3 Codex.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says that the model transforms Codex&amp;nbsp;from an agent that can merely “write and review code” to one that can do “nearly anything developers and professionals do on a computer, expanding who can build software and how work gets done.” Having tested its new model against a number of performance benchmarks, OpenAI claims that it can create “highly functional complex games and apps from scratch over the course of days.” &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI says that GPT-5.3 Codex is also 25% faster than its previous model (GPT-5.2) and that it was the company’s first model that “was instrumental in creating itself,” meaning that the company’s staff used early versions of the program to debug itself and evaluate how it was performing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new model release notably follows hot on the heels of a new agentic coding model released by its competitor, Anthropic. Indeed, OpenAI and Anthropic had originally planned to release their two agentic coding tools at the exact same time: 10 a.m. PST. However, not long before the original release time, Anthropic moved its release date up by 15 minutes, slightly besting OpenAI in the race to publicize the models.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/openai-launches-new-agentic-coding-model-only-minutes-after-anthropic-drops-its-own/</guid><pubDate>Thu, 05 Feb 2026 20:01:39 +0000</pubDate></item><item><title>[NEW] Helping AI agents search to get the best results out of large language models (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/helping-ai-agents-search-to-get-best-results-from-llms-0205</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/encompass2-mit-00_0.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-e7f34161-7fff-b8ec-7efe-e9bf6135715c"&gt;Whether you’re a scientist brainstorming research ideas or a CEO hoping to automate a task in human resources or finance, you’ll find that artificial intelligence tools are becoming the assistants you didn’t know you needed. In particular,&amp;nbsp;many professionals are tapping into the talents of semi-autonomous software systems called AI agents, which can call on AI at specific points to solve problems and complete tasks.&lt;/p&gt;&lt;p&gt;AI agents are particularly effective when they use large language models (LLMs) because those systems are powerful, efficient, and adaptable. One way to program such technology is by describing in code what you want your system to do (the “workflow”), including when it should use an LLM. If you were a software company trying to revamp your old codebase to use a more modern programming language for better optimizations and safety, you might build a system that uses an LLM to translate the codebase one file at a time, testing each file as you go.&lt;/p&gt;&lt;p&gt;But what happens when LLMs make mistakes? You’ll want the agent to backtrack to make another attempt, incorporating lessons it learned from previous mistakes. Coding this up can take as much effort as implementing the original agent; if your system for translating a codebase contained thousands of lines of code, then you’d be making thousands of lines of code changes or additions to support the logic for backtracking when LLMs make mistakes.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr" id="docs-internal-guid-e7f34161-7fff-b8ec-7efe-e9bf6135715c"&gt;To save programmers time and effort, researchers with MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Asari AI have developed a framework called “EnCompass.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;With EnCompass, you no longer have to make these changes yourself. Instead, when EnCompass runs your program, it automatically backtracks if LLMs make mistakes. EnCompass can also make clones of the program runtime to make multiple attempts in parallel in search of the best solution. In full generality, EnCompass searches over the different possible paths your agent could take as a result of the different possible outputs of all the LLM calls, looking for the path where the LLM finds the best solution.&lt;/p&gt;&lt;p&gt;Then, all you have to do is to annotate the locations where you may want to backtrack or clone the program runtime, as well as record any information that may be useful to the strategy used to search over the different possible execution paths of your agent (the search strategy). You can then separately specify the search strategy — you could either use one that EnCompass provides out of the box or, if desired, implement your own custom search strategy.&lt;/p&gt;&lt;p dir="ltr"&gt;“With EnCompass, we’ve separated the search strategy from the underlying workflow of an AI agent,” says lead author Zhening Li ’25, MEng ’25, who is an MIT electrical engineering and computer science (EECS) PhD student, CSAIL researcher, and research consultant at Asari AI. “Our framework lets programmers easily experiment with different search strategies to find the one that makes the AI agent perform the best.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;EnCompass was used for agents implemented as Python programs that call LLMs, where it demonstrated noticeable code savings. EnCompass reduced coding effort for implementing search by up to 80 percent across agents, such as an agent for translating code repositories and for discovering transformation rules of digital grids. In the future, EnCompass could enable agents to tackle large-scale tasks, including managing massive code libraries, designing and carrying out science experiments, and creating blueprints for rockets and other hardware.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Branching out&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;When programming your agent, you mark particular operations — such as calls to an LLM — where results may vary. These annotations are called “branchpoints.” If you imagine your agent program as generating a single plot line of a story, then adding branchpoints turns the story into a choose-your-own-adventure story game, where branchpoints are locations where the plot branches into multiple future plot lines.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;You can then specify the strategy that EnCompass uses to navigate that story game, in search of the best possible ending to the story. This can include launching parallel threads of execution or backtracking to a previous branchpoint when you get stuck in a dead end.&lt;/p&gt;&lt;p&gt;Users can also plug-and-play a few common search strategies provided by EnCompass out of the box, or define their own custom strategy. For example, you could opt for Monte Carlo tree search, which builds a search tree by balancing exploration and exploitation, or beam search, which keeps the best few outputs from every step. EnCompass makes it easy to experiment with different approaches to find the best strategy to maximize the likelihood of successfully completing your task.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;The coding efficiency of EnCompass&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;So just how code-efficient is EnCompass for adding search to agent programs? According to researchers’ findings, the framework drastically cut down how much programmers needed to add to their agent programs to add search, helping them experiment with different strategies to find the one that performs the best.&lt;/p&gt;&lt;p&gt;For example, the researchers applied EnCompass to an agent that translates a repository of code from the Java programming language, which is commonly used to program apps and enterprise software, to Python. They found that implementing search with EnCompass — mainly involving adding branchpoint annotations and annotations that record how well each step did — required 348 fewer lines of code (about 82 percent) than implementing it by hand. They also demonstrated how EnCompass enabled them to easily try out different search strategies, identifying the best strategy to be a two-level beam search algorithm, achieving an accuracy boost of 15 to 40 percent across five different repositories at a search budget of 16 times the LLM calls made by the agent without search.&lt;/p&gt;&lt;p dir="ltr"&gt;“As LLMs become a more integral part of everyday software, it becomes more important to understand how to efficiently build software that leverages their strengths and works around their limitations,” says co-author Armando Solar-Lezama, who is an MIT professor of EECS and CSAIL principal investigator. “EnCompass is an important step in that direction.”&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers add that EnCompass targets agents where a program specifies the steps of the high-level workflow; the current iteration of their framework is less applicable to agents that are entirely controlled by an LLM. “In those agents, instead of having a program that specifies the steps and then using an LLM to carry out those steps, the LLM itself decides everything,” says Li. “There is no underlying programmatic workflow, so you can execute inference-time search on whatever the LLM invents on the fly. In this case, there’s less need for a tool like EnCompass that modifies how a program executes with search and backtracking.”&lt;/p&gt;&lt;p dir="ltr"&gt;Li and his colleagues plan to extend EnCompass to more general search frameworks for AI agents. They also plan to test their system on more complex tasks to refine it for real-world uses, including at companies. What’s more, they’re evaluating how well EnCompass helps agents work with humans on tasks like brainstorming hardware designs or translating much larger code libraries. For now, EnCompass is a powerful building block that enables humans to tinker with AI agents more easily, improving their performance.&lt;/p&gt;&lt;p dir="ltr"&gt;“EnCompass arrives at a timely moment, as AI-driven agents and search-based techniques are beginning to reshape workflows in software engineering,” says Carnegie Mellon University Professor Yiming Yang, who wasn’t involved in the research. “By cleanly separating an agent’s programming logic from its inference-time search strategy, the framework offers a principled way to explore how structured search can enhance code generation, translation, and analysis. This abstraction provides a solid foundation for more systematic and reliable search-driven approaches to software development.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Li and Solar-Lezama wrote the paper with two Asari AI researchers: Caltech Professor Yisong Yue, an advisor at the company; and senior author Stephan Zheng, who is the founder and CEO. Their work was supported by Asari AI.&lt;/p&gt;&lt;p&gt;The team’s work was presented at the Conference on Neural Information Processing Systems (NeurIPS) in December.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/encompass2-mit-00_0.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-e7f34161-7fff-b8ec-7efe-e9bf6135715c"&gt;Whether you’re a scientist brainstorming research ideas or a CEO hoping to automate a task in human resources or finance, you’ll find that artificial intelligence tools are becoming the assistants you didn’t know you needed. In particular,&amp;nbsp;many professionals are tapping into the talents of semi-autonomous software systems called AI agents, which can call on AI at specific points to solve problems and complete tasks.&lt;/p&gt;&lt;p&gt;AI agents are particularly effective when they use large language models (LLMs) because those systems are powerful, efficient, and adaptable. One way to program such technology is by describing in code what you want your system to do (the “workflow”), including when it should use an LLM. If you were a software company trying to revamp your old codebase to use a more modern programming language for better optimizations and safety, you might build a system that uses an LLM to translate the codebase one file at a time, testing each file as you go.&lt;/p&gt;&lt;p&gt;But what happens when LLMs make mistakes? You’ll want the agent to backtrack to make another attempt, incorporating lessons it learned from previous mistakes. Coding this up can take as much effort as implementing the original agent; if your system for translating a codebase contained thousands of lines of code, then you’d be making thousands of lines of code changes or additions to support the logic for backtracking when LLMs make mistakes.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr" id="docs-internal-guid-e7f34161-7fff-b8ec-7efe-e9bf6135715c"&gt;To save programmers time and effort, researchers with MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Asari AI have developed a framework called “EnCompass.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;With EnCompass, you no longer have to make these changes yourself. Instead, when EnCompass runs your program, it automatically backtracks if LLMs make mistakes. EnCompass can also make clones of the program runtime to make multiple attempts in parallel in search of the best solution. In full generality, EnCompass searches over the different possible paths your agent could take as a result of the different possible outputs of all the LLM calls, looking for the path where the LLM finds the best solution.&lt;/p&gt;&lt;p&gt;Then, all you have to do is to annotate the locations where you may want to backtrack or clone the program runtime, as well as record any information that may be useful to the strategy used to search over the different possible execution paths of your agent (the search strategy). You can then separately specify the search strategy — you could either use one that EnCompass provides out of the box or, if desired, implement your own custom search strategy.&lt;/p&gt;&lt;p dir="ltr"&gt;“With EnCompass, we’ve separated the search strategy from the underlying workflow of an AI agent,” says lead author Zhening Li ’25, MEng ’25, who is an MIT electrical engineering and computer science (EECS) PhD student, CSAIL researcher, and research consultant at Asari AI. “Our framework lets programmers easily experiment with different search strategies to find the one that makes the AI agent perform the best.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;EnCompass was used for agents implemented as Python programs that call LLMs, where it demonstrated noticeable code savings. EnCompass reduced coding effort for implementing search by up to 80 percent across agents, such as an agent for translating code repositories and for discovering transformation rules of digital grids. In the future, EnCompass could enable agents to tackle large-scale tasks, including managing massive code libraries, designing and carrying out science experiments, and creating blueprints for rockets and other hardware.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Branching out&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;When programming your agent, you mark particular operations — such as calls to an LLM — where results may vary. These annotations are called “branchpoints.” If you imagine your agent program as generating a single plot line of a story, then adding branchpoints turns the story into a choose-your-own-adventure story game, where branchpoints are locations where the plot branches into multiple future plot lines.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;You can then specify the strategy that EnCompass uses to navigate that story game, in search of the best possible ending to the story. This can include launching parallel threads of execution or backtracking to a previous branchpoint when you get stuck in a dead end.&lt;/p&gt;&lt;p&gt;Users can also plug-and-play a few common search strategies provided by EnCompass out of the box, or define their own custom strategy. For example, you could opt for Monte Carlo tree search, which builds a search tree by balancing exploration and exploitation, or beam search, which keeps the best few outputs from every step. EnCompass makes it easy to experiment with different approaches to find the best strategy to maximize the likelihood of successfully completing your task.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;The coding efficiency of EnCompass&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;So just how code-efficient is EnCompass for adding search to agent programs? According to researchers’ findings, the framework drastically cut down how much programmers needed to add to their agent programs to add search, helping them experiment with different strategies to find the one that performs the best.&lt;/p&gt;&lt;p&gt;For example, the researchers applied EnCompass to an agent that translates a repository of code from the Java programming language, which is commonly used to program apps and enterprise software, to Python. They found that implementing search with EnCompass — mainly involving adding branchpoint annotations and annotations that record how well each step did — required 348 fewer lines of code (about 82 percent) than implementing it by hand. They also demonstrated how EnCompass enabled them to easily try out different search strategies, identifying the best strategy to be a two-level beam search algorithm, achieving an accuracy boost of 15 to 40 percent across five different repositories at a search budget of 16 times the LLM calls made by the agent without search.&lt;/p&gt;&lt;p dir="ltr"&gt;“As LLMs become a more integral part of everyday software, it becomes more important to understand how to efficiently build software that leverages their strengths and works around their limitations,” says co-author Armando Solar-Lezama, who is an MIT professor of EECS and CSAIL principal investigator. “EnCompass is an important step in that direction.”&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers add that EnCompass targets agents where a program specifies the steps of the high-level workflow; the current iteration of their framework is less applicable to agents that are entirely controlled by an LLM. “In those agents, instead of having a program that specifies the steps and then using an LLM to carry out those steps, the LLM itself decides everything,” says Li. “There is no underlying programmatic workflow, so you can execute inference-time search on whatever the LLM invents on the fly. In this case, there’s less need for a tool like EnCompass that modifies how a program executes with search and backtracking.”&lt;/p&gt;&lt;p dir="ltr"&gt;Li and his colleagues plan to extend EnCompass to more general search frameworks for AI agents. They also plan to test their system on more complex tasks to refine it for real-world uses, including at companies. What’s more, they’re evaluating how well EnCompass helps agents work with humans on tasks like brainstorming hardware designs or translating much larger code libraries. For now, EnCompass is a powerful building block that enables humans to tinker with AI agents more easily, improving their performance.&lt;/p&gt;&lt;p dir="ltr"&gt;“EnCompass arrives at a timely moment, as AI-driven agents and search-based techniques are beginning to reshape workflows in software engineering,” says Carnegie Mellon University Professor Yiming Yang, who wasn’t involved in the research. “By cleanly separating an agent’s programming logic from its inference-time search strategy, the framework offers a principled way to explore how structured search can enhance code generation, translation, and analysis. This abstraction provides a solid foundation for more systematic and reliable search-driven approaches to software development.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Li and Solar-Lezama wrote the paper with two Asari AI researchers: Caltech Professor Yisong Yue, an advisor at the company; and senior author Stephan Zheng, who is the founder and CEO. Their work was supported by Asari AI.&lt;/p&gt;&lt;p&gt;The team’s work was presented at the Conference on Neural Information Processing Systems (NeurIPS) in December.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/helping-ai-agents-search-to-get-best-results-from-llms-0205</guid><pubDate>Thu, 05 Feb 2026 21:30:00 +0000</pubDate></item><item><title>[NEW] With GPT-5.3-Codex, OpenAI pitches Codex for more than just writing code (AI - Ars Technica)</title><link>https://arstechnica.com/ai/2026/02/with-gpt-5-3-codex-openai-pitches-codex-for-more-than-just-writing-code/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The emphasis is on “mid-turn steering and frequent progress updates.”
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A screenshot of a simple panel, with conversations listed on the left" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Codex-dark-640x360.jpg" width="640" /&gt;
                  &lt;img alt="A screenshot of a simple panel, with conversations listed on the left" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Codex-dark-1152x648-1770052639.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Codex macOS app.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Today, OpenAI announced GPT-5.3-Codex, a new version of its frontier coding model that will be available via the command line, IDE extension, web interface, and the new macOS desktop app. (No API access yet, but it’s coming.)&lt;/p&gt;
&lt;p&gt;GPT-5.3-Codex outperforms GPT-5.2-Codex and GPT-5.2 in SWE-Bench Pro, Terminal-Bench 2.0, and other benchmarks, according to the company’s testing.&lt;/p&gt;
&lt;p&gt;There are already a few headlines out there saying “Codex built itself,” but let’s reality-check that, as that’s an overstatement. The domains OpenAI described using it for here are similar to the ones you see in some other enterprise software development firms now: managing deployments, debugging, and handling test results and evaluations. There is no claim here that GPT-5.3-Codex built itself.&lt;/p&gt;
&lt;p&gt;Instead, OpenAI says GPT-5.3-Codex was “instrumental in creating itself.” You can read more about what that means in the company’s blog post.&lt;/p&gt;
&lt;p&gt;But that’s part of the pitch with this model update—OpenAI is trying to position Codex as a tool that does more than generate lines of code. The goal is to make it useful for “all of the work in the software lifecycle—debugging, deploying, monitoring, writing PRDs, editing copy, user research, tests, metrics, and more.” There’s also an emphasis on steering the model mid-task and frequent status updates.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It’s worth mentioning that the general-purpose ChatGPT model is still version numbered 5.2. With Codex moving to 5.3, OpenAI may be planning a similar update for ChatGPT in the near future, but nothing has been announced yet.&lt;/p&gt;
&lt;p&gt;There are no announced changes to limits or pricing with this model, but OpenAI says that, as of this update, models will run 25 percent faster for Codex users, “thanks to improvements in our infrastructure and inference stack.”&lt;/p&gt;
&lt;p&gt;OpenAI also notes that “what’s next” for Codex is “moving beyond writing code to using it as a tool to operate a computer and get real work done end to end.” That’s something that people have already been using these tools for via MCP and other methods, and which Anthropic started rolling out with Claude Cowork a few weeks back.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The emphasis is on “mid-turn steering and frequent progress updates.”
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A screenshot of a simple panel, with conversations listed on the left" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Codex-dark-640x360.jpg" width="640" /&gt;
                  &lt;img alt="A screenshot of a simple panel, with conversations listed on the left" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Codex-dark-1152x648-1770052639.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Codex macOS app.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Today, OpenAI announced GPT-5.3-Codex, a new version of its frontier coding model that will be available via the command line, IDE extension, web interface, and the new macOS desktop app. (No API access yet, but it’s coming.)&lt;/p&gt;
&lt;p&gt;GPT-5.3-Codex outperforms GPT-5.2-Codex and GPT-5.2 in SWE-Bench Pro, Terminal-Bench 2.0, and other benchmarks, according to the company’s testing.&lt;/p&gt;
&lt;p&gt;There are already a few headlines out there saying “Codex built itself,” but let’s reality-check that, as that’s an overstatement. The domains OpenAI described using it for here are similar to the ones you see in some other enterprise software development firms now: managing deployments, debugging, and handling test results and evaluations. There is no claim here that GPT-5.3-Codex built itself.&lt;/p&gt;
&lt;p&gt;Instead, OpenAI says GPT-5.3-Codex was “instrumental in creating itself.” You can read more about what that means in the company’s blog post.&lt;/p&gt;
&lt;p&gt;But that’s part of the pitch with this model update—OpenAI is trying to position Codex as a tool that does more than generate lines of code. The goal is to make it useful for “all of the work in the software lifecycle—debugging, deploying, monitoring, writing PRDs, editing copy, user research, tests, metrics, and more.” There’s also an emphasis on steering the model mid-task and frequent status updates.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It’s worth mentioning that the general-purpose ChatGPT model is still version numbered 5.2. With Codex moving to 5.3, OpenAI may be planning a similar update for ChatGPT in the near future, but nothing has been announced yet.&lt;/p&gt;
&lt;p&gt;There are no announced changes to limits or pricing with this model, but OpenAI says that, as of this update, models will run 25 percent faster for Codex users, “thanks to improvements in our infrastructure and inference stack.”&lt;/p&gt;
&lt;p&gt;OpenAI also notes that “what’s next” for Codex is “moving beyond writing code to using it as a tool to operate a computer and get real work done end to end.” That’s something that people have already been using these tools for via MCP and other methods, and which Anthropic started rolling out with Claude Cowork a few weeks back.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2026/02/with-gpt-5-3-codex-openai-pitches-codex-for-more-than-just-writing-code/</guid><pubDate>Thu, 05 Feb 2026 21:47:06 +0000</pubDate></item><item><title>[NEW] Amazon and Google are winning the AI capex race — but what’s the prize? (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/amazon-and-google-are-winning-the-ai-capex-race-but-whats-the-prize/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2215577882.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sometimes, it can seem like the AI industry is racing to see who can spend the most money on data centers. Whoever builds the most data centers will have the most compute, the thinking goes, and thus be able to build the best AI products, which will guarantee victory in the years to come. There are limits to this way of thinking — traditionally, businesses eventually succeed by making &lt;em&gt;more&lt;/em&gt; money and spending &lt;em&gt;less&lt;/em&gt; — but it’s proven remarkably persuasive for large tech companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If that is the game, Amazon does seem to be winning. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company announced in its earnings on Thursday that it projects $200 billion in capital expenditures throughout 2026, across “AI, chips, robotics, and low earth orbit satellites.” That’s up from the $131.8 billion in capex in 2025. It’s tempting to attribute the whole capex budget to AI. But unlike most of its competitors, Amazon has a significant physical plant, some of which is being converted for use by expensive robots, so the non-AI expenses aren’t so easy to wave away.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is close behind. In its earnings on Wednesday, the company projected between $175 billion and $185 billion in capital expenditures for 2026, up from $91.4 billion the previous year. It’s significantly more than the company spent on fixed assets last year, and significantly more than most of its competitors are spending.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta, which reported last week, projected $115 billion to $135 billion in capex spending for 2026, while Oracle (once the poster child for AI infrastructure) projects a measly $50 billion. Microsoft doesn’t have an official projection for 2026 yet, but the most recent quarterly figure was $37.5 billion, which pencils out to roughly $150 billion, assuming it keeps up. It’s a notable increase, and one that has led to investor pressure on CEO Satya Nadella — but it still puts the company in third place.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From within the tech world, the logic here is simple. The revolutionary potential of AI is going to turn high-end compute into the scarce resource of the future, and only companies that control their own supply will survive. But while Google, Amazon, Microsoft, Meta, Oracle, and others are frantically prepping for the compute desert of the future, their investors aren’t convinced. Each company saw its stock price plummet as investors balked at the hundreds of billions of dollars being committed, and companies with higher spends tended to drop more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Crucially, this isn’t just a problem for companies like Meta that haven’t figured out their AI product strategy yet. It’s everyone — even companies like Microsoft and Amazon with a robust cloud business and a straightforward take on how to make money in the AI era. The numbers are simply too high for investor comfort.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Investor sentiment isn’t everything — and in this case, it may not do much to change the industry’s mind. If you believe AI is about to change everything (and the argument is pretty compelling at this point), you’d be a fool to change course just because Wall Street got jumpy. But going forward, Big Tech companies will be under a lot of pressure to downplay how expensive their AI ambitions really are.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2215577882.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sometimes, it can seem like the AI industry is racing to see who can spend the most money on data centers. Whoever builds the most data centers will have the most compute, the thinking goes, and thus be able to build the best AI products, which will guarantee victory in the years to come. There are limits to this way of thinking — traditionally, businesses eventually succeed by making &lt;em&gt;more&lt;/em&gt; money and spending &lt;em&gt;less&lt;/em&gt; — but it’s proven remarkably persuasive for large tech companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If that is the game, Amazon does seem to be winning. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company announced in its earnings on Thursday that it projects $200 billion in capital expenditures throughout 2026, across “AI, chips, robotics, and low earth orbit satellites.” That’s up from the $131.8 billion in capex in 2025. It’s tempting to attribute the whole capex budget to AI. But unlike most of its competitors, Amazon has a significant physical plant, some of which is being converted for use by expensive robots, so the non-AI expenses aren’t so easy to wave away.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is close behind. In its earnings on Wednesday, the company projected between $175 billion and $185 billion in capital expenditures for 2026, up from $91.4 billion the previous year. It’s significantly more than the company spent on fixed assets last year, and significantly more than most of its competitors are spending.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta, which reported last week, projected $115 billion to $135 billion in capex spending for 2026, while Oracle (once the poster child for AI infrastructure) projects a measly $50 billion. Microsoft doesn’t have an official projection for 2026 yet, but the most recent quarterly figure was $37.5 billion, which pencils out to roughly $150 billion, assuming it keeps up. It’s a notable increase, and one that has led to investor pressure on CEO Satya Nadella — but it still puts the company in third place.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From within the tech world, the logic here is simple. The revolutionary potential of AI is going to turn high-end compute into the scarce resource of the future, and only companies that control their own supply will survive. But while Google, Amazon, Microsoft, Meta, Oracle, and others are frantically prepping for the compute desert of the future, their investors aren’t convinced. Each company saw its stock price plummet as investors balked at the hundreds of billions of dollars being committed, and companies with higher spends tended to drop more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Crucially, this isn’t just a problem for companies like Meta that haven’t figured out their AI product strategy yet. It’s everyone — even companies like Microsoft and Amazon with a robust cloud business and a straightforward take on how to make money in the AI era. The numbers are simply too high for investor comfort.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Investor sentiment isn’t everything — and in this case, it may not do much to change the industry’s mind. If you believe AI is about to change everything (and the argument is pretty compelling at this point), you’d be a fool to change course just because Wall Street got jumpy. But going forward, Big Tech companies will be under a lot of pressure to downplay how expensive their AI ambitions really are.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/amazon-and-google-are-winning-the-ai-capex-race-but-whats-the-prize/</guid><pubDate>Thu, 05 Feb 2026 22:43:11 +0000</pubDate></item><item><title>[NEW] AI companies want you to stop chatting with bots and start managing them (AI - Ars Technica)</title><link>https://arstechnica.com/information-technology/2026/02/ai-companies-want-you-to-stop-chatting-with-bots-and-start-managing-them/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Claude Opus 4.6 and OpenAI Frontier pitch a future of supervising AI agents.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Business people supervising a robot work" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/lazy_workers-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Business people supervising a robot work" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/lazy_workers-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          demaerre via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Thursday, Anthropic and OpenAI shipped products built around the same idea: instead of chatting with a single AI assistant, users should be managing teams of AI agents that divide up work and run in parallel. The simultaneous releases are part of a gradual shift across the industry, from AI as a conversation partner to AI as a delegated workforce, and they arrive during a week when that very concept reportedly helped wipe $285 billion off software stocks.&lt;/p&gt;
&lt;p&gt;Whether that supervisory model works in practice remains an open question. Current AI agents still require heavy human intervention to catch errors, and no independent evaluation has confirmed that these multi-agent tools reliably outperform a single developer working alone.&lt;/p&gt;
&lt;p&gt;Even so, the companies are going all-in on agents. Anthropic’s contribution is Claude Opus 4.6, a new version of its most capable AI model, paired with a feature called “agent teams” in Claude Code. Agent teams let developers spin up multiple AI agents that split a task into independent pieces, coordinate autonomously, and run concurrently.&lt;/p&gt;
&lt;p&gt;In practice, agent teams look like a split-screen terminal environment: A developer can jump between subagents using Shift+Up/Down, take over any one directly, and watch the others keep working. Anthropic describes the feature as best suited for “tasks that split into independent, read-heavy work like codebase reviews.” It is available as a research preview.&lt;/p&gt;
&lt;p&gt;OpenAI, meanwhile, released Frontier, an enterprise platform it describes as a way to “hire AI co-workers who take on many of the tasks people already do on a computer.” Frontier assigns each AI agent its own identity, permissions, and memory, and it connects to existing business systems such as CRMs, ticketing tools, and data warehouses. “What we’re fundamentally doing is basically transitioning agents into true AI co-workers,” Barret Zoph, OpenAI’s general manager of business-to-business, told CNBC.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Despite the hype about these agents being co-workers, from our experience, these agents tend to work best if you think of them as tools that amplify existing skills, not as the autonomous co-workers the marketing language implies. They can produce impressive drafts fast but still require constant human course-correction.&lt;/p&gt;
&lt;p&gt;The Frontier launch came just three days after OpenAI released a new macOS desktop app for Codex, its AI coding tool, which OpenAI executives described as a “command center for agents.” The Codex app lets developers run multiple agent threads in parallel, each working on an isolated copy of a codebase via Git worktrees.&lt;/p&gt;
&lt;p&gt;OpenAI also released GPT-5.3-Codex on Thursday, a new AI model that powers the Codex app. OpenAI claims that the Codex team used early versions of GPT-5.3-Codex to debug the model’s own training run, manage its deployment, and diagnose test results, similar to what OpenAI told Ars Technica in a December interview.&lt;/p&gt;
&lt;p&gt;“Our team was blown away by how much Codex was able to accelerate its own development,” the company wrote. On Terminal-Bench 2.0, the agentic coding benchmark, GPT-5.3-Codex scored 77.3%, which exceeds Anthropic’s just-released Opus 4.6 by about 12 percentage points.&lt;/p&gt;
&lt;p&gt;The common thread across all of these products is a shift in the user’s role. Rather than merely typing a prompt and waiting for a single response, the developer or knowledge worker becomes more like a supervisor, dispatching tasks, monitoring progress, and stepping in when an agent needs direction.&lt;/p&gt;
&lt;p&gt;In this vision, developers and knowledge workers effectively become middle managers of AI. That is, not writing the code or doing the analysis themselves, but delegating tasks, reviewing output, and hoping the agents underneath them don’t quietly break things. Whether that will come to pass (or if it’s actually a good idea) is still widely debated.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;A new model under the Claude hood&lt;/h2&gt;
&lt;p&gt;Opus 4.6 is a substantial update to Anthropic’s flagship model. It succeeds Claude Opus 4.5, which Anthropic released in November. In a first for the Opus model family, it supports a context window of up to 1 million tokens (in beta), which means it can process much larger bodies of text or code in a single session.&lt;/p&gt;
&lt;p&gt;On benchmarks, Anthropic says Opus 4.6 tops OpenAI’s GPT-5.2 (an earlier model than the one released today) and Google’s Gemini 3 Pro across several evaluations, including Terminal-Bench 2.0 (an agentic coding test), Humanity’s Last Exam (a multidisciplinary reasoning test), and BrowseComp (a test of finding hard-to-locate information online)&lt;/p&gt;
&lt;p&gt;Although it should be noted that OpenAI’s GPT-5.3-Codex, released the same day, seemingly reclaimed the lead on Terminal-Bench. On ARC AGI 2, which attempts to test the ability to solve problems that are easy for humans but hard for AI models, Opus 4.6 scored 68.8 percent, compared to 37.6 percent for Opus 4.5, 54.2 percent for GPT-5.2, and 45.1 percent for Gemini 3 Pro.&lt;/p&gt;
&lt;p&gt;As always, take AI benchmarks with a grain of salt, since objectively measuring AI model capabilities is a relatively new and unsettled science.&lt;/p&gt;
&lt;p&gt;Anthropic also said that on a long-context retrieval benchmark called MRCR v2, Opus 4.6 scored 76 percent on the 1 million-token variant, compared to 18.5 percent for its Sonnet 4.5 model. That gap matters for the agent teams use case, since agents working across large codebases need to track information across hundreds of thousands of tokens without losing the thread.&lt;/p&gt;
&lt;p&gt;Pricing for the API stays the same as Opus 4.5 at $5 per million input tokens and $25 per million output tokens, with a premium rate of $10/$37.50 for prompts that exceed 200,000 tokens. Opus 4.6 is available on claude.ai, the Claude API, and all major cloud platforms.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The market fallout outside&lt;/h2&gt;
&lt;p&gt;These releases occurred during a week of exceptional volatility for software stocks. On January 30, Anthropic released 11 open source plugins for Cowork, its agentic productivity tool that launched on January 12. Cowork itself is a general-purpose tool that gives Claude access to local folders for work tasks, but the plugins extended it into specific professional domains: legal contract review, non-disclosure agreement triage, compliance workflows, financial analysis, sales, and marketing.&lt;/p&gt;
&lt;p&gt;By Tuesday, investors reportedly reacted to the release by erasing roughly $285 billion in market value across software, financial services, and asset management stocks. A Goldman Sachs basket of US software stocks fell 6 percent that day, its steepest single-session decline since April’s tariff-driven sell-off. Thomson Reuters led the rout with an 18 percent drop, and the pain spread to European and Asian markets.&lt;/p&gt;
&lt;p&gt;The purported fear among investors centers on AI model companies packaging complete workflows that compete with established software-as-a-service (SaaS) vendors, even if the verdict is still out on whether these tools can achieve those tasks.&lt;/p&gt;
&lt;p&gt;OpenAI’s Frontier might deepen that concern: its stated design lets AI agents log in to applications, execute tasks, and manage work with minimal human involvement, which Fortune described as a bid to become “the operating system of the enterprise.” OpenAI CEO of Applications Fidji Simo pushed back on the idea that Frontier replaces existing software, telling reporters, “Frontier is really a recognition that we’re not going to build everything ourselves.”&lt;/p&gt;
&lt;p&gt;Whether these co-working apps actually live up to their billing or not, the convergence is hard to miss. Anthropic’s Scott White, the company’s head of product for enterprise, gave the practice a name that is likely to roll a few eyes. “Everybody has seen this transformation happen with software engineering in the last year and a half, where vibe coding started to exist as a concept, and people could now do things with their ideas,” White told CNBC. “I think that we are now transitioning almost into vibe working.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Claude Opus 4.6 and OpenAI Frontier pitch a future of supervising AI agents.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Business people supervising a robot work" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/lazy_workers-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Business people supervising a robot work" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/lazy_workers-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          demaerre via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Thursday, Anthropic and OpenAI shipped products built around the same idea: instead of chatting with a single AI assistant, users should be managing teams of AI agents that divide up work and run in parallel. The simultaneous releases are part of a gradual shift across the industry, from AI as a conversation partner to AI as a delegated workforce, and they arrive during a week when that very concept reportedly helped wipe $285 billion off software stocks.&lt;/p&gt;
&lt;p&gt;Whether that supervisory model works in practice remains an open question. Current AI agents still require heavy human intervention to catch errors, and no independent evaluation has confirmed that these multi-agent tools reliably outperform a single developer working alone.&lt;/p&gt;
&lt;p&gt;Even so, the companies are going all-in on agents. Anthropic’s contribution is Claude Opus 4.6, a new version of its most capable AI model, paired with a feature called “agent teams” in Claude Code. Agent teams let developers spin up multiple AI agents that split a task into independent pieces, coordinate autonomously, and run concurrently.&lt;/p&gt;
&lt;p&gt;In practice, agent teams look like a split-screen terminal environment: A developer can jump between subagents using Shift+Up/Down, take over any one directly, and watch the others keep working. Anthropic describes the feature as best suited for “tasks that split into independent, read-heavy work like codebase reviews.” It is available as a research preview.&lt;/p&gt;
&lt;p&gt;OpenAI, meanwhile, released Frontier, an enterprise platform it describes as a way to “hire AI co-workers who take on many of the tasks people already do on a computer.” Frontier assigns each AI agent its own identity, permissions, and memory, and it connects to existing business systems such as CRMs, ticketing tools, and data warehouses. “What we’re fundamentally doing is basically transitioning agents into true AI co-workers,” Barret Zoph, OpenAI’s general manager of business-to-business, told CNBC.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Despite the hype about these agents being co-workers, from our experience, these agents tend to work best if you think of them as tools that amplify existing skills, not as the autonomous co-workers the marketing language implies. They can produce impressive drafts fast but still require constant human course-correction.&lt;/p&gt;
&lt;p&gt;The Frontier launch came just three days after OpenAI released a new macOS desktop app for Codex, its AI coding tool, which OpenAI executives described as a “command center for agents.” The Codex app lets developers run multiple agent threads in parallel, each working on an isolated copy of a codebase via Git worktrees.&lt;/p&gt;
&lt;p&gt;OpenAI also released GPT-5.3-Codex on Thursday, a new AI model that powers the Codex app. OpenAI claims that the Codex team used early versions of GPT-5.3-Codex to debug the model’s own training run, manage its deployment, and diagnose test results, similar to what OpenAI told Ars Technica in a December interview.&lt;/p&gt;
&lt;p&gt;“Our team was blown away by how much Codex was able to accelerate its own development,” the company wrote. On Terminal-Bench 2.0, the agentic coding benchmark, GPT-5.3-Codex scored 77.3%, which exceeds Anthropic’s just-released Opus 4.6 by about 12 percentage points.&lt;/p&gt;
&lt;p&gt;The common thread across all of these products is a shift in the user’s role. Rather than merely typing a prompt and waiting for a single response, the developer or knowledge worker becomes more like a supervisor, dispatching tasks, monitoring progress, and stepping in when an agent needs direction.&lt;/p&gt;
&lt;p&gt;In this vision, developers and knowledge workers effectively become middle managers of AI. That is, not writing the code or doing the analysis themselves, but delegating tasks, reviewing output, and hoping the agents underneath them don’t quietly break things. Whether that will come to pass (or if it’s actually a good idea) is still widely debated.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;A new model under the Claude hood&lt;/h2&gt;
&lt;p&gt;Opus 4.6 is a substantial update to Anthropic’s flagship model. It succeeds Claude Opus 4.5, which Anthropic released in November. In a first for the Opus model family, it supports a context window of up to 1 million tokens (in beta), which means it can process much larger bodies of text or code in a single session.&lt;/p&gt;
&lt;p&gt;On benchmarks, Anthropic says Opus 4.6 tops OpenAI’s GPT-5.2 (an earlier model than the one released today) and Google’s Gemini 3 Pro across several evaluations, including Terminal-Bench 2.0 (an agentic coding test), Humanity’s Last Exam (a multidisciplinary reasoning test), and BrowseComp (a test of finding hard-to-locate information online)&lt;/p&gt;
&lt;p&gt;Although it should be noted that OpenAI’s GPT-5.3-Codex, released the same day, seemingly reclaimed the lead on Terminal-Bench. On ARC AGI 2, which attempts to test the ability to solve problems that are easy for humans but hard for AI models, Opus 4.6 scored 68.8 percent, compared to 37.6 percent for Opus 4.5, 54.2 percent for GPT-5.2, and 45.1 percent for Gemini 3 Pro.&lt;/p&gt;
&lt;p&gt;As always, take AI benchmarks with a grain of salt, since objectively measuring AI model capabilities is a relatively new and unsettled science.&lt;/p&gt;
&lt;p&gt;Anthropic also said that on a long-context retrieval benchmark called MRCR v2, Opus 4.6 scored 76 percent on the 1 million-token variant, compared to 18.5 percent for its Sonnet 4.5 model. That gap matters for the agent teams use case, since agents working across large codebases need to track information across hundreds of thousands of tokens without losing the thread.&lt;/p&gt;
&lt;p&gt;Pricing for the API stays the same as Opus 4.5 at $5 per million input tokens and $25 per million output tokens, with a premium rate of $10/$37.50 for prompts that exceed 200,000 tokens. Opus 4.6 is available on claude.ai, the Claude API, and all major cloud platforms.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The market fallout outside&lt;/h2&gt;
&lt;p&gt;These releases occurred during a week of exceptional volatility for software stocks. On January 30, Anthropic released 11 open source plugins for Cowork, its agentic productivity tool that launched on January 12. Cowork itself is a general-purpose tool that gives Claude access to local folders for work tasks, but the plugins extended it into specific professional domains: legal contract review, non-disclosure agreement triage, compliance workflows, financial analysis, sales, and marketing.&lt;/p&gt;
&lt;p&gt;By Tuesday, investors reportedly reacted to the release by erasing roughly $285 billion in market value across software, financial services, and asset management stocks. A Goldman Sachs basket of US software stocks fell 6 percent that day, its steepest single-session decline since April’s tariff-driven sell-off. Thomson Reuters led the rout with an 18 percent drop, and the pain spread to European and Asian markets.&lt;/p&gt;
&lt;p&gt;The purported fear among investors centers on AI model companies packaging complete workflows that compete with established software-as-a-service (SaaS) vendors, even if the verdict is still out on whether these tools can achieve those tasks.&lt;/p&gt;
&lt;p&gt;OpenAI’s Frontier might deepen that concern: its stated design lets AI agents log in to applications, execute tasks, and manage work with minimal human involvement, which Fortune described as a bid to become “the operating system of the enterprise.” OpenAI CEO of Applications Fidji Simo pushed back on the idea that Frontier replaces existing software, telling reporters, “Frontier is really a recognition that we’re not going to build everything ourselves.”&lt;/p&gt;
&lt;p&gt;Whether these co-working apps actually live up to their billing or not, the convergence is hard to miss. Anthropic’s Scott White, the company’s head of product for enterprise, gave the practice a name that is likely to roll a few eyes. “Everybody has seen this transformation happen with software engineering in the last year and a half, where vibe coding started to exist as a concept, and people could now do things with their ideas,” White told CNBC. “I think that we are now transitioning almost into vibe working.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2026/02/ai-companies-want-you-to-stop-chatting-with-bots-and-start-managing-them/</guid><pubDate>Thu, 05 Feb 2026 22:47:54 +0000</pubDate></item><item><title>[NEW] AWS revenue continues to soar as cloud demand remains high (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/aws-revenue-continues-to-soar-as-cloud-demand-remains-high/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/IMG_4752.jpg?resize=1200,797" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services ended 2025 with its strongest quarterly growth rate in more than three years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company reported Thursday that its cloud service business recorded $35.6 billion in revenue in the fourth quarter of 2025. This figure marks a 24% year-on-year increase and the business segment’s largest growth rate in 13 quarters. Annual revenue run rate for the business segment is $142 billion, according to Amazon. The cloud service also saw an increase in its operating income from $12.5 billion in the fourth quarter compared to $10.6 billion in the same period in 2024.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“It’s very different having 24% year-over-year growth on $142 billion annualized run rate than to have a higher percentage growth on a meaningfully smaller base, which is the case with our competitors,” Amazon CEO Andy Jassy said during the company’s fourth-quarter earnings call. “We continue to add more incremental revenue and capacity than others, and extend our leadership position.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That fourth-quarter growth was fueled by new agreements with Salesforce, BlackRock, Perplexity, and the U.S. Air Force, among other companies and government entities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“More of the top 500 U.S. startups use AWS as their primary cloud provider than the next two providers combined,” Jassy said. “We’re adding significant easy to core computing capacity each day.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also added more than a gigawatt of power to its data center network in the fourth quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jassy said AWS still sees a fair amount of its business coming from enterprises that want to move infrastructure from on-premise to the cloud. AWS is, of course, also seeing a boost from the AI boom, and Jassy credited AWS’s top-to-bottom AI stack functionality.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We consistently see customers wanting to run their AI workloads where the rest of their applications and data are,” Jassy said. “We’re also seeing that as customers run large AI workloads on AWS, they’re adding to their core AWS footprint as well.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS made up 16.6% of Amazon’s overall $213.4 billion revenue in the fourth quarter. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS’s success wasn’t enough to appease Amazon investors, however. Amazon shares fell 10% in after-hours trading after investors reacted to the company’s plan to boost capital expenditures and missed Wall Street’s expectations on earnings per share.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/IMG_4752.jpg?resize=1200,797" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services ended 2025 with its strongest quarterly growth rate in more than three years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company reported Thursday that its cloud service business recorded $35.6 billion in revenue in the fourth quarter of 2025. This figure marks a 24% year-on-year increase and the business segment’s largest growth rate in 13 quarters. Annual revenue run rate for the business segment is $142 billion, according to Amazon. The cloud service also saw an increase in its operating income from $12.5 billion in the fourth quarter compared to $10.6 billion in the same period in 2024.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“It’s very different having 24% year-over-year growth on $142 billion annualized run rate than to have a higher percentage growth on a meaningfully smaller base, which is the case with our competitors,” Amazon CEO Andy Jassy said during the company’s fourth-quarter earnings call. “We continue to add more incremental revenue and capacity than others, and extend our leadership position.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That fourth-quarter growth was fueled by new agreements with Salesforce, BlackRock, Perplexity, and the U.S. Air Force, among other companies and government entities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“More of the top 500 U.S. startups use AWS as their primary cloud provider than the next two providers combined,” Jassy said. “We’re adding significant easy to core computing capacity each day.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also added more than a gigawatt of power to its data center network in the fourth quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jassy said AWS still sees a fair amount of its business coming from enterprises that want to move infrastructure from on-premise to the cloud. AWS is, of course, also seeing a boost from the AI boom, and Jassy credited AWS’s top-to-bottom AI stack functionality.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We consistently see customers wanting to run their AI workloads where the rest of their applications and data are,” Jassy said. “We’re also seeing that as customers run large AI workloads on AWS, they’re adding to their core AWS footprint as well.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS made up 16.6% of Amazon’s overall $213.4 billion revenue in the fourth quarter. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS’s success wasn’t enough to appease Amazon investors, however. Amazon shares fell 10% in after-hours trading after investors reacted to the company’s plan to boost capital expenditures and missed Wall Street’s expectations on earnings per share.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/aws-revenue-continues-to-soar-as-cloud-demand-remains-high/</guid><pubDate>Thu, 05 Feb 2026 23:11:37 +0000</pubDate></item><item><title>[NEW] Reddit looks to AI search as its next big opportunity (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/reddit-looks-to-ai-search-as-its-next-big-opportunity/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/reddit-ipo-v2.webp?resize=1200,674" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Reddit suggested on Thursday that its AI-powered search engine could be the next big opportunity for its business — not just in terms of product, but also as a revenue driver impacting its bottom line. During the company’s fourth-quarter earnings call on Thursday, it offered an update on its plans to merge traditional and AI search together and hinted that although search is not yet monetized, “it’s an enormous market and opportunity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In particular, the company believes that generative AI search will be “better for most queries.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There’s a type of query we’re, I think, particularly good at — I would argue, the best on the internet — which is questions that have no answers, where the answer actually is multiple perspectives from lots of people,” said Reddit CEO Steve Huffman.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Traditional search, meanwhile, is more like navigation — it’s a way to find the right link to a topic or subreddit. But LLMs can be good at this, too, if not better, he said. “So that’s the direction we’re going.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The exec also noted that weekly active users for search over the past year grew 30% from 60 million users to 80 million users. Meanwhile, the weekly active users for the AI-powered Reddit Answers grew from 1 million in the first quarter of 2025 to 15 million by the fourth quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re seeing a lot of growth there, and I think there’s a lot of potential too,” Huffman added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reddit said it’s working to modernize the AI answers interface by making its responses more media-rich, and pilots of this are already underway.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also thinking about how it can position itself when it’s not just a social site, but a place people come for answers. Reddit told investors on the call that it’s doing away with the distinction between logged-in and logged-out users starting in Q3 2026, as it will aim to personalize the site — using AI and machine learning — and make it relevant to whoever shows up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced in 2025 it was planning to combine its AI search feature, Reddit Answers, with its traditional search engine to improve the experience for end users. In the fourth quarter, Reddit said it had made “significant progress” in unifying its core search and its AI feature. It also released five new languages on Reddit Answers and is piloting dynamic agents along with search results that include “media beyond text.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though Reddit sees value in its AI answers, it’s not been keeping that to itself. The company’s content licensing business, which allows other companies to train their AI models on its data, is growing, too. That business revenue is reported as part of Reddit’s “other” revenues (i.e., its non-ad revenue). This “other” revenue increased by 8% year-over-year to reach $36 million in Q4 and was up 22% to reach $140 million for 2025.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/reddit-ipo-v2.webp?resize=1200,674" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Reddit suggested on Thursday that its AI-powered search engine could be the next big opportunity for its business — not just in terms of product, but also as a revenue driver impacting its bottom line. During the company’s fourth-quarter earnings call on Thursday, it offered an update on its plans to merge traditional and AI search together and hinted that although search is not yet monetized, “it’s an enormous market and opportunity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In particular, the company believes that generative AI search will be “better for most queries.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There’s a type of query we’re, I think, particularly good at — I would argue, the best on the internet — which is questions that have no answers, where the answer actually is multiple perspectives from lots of people,” said Reddit CEO Steve Huffman.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Traditional search, meanwhile, is more like navigation — it’s a way to find the right link to a topic or subreddit. But LLMs can be good at this, too, if not better, he said. “So that’s the direction we’re going.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The exec also noted that weekly active users for search over the past year grew 30% from 60 million users to 80 million users. Meanwhile, the weekly active users for the AI-powered Reddit Answers grew from 1 million in the first quarter of 2025 to 15 million by the fourth quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re seeing a lot of growth there, and I think there’s a lot of potential too,” Huffman added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reddit said it’s working to modernize the AI answers interface by making its responses more media-rich, and pilots of this are already underway.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also thinking about how it can position itself when it’s not just a social site, but a place people come for answers. Reddit told investors on the call that it’s doing away with the distinction between logged-in and logged-out users starting in Q3 2026, as it will aim to personalize the site — using AI and machine learning — and make it relevant to whoever shows up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced in 2025 it was planning to combine its AI search feature, Reddit Answers, with its traditional search engine to improve the experience for end users. In the fourth quarter, Reddit said it had made “significant progress” in unifying its core search and its AI feature. It also released five new languages on Reddit Answers and is piloting dynamic agents along with search results that include “media beyond text.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though Reddit sees value in its AI answers, it’s not been keeping that to itself. The company’s content licensing business, which allows other companies to train their AI models on its data, is growing, too. That business revenue is reported as part of Reddit’s “other” revenues (i.e., its non-ad revenue). This “other” revenue increased by 8% year-over-year to reach $36 million in Q4 and was up 22% to reach $140 million for 2025.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/reddit-looks-to-ai-search-as-its-next-big-opportunity/</guid><pubDate>Thu, 05 Feb 2026 23:20:27 +0000</pubDate></item><item><title>[NEW] Sapiom raises $15M to help AI agents buy their own tech tools (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/sapiom-raises-15m-to-help-ai-agents-buy-their-own-tech-tools/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Ilan_Zerbib-3269-067-p.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;People without coding backgrounds are discovering that they can build their own custom apps using vibe coding — solutions like Lovable that turn plain-language descriptions into working code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While these prompt-to-code tools can help create nice prototypes, launching them into full-scale production (as this reporter recently discovered) can be tricky without figuring out how to connect the application with external tech services, such as those that can send text messages via SMS, email, and process Stripe payments.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ilan Zerbib, who spent five years as Shopify’s director of engineering for payments, is building a solution that could eliminate these back-end infrastructure headaches for nontechnical creators.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last summer, Zerbib launched Sapiom, a startup developing the financial layer that allows AI agents to securely purchase and access software, APIs, data, and compute — essentially creating a payment system that lets AI automatically buy the services it needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Every time an AI agent connects to an external tool like Twilio for SMS, it requires authentication and a micro-payment. Sapiom’s goal is to make this whole process seamless, letting the AI agent decide what to buy and when without human intervention.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In the future, apps are going to consume services which require payments.&lt;strong&gt; &lt;/strong&gt;Right now, there’s no easy way for agents to actually access all of that,” said Amit Kumar, a partner at Accel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kumar has met with dozens of startups in the AI payments space, but he believes Zerbib’s focus on the financial layer for enterprises, rather than consumers, is what’s truly needed to make AI agents work. That’s why Accel is leading Sapiom’s $15 million seed round, with participation from Okta Ventures, Gradient Ventures, Array Ventures, Menlo Ventures, Anthropic, and Coinbase Ventures.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“If you really think about it, every API call is a payment. Every time you send a text message, it’s a payment. Every time you spin up a server for AWS, it’s a payment,” Kumar told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While it’s still early days for Sapiom, the startup hopes that its infrastructure solution will be adopted by vibe-coding companies and other companies creating AI agents that will eventually be tasked with doing many things on their own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, anyone who has vibe-coded an app with SMS capabilities won’t have to manually sign up for Twilio, add a credit card, and copy an API key into their code. Instead, Sapiom handles all of that in the background, and the person building the micro-app will be charged for Twilio’s services as a pass-through fee by Lovable, Bolt, or another vibe-coding platform.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While Sapiom is currently focused on B2B solutions, its technology could eventually empower personal AI agents to handle consumer transactions. The expectation is that individuals will one day trust agents to make independent financial decisions, such as ordering an Uber or shopping on Amazon. While that future is exciting, Zerbib believes that AI won’t magically make people buy more things, which is why he’s focusing on creating financial layers for businesses instead.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Ilan_Zerbib-3269-067-p.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;People without coding backgrounds are discovering that they can build their own custom apps using vibe coding — solutions like Lovable that turn plain-language descriptions into working code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While these prompt-to-code tools can help create nice prototypes, launching them into full-scale production (as this reporter recently discovered) can be tricky without figuring out how to connect the application with external tech services, such as those that can send text messages via SMS, email, and process Stripe payments.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ilan Zerbib, who spent five years as Shopify’s director of engineering for payments, is building a solution that could eliminate these back-end infrastructure headaches for nontechnical creators.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last summer, Zerbib launched Sapiom, a startup developing the financial layer that allows AI agents to securely purchase and access software, APIs, data, and compute — essentially creating a payment system that lets AI automatically buy the services it needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Every time an AI agent connects to an external tool like Twilio for SMS, it requires authentication and a micro-payment. Sapiom’s goal is to make this whole process seamless, letting the AI agent decide what to buy and when without human intervention.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In the future, apps are going to consume services which require payments.&lt;strong&gt; &lt;/strong&gt;Right now, there’s no easy way for agents to actually access all of that,” said Amit Kumar, a partner at Accel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kumar has met with dozens of startups in the AI payments space, but he believes Zerbib’s focus on the financial layer for enterprises, rather than consumers, is what’s truly needed to make AI agents work. That’s why Accel is leading Sapiom’s $15 million seed round, with participation from Okta Ventures, Gradient Ventures, Array Ventures, Menlo Ventures, Anthropic, and Coinbase Ventures.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“If you really think about it, every API call is a payment. Every time you send a text message, it’s a payment. Every time you spin up a server for AWS, it’s a payment,” Kumar told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While it’s still early days for Sapiom, the startup hopes that its infrastructure solution will be adopted by vibe-coding companies and other companies creating AI agents that will eventually be tasked with doing many things on their own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, anyone who has vibe-coded an app with SMS capabilities won’t have to manually sign up for Twilio, add a credit card, and copy an API key into their code. Instead, Sapiom handles all of that in the background, and the person building the micro-app will be charged for Twilio’s services as a pass-through fee by Lovable, Bolt, or another vibe-coding platform.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While Sapiom is currently focused on B2B solutions, its technology could eventually empower personal AI agents to handle consumer transactions. The expectation is that individuals will one day trust agents to make independent financial decisions, such as ordering an Uber or shopping on Amazon. While that future is exciting, Zerbib believes that AI won’t magically make people buy more things, which is why he’s focusing on creating financial layers for businesses instead.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/sapiom-raises-15m-to-help-ai-agents-buy-their-own-tech-tools/</guid><pubDate>Thu, 05 Feb 2026 23:53:42 +0000</pubDate></item></channel></rss>