<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 11 Sep 2025 01:39:24 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>AI vs. MAGA: Populists alarmed by Trump’s embrace of AI, Big Tech (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/09/ai-vs-maga-populists-alarmed-by-trumps-embrace-of-ai-big-tech/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        AI “threatens the common man’s liberty," says GOP Sen. Josh Hawley.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Picture of white house dinner" class="absolute inset-0 w-full h-full object-cover hidden" height="213" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/dinner-640x213.jpg" width="640" /&gt;
                  &lt;img alt="Picture of white house dinner" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/dinner-1152x640.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      President Donald Trump and first lady Melania Trump host a private dinner for technology and business leaders at the White House earlier this month.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Brian Snyder/Reuters

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Flanked by Silicon Valley’s most powerful executives in the White House last week, Melania Trump hailed artificial intelligence as potentially “the greatest engine of progress in the history of the United States of America.”&lt;/p&gt;
&lt;p&gt;Less than a mile from the first lady, in a hotel ballroom packed with MAGA faithful, top Republican Josh Hawley had a different message.&lt;/p&gt;
&lt;p&gt;AI “threatens the common man’s liberty” and could even undermine the Republic itself, the senior US senator from Missouri said.&lt;/p&gt;
&lt;p&gt;“The problem with the AI revolution as it’s currently going is that it only entrenches the power of the people who are already the most powerful people in the world,” he said. “The goal is to replace... the farmer, the assembly line man, the construction worker.”&lt;/p&gt;
&lt;p&gt;Hawley is a frequent critic of Big Tech. But his comments are endorsed by a growing chorus on America’s right—even as President Donald Trump’s administration scraps regulatory barriers and accelerates AI’s adoption across the land.&lt;/p&gt;
&lt;p&gt;It presages an unexpected clash at the heart of the MAGA world.&lt;/p&gt;
&lt;p&gt;Evangelical pastors, political strategists, and academics gathered at the recent National Conservatism Conference—the MAGA movement’s ideological nerve center—were full of contempt for the technology.&lt;/p&gt;
&lt;p&gt;“There’s a lot of people who are basically worried about... what actually is going to happen to unemployment and families and the culture and education with advanced AI, even if it doesn’t make it to artificial superintelligence,” said Geoffrey Miller, an evolutionary psychologist who spoke on a panel at the conference.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The “AI industry shares virtually no ideological overlaps with national conservatism,” he said.&lt;/p&gt;
&lt;p&gt;Attendees came from groups that helped shape the Trump administration’s policy platform but were “overwhelmingly positive” about the speech, Miller said. “There’s a lot of people [who were] just like, what do I read to learn more?”&lt;/p&gt;
&lt;p&gt;Some on the MAGA right have long been skeptical of Big Tech’s conversion to Trump.&lt;/p&gt;
&lt;p&gt;Former White House chief strategist Steve Bannon has called for Mark Zuckerberg—who sat next to Trump at a recent White House dinner with other AI leaders—to be jailed for using his Facebook platform to help Democrats.&lt;/p&gt;
&lt;p&gt;But AI’s rise has given the skeptics another cudgel with which to beat Silicon Valley elites.&lt;/p&gt;
&lt;p&gt;A warning by Anthropic chief executive Dario Amodei that AI could wipe out half of all entry-level, white-collar jobs within five years was seized upon by some on the right, who called for Trump to constrain the technology.&lt;/p&gt;
&lt;p&gt;Christian conservatives fear that the kind of companionship offered by AI bots will damage society or even dissuade people from marrying. They also worry about AI pornography and the use of the technology to “undress” humans.&lt;/p&gt;
&lt;p&gt;The conservative pushback intensified after reports of people committing murder and teenagers dying by suicide after prolonged interactions with chatbots including OpenAI’s ChatGPT.&lt;/p&gt;
&lt;p&gt;The company is being sued by the family of Adam Raine, a 16-year-old who killed himself in April after seeking mental health support from the product. Raine’s family claim the chatbot even provided tips on the best materials for a noose. (Following the filing, OpenAI announced new safety protocols for teens on ChatGPT.)&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Conservative media figures, including Megyn Kelly, decried the incident as “horrific.” Mike Davis, a Trump ally who helped push his Supreme Court nominees through the US Senate, urged people to “watch and remember” a TV interview with Raine’s parents, “when the AI oligarchs go begging Congress again” for legal relief.&lt;/p&gt;
&lt;p&gt;An initial sign of the AI backlash on the right came this summer when Bannon and Davies convinced Republicans to scrap part of Trump’s “big, beautiful, bill” that would have stopped states regulating AI themselves.&lt;/p&gt;
&lt;p&gt;The AI industry had lobbied hard for the moratorium, saying it would give the sector legal clarity.&lt;/p&gt;
&lt;p&gt;Polling showed that the skeptics on the right better understood the country’s mood. A YouGov survey found that more than 55 percent of voters objected to the provision, rising to 70 percent of 18- to 34-year-olds.&lt;/p&gt;
&lt;p&gt;“Even the bill sponsor Senator Ted Cruz... voted against his own moratorium, because it was quite clear that the winds had shifted so forcefully in another direction, there was no resisting it,” said Michael Toscano, the director of the Family First Technology Initiative at the Institute of Family Studies. He has described AI as a “malicious technology.”&lt;/p&gt;
&lt;p&gt;The defeat of the moratorium was “a major moment in American technology policy,” said Adam Thierer, a senior fellow at the R Street Institute, which pushes for free-market policies.&lt;/p&gt;
&lt;p&gt;Despite the Trump administration’s embrace of Big Tech—with boosters such as Silicon Valley investor David Sacks directing AI policy for the president—animus on the right was building, said Thierer.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Some Republicans are still angry over the deplatforming of Trump by tech executives once known for their progressive politics. They had been joined by a “vocal and growing group of conservatives who are fundamentally suspicious of the benefits of technological innovation,” Thierer said.&lt;/p&gt;
&lt;p&gt;With MAGA skeptics on one side and Big Tech allies of the president on the other, a “battle for the soul of the conservative movement” is under way.&lt;/p&gt;
&lt;p&gt;Popular resentment is now a threat to Trump’s Republican Party, warn some of its biggest supporters—especially if AI begins displacing jobs as many of its exponents suggest.&lt;/p&gt;
&lt;p&gt;“You can displace farm workers—what are they going to do about it? You can displace factory workers—they will just kill themselves with drugs and fast food,” Tucker Carlson, one of the MAGA movement's most prominent media figures, told a tech conference on Monday.&lt;/p&gt;
&lt;p&gt;“If you do that to lawyers and non-profit sector employees, you will get a revolution.”&lt;/p&gt;
&lt;p&gt;It made Trump’s embrace of Silicon Valley bosses a “significant risk” for his administration ahead of next year’s midterm elections, a leading Republican strategist said.&lt;/p&gt;
&lt;p&gt;“It’s a real double-edged sword—the administration is forced to embrace [AI] because if the US is not the leader in AI, China will be,” the strategist said, echoing the kind of argument made by Sacks and fellow Trump adviser Michael Kratsios for their AI policy platform.&lt;/p&gt;
&lt;p&gt;“But you could see unemployment spiking over the next year,” the strategist said.&lt;/p&gt;
&lt;p&gt;Other MAGA supporters are urging Trump to tone down at least his public cheerleading for an AI sector so many of them consider a threat.&lt;/p&gt;
&lt;p&gt;“The pressure that is being placed on conservatives to fall in line... is a recipe for discontent,” said Toscano.&lt;/p&gt;
&lt;p&gt;By courting AI bosses, the Republican Party, which claims to represent the pro-family movement, religious communities, and American workers, appeared to be embracing those who are antithetical to all of those groups, he warned.&lt;/p&gt;
&lt;p&gt;“The current view of things suggests that the most important members of the party are those that are from Silicon Valley,” Toscano said.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Additional reporting by Cristina Criddle in San Francisco.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2025 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        AI “threatens the common man’s liberty," says GOP Sen. Josh Hawley.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Picture of white house dinner" class="absolute inset-0 w-full h-full object-cover hidden" height="213" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/dinner-640x213.jpg" width="640" /&gt;
                  &lt;img alt="Picture of white house dinner" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/dinner-1152x640.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      President Donald Trump and first lady Melania Trump host a private dinner for technology and business leaders at the White House earlier this month.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Brian Snyder/Reuters

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Flanked by Silicon Valley’s most powerful executives in the White House last week, Melania Trump hailed artificial intelligence as potentially “the greatest engine of progress in the history of the United States of America.”&lt;/p&gt;
&lt;p&gt;Less than a mile from the first lady, in a hotel ballroom packed with MAGA faithful, top Republican Josh Hawley had a different message.&lt;/p&gt;
&lt;p&gt;AI “threatens the common man’s liberty” and could even undermine the Republic itself, the senior US senator from Missouri said.&lt;/p&gt;
&lt;p&gt;“The problem with the AI revolution as it’s currently going is that it only entrenches the power of the people who are already the most powerful people in the world,” he said. “The goal is to replace... the farmer, the assembly line man, the construction worker.”&lt;/p&gt;
&lt;p&gt;Hawley is a frequent critic of Big Tech. But his comments are endorsed by a growing chorus on America’s right—even as President Donald Trump’s administration scraps regulatory barriers and accelerates AI’s adoption across the land.&lt;/p&gt;
&lt;p&gt;It presages an unexpected clash at the heart of the MAGA world.&lt;/p&gt;
&lt;p&gt;Evangelical pastors, political strategists, and academics gathered at the recent National Conservatism Conference—the MAGA movement’s ideological nerve center—were full of contempt for the technology.&lt;/p&gt;
&lt;p&gt;“There’s a lot of people who are basically worried about... what actually is going to happen to unemployment and families and the culture and education with advanced AI, even if it doesn’t make it to artificial superintelligence,” said Geoffrey Miller, an evolutionary psychologist who spoke on a panel at the conference.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The “AI industry shares virtually no ideological overlaps with national conservatism,” he said.&lt;/p&gt;
&lt;p&gt;Attendees came from groups that helped shape the Trump administration’s policy platform but were “overwhelmingly positive” about the speech, Miller said. “There’s a lot of people [who were] just like, what do I read to learn more?”&lt;/p&gt;
&lt;p&gt;Some on the MAGA right have long been skeptical of Big Tech’s conversion to Trump.&lt;/p&gt;
&lt;p&gt;Former White House chief strategist Steve Bannon has called for Mark Zuckerberg—who sat next to Trump at a recent White House dinner with other AI leaders—to be jailed for using his Facebook platform to help Democrats.&lt;/p&gt;
&lt;p&gt;But AI’s rise has given the skeptics another cudgel with which to beat Silicon Valley elites.&lt;/p&gt;
&lt;p&gt;A warning by Anthropic chief executive Dario Amodei that AI could wipe out half of all entry-level, white-collar jobs within five years was seized upon by some on the right, who called for Trump to constrain the technology.&lt;/p&gt;
&lt;p&gt;Christian conservatives fear that the kind of companionship offered by AI bots will damage society or even dissuade people from marrying. They also worry about AI pornography and the use of the technology to “undress” humans.&lt;/p&gt;
&lt;p&gt;The conservative pushback intensified after reports of people committing murder and teenagers dying by suicide after prolonged interactions with chatbots including OpenAI’s ChatGPT.&lt;/p&gt;
&lt;p&gt;The company is being sued by the family of Adam Raine, a 16-year-old who killed himself in April after seeking mental health support from the product. Raine’s family claim the chatbot even provided tips on the best materials for a noose. (Following the filing, OpenAI announced new safety protocols for teens on ChatGPT.)&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Conservative media figures, including Megyn Kelly, decried the incident as “horrific.” Mike Davis, a Trump ally who helped push his Supreme Court nominees through the US Senate, urged people to “watch and remember” a TV interview with Raine’s parents, “when the AI oligarchs go begging Congress again” for legal relief.&lt;/p&gt;
&lt;p&gt;An initial sign of the AI backlash on the right came this summer when Bannon and Davies convinced Republicans to scrap part of Trump’s “big, beautiful, bill” that would have stopped states regulating AI themselves.&lt;/p&gt;
&lt;p&gt;The AI industry had lobbied hard for the moratorium, saying it would give the sector legal clarity.&lt;/p&gt;
&lt;p&gt;Polling showed that the skeptics on the right better understood the country’s mood. A YouGov survey found that more than 55 percent of voters objected to the provision, rising to 70 percent of 18- to 34-year-olds.&lt;/p&gt;
&lt;p&gt;“Even the bill sponsor Senator Ted Cruz... voted against his own moratorium, because it was quite clear that the winds had shifted so forcefully in another direction, there was no resisting it,” said Michael Toscano, the director of the Family First Technology Initiative at the Institute of Family Studies. He has described AI as a “malicious technology.”&lt;/p&gt;
&lt;p&gt;The defeat of the moratorium was “a major moment in American technology policy,” said Adam Thierer, a senior fellow at the R Street Institute, which pushes for free-market policies.&lt;/p&gt;
&lt;p&gt;Despite the Trump administration’s embrace of Big Tech—with boosters such as Silicon Valley investor David Sacks directing AI policy for the president—animus on the right was building, said Thierer.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Some Republicans are still angry over the deplatforming of Trump by tech executives once known for their progressive politics. They had been joined by a “vocal and growing group of conservatives who are fundamentally suspicious of the benefits of technological innovation,” Thierer said.&lt;/p&gt;
&lt;p&gt;With MAGA skeptics on one side and Big Tech allies of the president on the other, a “battle for the soul of the conservative movement” is under way.&lt;/p&gt;
&lt;p&gt;Popular resentment is now a threat to Trump’s Republican Party, warn some of its biggest supporters—especially if AI begins displacing jobs as many of its exponents suggest.&lt;/p&gt;
&lt;p&gt;“You can displace farm workers—what are they going to do about it? You can displace factory workers—they will just kill themselves with drugs and fast food,” Tucker Carlson, one of the MAGA movement's most prominent media figures, told a tech conference on Monday.&lt;/p&gt;
&lt;p&gt;“If you do that to lawyers and non-profit sector employees, you will get a revolution.”&lt;/p&gt;
&lt;p&gt;It made Trump’s embrace of Silicon Valley bosses a “significant risk” for his administration ahead of next year’s midterm elections, a leading Republican strategist said.&lt;/p&gt;
&lt;p&gt;“It’s a real double-edged sword—the administration is forced to embrace [AI] because if the US is not the leader in AI, China will be,” the strategist said, echoing the kind of argument made by Sacks and fellow Trump adviser Michael Kratsios for their AI policy platform.&lt;/p&gt;
&lt;p&gt;“But you could see unemployment spiking over the next year,” the strategist said.&lt;/p&gt;
&lt;p&gt;Other MAGA supporters are urging Trump to tone down at least his public cheerleading for an AI sector so many of them consider a threat.&lt;/p&gt;
&lt;p&gt;“The pressure that is being placed on conservatives to fall in line... is a recipe for discontent,” said Toscano.&lt;/p&gt;
&lt;p&gt;By courting AI bosses, the Republican Party, which claims to represent the pro-family movement, religious communities, and American workers, appeared to be embracing those who are antithetical to all of those groups, he warned.&lt;/p&gt;
&lt;p&gt;“The current view of things suggests that the most important members of the party are those that are from Silicon Valley,” Toscano said.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Additional reporting by Cristina Criddle in San Francisco.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2025 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/09/ai-vs-maga-populists-alarmed-by-trumps-embrace-of-ai-big-tech/</guid><pubDate>Wed, 10 Sep 2025 13:41:17 +0000</pubDate></item><item><title>After selling to Spotify, Anchor’s co-founders are back with Oboe, an AI-powered app for learning (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/10/after-selling-to-spotify-anchors-co-founders-are-back-with-oboe-an-ai-powered-app-for-learning/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The co-founders who sold their last startup Anchor to Spotify are launching their next project: Oboe, an AI-powered educational app that enables anyone to create lightweight, flexible learning courses on nearly any topic they choose, simply by entering a prompt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These courses can span a variety of verticals, including topics like science, history, foreign language, news, pop culture, preparing for life changes, and more. At launch, Oboe — a name inspired by the root of the Japanese word meaning “to learn” — will offer nine different course formats. These allow users to learn in the way they prefer, Oboe co-founder Nir Zicherman explained to TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Zicherman founded the company along with Anchor co-founder Michael Mignano after leaving Spotify in October 2023 and taking a brief period to recharge. Zicherman said he was inspired to work on an AI educational product after working to scale Spotify’s audiobooks business, which made it easier for people to gain access to high-quality and educational content, as it was bundled with their music subscription.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike AI chatbots, you don’t have to engage in back-and-forth conversations to learn with Oboe. Instead, you can opt for text and visuals, audio courses, games, interactive tests, and more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For those who want to learn on the go, Oboe offers two audio formats. One feels more like listening to a university-style lecture, while the other is akin to Google’s podcast-like NotebookLM, as it features two hosts talking in depth about the topic.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="a pair of screenshots showing the Oboe app" class="wp-image-3044805" height="680" src="https://techcrunch.com/wp-content/uploads/2025/09/OboeScreenshot1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Oboe&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“The real magic here comes from an internal architecture that we’ve built that I would describe as a complex, multi-agent architecture that we built from scratch, each part of which is orchestrated to run in parallel as we generate a course,” Zicherman says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The challenge is, how do you create courses that are both high quality, entirely personalized to what the user wants to see, and also get generated extremely quickly? This all happens within seconds,” he says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We have agents that, in parallel, are responsible for everything from developing the course architecture to developing and verifying the base material that’s being taught, writing the script for the podcast, pulling in real images from the internet — not AI-generated images, but real images and visuals into the reading formats that we offer,” he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of Oboe’s agents audit the content to ensure the courses are accurate, high-quality, and personalized to what the user wants to learn. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="another pair of screenshots showing a deep dive and podcast episode in the Oboe app." class="wp-image-3044806" height="680" src="https://techcrunch.com/wp-content/uploads/2025/09/OboeScreenshot2.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Oboe&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The courses are meant to be lightweight, engaging, and fun. Plus, Oboe’s team is working on a recommendation engine that will help you continually go deeper on a topic, if you prefer. That leaves it up to the user as to whether they want to gain some surface-level knowledge about a new topic or whether they want to get more in-depth.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This, combined with the variety of formats, will help Oboe appeal to a broader audience, the team believes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“To me, education conjures up images of more formal academic settings and the types of prescriptive curricula that students are used to as they grow up,” Zicherman tells TechCrunch. “But the truth is, we are all lifelong learners&amp;nbsp;… So much of the time that we spend on the internet these days is spent trying to better understand things, but the truth is that the internet was built to grab our attention, not to teach effectively.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re very excited to build a platform that is intended to be the one-stop shop to serve that intrinsic thirst for knowledge that exists in every person,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At launch, users can consume any course created by others for free and can create up to five free courses per month. After that, there are two paid tiers: Oboe Plus, which offers 30 additional courses for $15 per month, and Oboe Pro, which offers 100 courses for $40 per month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The service will first be available on the web (and mobile web), but native apps for iOS and Android are on the way.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Oboe is a team of five full-time, including Zicherman. Mignano remains a full-time partner at VC firm Lightspeed but sits on Oboe’s board and shares the co-founder title.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s $4 million seed round was led by Eniac Ventures, the VC firm that led Anchor’s seed. The round also includes investment from Haystack, Factorial Capital, Homebrew, Offline Ventures, Scott Belsky, Kayvon Beykpour, Nikita Bier, Tim Ferriss, and Matt Lieber.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The co-founders who sold their last startup Anchor to Spotify are launching their next project: Oboe, an AI-powered educational app that enables anyone to create lightweight, flexible learning courses on nearly any topic they choose, simply by entering a prompt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These courses can span a variety of verticals, including topics like science, history, foreign language, news, pop culture, preparing for life changes, and more. At launch, Oboe — a name inspired by the root of the Japanese word meaning “to learn” — will offer nine different course formats. These allow users to learn in the way they prefer, Oboe co-founder Nir Zicherman explained to TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Zicherman founded the company along with Anchor co-founder Michael Mignano after leaving Spotify in October 2023 and taking a brief period to recharge. Zicherman said he was inspired to work on an AI educational product after working to scale Spotify’s audiobooks business, which made it easier for people to gain access to high-quality and educational content, as it was bundled with their music subscription.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike AI chatbots, you don’t have to engage in back-and-forth conversations to learn with Oboe. Instead, you can opt for text and visuals, audio courses, games, interactive tests, and more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For those who want to learn on the go, Oboe offers two audio formats. One feels more like listening to a university-style lecture, while the other is akin to Google’s podcast-like NotebookLM, as it features two hosts talking in depth about the topic.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="a pair of screenshots showing the Oboe app" class="wp-image-3044805" height="680" src="https://techcrunch.com/wp-content/uploads/2025/09/OboeScreenshot1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Oboe&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“The real magic here comes from an internal architecture that we’ve built that I would describe as a complex, multi-agent architecture that we built from scratch, each part of which is orchestrated to run in parallel as we generate a course,” Zicherman says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The challenge is, how do you create courses that are both high quality, entirely personalized to what the user wants to see, and also get generated extremely quickly? This all happens within seconds,” he says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We have agents that, in parallel, are responsible for everything from developing the course architecture to developing and verifying the base material that’s being taught, writing the script for the podcast, pulling in real images from the internet — not AI-generated images, but real images and visuals into the reading formats that we offer,” he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of Oboe’s agents audit the content to ensure the courses are accurate, high-quality, and personalized to what the user wants to learn. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="another pair of screenshots showing a deep dive and podcast episode in the Oboe app." class="wp-image-3044806" height="680" src="https://techcrunch.com/wp-content/uploads/2025/09/OboeScreenshot2.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Oboe&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The courses are meant to be lightweight, engaging, and fun. Plus, Oboe’s team is working on a recommendation engine that will help you continually go deeper on a topic, if you prefer. That leaves it up to the user as to whether they want to gain some surface-level knowledge about a new topic or whether they want to get more in-depth.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This, combined with the variety of formats, will help Oboe appeal to a broader audience, the team believes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“To me, education conjures up images of more formal academic settings and the types of prescriptive curricula that students are used to as they grow up,” Zicherman tells TechCrunch. “But the truth is, we are all lifelong learners&amp;nbsp;… So much of the time that we spend on the internet these days is spent trying to better understand things, but the truth is that the internet was built to grab our attention, not to teach effectively.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re very excited to build a platform that is intended to be the one-stop shop to serve that intrinsic thirst for knowledge that exists in every person,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At launch, users can consume any course created by others for free and can create up to five free courses per month. After that, there are two paid tiers: Oboe Plus, which offers 30 additional courses for $15 per month, and Oboe Pro, which offers 100 courses for $40 per month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The service will first be available on the web (and mobile web), but native apps for iOS and Android are on the way.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Oboe is a team of five full-time, including Zicherman. Mignano remains a full-time partner at VC firm Lightspeed but sits on Oboe’s board and shares the co-founder title.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s $4 million seed round was led by Eniac Ventures, the VC firm that led Anchor’s seed. The round also includes investment from Haystack, Factorial Capital, Homebrew, Offline Ventures, Scott Belsky, Kayvon Beykpour, Nikita Bier, Tim Ferriss, and Matt Lieber.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/10/after-selling-to-spotify-anchors-co-founders-are-back-with-oboe-an-ai-powered-app-for-learning/</guid><pubDate>Wed, 10 Sep 2025 14:27:55 +0000</pubDate></item><item><title>Exploring the future of voice AI with Mati Staniszewski at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/10/exploring-the-future-of-voice-ai-with-mati-staniszewski-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;br /&gt;Synthetic speech is no longer the stuff of science fiction. From audiobooks and dubbing to gaming and avatars, AI-generated voice is breaking into the mainstream — and &lt;strong&gt;Mati Staniszewski&lt;/strong&gt;, CEO and co-founder of ElevenLabs, is helping lead the charge. At &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, he’ll take the stage to talk about what it takes to make voice AI truly human.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Mati Staniszewski" class="wp-image-3025392" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_-Mati-Staniszewski-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-shaping-the-future-of-sound"&gt;&lt;br /&gt;&lt;strong&gt;Shaping the future of sound&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;ElevenLabs has quickly become a key player in the generative AI space, known for pushing the boundaries of synthetic voice technology. In this session, Mati will explore how ElevenLabs built a platform that can replicate natural speech with remarkable nuance and realism — and why that opens the door to new possibilities across entertainment, accessibility, education, and creative storytelling.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-this-session-matters"&gt;Why this session matters&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Voice is one of the most personal, expressive human traits. Creating AI that can replicate it accurately — and ethically — presents unique technical and social challenges. This conversation will unpack those challenges, explore real-world use cases, and look ahead at how AI voice tools will shape the way we listen, learn, and connect.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-miss-this-ai-session-and-the-savings"&gt;Don’t miss this AI session and the savings&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Join 10,000 startup and VC leaders at Disrupt 2025 for bold conversations shaping the future of AI — and breakthroughs across five industry-specific stages. Get your ticket now and &lt;strong&gt;save up to $668&lt;/strong&gt;. Prices go up after September 26.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;br /&gt;Synthetic speech is no longer the stuff of science fiction. From audiobooks and dubbing to gaming and avatars, AI-generated voice is breaking into the mainstream — and &lt;strong&gt;Mati Staniszewski&lt;/strong&gt;, CEO and co-founder of ElevenLabs, is helping lead the charge. At &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, he’ll take the stage to talk about what it takes to make voice AI truly human.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Mati Staniszewski" class="wp-image-3025392" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_-Mati-Staniszewski-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-shaping-the-future-of-sound"&gt;&lt;br /&gt;&lt;strong&gt;Shaping the future of sound&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;ElevenLabs has quickly become a key player in the generative AI space, known for pushing the boundaries of synthetic voice technology. In this session, Mati will explore how ElevenLabs built a platform that can replicate natural speech with remarkable nuance and realism — and why that opens the door to new possibilities across entertainment, accessibility, education, and creative storytelling.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-this-session-matters"&gt;Why this session matters&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Voice is one of the most personal, expressive human traits. Creating AI that can replicate it accurately — and ethically — presents unique technical and social challenges. This conversation will unpack those challenges, explore real-world use cases, and look ahead at how AI voice tools will shape the way we listen, learn, and connect.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-miss-this-ai-session-and-the-savings"&gt;Don’t miss this AI session and the savings&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Join 10,000 startup and VC leaders at Disrupt 2025 for bold conversations shaping the future of AI — and breakthroughs across five industry-specific stages. Get your ticket now and &lt;strong&gt;save up to $668&lt;/strong&gt;. Prices go up after September 26.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/10/exploring-the-future-of-voice-ai-with-mati-staniszewski-at-techcrunch-disrupt-2025/</guid><pubDate>Wed, 10 Sep 2025 15:00:00 +0000</pubDate></item><item><title>Microsoft ends OpenAI exclusivity in Office, adds rival Anthropic (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/09/report-microsoft-taps-rival-anthropics-ai-for-office-after-it-beats-openai-at-some-tasks/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Microsoft will end OpenAI's exclusive hold on its productivity suite, adding second AI supplier.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The Microsoft Copilot logo." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/copilot_logo3-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The Microsoft Copilot logo." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/copilot_logo3-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Microsoft

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Microsoft's Office 365 suite will soon incorporate AI models from Anthropic alongside existing OpenAI technology, The Information reported, ending years of exclusive reliance on OpenAI for generative AI features across Word, Excel, PowerPoint, and Outlook.&lt;/p&gt;
&lt;p&gt;The shift reportedly follows internal testing that revealed Anthropic's Claude Sonnet 4 model excels at specific Office tasks where OpenAI's models fall short, particularly in visual design and spreadsheet automation, according to sources familiar with the project cited by The Information, who stressed the move is not a negotiating tactic.&lt;/p&gt;
&lt;p&gt;Anthropic did not immediately respond to Ars Technica's request for comment.&lt;/p&gt;
&lt;p&gt;In an unusual arrangement showing the tangled alliances of the AI industry, Microsoft will reportedly purchase access to Anthropic's models through Amazon Web Services—both a cloud computing rival and one of Anthropic's major investors. The integration is expected to be announced within weeks, with subscription pricing for Office's AI tools remaining unchanged, the report says.&lt;/p&gt;
&lt;p&gt;Microsoft maintains that its OpenAI relationship remains intact. "As we've said, OpenAI will continue to be our partner on frontier models and we remain committed to our long-term partnership," a Microsoft spokesperson told Reuters following the report. The tech giant has poured over $13 billion into OpenAI to date and is currently negotiating terms for continued access to OpenAI's models amid ongoing negotiations about their partnership terms.&lt;/p&gt;
&lt;p&gt;Stretching back to 2019, Microsoft's tight partnership with OpenAI until recently gave the tech giant a head start in AI assistants based on language models, allowing for a rapid (though bumpy) deployment of OpenAI-technology-based features in Bing search and the rollout of Copilot assistants throughout its software ecosystem. It's worth noting, however, that a recent report from the UK government found no clear productivity boost from using Copilot AI in daily work tasks among study participants.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Strategic hedging in the AI arms race&lt;/h2&gt;
&lt;p&gt;For its part, OpenAI has also begun to distance itself from its primary investor. In June, it struck a deal to use Google's cloud computing infrastructure for AI despite the two companies' fierce competition in the space, marking a shift in OpenAI's strategy to diversify its computing resources beyond Microsoft Azure, which had been its exclusive cloud provider until January.&lt;/p&gt;
&lt;p&gt;OpenAI is also planning to launch a jobs platform that may compete with Microsoft's LinkedIn, and OpenAI plans to begin mass-producing its own AI chips with Broadcom in 2026 to lessen its dependence on outside providers.&lt;/p&gt;
&lt;p&gt;This type of diversification likely reflects evolving AI strategies for both OpenAI and Microsoft as demand for AI features outpaces any single provider's capacity. Beyond the Anthropic addition, Microsoft has been developing its own proprietary AI models and began offering DeepSeek's technology through its Azure cloud platform in January. Microsoft already offers multiple AI models, including Claude, through its GitHub Copilot development platform.&lt;/p&gt;
&lt;p&gt;For Anthropic, the Microsoft deal represents a significant paper win against its rival. Founded in 2021 by ex-OpenAI executives, including CEO Dario Amodei, Anthropic has positioned its Claude models as "more steerable" alternatives to ChatGPT. Amazon's $4 billion investment (which began in 2023) in Anthropic provided both capital and computing infrastructure through Amazon Web Services —resources Microsoft will now indirectly use to power its own products.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Microsoft will end OpenAI's exclusive hold on its productivity suite, adding second AI supplier.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The Microsoft Copilot logo." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/copilot_logo3-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The Microsoft Copilot logo." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/copilot_logo3-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Microsoft

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Microsoft's Office 365 suite will soon incorporate AI models from Anthropic alongside existing OpenAI technology, The Information reported, ending years of exclusive reliance on OpenAI for generative AI features across Word, Excel, PowerPoint, and Outlook.&lt;/p&gt;
&lt;p&gt;The shift reportedly follows internal testing that revealed Anthropic's Claude Sonnet 4 model excels at specific Office tasks where OpenAI's models fall short, particularly in visual design and spreadsheet automation, according to sources familiar with the project cited by The Information, who stressed the move is not a negotiating tactic.&lt;/p&gt;
&lt;p&gt;Anthropic did not immediately respond to Ars Technica's request for comment.&lt;/p&gt;
&lt;p&gt;In an unusual arrangement showing the tangled alliances of the AI industry, Microsoft will reportedly purchase access to Anthropic's models through Amazon Web Services—both a cloud computing rival and one of Anthropic's major investors. The integration is expected to be announced within weeks, with subscription pricing for Office's AI tools remaining unchanged, the report says.&lt;/p&gt;
&lt;p&gt;Microsoft maintains that its OpenAI relationship remains intact. "As we've said, OpenAI will continue to be our partner on frontier models and we remain committed to our long-term partnership," a Microsoft spokesperson told Reuters following the report. The tech giant has poured over $13 billion into OpenAI to date and is currently negotiating terms for continued access to OpenAI's models amid ongoing negotiations about their partnership terms.&lt;/p&gt;
&lt;p&gt;Stretching back to 2019, Microsoft's tight partnership with OpenAI until recently gave the tech giant a head start in AI assistants based on language models, allowing for a rapid (though bumpy) deployment of OpenAI-technology-based features in Bing search and the rollout of Copilot assistants throughout its software ecosystem. It's worth noting, however, that a recent report from the UK government found no clear productivity boost from using Copilot AI in daily work tasks among study participants.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Strategic hedging in the AI arms race&lt;/h2&gt;
&lt;p&gt;For its part, OpenAI has also begun to distance itself from its primary investor. In June, it struck a deal to use Google's cloud computing infrastructure for AI despite the two companies' fierce competition in the space, marking a shift in OpenAI's strategy to diversify its computing resources beyond Microsoft Azure, which had been its exclusive cloud provider until January.&lt;/p&gt;
&lt;p&gt;OpenAI is also planning to launch a jobs platform that may compete with Microsoft's LinkedIn, and OpenAI plans to begin mass-producing its own AI chips with Broadcom in 2026 to lessen its dependence on outside providers.&lt;/p&gt;
&lt;p&gt;This type of diversification likely reflects evolving AI strategies for both OpenAI and Microsoft as demand for AI features outpaces any single provider's capacity. Beyond the Anthropic addition, Microsoft has been developing its own proprietary AI models and began offering DeepSeek's technology through its Azure cloud platform in January. Microsoft already offers multiple AI models, including Claude, through its GitHub Copilot development platform.&lt;/p&gt;
&lt;p&gt;For Anthropic, the Microsoft deal represents a significant paper win against its rival. Founded in 2021 by ex-OpenAI executives, including CEO Dario Amodei, Anthropic has positioned its Claude models as "more steerable" alternatives to ChatGPT. Amazon's $4 billion investment (which began in 2023) in Anthropic provided both capital and computing infrastructure through Amazon Web Services —resources Microsoft will now indirectly use to power its own products.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/09/report-microsoft-taps-rival-anthropics-ai-for-office-after-it-beats-openai-at-some-tasks/</guid><pubDate>Wed, 10 Sep 2025 15:41:42 +0000</pubDate></item><item><title>DOE selects MIT to establish a Center for the Exascale Simulation of Coupled High-Enthalpy Fluid–Solid Interactions (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/mit-center-exascale-simulation-coupled-high-enthalpy-fluid-solid-interactions-0910</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/orion-reentry-00.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;The U.S. Department of Energy’s National Nuclear Security Administration (DOE/NNSA) recently&amp;nbsp;announced that it has selected MIT to establish a new research center dedicated to advancing the predictive simulation of extreme environments, such as those encountered in hypersonic flight and atmospheric re-entry. The center will be part of the fourth phase of NNSA's&amp;nbsp;Predictive Science Academic Alliance Program (PSAAP-IV), which supports frontier research advancing the predictive capabilities of high-performance computing for open science and engineering applications relevant to national security mission spaces.&lt;/p&gt;&lt;p&gt;The Center for the Exascale Simulation of Coupled High-Enthalpy Fluid–Solid Interactions (CHEFSI) — a joint effort of the&amp;nbsp;MIT Center for Computational Science and Engineering, the&amp;nbsp;MIT Schwarzman College of Computing, and the&amp;nbsp;MIT Institute for Soldier Nanotechnologies (ISN) —&amp;nbsp;plans to harness cutting-edge exascale supercomputers and next-generation algorithms to simulate with unprecedented detail how extremely hot, fast-moving gaseous and solid materials interact. The understanding of these extreme environments — characterized by temperatures of more than 1,500 degrees Celsius and speeds as high as Mach&amp;nbsp;25 — and their effect on vehicles is central to national security, space exploration, and the development of advanced thermal protection systems.&lt;/p&gt;&lt;p&gt;“CHEFSI will capitalize on MIT’s deep strengths in predictive modeling, high-performance computing, and STEM education to help ensure the United States remains at the forefront of scientific and technological innovation,” says Ian A. Waitz, MIT’s vice president for research. “The center’s particular relevance to national security and advanced technologies exemplifies MIT’s commitment to advancing research with broad societal benefit.”&lt;/p&gt;&lt;p&gt;CHEFSI is one of five new Predictive Simulation Centers announced by the NNSA as part of a program expected to provide up to $17.5 million to each center over five years.&lt;/p&gt;&lt;p&gt;CHEFSI’s research aims to couple detailed simulations of high-enthalpy gas flows with models of the chemical, thermal, and mechanical behavior of solid materials, capturing phenomena such as oxidation, nitridation, ablation, and fracture. Advanced computational models — validated by carefully designed experiments — can address the limitations of flight testing by providing critical insights into material performance and failure.&lt;/p&gt;&lt;p&gt;“By integrating high-fidelity physics models with artificial intelligence-based surrogate models, experimental validation, and state-of-the-art exascale computational tools, CHEFSI will help us understand and predict how thermal protection systems perform under some of the harshest conditions encountered in engineering systems,” says Raúl Radovitzky, the Jerome C. Hunsaker Professor of Aeronautics and Astronautics, associate director of the ISN, and director of CHEFSI. “This knowledge will help in the design of resilient systems for applications ranging from reusable spacecraft to hypersonic vehicles.”&lt;/p&gt;&lt;p&gt;Radovitzky will be joined on the center’s leadership team by Youssef Marzouk, the Breene M. Kerr (1951) Professor of Aeronautics and Astronautics, co-director of the MIT Center for Computational Science and Engineering (CCSE), and recently named the associate dean of the MIT Schwarzman College of Computing; and Nicolas Hadjiconstantinou, the Quentin Berg (1937) Professor of Mechanical Engineering and co-director of CCSE, who will serve as associate directors. The center&amp;nbsp;co-principal investigators include MIT faculty members across the departments of Aeronautics and Astronautics, Electrical Engineering and Computer Science, Materials Science and Engineering, Mathematics, and Mechanical Engineering. Franklin Hadley will lead center operations, with administration and finance under the&amp;nbsp;purview of Joshua Freedman. Hadley and Freedman are both members of the ISN headquarters team.&amp;nbsp;&lt;/p&gt;&lt;p&gt;CHEFSI expects to collaborate extensively with the DoE/NNSA national laboratories —&amp;nbsp;Lawrence Livermore National Laboratory, Los Alamos National Laboratory, and Sandia National Laboratories — and, in doing so, offer graduate students and postdocs immersive research experiences and internships at these facilities.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/orion-reentry-00.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;The U.S. Department of Energy’s National Nuclear Security Administration (DOE/NNSA) recently&amp;nbsp;announced that it has selected MIT to establish a new research center dedicated to advancing the predictive simulation of extreme environments, such as those encountered in hypersonic flight and atmospheric re-entry. The center will be part of the fourth phase of NNSA's&amp;nbsp;Predictive Science Academic Alliance Program (PSAAP-IV), which supports frontier research advancing the predictive capabilities of high-performance computing for open science and engineering applications relevant to national security mission spaces.&lt;/p&gt;&lt;p&gt;The Center for the Exascale Simulation of Coupled High-Enthalpy Fluid–Solid Interactions (CHEFSI) — a joint effort of the&amp;nbsp;MIT Center for Computational Science and Engineering, the&amp;nbsp;MIT Schwarzman College of Computing, and the&amp;nbsp;MIT Institute for Soldier Nanotechnologies (ISN) —&amp;nbsp;plans to harness cutting-edge exascale supercomputers and next-generation algorithms to simulate with unprecedented detail how extremely hot, fast-moving gaseous and solid materials interact. The understanding of these extreme environments — characterized by temperatures of more than 1,500 degrees Celsius and speeds as high as Mach&amp;nbsp;25 — and their effect on vehicles is central to national security, space exploration, and the development of advanced thermal protection systems.&lt;/p&gt;&lt;p&gt;“CHEFSI will capitalize on MIT’s deep strengths in predictive modeling, high-performance computing, and STEM education to help ensure the United States remains at the forefront of scientific and technological innovation,” says Ian A. Waitz, MIT’s vice president for research. “The center’s particular relevance to national security and advanced technologies exemplifies MIT’s commitment to advancing research with broad societal benefit.”&lt;/p&gt;&lt;p&gt;CHEFSI is one of five new Predictive Simulation Centers announced by the NNSA as part of a program expected to provide up to $17.5 million to each center over five years.&lt;/p&gt;&lt;p&gt;CHEFSI’s research aims to couple detailed simulations of high-enthalpy gas flows with models of the chemical, thermal, and mechanical behavior of solid materials, capturing phenomena such as oxidation, nitridation, ablation, and fracture. Advanced computational models — validated by carefully designed experiments — can address the limitations of flight testing by providing critical insights into material performance and failure.&lt;/p&gt;&lt;p&gt;“By integrating high-fidelity physics models with artificial intelligence-based surrogate models, experimental validation, and state-of-the-art exascale computational tools, CHEFSI will help us understand and predict how thermal protection systems perform under some of the harshest conditions encountered in engineering systems,” says Raúl Radovitzky, the Jerome C. Hunsaker Professor of Aeronautics and Astronautics, associate director of the ISN, and director of CHEFSI. “This knowledge will help in the design of resilient systems for applications ranging from reusable spacecraft to hypersonic vehicles.”&lt;/p&gt;&lt;p&gt;Radovitzky will be joined on the center’s leadership team by Youssef Marzouk, the Breene M. Kerr (1951) Professor of Aeronautics and Astronautics, co-director of the MIT Center for Computational Science and Engineering (CCSE), and recently named the associate dean of the MIT Schwarzman College of Computing; and Nicolas Hadjiconstantinou, the Quentin Berg (1937) Professor of Mechanical Engineering and co-director of CCSE, who will serve as associate directors. The center&amp;nbsp;co-principal investigators include MIT faculty members across the departments of Aeronautics and Astronautics, Electrical Engineering and Computer Science, Materials Science and Engineering, Mathematics, and Mechanical Engineering. Franklin Hadley will lead center operations, with administration and finance under the&amp;nbsp;purview of Joshua Freedman. Hadley and Freedman are both members of the ISN headquarters team.&amp;nbsp;&lt;/p&gt;&lt;p&gt;CHEFSI expects to collaborate extensively with the DoE/NNSA national laboratories —&amp;nbsp;Lawrence Livermore National Laboratory, Los Alamos National Laboratory, and Sandia National Laboratories — and, in doing so, offer graduate students and postdocs immersive research experiences and internships at these facilities.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/mit-center-exascale-simulation-coupled-high-enthalpy-fluid-solid-interactions-0910</guid><pubDate>Wed, 10 Sep 2025 15:45:00 +0000</pubDate></item><item><title>RenderFormer: How neural networks are reshaping 3D rendering (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/renderformer-how-neural-networks-are-reshaping-3d-rendering/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white icons on a gradient background transitioning from blue to green. From left to right: network node icon, lightbulb-shaped icon with a path tool icon in the center; a monitor icon showing a web browser icon" class="wp-image-1149127" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;3D rendering—the process of converting three-dimensional models into two-dimensional images—is a foundational technology in computer graphics, widely used across gaming, film, virtual reality, and architectural visualization. Traditionally, this process has depended on physics-based techniques like ray tracing and rasterization, which simulate light behavior through mathematical formulas and expert-designed models.&lt;/p&gt;



&lt;p&gt;Now, thanks to advances in AI, especially neural networks, researchers are beginning to replace these conventional approaches with machine learning (ML). This shift is giving rise to a new field known as neural rendering.&lt;/p&gt;



&lt;p&gt;Neural rendering combines deep learning with traditional graphics techniques, allowing models to simulate complex light transport without explicitly modeling physical optics. This approach offers significant advantages: it eliminates the need for handcrafted rules, supports end-to-end training, and can be optimized for specific tasks. Yet, most current neural rendering methods rely on 2D image inputs, lack support for raw 3D geometry and material data, and often require retraining for each new scene—limiting their generalizability.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="renderformer-toward-a-general-purpose-neural-rendering-model"&gt;RenderFormer: Toward a general-purpose neural rendering model&lt;/h2&gt;



&lt;p&gt;To overcome these limitations, researchers at Microsoft Research have developed RenderFormer, a new neural architecture designed to support full-featured 3D rendering using only ML—no traditional graphics computation required. RenderFormer is the first model to demonstrate that a neural network can learn a complete graphics rendering pipeline, including support for arbitrary 3D scenes and global illumination, without relying on ray tracing or rasterization. This work has been accepted at SIGGRAPH 2025 and is open-sourced on GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="architecture-overview"&gt;Architecture overview&lt;/h2&gt;



&lt;p&gt;As shown in Figure 1, RenderFormer represents the entire 3D scene using triangle tokens—each one encoding spatial position, surface normal, and physical material properties such as diffuse color, specular color, and roughness. Lighting is also modeled as triangle tokens, with emission values indicating intensity.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1: The figure illustrates the architecture of RenderFormer. It includes a Triangle Mesh Scene with a 3D rabbit model inside a colored cube, a Camera Ray Map grid, a View Independent Transformer (12 layers of Self-Attention and Feed Forward Network), a View Dependent Transformer (6 layers with Cross-Attention and Self-Attention), and a DPT Decoder. Scene attributes—Vertex Normal, Reflectance (Diffuse, Specular, Roughness), Emission, and Position—are embedded into Triangle Tokens via Linear + Norm operations. These tokens and Ray Bundle Tokens (from the Camera Ray Map) are processed by the respective transformers and decoded to produce a rendered image of a glossy rabbit in a colored room." class="wp-image-1149133" height="1008" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1.png" width="2419" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Architecture of RenderFormer&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;To describe the viewing direction, the model uses ray bundle tokens derived from a ray map—each pixel in the output image corresponds to one of these rays. To improve computational efficiency, pixels are grouped into rectangular blocks, with all rays in a block processed together.&lt;/p&gt;



&lt;p&gt;The model outputs a set of tokens that are decoded into image pixels, completing the rendering process entirely within the neural network.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="dual-branch-design-for-view-independent-and-view-dependent-effects"&gt;Dual-branch design for view-independent and view-dependent effects&lt;/h2&gt;



&lt;p&gt;The RenderFormer architecture is built around two transformers: one for view-independent features and another for view-dependent ones.&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;The &lt;strong&gt;view-independent transformer&lt;/strong&gt; captures scene information unrelated to viewpoint, such as shadowing and diffuse light transport, using self-attention between triangle tokens.&lt;/li&gt;



&lt;li&gt;The &lt;strong&gt;view-dependent transformer&lt;/strong&gt; models effects like visibility, reflections, and specular highlights through cross-attention between triangle and ray bundle tokens.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Additional image-space effects, such as anti-aliasing and screen-space reflections, are handled via self-attention among ray bundle tokens.&lt;/p&gt;



&lt;p&gt;To validate the architecture, the team conducted ablation studies and visual analyses, confirming the importance of each component in the rendering pipeline.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Table 1: A table comparing the performance of different network variants in an ablation study. The columns are labeled Variant, PSNR (↑), SSIM (↑), LPIPS (↓), and FLIP (↓). Variants include configurations such as " class="wp-image-1149129" for="for" height="509" rows="rows" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1.png" width="963" /&gt;&lt;figcaption class="wp-element-caption"&gt;Table 1. Ablation study analyzing the impact of different components and attention mechanisms on the final performance of the trained network. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;To test the capabilities of the view-independent transformer, researchers trained a decoder to produce diffuse-only renderings. The results, shown in Figure 2, demonstrate that the model can accurately simulate shadows and other indirect lighting effects.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2: The figure displays four 3D-rendered objects showcasing view-independent rendering effects. From left to right: a purple teapot on a green surface, a blue rectangular object on a red surface, an upside-down table casting shadows on a green surface, and a green apple-like object on a blue surface. Each object features diffuse lighting and coarse shadow effects, with distinct highlights and shadows produced by directional light sources." class="wp-image-1149132" height="240" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2.png" width="943" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. View-independent rendering effects decoded directly from the view-independent transformer, including diffuse lighting and coarse shadow effects. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The view-dependent transformer was evaluated through attention visualizations. For example, in Figure 3, the attention map reveals a pixel on a teapot attending to its surface triangle and to a nearby wall—capturing the effect of specular reflection. These visualizations also show how material changes influence the sharpness and intensity of reflections.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3: The figure contains six panels arranged in two rows and three columns. The top row displays a teapot in a room with red and green walls under three different roughness values: 0.3, 0.7, and 0.99 (left to right). The bottom row shows the corresponding attention outputs for each roughness setting, featuring the teapot silhouette against a dark background with distinct light patterns that vary with roughness." class="wp-image-1149131" height="620" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3.png" width="947" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. Visualization of attention outputs&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="training-methodology-and-dataset-design"&gt;Training methodology and dataset design&lt;/h2&gt;



&lt;p&gt;RenderFormer was trained using the Objaverse dataset, a collection of more than 800,000 annotated 3D objects that is designed to advance research in 3D modeling, computer vision, and related fields. The researchers designed four scene templates, populating each with 1–3 randomly selected objects and materials. Scenes were rendered in high dynamic range (HDR) using Blender’s Cycles renderer, under varied lighting conditions and camera angles.&lt;/p&gt;



&lt;p&gt;The base model, consisting of 205 million parameters, was trained in two phases using the AdamW optimizer:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;500,000 steps at 256×256 resolution with up to 1,536 triangles&lt;/li&gt;



&lt;li&gt;100,000 steps at 512×512 resolution with up to 4,096 triangles&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;The model supports arbitrary triangle-based input and generalizes well to complex real-world scenes. As shown in Figure 4, it accurately reproduces shadows, diffuse shading, and specular highlights.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4: The figure presents a 3×3 grid of diverse 3D scenes rendered by RenderFormer. In the top row, the first scene shows a room with red, green, and white walls containing two rectangular prisms; the second features a metallic tree-like structure in a blue-walled room with a reflective floor; and the third depicts a red animal figure, a black abstract shape, and a multi-faceted sphere in a purple container on a yellow surface. The middle row includes three constant width bodies (black, red, and blue) floating above a colorful checkered floor; a green shader ball with a square cavity inside a gray-walled room; and crystal-like structures in green, purple, and red on a reflective surface. The bottom row showcases a low-poly fox near a pink tree emitting particles on grassy terrain; a golden horse statue beside a heart-shaped object split into red and grey halves on a reflective surface; and a wicker basket, a banana and a bottle placed on a white platform." class="wp-image-1149130" height="805" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4.jpg" width="805" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4. Rendered results of different 3D scenes generated by RenderFormer &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;RenderFormer can also generate continuous video by rendering individual frames, thanks to its ability to model viewpoint changes and scene dynamics.&lt;/p&gt;



&lt;figure class="wp-block-video aligncenter"&gt;&lt;video controls="controls" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_animate.mp4"&gt;&lt;/video&gt;&lt;figcaption class="wp-element-caption"&gt;3D animation sequence rendered by RenderFormer &lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="looking-ahead-opportunities-and-challenges"&gt;Looking ahead: Opportunities and challenges&lt;/h2&gt;



&lt;p&gt;RenderFormer represents a significant step forward for neural rendering. It demonstrates that deep learning can replicate and potentially replace the traditional rendering pipeline, supporting arbitrary 3D inputs and realistic global illumination—all without any hand-coded graphics computations.&lt;/p&gt;



&lt;p&gt;However, key challenges remain. Scaling to larger and more complex scenes with intricate geometry, advanced materials, and diverse lighting conditions will require further research. Still, the transformer-based architecture provides a solid foundation for future integration with broader AI systems, including video generation, image synthesis, robotics, and embodied AI.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Researchers hope that RenderFormer will serve as a building block for future breakthroughs in both graphics and AI, opening new possibilities for visual computing and intelligent environments.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white icons on a gradient background transitioning from blue to green. From left to right: network node icon, lightbulb-shaped icon with a path tool icon in the center; a monitor icon showing a web browser icon" class="wp-image-1149127" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;3D rendering—the process of converting three-dimensional models into two-dimensional images—is a foundational technology in computer graphics, widely used across gaming, film, virtual reality, and architectural visualization. Traditionally, this process has depended on physics-based techniques like ray tracing and rasterization, which simulate light behavior through mathematical formulas and expert-designed models.&lt;/p&gt;



&lt;p&gt;Now, thanks to advances in AI, especially neural networks, researchers are beginning to replace these conventional approaches with machine learning (ML). This shift is giving rise to a new field known as neural rendering.&lt;/p&gt;



&lt;p&gt;Neural rendering combines deep learning with traditional graphics techniques, allowing models to simulate complex light transport without explicitly modeling physical optics. This approach offers significant advantages: it eliminates the need for handcrafted rules, supports end-to-end training, and can be optimized for specific tasks. Yet, most current neural rendering methods rely on 2D image inputs, lack support for raw 3D geometry and material data, and often require retraining for each new scene—limiting their generalizability.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="renderformer-toward-a-general-purpose-neural-rendering-model"&gt;RenderFormer: Toward a general-purpose neural rendering model&lt;/h2&gt;



&lt;p&gt;To overcome these limitations, researchers at Microsoft Research have developed RenderFormer, a new neural architecture designed to support full-featured 3D rendering using only ML—no traditional graphics computation required. RenderFormer is the first model to demonstrate that a neural network can learn a complete graphics rendering pipeline, including support for arbitrary 3D scenes and global illumination, without relying on ray tracing or rasterization. This work has been accepted at SIGGRAPH 2025 and is open-sourced on GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="architecture-overview"&gt;Architecture overview&lt;/h2&gt;



&lt;p&gt;As shown in Figure 1, RenderFormer represents the entire 3D scene using triangle tokens—each one encoding spatial position, surface normal, and physical material properties such as diffuse color, specular color, and roughness. Lighting is also modeled as triangle tokens, with emission values indicating intensity.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1: The figure illustrates the architecture of RenderFormer. It includes a Triangle Mesh Scene with a 3D rabbit model inside a colored cube, a Camera Ray Map grid, a View Independent Transformer (12 layers of Self-Attention and Feed Forward Network), a View Dependent Transformer (6 layers with Cross-Attention and Self-Attention), and a DPT Decoder. Scene attributes—Vertex Normal, Reflectance (Diffuse, Specular, Roughness), Emission, and Position—are embedded into Triangle Tokens via Linear + Norm operations. These tokens and Ray Bundle Tokens (from the Camera Ray Map) are processed by the respective transformers and decoded to produce a rendered image of a glossy rabbit in a colored room." class="wp-image-1149133" height="1008" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1.png" width="2419" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Architecture of RenderFormer&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;To describe the viewing direction, the model uses ray bundle tokens derived from a ray map—each pixel in the output image corresponds to one of these rays. To improve computational efficiency, pixels are grouped into rectangular blocks, with all rays in a block processed together.&lt;/p&gt;



&lt;p&gt;The model outputs a set of tokens that are decoded into image pixels, completing the rendering process entirely within the neural network.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="dual-branch-design-for-view-independent-and-view-dependent-effects"&gt;Dual-branch design for view-independent and view-dependent effects&lt;/h2&gt;



&lt;p&gt;The RenderFormer architecture is built around two transformers: one for view-independent features and another for view-dependent ones.&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;The &lt;strong&gt;view-independent transformer&lt;/strong&gt; captures scene information unrelated to viewpoint, such as shadowing and diffuse light transport, using self-attention between triangle tokens.&lt;/li&gt;



&lt;li&gt;The &lt;strong&gt;view-dependent transformer&lt;/strong&gt; models effects like visibility, reflections, and specular highlights through cross-attention between triangle and ray bundle tokens.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Additional image-space effects, such as anti-aliasing and screen-space reflections, are handled via self-attention among ray bundle tokens.&lt;/p&gt;



&lt;p&gt;To validate the architecture, the team conducted ablation studies and visual analyses, confirming the importance of each component in the rendering pipeline.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Table 1: A table comparing the performance of different network variants in an ablation study. The columns are labeled Variant, PSNR (↑), SSIM (↑), LPIPS (↓), and FLIP (↓). Variants include configurations such as " class="wp-image-1149129" for="for" height="509" rows="rows" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1.png" width="963" /&gt;&lt;figcaption class="wp-element-caption"&gt;Table 1. Ablation study analyzing the impact of different components and attention mechanisms on the final performance of the trained network. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;To test the capabilities of the view-independent transformer, researchers trained a decoder to produce diffuse-only renderings. The results, shown in Figure 2, demonstrate that the model can accurately simulate shadows and other indirect lighting effects.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2: The figure displays four 3D-rendered objects showcasing view-independent rendering effects. From left to right: a purple teapot on a green surface, a blue rectangular object on a red surface, an upside-down table casting shadows on a green surface, and a green apple-like object on a blue surface. Each object features diffuse lighting and coarse shadow effects, with distinct highlights and shadows produced by directional light sources." class="wp-image-1149132" height="240" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2.png" width="943" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. View-independent rendering effects decoded directly from the view-independent transformer, including diffuse lighting and coarse shadow effects. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The view-dependent transformer was evaluated through attention visualizations. For example, in Figure 3, the attention map reveals a pixel on a teapot attending to its surface triangle and to a nearby wall—capturing the effect of specular reflection. These visualizations also show how material changes influence the sharpness and intensity of reflections.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3: The figure contains six panels arranged in two rows and three columns. The top row displays a teapot in a room with red and green walls under three different roughness values: 0.3, 0.7, and 0.99 (left to right). The bottom row shows the corresponding attention outputs for each roughness setting, featuring the teapot silhouette against a dark background with distinct light patterns that vary with roughness." class="wp-image-1149131" height="620" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3.png" width="947" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. Visualization of attention outputs&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="training-methodology-and-dataset-design"&gt;Training methodology and dataset design&lt;/h2&gt;



&lt;p&gt;RenderFormer was trained using the Objaverse dataset, a collection of more than 800,000 annotated 3D objects that is designed to advance research in 3D modeling, computer vision, and related fields. The researchers designed four scene templates, populating each with 1–3 randomly selected objects and materials. Scenes were rendered in high dynamic range (HDR) using Blender’s Cycles renderer, under varied lighting conditions and camera angles.&lt;/p&gt;



&lt;p&gt;The base model, consisting of 205 million parameters, was trained in two phases using the AdamW optimizer:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;500,000 steps at 256×256 resolution with up to 1,536 triangles&lt;/li&gt;



&lt;li&gt;100,000 steps at 512×512 resolution with up to 4,096 triangles&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;The model supports arbitrary triangle-based input and generalizes well to complex real-world scenes. As shown in Figure 4, it accurately reproduces shadows, diffuse shading, and specular highlights.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4: The figure presents a 3×3 grid of diverse 3D scenes rendered by RenderFormer. In the top row, the first scene shows a room with red, green, and white walls containing two rectangular prisms; the second features a metallic tree-like structure in a blue-walled room with a reflective floor; and the third depicts a red animal figure, a black abstract shape, and a multi-faceted sphere in a purple container on a yellow surface. The middle row includes three constant width bodies (black, red, and blue) floating above a colorful checkered floor; a green shader ball with a square cavity inside a gray-walled room; and crystal-like structures in green, purple, and red on a reflective surface. The bottom row showcases a low-poly fox near a pink tree emitting particles on grassy terrain; a golden horse statue beside a heart-shaped object split into red and grey halves on a reflective surface; and a wicker basket, a banana and a bottle placed on a white platform." class="wp-image-1149130" height="805" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4.jpg" width="805" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4. Rendered results of different 3D scenes generated by RenderFormer &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;RenderFormer can also generate continuous video by rendering individual frames, thanks to its ability to model viewpoint changes and scene dynamics.&lt;/p&gt;



&lt;figure class="wp-block-video aligncenter"&gt;&lt;video controls="controls" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_animate.mp4"&gt;&lt;/video&gt;&lt;figcaption class="wp-element-caption"&gt;3D animation sequence rendered by RenderFormer &lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="looking-ahead-opportunities-and-challenges"&gt;Looking ahead: Opportunities and challenges&lt;/h2&gt;



&lt;p&gt;RenderFormer represents a significant step forward for neural rendering. It demonstrates that deep learning can replicate and potentially replace the traditional rendering pipeline, supporting arbitrary 3D inputs and realistic global illumination—all without any hand-coded graphics computations.&lt;/p&gt;



&lt;p&gt;However, key challenges remain. Scaling to larger and more complex scenes with intricate geometry, advanced materials, and diverse lighting conditions will require further research. Still, the transformer-based architecture provides a solid foundation for future integration with broader AI systems, including video generation, image synthesis, robotics, and embodied AI.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Researchers hope that RenderFormer will serve as a building block for future breakthroughs in both graphics and AI, opening new possibilities for visual computing and intelligent environments.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/renderformer-how-neural-networks-are-reshaping-3d-rendering/</guid><pubDate>Wed, 10 Sep 2025 16:00:00 +0000</pubDate></item><item><title>Spotify peeved after 10,000 users sold data to build AI tools (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/09/spotify-peeved-after-10000-users-sold-data-to-build-ai-tools/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Spotify sent a warning to stop data sales, but developers say they never got it.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/spotify-data-mining-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/spotify-data-mining-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;For millions of Spotify users, the "Wrapped" feature—which crunches the numbers on their annual listening habits—is a highlight of every year's end, ever since it debuted in 2015. NPR once broke down exactly why our brains find the feature so "irresistible," while Cosmopolitan last year declared that sharing Wrapped screenshots of top artists and songs had by now become "the ultimate status symbol" for tens of millions of music fans.&lt;/p&gt;
&lt;p&gt;It's no surprise then that, after a decade, some Spotify users who are especially eager to see Wrapped evolve are no longer willing to wait to see if Spotify will ever deliver the more creative streaming insights they crave.&lt;/p&gt;
&lt;p&gt;With the help of AI, these users expect that their data can be more quickly analyzed to potentially uncover overlooked or never-considered patterns that could offer even more insights into what their listening habits say about them.&lt;/p&gt;
&lt;p&gt;Imagine, for example, accessing a music recap that encapsulates a user's full listening history—not just their top songs and artists. With that unlocked, users could track emotional patterns, analyzing how their music tastes reflected their moods over time and perhaps helping them adjust their listening habits to better cope with stress or major life events. And for users particularly intrigued by their own data, there's even the potential to use AI to cross data streams from different platforms and perhaps understand even more about how their music choices impact their lives and tastes more broadly.&lt;/p&gt;
&lt;p&gt;Likely just as appealing as gleaning deeper personal insights, though, users could also potentially build AI tools to compare listening habits with their friends. That could lead to nearly endless fun for the most invested music fans, where AI could be tapped to assess all kinds of random data points, like whose breakup playlists are more intense or who really spends the most time listening to a shared favorite artist.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In pursuit of supporting developers offering novel insights like these, more than 18,000 Spotify users have joined "Unwrapped," a collective launched in February that allows them to pool and monetize their data.&lt;/p&gt;
&lt;p&gt;Voting as a group through the decentralized data platform Vana—which Wired profiled earlier this year—these users can elect to sell their dataset to developers who are building AI tools offering fresh ways for users to analyze streaming data in ways that Spotify likely couldn't or wouldn't.&lt;/p&gt;
&lt;p&gt;In June, the group made its first sale, with 99.5 percent of members voting yes. Vana co-founder Anna Kazlauskas told Ars that the collective—at the time about 10,000 members strong—sold a "small portion" of its data (users' artist preferences) for $55,000 to Solo AI.&lt;/p&gt;
&lt;p&gt;While each Spotify user only earned about $5 in cryptocurrency tokens—which Kazlauskas suggested was not "ideal," wishing the users had earned about "a hundred times" more—she said the deal was "meaningful" in showing Spotify users that their data "is actually worth something."&lt;/p&gt;
&lt;p&gt;"I think this is what shows how these pools of data really act like a labor union," Kazlauskas said. "A single Spotify user, you're not going to be able to go say like, 'Hey, I want to sell you my individual data.' You actually need enough of a pool to sort of make it work."&lt;/p&gt;
&lt;h2&gt;Spotify sent warning to Unwrapped&lt;/h2&gt;
&lt;p&gt;Unsurprisingly, Spotify is not happy about Unwrapped, which is perhaps a little too closely named to its popular branded feature for the streaming giant's comfort. A spokesperson told Ars that Spotify sent a letter to the contact info listed for Unwrapped developers on their site, outlining concerns that the collective could be infringing on Spotify's Wrapped trademark.&lt;/p&gt;
&lt;p&gt;Further, the letter warned that Unwrapped violates Spotify's developer policy, which bans using the Spotify platform or any Spotify content to build machine learning or AI models. And developers may also be violating terms by facilitating users' sale of streaming data.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;"Spotify honors our users’ privacy rights, including the right of portability," Spotify's spokesperson said. "All of our users can receive a copy of their personal data to use as they see fit. That said, UnwrappedData.org is in violation of our Developer Terms which prohibit the collection, aggregation, and sale of Spotify user data to third parties."&lt;/p&gt;
&lt;p&gt;But while Spotify suggests it has already taken steps to stop Unwrapped, the Unwrapped team told Ars that it never received any communication from Spotify. It plans to defend users' right to "access, control, and benefit from their own data," its statement said, while providing reassurances that it will "respect Spotify's position as a global music leader."&lt;/p&gt;
&lt;p&gt;Unwrapped "does not distribute Spotify’s content, nor does it interfere with Spotify’s business," developers argued. "What it provides is community-owned infrastructure that allows individuals to exercise rights they already hold under widely recognized data protection frameworks—rights to access their own listening history, preferences, and usage data."&lt;/p&gt;
&lt;p&gt;"When listeners choose to share or monetize their data together, they are not taking anything away from Spotify," developers said. "They are simply exercising digital self-determination. To suggest otherwise is to claim that users do not truly own their data—that Spotify owns it for them."&lt;/p&gt;
&lt;p&gt;Jacob Hoffman-Andrews, a senior staff technologist for the digital rights group the Electronic Frontier Foundation, told Ars that—while EFF objects to data dividend schemes "where users are encouraged to share personal information in exchange for payment"—Spotify users should nevertheless always maintain control of their data.&lt;/p&gt;
&lt;p&gt;"In general, listeners should have control of their own data, which includes exporting it for their own use," Hoffman-Andrews said. "An individual's musical history is of use not just to Spotify but also to the individual who created it. And there's a long history of services that enable this sort of data portability, for instance Last.fm, which integrates with Spotify and many other services."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;To EFF, it seems ill-advised to sell data to AI companies, Hoffman-Andrews said, emphasizing "privacy isn't a market commodity, it's a fundamental right."&lt;/p&gt;
&lt;p&gt;"Of course, so is the right to control one's own data," Hoffman-Andrews noted, seeming to agree with Unwrapped developers in concluding that "ultimately, listeners should get to do what they want with their own information."&lt;/p&gt;
&lt;p&gt;Users' right to privacy is the primary reason why Unwrapped developers told Ars that they're hoping Spotify won't try to block users from selling data to build AI.&lt;/p&gt;
&lt;p&gt;"This is the heart of the issue: If Spotify seeks to restrict or penalize people for exercising these rights, it sends a chilling message that its listeners should have no say in how their own data is used," the Unwrapped team's statement said. "That is out of step not only with privacy law, but with the values of transparency, fairness, and community-driven innovation that define the next era of the Internet."&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Unwrapped sign-ups limited due to alleged Spotify issues&lt;/h2&gt;
&lt;p&gt;There could be more interest in Unwrapped. But Kazlauskas alleged to Ars that in the more than six months since Unwrapped's launch, "Spotify has made it extraordinarily difficult" for users to port over their data. She claimed that developers have found that "every time they have an easy way for users to get their data," Spotify shuts it down "in some way."&lt;/p&gt;
&lt;p&gt;Supposedly because of Spotify's interference, Unwrapped remains in an early launch phase and can only offer limited spots for new users seeking to sell their data. Kazlauskas told Ars that about 300 users can be added each day due to the cumbersome and allegedly shifting process for porting over data.&lt;/p&gt;
&lt;p&gt;Currently, however, Unwrapped is working on an update that could make that process more stable, Kazlauskas said, as well as changes to help users regularly update their streaming data. Those updates could perhaps attract more users to the collective.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Critics of Vana, like TechCrunch's Kyle Wiggers, have suggested that data pools like Unwrapped will never reach "critical mass," likely only appealing to niche users drawn to decentralization movements. Kazlauskas told Ars that data sale payments issued in cryptocurrency are one barrier for crypto-averse or crypto-shy users interested in Vana.&lt;/p&gt;
&lt;p&gt;"The No. 1 thing I would say is, this kind of user experience problem where when you're using any new kind of decentralized technology, you need to set up a wallet, then you're getting tokens," Kazlauskas explained. Users may feel culture shock, wondering, "What does that even mean? How do I vote with this thing? Is this real money?"&lt;/p&gt;
&lt;p&gt;Kazlauskas is hoping that Vana supports a culture shift, striving to reach critical mass by giving users a "commercial lens" to start caring about data ownership. She also supports legislation like the Digital Choice Act in Utah, which "requires actually real-time API access, so people can get their data." If the US had a federal law like that, Kazlauskas suspects that launching Unwrapped would have been "so much easier."&lt;/p&gt;
&lt;p&gt;Although regulations like Utah's law could serve as a harbinger of a sea change, Kazlauskas noted that Big Tech companies that currently control AI markets employ a fierce lobbying force to maintain control over user data that decentralized movements just don't have.&lt;/p&gt;
&lt;p&gt;As Vana partners with Flower AI, striving, as Wired reported, to "shake up the AI industry" by releasing "a giant 100 billion-parameter model" later this year, Kazlauskas remains committed to ensuring that users are in control and "not just consumed." She fears a future where tech giants may be motivated to use AI to surveil, influence, or manipulate users, when instead users could choose to band together and benefit from building more ethical AI.&lt;/p&gt;
&lt;p&gt;"A world where a single company controls AI is honestly really dystopian," Kazlauskas told Ars. "I think that it is really scary. And so I think that the path that decentralized AI offers is one where a large group of people are still in control, and you still get really powerful technology."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Spotify sent a warning to stop data sales, but developers say they never got it.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/spotify-data-mining-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/spotify-data-mining-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;For millions of Spotify users, the "Wrapped" feature—which crunches the numbers on their annual listening habits—is a highlight of every year's end, ever since it debuted in 2015. NPR once broke down exactly why our brains find the feature so "irresistible," while Cosmopolitan last year declared that sharing Wrapped screenshots of top artists and songs had by now become "the ultimate status symbol" for tens of millions of music fans.&lt;/p&gt;
&lt;p&gt;It's no surprise then that, after a decade, some Spotify users who are especially eager to see Wrapped evolve are no longer willing to wait to see if Spotify will ever deliver the more creative streaming insights they crave.&lt;/p&gt;
&lt;p&gt;With the help of AI, these users expect that their data can be more quickly analyzed to potentially uncover overlooked or never-considered patterns that could offer even more insights into what their listening habits say about them.&lt;/p&gt;
&lt;p&gt;Imagine, for example, accessing a music recap that encapsulates a user's full listening history—not just their top songs and artists. With that unlocked, users could track emotional patterns, analyzing how their music tastes reflected their moods over time and perhaps helping them adjust their listening habits to better cope with stress or major life events. And for users particularly intrigued by their own data, there's even the potential to use AI to cross data streams from different platforms and perhaps understand even more about how their music choices impact their lives and tastes more broadly.&lt;/p&gt;
&lt;p&gt;Likely just as appealing as gleaning deeper personal insights, though, users could also potentially build AI tools to compare listening habits with their friends. That could lead to nearly endless fun for the most invested music fans, where AI could be tapped to assess all kinds of random data points, like whose breakup playlists are more intense or who really spends the most time listening to a shared favorite artist.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In pursuit of supporting developers offering novel insights like these, more than 18,000 Spotify users have joined "Unwrapped," a collective launched in February that allows them to pool and monetize their data.&lt;/p&gt;
&lt;p&gt;Voting as a group through the decentralized data platform Vana—which Wired profiled earlier this year—these users can elect to sell their dataset to developers who are building AI tools offering fresh ways for users to analyze streaming data in ways that Spotify likely couldn't or wouldn't.&lt;/p&gt;
&lt;p&gt;In June, the group made its first sale, with 99.5 percent of members voting yes. Vana co-founder Anna Kazlauskas told Ars that the collective—at the time about 10,000 members strong—sold a "small portion" of its data (users' artist preferences) for $55,000 to Solo AI.&lt;/p&gt;
&lt;p&gt;While each Spotify user only earned about $5 in cryptocurrency tokens—which Kazlauskas suggested was not "ideal," wishing the users had earned about "a hundred times" more—she said the deal was "meaningful" in showing Spotify users that their data "is actually worth something."&lt;/p&gt;
&lt;p&gt;"I think this is what shows how these pools of data really act like a labor union," Kazlauskas said. "A single Spotify user, you're not going to be able to go say like, 'Hey, I want to sell you my individual data.' You actually need enough of a pool to sort of make it work."&lt;/p&gt;
&lt;h2&gt;Spotify sent warning to Unwrapped&lt;/h2&gt;
&lt;p&gt;Unsurprisingly, Spotify is not happy about Unwrapped, which is perhaps a little too closely named to its popular branded feature for the streaming giant's comfort. A spokesperson told Ars that Spotify sent a letter to the contact info listed for Unwrapped developers on their site, outlining concerns that the collective could be infringing on Spotify's Wrapped trademark.&lt;/p&gt;
&lt;p&gt;Further, the letter warned that Unwrapped violates Spotify's developer policy, which bans using the Spotify platform or any Spotify content to build machine learning or AI models. And developers may also be violating terms by facilitating users' sale of streaming data.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;"Spotify honors our users’ privacy rights, including the right of portability," Spotify's spokesperson said. "All of our users can receive a copy of their personal data to use as they see fit. That said, UnwrappedData.org is in violation of our Developer Terms which prohibit the collection, aggregation, and sale of Spotify user data to third parties."&lt;/p&gt;
&lt;p&gt;But while Spotify suggests it has already taken steps to stop Unwrapped, the Unwrapped team told Ars that it never received any communication from Spotify. It plans to defend users' right to "access, control, and benefit from their own data," its statement said, while providing reassurances that it will "respect Spotify's position as a global music leader."&lt;/p&gt;
&lt;p&gt;Unwrapped "does not distribute Spotify’s content, nor does it interfere with Spotify’s business," developers argued. "What it provides is community-owned infrastructure that allows individuals to exercise rights they already hold under widely recognized data protection frameworks—rights to access their own listening history, preferences, and usage data."&lt;/p&gt;
&lt;p&gt;"When listeners choose to share or monetize their data together, they are not taking anything away from Spotify," developers said. "They are simply exercising digital self-determination. To suggest otherwise is to claim that users do not truly own their data—that Spotify owns it for them."&lt;/p&gt;
&lt;p&gt;Jacob Hoffman-Andrews, a senior staff technologist for the digital rights group the Electronic Frontier Foundation, told Ars that—while EFF objects to data dividend schemes "where users are encouraged to share personal information in exchange for payment"—Spotify users should nevertheless always maintain control of their data.&lt;/p&gt;
&lt;p&gt;"In general, listeners should have control of their own data, which includes exporting it for their own use," Hoffman-Andrews said. "An individual's musical history is of use not just to Spotify but also to the individual who created it. And there's a long history of services that enable this sort of data portability, for instance Last.fm, which integrates with Spotify and many other services."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;To EFF, it seems ill-advised to sell data to AI companies, Hoffman-Andrews said, emphasizing "privacy isn't a market commodity, it's a fundamental right."&lt;/p&gt;
&lt;p&gt;"Of course, so is the right to control one's own data," Hoffman-Andrews noted, seeming to agree with Unwrapped developers in concluding that "ultimately, listeners should get to do what they want with their own information."&lt;/p&gt;
&lt;p&gt;Users' right to privacy is the primary reason why Unwrapped developers told Ars that they're hoping Spotify won't try to block users from selling data to build AI.&lt;/p&gt;
&lt;p&gt;"This is the heart of the issue: If Spotify seeks to restrict or penalize people for exercising these rights, it sends a chilling message that its listeners should have no say in how their own data is used," the Unwrapped team's statement said. "That is out of step not only with privacy law, but with the values of transparency, fairness, and community-driven innovation that define the next era of the Internet."&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Unwrapped sign-ups limited due to alleged Spotify issues&lt;/h2&gt;
&lt;p&gt;There could be more interest in Unwrapped. But Kazlauskas alleged to Ars that in the more than six months since Unwrapped's launch, "Spotify has made it extraordinarily difficult" for users to port over their data. She claimed that developers have found that "every time they have an easy way for users to get their data," Spotify shuts it down "in some way."&lt;/p&gt;
&lt;p&gt;Supposedly because of Spotify's interference, Unwrapped remains in an early launch phase and can only offer limited spots for new users seeking to sell their data. Kazlauskas told Ars that about 300 users can be added each day due to the cumbersome and allegedly shifting process for porting over data.&lt;/p&gt;
&lt;p&gt;Currently, however, Unwrapped is working on an update that could make that process more stable, Kazlauskas said, as well as changes to help users regularly update their streaming data. Those updates could perhaps attract more users to the collective.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Critics of Vana, like TechCrunch's Kyle Wiggers, have suggested that data pools like Unwrapped will never reach "critical mass," likely only appealing to niche users drawn to decentralization movements. Kazlauskas told Ars that data sale payments issued in cryptocurrency are one barrier for crypto-averse or crypto-shy users interested in Vana.&lt;/p&gt;
&lt;p&gt;"The No. 1 thing I would say is, this kind of user experience problem where when you're using any new kind of decentralized technology, you need to set up a wallet, then you're getting tokens," Kazlauskas explained. Users may feel culture shock, wondering, "What does that even mean? How do I vote with this thing? Is this real money?"&lt;/p&gt;
&lt;p&gt;Kazlauskas is hoping that Vana supports a culture shift, striving to reach critical mass by giving users a "commercial lens" to start caring about data ownership. She also supports legislation like the Digital Choice Act in Utah, which "requires actually real-time API access, so people can get their data." If the US had a federal law like that, Kazlauskas suspects that launching Unwrapped would have been "so much easier."&lt;/p&gt;
&lt;p&gt;Although regulations like Utah's law could serve as a harbinger of a sea change, Kazlauskas noted that Big Tech companies that currently control AI markets employ a fierce lobbying force to maintain control over user data that decentralized movements just don't have.&lt;/p&gt;
&lt;p&gt;As Vana partners with Flower AI, striving, as Wired reported, to "shake up the AI industry" by releasing "a giant 100 billion-parameter model" later this year, Kazlauskas remains committed to ensuring that users are in control and "not just consumed." She fears a future where tech giants may be motivated to use AI to surveil, influence, or manipulate users, when instead users could choose to band together and benefit from building more ethical AI.&lt;/p&gt;
&lt;p&gt;"A world where a single company controls AI is honestly really dystopian," Kazlauskas told Ars. "I think that it is really scary. And so I think that the path that decentralized AI offers is one where a large group of people are still in control, and you still get really powerful technology."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/09/spotify-peeved-after-10000-users-sold-data-to-build-ai-tools/</guid><pubDate>Wed, 10 Sep 2025 16:23:25 +0000</pubDate></item><item><title>Humanoids, AVs, and what’s next in AI hardware at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/10/humanoids-avs-and-whats-next-in-ai-hardware-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; hits Moscone West in San Francisco from October 27 to 29, bringing together 10,000+ startup and VC leaders for three days of bold ideas, groundbreaking tech, and future-shaping conversations. One of the most highly anticipated &lt;strong&gt;sessions happening on one of the two AI Stages&lt;/strong&gt; will spotlight where AI hardware is heading next, featuring a live look at the robotics and autonomous systems pushing boundaries in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;br /&gt;AI may be reshaping software, but when it comes to robotics and autonomous systems, the big breakout moment is still on the horizon. That’s what makes this session at TechCrunch Disrupt 2025 so compelling. &lt;strong&gt;Raquel Urtasun&lt;/strong&gt;, founder and CEO of &lt;strong&gt;Waabi&lt;/strong&gt;, and &lt;strong&gt;Jeff Cardenas&lt;/strong&gt;, co-founder and CEO of &lt;strong&gt;Apptronik&lt;/strong&gt;, are joining forces on the AI Stage to talk about what it takes to put intelligence into motion — whether it’s behind the wheel or on two legs.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Jeff Cardenas Raquel Urtasun" class="wp-image-3026927" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_CardenasUrtasun-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-ai-meets-real-world-physics"&gt;&lt;br /&gt;AI meets real-world physics&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This conversation dives into the complex systems that power autonomous vehicles and humanoid robots — and the simulation, sensors, and software infrastructure needed to scale them safely. Both Waabi and Apptronik are pushing the limits of what’s possible in the physical world. At Disrupt, they’ll walk us through the breakthroughs and bottlenecks shaping the next generation of intelligent machines.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-this-session-matters"&gt;Why this session matters&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AI is already changing how we build, ship, and move — but physical deployment brings a unique set of constraints and opportunities. Expect a grounded, forward-looking discussion on how the smartest robots and self-driving platforms are coming to life, and what that means for the future of industry, labor, and infrastructure.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Catch Raquel Urtasun and Jeff Cardenas on the AI Stage at TechCrunch Disrupt 2025, happening October 27 to 29 at Moscone West in San Francisco. &lt;strong&gt;Register now&lt;/strong&gt; to join more than 10,000 startup and VC leaders and &lt;strong&gt;save up to $668&lt;/strong&gt; before prices increase.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; hits Moscone West in San Francisco from October 27 to 29, bringing together 10,000+ startup and VC leaders for three days of bold ideas, groundbreaking tech, and future-shaping conversations. One of the most highly anticipated &lt;strong&gt;sessions happening on one of the two AI Stages&lt;/strong&gt; will spotlight where AI hardware is heading next, featuring a live look at the robotics and autonomous systems pushing boundaries in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;br /&gt;AI may be reshaping software, but when it comes to robotics and autonomous systems, the big breakout moment is still on the horizon. That’s what makes this session at TechCrunch Disrupt 2025 so compelling. &lt;strong&gt;Raquel Urtasun&lt;/strong&gt;, founder and CEO of &lt;strong&gt;Waabi&lt;/strong&gt;, and &lt;strong&gt;Jeff Cardenas&lt;/strong&gt;, co-founder and CEO of &lt;strong&gt;Apptronik&lt;/strong&gt;, are joining forces on the AI Stage to talk about what it takes to put intelligence into motion — whether it’s behind the wheel or on two legs.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Jeff Cardenas Raquel Urtasun" class="wp-image-3026927" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_CardenasUrtasun-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-ai-meets-real-world-physics"&gt;&lt;br /&gt;AI meets real-world physics&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This conversation dives into the complex systems that power autonomous vehicles and humanoid robots — and the simulation, sensors, and software infrastructure needed to scale them safely. Both Waabi and Apptronik are pushing the limits of what’s possible in the physical world. At Disrupt, they’ll walk us through the breakthroughs and bottlenecks shaping the next generation of intelligent machines.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-this-session-matters"&gt;Why this session matters&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AI is already changing how we build, ship, and move — but physical deployment brings a unique set of constraints and opportunities. Expect a grounded, forward-looking discussion on how the smartest robots and self-driving platforms are coming to life, and what that means for the future of industry, labor, and infrastructure.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Catch Raquel Urtasun and Jeff Cardenas on the AI Stage at TechCrunch Disrupt 2025, happening October 27 to 29 at Moscone West in San Francisco. &lt;strong&gt;Register now&lt;/strong&gt; to join more than 10,000 startup and VC leaders and &lt;strong&gt;save up to $668&lt;/strong&gt; before prices increase.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/10/humanoids-avs-and-whats-next-in-ai-hardware-at-techcrunch-disrupt-2025/</guid><pubDate>Wed, 10 Sep 2025 17:00:00 +0000</pubDate></item><item><title>Anthropic reports outages, Claude and Console impacted (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/10/anthropic-reports-outages-claude-and-console-impacted/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Claude-Chrome-Ext_email-hero-hero.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic reported a service outage impacting APIs, Console, and Claude earlier this afternoon.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users on GitHub and Hacker News noted issues with Claude at around 12:20 p.m. ET, with Anthropic releasing a status update eight minutes later, noting that its APIs, Console, and Claude AI were down. At press time, the company said it had implemented several fixes and was monitoring the results.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re aware of a very brief outage of our API today shortly before 9:30 a.m. PT,” an Anthropic spokesperson told TechCrunch. “Service was quickly restored.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic is no stranger to errors or bugs on its platform and has had some issues in the past few months, especially regarding Claude and its models.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Claude users awaited its reboot, some joked about what they were supposed to do since the system was down. One user on GitHub wrote about how the software engineering community was now twiddling its thumbs, while another on Hacker News quoted what someone said last time something like this happened: “Nooooo I’m going to have to use my brain again and write 100% of my code like a caveman from December 2024.”&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Claude-Chrome-Ext_email-hero-hero.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic reported a service outage impacting APIs, Console, and Claude earlier this afternoon.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users on GitHub and Hacker News noted issues with Claude at around 12:20 p.m. ET, with Anthropic releasing a status update eight minutes later, noting that its APIs, Console, and Claude AI were down. At press time, the company said it had implemented several fixes and was monitoring the results.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re aware of a very brief outage of our API today shortly before 9:30 a.m. PT,” an Anthropic spokesperson told TechCrunch. “Service was quickly restored.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic is no stranger to errors or bugs on its platform and has had some issues in the past few months, especially regarding Claude and its models.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Claude users awaited its reboot, some joked about what they were supposed to do since the system was down. One user on GitHub wrote about how the software engineering community was now twiddling its thumbs, while another on Hacker News quoted what someone said last time something like this happened: “Nooooo I’m going to have to use my brain again and write 100% of my code like a caveman from December 2024.”&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/10/anthropic-reports-outages-claude-and-console-impacted/</guid><pubDate>Wed, 10 Sep 2025 17:33:05 +0000</pubDate></item><item><title>YouTube’s multi-language audio feature for dubbing videos rolls out to all creators (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/10/youtubes-multi-language-audio-feature-for-dubbing-videos-rolls-out-to-all-creators/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/04/youtube-ios-app.webp?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;YouTube announced on Wednesday that its multi-language audio feature has officially launched after a two-year-long pilot. Now, millions of YouTubers can add dubbing to their videos in different languages, helping them reach a wider global audience. The rollout is expected to happen over the coming weeks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature initially launched as a pilot in 2023, available to a limited number of creators, including MrBeast, Mark Rober, and chef Jamie Oliver. Creators had to work with third-party dubbing services until YouTube introduced an AI-powered auto-dubbing tool that leverages Google’s Gemini technology to replicate a creator’s tone and emotions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Since its launch, YouTube reports that several testers have seen success with this feature. On average, those who uploaded multi-language audio tracks saw over 25% of their watch time coming from views in the video’s non-primary language. Jamie Oliver’s channel, for instance, tripled in views after using multi-language audio tracks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, the company has been testing multi-language thumbnails with a select group of creators. Since June, creators have been able to customize thumbnails to display text in other languages, catering to their international audience. The localized thumbnails are designed to include text that matches the viewers’ preferred languages.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/04/youtube-ios-app.webp?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;YouTube announced on Wednesday that its multi-language audio feature has officially launched after a two-year-long pilot. Now, millions of YouTubers can add dubbing to their videos in different languages, helping them reach a wider global audience. The rollout is expected to happen over the coming weeks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature initially launched as a pilot in 2023, available to a limited number of creators, including MrBeast, Mark Rober, and chef Jamie Oliver. Creators had to work with third-party dubbing services until YouTube introduced an AI-powered auto-dubbing tool that leverages Google’s Gemini technology to replicate a creator’s tone and emotions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Since its launch, YouTube reports that several testers have seen success with this feature. On average, those who uploaded multi-language audio tracks saw over 25% of their watch time coming from views in the video’s non-primary language. Jamie Oliver’s channel, for instance, tripled in views after using multi-language audio tracks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, the company has been testing multi-language thumbnails with a select group of creators. Since June, creators have been able to customize thumbnails to display text in other languages, catering to their international audience. The localized thumbnails are designed to include text that matches the viewers’ preferred languages.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/10/youtubes-multi-language-audio-feature-for-dubbing-videos-rolls-out-to-all-creators/</guid><pubDate>Wed, 10 Sep 2025 17:58:32 +0000</pubDate></item><item><title>Developers joke about “coding like cavemen” as AI service suffers major outage (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/09/developers-joke-about-coding-like-cavemen-as-ai-service-suffers-major-outage/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Anthropic outage takes down AI tools some developers rely on to create software.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-300x169.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      This is fine.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday afternoon, Anthropic experienced a brief but complete service outage that took down its AI infrastructure, leaving developers unable to access Claude.ai, the API, Claude Code, or the management console for around half an hour. The outage affected all three of Anthropic's main services simultaneously, with the company posting at 12:28 pm Eastern that "APIs, Console, and Claude.ai are down. Services will be restored as soon as possible." As of press time, the services appear to be restored.&lt;/p&gt;
&lt;p&gt;The disruption, though lasting only about 30 minutes, quickly took the top spot on tech link-sharing site Hacker News for a short time and inspired immediate reactions from developers who have become increasingly reliant on AI coding tools for their daily work. "Everyone will just have to learn how to do it like we did in the old days, and blindly copy and paste from Stack Overflow," joked one Hacker News commenter. Another user recalled a joke from a previous AI outage: "Nooooo I'm going to have to use my brain again and write 100% of my code like a caveman from December 2024."&lt;/p&gt;
&lt;p&gt;The most recent outage came at an inopportune time, affecting developers across the US who have integrated Claude into their workflows. One Hacker News user observed: "It's like every other day, the moment US working hours start, AI (in my case I mostly use Anthropic, others may be better) starts dying or at least getting intermittent errors. In EU working hours there's rarely any outages." Another user also noted this pattern, saying that "early morning here in the UK everything is fine, as soon as most of the US is up and at it, then it slowly turns to treacle."&lt;/p&gt;
&lt;p&gt;While some users criticized Anthropic for reliability issues in recent months, the company's status page acknowledged the issue within 39 minutes of the initial reports, and by 12:55 pm Eastern announced that a fix had been implemented and that the company's teams were monitoring the results.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Growing dependency on AI coding tools&lt;/h2&gt;
&lt;p&gt;The speed at which news of the outage spread shows how deeply embedded AI coding assistants have already become in modern software development. Claude Code, announced in February and widely launched in May, is Anthropic's terminal-based coding agent that can perform multi-step coding tasks across an existing code base.&lt;/p&gt;
&lt;p&gt;The tool competes with OpenAI's Codex feature, a coding agent that generates production-ready code in isolated containers, Google's Gemini CLI, Microsoft's GitHub Copilot, which itself can use Claude models for code, and Cursor, a popular AI-powered IDE built on VS Code that also integrates multiple AI models, including Claude.&lt;/p&gt;
&lt;p&gt;During today's outage, some developers turned to alternative solutions. "Z.AI works fine. Qwen works fine. Glad I switched," posted one user on Hacker News. Others joked about reverting to older methods, with one suggesting the "pseudo-LLM experience" could be achieved with a Python package that imports code directly from Stack Overflow.&lt;/p&gt;
&lt;p&gt;While AI coding assistants have accelerated development for some users, they've also caused problems for others who rely on them too heavily. The emerging practice of so-called "vibe coding"—using natural language to generate and execute code through AI models without fully understanding the underlying operations—has led to catastrophic failures.&lt;/p&gt;
&lt;p&gt;In recent incidents, Google's Gemini CLI destroyed user files while attempting to reorganize them, and Replit's AI coding service deleted a production database despite explicit instructions not to modify code. These failures occurred when the AI models confabulated successful operations and built subsequent actions on false premises, highlighting the risks of depending on AI assistants that can misinterpret file structures or fabricate data to hide their errors.&lt;/p&gt;
&lt;p&gt;Wednesday's outage served as a reminder that as dependency on AI grows, even minor service disruptions can become major events that affect an entire profession. But perhaps that could be a good thing if it's an excuse to take a break from a stressful workload. As one commenter joked, it might be "time to go outside and touch some grass again."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Anthropic outage takes down AI tools some developers rely on to create software.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-300x169.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      This is fine.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday afternoon, Anthropic experienced a brief but complete service outage that took down its AI infrastructure, leaving developers unable to access Claude.ai, the API, Claude Code, or the management console for around half an hour. The outage affected all three of Anthropic's main services simultaneously, with the company posting at 12:28 pm Eastern that "APIs, Console, and Claude.ai are down. Services will be restored as soon as possible." As of press time, the services appear to be restored.&lt;/p&gt;
&lt;p&gt;The disruption, though lasting only about 30 minutes, quickly took the top spot on tech link-sharing site Hacker News for a short time and inspired immediate reactions from developers who have become increasingly reliant on AI coding tools for their daily work. "Everyone will just have to learn how to do it like we did in the old days, and blindly copy and paste from Stack Overflow," joked one Hacker News commenter. Another user recalled a joke from a previous AI outage: "Nooooo I'm going to have to use my brain again and write 100% of my code like a caveman from December 2024."&lt;/p&gt;
&lt;p&gt;The most recent outage came at an inopportune time, affecting developers across the US who have integrated Claude into their workflows. One Hacker News user observed: "It's like every other day, the moment US working hours start, AI (in my case I mostly use Anthropic, others may be better) starts dying or at least getting intermittent errors. In EU working hours there's rarely any outages." Another user also noted this pattern, saying that "early morning here in the UK everything is fine, as soon as most of the US is up and at it, then it slowly turns to treacle."&lt;/p&gt;
&lt;p&gt;While some users criticized Anthropic for reliability issues in recent months, the company's status page acknowledged the issue within 39 minutes of the initial reports, and by 12:55 pm Eastern announced that a fix had been implemented and that the company's teams were monitoring the results.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Growing dependency on AI coding tools&lt;/h2&gt;
&lt;p&gt;The speed at which news of the outage spread shows how deeply embedded AI coding assistants have already become in modern software development. Claude Code, announced in February and widely launched in May, is Anthropic's terminal-based coding agent that can perform multi-step coding tasks across an existing code base.&lt;/p&gt;
&lt;p&gt;The tool competes with OpenAI's Codex feature, a coding agent that generates production-ready code in isolated containers, Google's Gemini CLI, Microsoft's GitHub Copilot, which itself can use Claude models for code, and Cursor, a popular AI-powered IDE built on VS Code that also integrates multiple AI models, including Claude.&lt;/p&gt;
&lt;p&gt;During today's outage, some developers turned to alternative solutions. "Z.AI works fine. Qwen works fine. Glad I switched," posted one user on Hacker News. Others joked about reverting to older methods, with one suggesting the "pseudo-LLM experience" could be achieved with a Python package that imports code directly from Stack Overflow.&lt;/p&gt;
&lt;p&gt;While AI coding assistants have accelerated development for some users, they've also caused problems for others who rely on them too heavily. The emerging practice of so-called "vibe coding"—using natural language to generate and execute code through AI models without fully understanding the underlying operations—has led to catastrophic failures.&lt;/p&gt;
&lt;p&gt;In recent incidents, Google's Gemini CLI destroyed user files while attempting to reorganize them, and Replit's AI coding service deleted a production database despite explicit instructions not to modify code. These failures occurred when the AI models confabulated successful operations and built subsequent actions on false premises, highlighting the risks of depending on AI assistants that can misinterpret file structures or fabricate data to hide their errors.&lt;/p&gt;
&lt;p&gt;Wednesday's outage served as a reminder that as dependency on AI grows, even minor service disruptions can become major events that affect an entire profession. But perhaps that could be a good thing if it's an excuse to take a break from a stressful workload. As one commenter joked, it might be "time to go outside and touch some grass again."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/09/developers-joke-about-coding-like-cavemen-as-ai-service-suffers-major-outage/</guid><pubDate>Wed, 10 Sep 2025 18:08:49 +0000</pubDate></item><item><title>[NEW] OpenAI and Oracle reportedly ink historic cloud computing deal (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/10/openai-and-oracle-reportedly-ink-historic-cloud-computing-deal/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2197181602.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Oracle sent its shares soaring after markets closed yesterday after reporting that it signed multiple multi-billion-dollar contracts with several customers. Now, we have an idea of who those customers might be.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Oracle signed a deal with OpenAI for the AI company to purchase $300 billion worth of compute power over a span of about five years, according to reporting from the Wall Street Journal. OpenAI would start purchasing this compute in 2027.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If the WSJ’s reporting is correct, this would be one of the largest cloud contracts ever signed. Oracle declined to comment. OpenAI did not respond to a request for confirmation or comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Oracle is no stranger to working with OpenAI. OpenAI started tapping Oracle for compute in the summer of 2024. The AI giant also moved further away from exclusively using Microsoft Azure as its only cloud provider in January.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This move away from Microsoft was timed with OpenAI’s involvement with the Stargate Project, in which OpenAI, SoftBank, and Oracle have committed to invest $500 billion into domestic data center projects over the next four years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI clearly needs as much compute as it can get. The company reportedly signed a cloud deal with Google, according to Reuters, this spring despite the fact that the two companies are racing against each other for AI supremacy.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2197181602.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Oracle sent its shares soaring after markets closed yesterday after reporting that it signed multiple multi-billion-dollar contracts with several customers. Now, we have an idea of who those customers might be.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Oracle signed a deal with OpenAI for the AI company to purchase $300 billion worth of compute power over a span of about five years, according to reporting from the Wall Street Journal. OpenAI would start purchasing this compute in 2027.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If the WSJ’s reporting is correct, this would be one of the largest cloud contracts ever signed. Oracle declined to comment. OpenAI did not respond to a request for confirmation or comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Oracle is no stranger to working with OpenAI. OpenAI started tapping Oracle for compute in the summer of 2024. The AI giant also moved further away from exclusively using Microsoft Azure as its only cloud provider in January.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This move away from Microsoft was timed with OpenAI’s involvement with the Stargate Project, in which OpenAI, SoftBank, and Oracle have committed to invest $500 billion into domestic data center projects over the next four years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI clearly needs as much compute as it can get. The company reportedly signed a cloud deal with Google, according to Reuters, this spring despite the fact that the two companies are racing against each other for AI supremacy.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/10/openai-and-oracle-reportedly-ink-historic-cloud-computing-deal/</guid><pubDate>Wed, 10 Sep 2025 19:00:34 +0000</pubDate></item><item><title>[NEW] One of Google’s new Pixel 10 AI features has already been removed (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/09/google-pulls-daily-hub-ai-feature-from-pixel-10-phones/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Pixel 10 Daily Hub has vanished, but Google says it will come back when it's ready.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Daily Hub on phone" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/Daily-Hub-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Daily Hub on phone" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/Daily-Hub-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google has "paused" Daily Hub on Pixel 10 phones. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google is one of the most ardent proponents of generative AI technology, as evidenced by the recent launch of the Pixel 10 series. The phones were announced with more than 20 new AI experiences, according to Google. However, one of them is already being pulled from the company's phones. If you go looking for your Daily Hub, you may be disappointed. Not &lt;em&gt;that&lt;/em&gt; disappointed, though, as it has been pulled because it didn't do very much.&lt;/p&gt;
&lt;p&gt;Many of Google's new AI features only make themselves known in specific circumstances, for example when Magic Cue finds an opportunity to suggest an address or calendar appointment based on your screen context. The Daily Hub, on the other hand, asserted itself multiple times throughout the day. It appeared at the top of the Google Discover feed, as well as in the At a Glance widget right at the top of the home screen.&lt;/p&gt;
&lt;p&gt;Just a few weeks after release, Google has pulled the Daily Hub preview from Pixel 10 devices. You will no longer see it in Google Discover nor in the home screen widget. After being spotted by 9to5Google, the company has issued a statement explaining its plans.&lt;/p&gt;
&lt;p&gt;"To ensure the best possible experience on Pixel, we’re temporarily pausing the public preview of Daily Hub for users. Our teams are actively working to enhance its performance and refine the personalized experience. We look forward to reintroducing an improved Daily Hub when it’s ready," a Google spokesperson said.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It's not surprising that Google has hit pause on Daily Hub. Google's approach to mobile AI relies on the Tensor processor's capable NPU. Pixel phones process your personal data on-device rather than sending it to the cloud, which is how Daily Hub is supposed to glean insights into your life. The problem is that it doesn't do that very well.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2116355 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Daily Hub" class="fullwidth full" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/daily-hub-lol.jpg" width="1280" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Daily Hub wasn't smart enough to understand the "nail services" were on my wife's shared calendar.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;As we mentioned in our review of the Pixel 10 series, Daily Hub's premise is similar to Samsung's Now Brief. They both pull in data from your apps and run it all through on-device AI models to create a digest of your life that is refreshed multiple times per day. Samsung has continued promoting Now Brief as its chief AI innovation despite the fact that it rarely offers more than the weather and calendar reminders. Google's Daily Hub was in a similar place, but it may have been even less useful.&lt;/p&gt;
&lt;p&gt;In our testing, Daily Hub rarely showed anything beyond the weather, suggested videos, and AI search prompts. When it did integrate calendar data, it seemed unable to differentiate between the user's own calendar and data from shared calendars. This largely useless report was pushed to the At a Glance widget multiple times per day, making it more of a nuisance than helpful.&lt;/p&gt;
&lt;p&gt;Smartphones are overflowing with personal data, which is one of the reasons they are such attractive targets for hackers. Despite this, both Samsung and Google have found little success generating useful daily insights with on-device AI models. We wait with bated breath to see if Google can make Daily Hub worth using when it returns.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Pixel 10 Daily Hub has vanished, but Google says it will come back when it's ready.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Daily Hub on phone" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/Daily-Hub-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Daily Hub on phone" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/Daily-Hub-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google has "paused" Daily Hub on Pixel 10 phones. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google is one of the most ardent proponents of generative AI technology, as evidenced by the recent launch of the Pixel 10 series. The phones were announced with more than 20 new AI experiences, according to Google. However, one of them is already being pulled from the company's phones. If you go looking for your Daily Hub, you may be disappointed. Not &lt;em&gt;that&lt;/em&gt; disappointed, though, as it has been pulled because it didn't do very much.&lt;/p&gt;
&lt;p&gt;Many of Google's new AI features only make themselves known in specific circumstances, for example when Magic Cue finds an opportunity to suggest an address or calendar appointment based on your screen context. The Daily Hub, on the other hand, asserted itself multiple times throughout the day. It appeared at the top of the Google Discover feed, as well as in the At a Glance widget right at the top of the home screen.&lt;/p&gt;
&lt;p&gt;Just a few weeks after release, Google has pulled the Daily Hub preview from Pixel 10 devices. You will no longer see it in Google Discover nor in the home screen widget. After being spotted by 9to5Google, the company has issued a statement explaining its plans.&lt;/p&gt;
&lt;p&gt;"To ensure the best possible experience on Pixel, we’re temporarily pausing the public preview of Daily Hub for users. Our teams are actively working to enhance its performance and refine the personalized experience. We look forward to reintroducing an improved Daily Hub when it’s ready," a Google spokesperson said.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It's not surprising that Google has hit pause on Daily Hub. Google's approach to mobile AI relies on the Tensor processor's capable NPU. Pixel phones process your personal data on-device rather than sending it to the cloud, which is how Daily Hub is supposed to glean insights into your life. The problem is that it doesn't do that very well.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2116355 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Daily Hub" class="fullwidth full" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/daily-hub-lol.jpg" width="1280" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Daily Hub wasn't smart enough to understand the "nail services" were on my wife's shared calendar.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;As we mentioned in our review of the Pixel 10 series, Daily Hub's premise is similar to Samsung's Now Brief. They both pull in data from your apps and run it all through on-device AI models to create a digest of your life that is refreshed multiple times per day. Samsung has continued promoting Now Brief as its chief AI innovation despite the fact that it rarely offers more than the weather and calendar reminders. Google's Daily Hub was in a similar place, but it may have been even less useful.&lt;/p&gt;
&lt;p&gt;In our testing, Daily Hub rarely showed anything beyond the weather, suggested videos, and AI search prompts. When it did integrate calendar data, it seemed unable to differentiate between the user's own calendar and data from shared calendars. This largely useless report was pushed to the At a Glance widget multiple times per day, making it more of a nuisance than helpful.&lt;/p&gt;
&lt;p&gt;Smartphones are overflowing with personal data, which is one of the reasons they are such attractive targets for hackers. Despite this, both Samsung and Google have found little success generating useful daily insights with on-device AI models. We wait with bated breath to see if Google can make Daily Hub worth using when it returns.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/09/google-pulls-daily-hub-ai-feature-from-pixel-10-phones/</guid><pubDate>Wed, 10 Sep 2025 19:00:41 +0000</pubDate></item><item><title>[NEW] Thinking Machines Lab wants to make AI models more consistent (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/10/thinking-machines-lab-wants-to-make-ai-models-more-consistent/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2188124206.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;There’s been great interest in what Mira Murati’s Thinking Machines Lab is building with its $2 billion in seed funding and the all-star team of former OpenAI researchers who have joined the lab. In a blog post published on Wednesday, Murati’s research lab gave the world its first look into one of its projects: creating AI models with reproducible responses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The research blog post, titled “Defeating Nondeterminism in LLM Inference,” tries to unpack the root cause of what introduces randomness in AI model responses. For example, ask ChatGPT the same question a few times over, and you’re likely to get a wide range of answers. This has largely been accepted in the AI community as a fact — today’s AI models are considered to be non-deterministic systems— but Thinking Machines Lab sees this as a solvable problem.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Today Thinking Machines Lab is launching our research blog, Connectionism. Our first blog post is “Defeating Nondeterminism in LLM Inference”&lt;/p&gt;&lt;p&gt;We believe that science is better when shared. Connectionism will cover topics as varied as our research is: from kernel numerics to… pic.twitter.com/jMFL3xt67C&lt;/p&gt;— Thinking Machines (@thinkymachines) September 10, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The post, authored by Thinking Machines Lab researcher Horace He, argues that the root cause of AI models’ randomness is the way GPU kernels — the small programs that run inside of Nvidia’s computer chips — are stitched together in inference processing (everything that happens after you press enter in ChatGPT). He suggests that by carefully controlling this layer of orchestration, it’s possible to make AI models more deterministic.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Beyond creating more reliable responses for enterprises and scientists, He notes that getting AI models to generate reproducible responses could also improve reinforcement learning (RL) training. RL is the process of rewarding AI models for correct answers, but if the answers are all slightly different, then the data gets a bit noisy. Creating more consistent AI model responses could make the whole RL process “smoother,” according to He. Thinking Machines Lab has told investors that it plans to use RL to customize AI models for businesses, The Information previously reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Murati, OpenAI’s former chief technology officer, said in July that Thinking Machines Lab’s first product will be unveiled in the coming months, and that it will be “useful for researchers and startups developing custom models.” It’s still unclear what that product is, or whether it will use techniques from this research to generate more reproducible responses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Thinking Machines Lab has also said that it plans to frequently publish blog posts, code, and other information about its research in an effort to “benefit the public, but also improve our own research culture.” This post, the first in the company’s new blog series called “Connectionism,” seems to be part of that effort. OpenAI also made a commitment to open research when it was founded, but the company has become more closed off as it’s become larger. We’ll see if Murati’s research lab stays true to this claim.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The research blog offers a rare glimpse inside one of Silicon Valley’s most secretive AI startups. While it doesn’t exactly reveal where the technology is going, it indicates that Thinking Machines Lab is tackling some of the largest question on the frontier of AI research. The real test is whether Thinking Machines Lab can solve these problems, and make products around its research to justify its $12 billion valuation.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2188124206.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;There’s been great interest in what Mira Murati’s Thinking Machines Lab is building with its $2 billion in seed funding and the all-star team of former OpenAI researchers who have joined the lab. In a blog post published on Wednesday, Murati’s research lab gave the world its first look into one of its projects: creating AI models with reproducible responses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The research blog post, titled “Defeating Nondeterminism in LLM Inference,” tries to unpack the root cause of what introduces randomness in AI model responses. For example, ask ChatGPT the same question a few times over, and you’re likely to get a wide range of answers. This has largely been accepted in the AI community as a fact — today’s AI models are considered to be non-deterministic systems— but Thinking Machines Lab sees this as a solvable problem.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Today Thinking Machines Lab is launching our research blog, Connectionism. Our first blog post is “Defeating Nondeterminism in LLM Inference”&lt;/p&gt;&lt;p&gt;We believe that science is better when shared. Connectionism will cover topics as varied as our research is: from kernel numerics to… pic.twitter.com/jMFL3xt67C&lt;/p&gt;— Thinking Machines (@thinkymachines) September 10, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The post, authored by Thinking Machines Lab researcher Horace He, argues that the root cause of AI models’ randomness is the way GPU kernels — the small programs that run inside of Nvidia’s computer chips — are stitched together in inference processing (everything that happens after you press enter in ChatGPT). He suggests that by carefully controlling this layer of orchestration, it’s possible to make AI models more deterministic.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Beyond creating more reliable responses for enterprises and scientists, He notes that getting AI models to generate reproducible responses could also improve reinforcement learning (RL) training. RL is the process of rewarding AI models for correct answers, but if the answers are all slightly different, then the data gets a bit noisy. Creating more consistent AI model responses could make the whole RL process “smoother,” according to He. Thinking Machines Lab has told investors that it plans to use RL to customize AI models for businesses, The Information previously reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Murati, OpenAI’s former chief technology officer, said in July that Thinking Machines Lab’s first product will be unveiled in the coming months, and that it will be “useful for researchers and startups developing custom models.” It’s still unclear what that product is, or whether it will use techniques from this research to generate more reproducible responses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Thinking Machines Lab has also said that it plans to frequently publish blog posts, code, and other information about its research in an effort to “benefit the public, but also improve our own research culture.” This post, the first in the company’s new blog series called “Connectionism,” seems to be part of that effort. OpenAI also made a commitment to open research when it was founded, but the company has become more closed off as it’s become larger. We’ll see if Murati’s research lab stays true to this claim.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The research blog offers a rare glimpse inside one of Silicon Valley’s most secretive AI startups. While it doesn’t exactly reveal where the technology is going, it indicates that Thinking Machines Lab is tackling some of the largest question on the frontier of AI research. The real test is whether Thinking Machines Lab can solve these problems, and make products around its research to justify its $12 billion valuation.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/10/thinking-machines-lab-wants-to-make-ai-models-more-consistent/</guid><pubDate>Wed, 10 Sep 2025 21:30:11 +0000</pubDate></item></channel></rss>