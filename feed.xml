<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 12 Nov 2025 12:47:28 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>[NEW] The global race for the AI app layer is still on (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/11/the-global-race-for-the-ai-app-layer-is-still-on/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The U.S. is far ahead of Europe in the race for large AI models — but the picture is different for the application layer, with emerging category leaders such as Lovable, the vibe-coding startup, and Synthesia, which makes AI-generated video for the enterprise. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s the conclusion made by global VC firm Accel in its 2025 Globalscape report, which focuses on the AI and cloud market. Worth noting: Accel is an investor in both Lovable and Synthesia.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to Accel, European and Israeli cloud and AI applications have raised 66 cents for every dollar raised by their American counterparts in 2025 so far. “When we started this report 10 years ago, Europe was one tenth of the U.S.,” Accel partner Philippe Botteri told TechCrunch.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3066813" height="382" src="https://techcrunch.com/wp-content/uploads/2025/11/Accel-2025-Globalscape-slide.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Courtesy of Accel&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Botteri says the ratio has increased because the region has developed an ecosystem of founders and investors “who really understand how to build great software companies, and that flywheel has been running for 10 years.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s also a reminder that Europeans and Israelis can do more than just staff Big Tech AI labs — an observation also shared by Jonathan Userovici, a Paris-based general partner at Headline. “Across every vertical, from legal and healthcare to manufacturing and marketing, we’re seeing founders who combine world-class technical talent with deep market expertise,” Userovici told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This aligns with the findings of the AI Europe 100 report published by Headline earlier this year, which curated AI-native application startups around Europe that the firm sees as having “the potential to become tomorrow’s winners in Europe” thanks to a combination of growth velocity, team, and tech advancement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That growth velocity is one of the key differences that Accel sees between this AI wave and previous ones. A new breed of AI-native applications has reached $100 million in annual recurring revenue in a matter of years, a feat that used to take decades.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“They’re growing faster than anything we’ve seen in the past, and they’re doing this with an incredible level of efficiency, meaning that revenue per headcount is the highest we’ve ever seen for software companies,” Botteri said. “And that’s happening on both sides of the [Atlantic] ocean.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, he noted that “existing cloud software companies are not going away.” Accel’s Public Cloud Index is up 25% year-over-year, and these players are “all adding agentic capabilities to their products.” As for private companies, some are integrating AI so deeply that they can be considered AI-native, he argued, naming Accel portfolio company Doctolib as an example.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Europe has kept high hopes for homegrown foundation model companies like Mistral AI, Accel’s outlook for European model companies is less sunny. But Botteri didn’t dismiss it entirely as a space for future leaders to emerge. It could still happen for smaller models but “it is not a very target-rich environment,” he added.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In contrast, VCs are actively competing for investment opportunities in the AI application layer, despite recurring questions about defensibility. According to Botteri, there is still defensibility in building a product-centric offering with fast adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another false dichotomy is the thought that there is no space outside of models and applications. “We see that most of the market today is chasing models, compute and applications, and we think that data is undervalued at the moment,” said Lotan Levkowitz, a managing partner at Israeli VC firm Grove Ventures. “We strongly believe that companies focused on proprietary data and data flywheels are indeed very lucrative.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The U.S. is far ahead of Europe in the race for large AI models — but the picture is different for the application layer, with emerging category leaders such as Lovable, the vibe-coding startup, and Synthesia, which makes AI-generated video for the enterprise. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s the conclusion made by global VC firm Accel in its 2025 Globalscape report, which focuses on the AI and cloud market. Worth noting: Accel is an investor in both Lovable and Synthesia.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to Accel, European and Israeli cloud and AI applications have raised 66 cents for every dollar raised by their American counterparts in 2025 so far. “When we started this report 10 years ago, Europe was one tenth of the U.S.,” Accel partner Philippe Botteri told TechCrunch.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3066813" height="382" src="https://techcrunch.com/wp-content/uploads/2025/11/Accel-2025-Globalscape-slide.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Courtesy of Accel&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Botteri says the ratio has increased because the region has developed an ecosystem of founders and investors “who really understand how to build great software companies, and that flywheel has been running for 10 years.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s also a reminder that Europeans and Israelis can do more than just staff Big Tech AI labs — an observation also shared by Jonathan Userovici, a Paris-based general partner at Headline. “Across every vertical, from legal and healthcare to manufacturing and marketing, we’re seeing founders who combine world-class technical talent with deep market expertise,” Userovici told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This aligns with the findings of the AI Europe 100 report published by Headline earlier this year, which curated AI-native application startups around Europe that the firm sees as having “the potential to become tomorrow’s winners in Europe” thanks to a combination of growth velocity, team, and tech advancement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That growth velocity is one of the key differences that Accel sees between this AI wave and previous ones. A new breed of AI-native applications has reached $100 million in annual recurring revenue in a matter of years, a feat that used to take decades.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“They’re growing faster than anything we’ve seen in the past, and they’re doing this with an incredible level of efficiency, meaning that revenue per headcount is the highest we’ve ever seen for software companies,” Botteri said. “And that’s happening on both sides of the [Atlantic] ocean.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, he noted that “existing cloud software companies are not going away.” Accel’s Public Cloud Index is up 25% year-over-year, and these players are “all adding agentic capabilities to their products.” As for private companies, some are integrating AI so deeply that they can be considered AI-native, he argued, naming Accel portfolio company Doctolib as an example.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Europe has kept high hopes for homegrown foundation model companies like Mistral AI, Accel’s outlook for European model companies is less sunny. But Botteri didn’t dismiss it entirely as a space for future leaders to emerge. It could still happen for smaller models but “it is not a very target-rich environment,” he added.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In contrast, VCs are actively competing for investment opportunities in the AI application layer, despite recurring questions about defensibility. According to Botteri, there is still defensibility in building a product-centric offering with fast adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another false dichotomy is the thought that there is no space outside of models and applications. “We see that most of the market today is chasing models, compute and applications, and we think that data is undervalued at the moment,” said Lotan Levkowitz, a managing partner at Israeli VC firm Grove Ventures. “We strongly believe that companies focused on proprietary data and data flywheels are indeed very lucrative.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/11/the-global-race-for-the-ai-app-layer-is-still-on/</guid><pubDate>Wed, 12 Nov 2025 07:01:00 +0000</pubDate></item><item><title>[NEW] Google reveals its own version of Apple’s AI cloud (AI News)</title><link>https://www.artificialintelligence-news.com/news/google-reveals-its-own-version-of-apple-ai-cloud/</link><description>&lt;p&gt;Google has rolled out Private AI Compute, a new cloud-based processing system designed to bring the privacy of on-device AI to the cloud. The platform aims to give users faster, more capable AI experiences without compromising data security. It combines Google’s most advanced Gemini models with strict privacy safeguards, reflecting the company’s ongoing effort to make AI both powerful and responsible.&lt;/p&gt;&lt;p&gt;The feature closely resembles Apple’s Private Cloud Compute, signalling how major tech firms are rethinking privacy in the age of large-scale AI. Both companies are trying to balance two competing needs — the huge computing power required to run advanced AI models and users’ expectations for data privacy.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-google-built-private-ai-compute"&gt;Why Google built Private AI Compute&lt;/h3&gt;&lt;p&gt;As AI systems get smarter, they’re also becoming more personal. What started as tools that completed simple tasks or answered direct questions are now systems that can anticipate user needs, suggest actions, and handle complex processes in real time. That kind of intelligence demands a level of reasoning and computation that often exceeds what’s possible on a single device.&lt;/p&gt;&lt;p&gt;Private AI Compute bridges that gap. It lets Gemini models in the cloud process data faster and more efficiently while ensuring that sensitive information remains private and inaccessible to anyone else — not even Google engineers. Google describes it as combining the power of cloud AI with the security users expect from local processing.&lt;/p&gt;&lt;p&gt;In practical terms, this means you could get quicker responses, smarter suggestions, and more personalised results without your personal data ever leaving your control.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-private-ai-compute-keeps-data-secure"&gt;How Private AI Compute keeps data secure&lt;/h3&gt;&lt;p&gt;Google claims the new platform is based on the same principles that underpin its broader AI and privacy strategy: giving users control, maintaining security, and earning trust. The system acts as a protected computing environment, isolating data so it can be processed safely and privately.&lt;/p&gt;&lt;p&gt;It uses a multi-layered design centred on three key components:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Unified Google tech stack:&lt;/strong&gt; Private AI Compute runs entirely on Google’s own infrastructure, powered by custom Tensor Processing Units (TPUs). It’s secured through Titanium Intelligence Enclaves (TIE), which create an additional layer of protection for data processed in the cloud.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Encrypted connections:&lt;/strong&gt; Before data is sent for processing, remote attestation and encryption verify that it’s connecting to a trusted, hardware-secured environment. Once inside this sealed cloud space, information stays private to the user.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Zero access assurance:&lt;/strong&gt; Google says the system is designed so that no one — not even the company itself — can access the data processed within Private AI Compute.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This design builds on Google’s Secure AI Framework (SAIF), AI Principles, and Privacy Principles, which outline how the company develops and deploys AI responsibly.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-users-can-expect"&gt;What users can expect&lt;/h3&gt;&lt;p&gt;Private AI Compute also improves the performance of AI features that are already running on devices. Magic Cue on the Pixel 10 can now offer more relevant and timely suggestions by leveraging cloud-level processing power. Similarly, the Recorder app can use the system to summarise transcriptions across a wider range of languages — something that would be difficult to do entirely on-device.&lt;/p&gt;&lt;p&gt;These examples hint at what’s ahead. With Private AI Compute, Google can deliver AI experiences that combine the privacy of local models with the intelligence of cloud-based ones. It’s an approach that could eventually apply to everything from personal assistants and photo organisation to productivity and accessibility tools.&lt;/p&gt;&lt;p&gt;Google calls this launch “just the beginning.” The company says Private AI Compute opens the door to a new generation of AI tools that are both more capable and more private. As AI becomes increasingly woven into everyday tasks, users are demanding greater transparency and control over how their data is used — and Google appears to be positioning this technology as part of that answer.&lt;/p&gt;&lt;p&gt;For those interested in the technical details, Google has published a technical brief explaining how Private AI Compute works and how it fits into the company’s larger vision for responsible AI development.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Solen Feyissa)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Apple plans big Siri update with help from Google AI&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-110522" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Google has rolled out Private AI Compute, a new cloud-based processing system designed to bring the privacy of on-device AI to the cloud. The platform aims to give users faster, more capable AI experiences without compromising data security. It combines Google’s most advanced Gemini models with strict privacy safeguards, reflecting the company’s ongoing effort to make AI both powerful and responsible.&lt;/p&gt;&lt;p&gt;The feature closely resembles Apple’s Private Cloud Compute, signalling how major tech firms are rethinking privacy in the age of large-scale AI. Both companies are trying to balance two competing needs — the huge computing power required to run advanced AI models and users’ expectations for data privacy.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-google-built-private-ai-compute"&gt;Why Google built Private AI Compute&lt;/h3&gt;&lt;p&gt;As AI systems get smarter, they’re also becoming more personal. What started as tools that completed simple tasks or answered direct questions are now systems that can anticipate user needs, suggest actions, and handle complex processes in real time. That kind of intelligence demands a level of reasoning and computation that often exceeds what’s possible on a single device.&lt;/p&gt;&lt;p&gt;Private AI Compute bridges that gap. It lets Gemini models in the cloud process data faster and more efficiently while ensuring that sensitive information remains private and inaccessible to anyone else — not even Google engineers. Google describes it as combining the power of cloud AI with the security users expect from local processing.&lt;/p&gt;&lt;p&gt;In practical terms, this means you could get quicker responses, smarter suggestions, and more personalised results without your personal data ever leaving your control.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-private-ai-compute-keeps-data-secure"&gt;How Private AI Compute keeps data secure&lt;/h3&gt;&lt;p&gt;Google claims the new platform is based on the same principles that underpin its broader AI and privacy strategy: giving users control, maintaining security, and earning trust. The system acts as a protected computing environment, isolating data so it can be processed safely and privately.&lt;/p&gt;&lt;p&gt;It uses a multi-layered design centred on three key components:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Unified Google tech stack:&lt;/strong&gt; Private AI Compute runs entirely on Google’s own infrastructure, powered by custom Tensor Processing Units (TPUs). It’s secured through Titanium Intelligence Enclaves (TIE), which create an additional layer of protection for data processed in the cloud.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Encrypted connections:&lt;/strong&gt; Before data is sent for processing, remote attestation and encryption verify that it’s connecting to a trusted, hardware-secured environment. Once inside this sealed cloud space, information stays private to the user.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Zero access assurance:&lt;/strong&gt; Google says the system is designed so that no one — not even the company itself — can access the data processed within Private AI Compute.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This design builds on Google’s Secure AI Framework (SAIF), AI Principles, and Privacy Principles, which outline how the company develops and deploys AI responsibly.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-users-can-expect"&gt;What users can expect&lt;/h3&gt;&lt;p&gt;Private AI Compute also improves the performance of AI features that are already running on devices. Magic Cue on the Pixel 10 can now offer more relevant and timely suggestions by leveraging cloud-level processing power. Similarly, the Recorder app can use the system to summarise transcriptions across a wider range of languages — something that would be difficult to do entirely on-device.&lt;/p&gt;&lt;p&gt;These examples hint at what’s ahead. With Private AI Compute, Google can deliver AI experiences that combine the privacy of local models with the intelligence of cloud-based ones. It’s an approach that could eventually apply to everything from personal assistants and photo organisation to productivity and accessibility tools.&lt;/p&gt;&lt;p&gt;Google calls this launch “just the beginning.” The company says Private AI Compute opens the door to a new generation of AI tools that are both more capable and more private. As AI becomes increasingly woven into everyday tasks, users are demanding greater transparency and control over how their data is used — and Google appears to be positioning this technology as part of that answer.&lt;/p&gt;&lt;p&gt;For those interested in the technical details, Google has published a technical brief explaining how Private AI Compute works and how it fits into the company’s larger vision for responsible AI development.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Solen Feyissa)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Apple plans big Siri update with help from Google AI&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-110522" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/google-reveals-its-own-version-of-apple-ai-cloud/</guid><pubDate>Wed, 12 Nov 2025 09:00:00 +0000</pubDate></item><item><title>[NEW] Figma bets on India to expand beyond design (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/12/figma-bets-on-india-to-expand-beyond-design/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Figma is expanding its presence in India by setting up a local office and hiring Indian talent as it seeks to deepen ties with one of its largest user communities and make a broader push to better win over developers alongside the designers who already rely on the platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2012 by Dylan Field and Evan Wallace, Figma broke through by offering a browser-based interface at a time when most designers were still tied to desktop software. The approach was initially met with skepticism, but the platform eventually became a go-to collaboration tool for UX and product teams. Now, the company is looking to replicate that trajectory with developers — and sees India as a key market to accelerate that evolution.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India has one of the world’s largest developer communities — an advantage already recognized by tech giants such as Microsoft, which counts nearly 22 million Indian developers on GitHub. As much as 33% of Figma’s users globally are developers, and the company has been rolling out features aimed at bridging design and engineering workflows. However, Figma still faces a perception challenge: many Indian developers continue to see Figma primarily as a design tool rather than a platform for end-to-end product creation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“India has such a large population of developers who might not currently think of Figma as their tool, and that’s the thing that we want to do,” said Abhishek Mathur, VP of Engineering at Figma, in an interview. “A lot of it is being done by the community, but we want to be part of that activity as well — and share our story of enabling developers to be more than just writing code.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday, Figma opened a new office in Bengaluru, India, as part of its continuing expansion outside the U.S. The San Francisco-headquartered company already has offices in Tokyo, Singapore, London, Paris, Berlin, Sydney, and São Paulo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, Figma had been supporting users in India remotely through its Singapore team. The company now recognizes the value of establishing a local presence, as its user base and community activity in the country have continued to expand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“India has always been a global hub of innovation, and particularly, for Figma, international markets are a big part of usage,” Mathur told TechCrunch.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As much as 85% of Figma’s overall usage is international, and India is its second-largest user base after the U.S., Mathur noted. The company said it was serving users in 85% of India’s 28 official states as of the third quarter of 2025. As of September, more than 40% of the top 100 companies listed on the Bombay Stock Exchange were Figma customers, it added.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figma's users in Indian states" class="wp-image-3066895" height="1421" src="https://techcrunch.com/wp-content/uploads/2025/11/figma-india-users-map_51bb09.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Figma&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Figma counts 13 million weekly active users worldwide. The company did not share specific user numbers for India, though Mathur described the country as “a very large portion” of its base. Its India community, called Friends of Figma, alone includes more than 25,000 members.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In May, Figma introduced a new range of AI-powered features designed to extend its software’s value beyond design teams, positioning it in competition not just with Adobe and Canva, but also with AI coding platforms such as Replit and Lovable. One of those features, Figma Make, allows users to generate working web applications from natural-language prompts and collaborate on both design and code within the same workspace.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mathur said India has been the largest market for Figma Make, with users in the country generating over 800,000 prototypes so far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Figma also sees increased adoption among developers in India, particularly for its dev mode, which debuted in 2023 to help developers quickly translate designs into code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The first spectrum of imagination to production is what we are seeing in terms of differences between India and the rest of the globe,” Mathur said. “The usage patterns are similar, but the scale of operations in some of the things is very challenging.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Figma’s Bengaluru office will initially focus on strengthening the company’s sales and marketing operations in the country. Its users in India include consumer-facing startups such as CRED, Groww, Fynd, Swiggy, and Zomato, as well as IT services giants including Infosys and TCS and consumer companies such as Airtel, CARS24, and Myntra.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2024, Figma generated about half of its revenue from markets outside the U.S., and Mathur described India as an “important market” for the company, though he did not disclose its specific contribution to global revenue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India’s user base is already influencing Figma’s product development. For instance, feedback from its community in India led the company to introduce improved code-export options that produce higher-quality code — a direct response to requests from Indian users seeking better output.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want to continue to do events, understand and work with our customers — small to large — and as time progresses, we might add other possibilities as well,” Mathur said.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Figma is expanding its presence in India by setting up a local office and hiring Indian talent as it seeks to deepen ties with one of its largest user communities and make a broader push to better win over developers alongside the designers who already rely on the platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2012 by Dylan Field and Evan Wallace, Figma broke through by offering a browser-based interface at a time when most designers were still tied to desktop software. The approach was initially met with skepticism, but the platform eventually became a go-to collaboration tool for UX and product teams. Now, the company is looking to replicate that trajectory with developers — and sees India as a key market to accelerate that evolution.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India has one of the world’s largest developer communities — an advantage already recognized by tech giants such as Microsoft, which counts nearly 22 million Indian developers on GitHub. As much as 33% of Figma’s users globally are developers, and the company has been rolling out features aimed at bridging design and engineering workflows. However, Figma still faces a perception challenge: many Indian developers continue to see Figma primarily as a design tool rather than a platform for end-to-end product creation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“India has such a large population of developers who might not currently think of Figma as their tool, and that’s the thing that we want to do,” said Abhishek Mathur, VP of Engineering at Figma, in an interview. “A lot of it is being done by the community, but we want to be part of that activity as well — and share our story of enabling developers to be more than just writing code.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday, Figma opened a new office in Bengaluru, India, as part of its continuing expansion outside the U.S. The San Francisco-headquartered company already has offices in Tokyo, Singapore, London, Paris, Berlin, Sydney, and São Paulo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, Figma had been supporting users in India remotely through its Singapore team. The company now recognizes the value of establishing a local presence, as its user base and community activity in the country have continued to expand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“India has always been a global hub of innovation, and particularly, for Figma, international markets are a big part of usage,” Mathur told TechCrunch.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As much as 85% of Figma’s overall usage is international, and India is its second-largest user base after the U.S., Mathur noted. The company said it was serving users in 85% of India’s 28 official states as of the third quarter of 2025. As of September, more than 40% of the top 100 companies listed on the Bombay Stock Exchange were Figma customers, it added.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figma's users in Indian states" class="wp-image-3066895" height="1421" src="https://techcrunch.com/wp-content/uploads/2025/11/figma-india-users-map_51bb09.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Figma&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Figma counts 13 million weekly active users worldwide. The company did not share specific user numbers for India, though Mathur described the country as “a very large portion” of its base. Its India community, called Friends of Figma, alone includes more than 25,000 members.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In May, Figma introduced a new range of AI-powered features designed to extend its software’s value beyond design teams, positioning it in competition not just with Adobe and Canva, but also with AI coding platforms such as Replit and Lovable. One of those features, Figma Make, allows users to generate working web applications from natural-language prompts and collaborate on both design and code within the same workspace.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mathur said India has been the largest market for Figma Make, with users in the country generating over 800,000 prototypes so far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Figma also sees increased adoption among developers in India, particularly for its dev mode, which debuted in 2023 to help developers quickly translate designs into code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The first spectrum of imagination to production is what we are seeing in terms of differences between India and the rest of the globe,” Mathur said. “The usage patterns are similar, but the scale of operations in some of the things is very challenging.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Figma’s Bengaluru office will initially focus on strengthening the company’s sales and marketing operations in the country. Its users in India include consumer-facing startups such as CRED, Groww, Fynd, Swiggy, and Zomato, as well as IT services giants including Infosys and TCS and consumer companies such as Airtel, CARS24, and Myntra.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2024, Figma generated about half of its revenue from markets outside the U.S., and Mathur described India as an “important market” for the company, though he did not disclose its specific contribution to global revenue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India’s user base is already influencing Figma’s product development. For instance, feedback from its community in India led the company to introduce improved code-export options that produce higher-quality code — a direct response to requests from Indian users seeking better output.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want to continue to do events, understand and work with our customers — small to large — and as time progresses, we might add other possibilities as well,” Mathur said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/12/figma-bets-on-india-to-expand-beyond-design/</guid><pubDate>Wed, 12 Nov 2025 10:00:00 +0000</pubDate></item><item><title>[NEW] Improving VMware migration workflows with agentic AI (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/12/1124919/improving-vmware-migration-workflows-with-agentic-ai/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;EPAM&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;For years, many chief information officers (CIOs) looked at VMware-to-cloud migrations with a wary pragmatism. Manually mapping dependencies and rewriting legacy apps mid-flight was not an enticing, low-lift proposition for enterprise IT teams.&lt;/p&gt;  &lt;p&gt;But the calculus for such decisions has changed dramatically in a short period of time. Following recent VMware licensing changes, organizations are seeing greater uncertainty around the platform’s future. At the same time, cloud-native innovation is accelerating. According to the &lt;strong&gt;CNCF’s 2024 Annual Survey,&lt;/strong&gt; 89% of organizations have already adopted at least some cloud-native techniques, and the share of companies reporting nearly all development and deployment as cloud-native grew sharply from 2023 to 2024 (20% to 24%). And market research firm &lt;strong&gt;IDC reports&lt;/strong&gt; that cloud providers have become top strategic partners for generative AI initiatives.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1124925" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/EPAM-Landing-Page-Card-1.png" /&gt;&lt;/figure&gt;    &lt;p&gt;This is all happening amid escalating pressure to innovate faster and more cost-effectively to meet the demands of an AI-first future. As enterprises prepare for that inevitability, they are facing compute demands that are difficult, if not prohibitively expensive, to maintain exclusively on-premises.&lt;/p&gt;  &lt;p&gt;Download the full article.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;EPAM&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;For years, many chief information officers (CIOs) looked at VMware-to-cloud migrations with a wary pragmatism. Manually mapping dependencies and rewriting legacy apps mid-flight was not an enticing, low-lift proposition for enterprise IT teams.&lt;/p&gt;  &lt;p&gt;But the calculus for such decisions has changed dramatically in a short period of time. Following recent VMware licensing changes, organizations are seeing greater uncertainty around the platform’s future. At the same time, cloud-native innovation is accelerating. According to the &lt;strong&gt;CNCF’s 2024 Annual Survey,&lt;/strong&gt; 89% of organizations have already adopted at least some cloud-native techniques, and the share of companies reporting nearly all development and deployment as cloud-native grew sharply from 2023 to 2024 (20% to 24%). And market research firm &lt;strong&gt;IDC reports&lt;/strong&gt; that cloud providers have become top strategic partners for generative AI initiatives.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1124925" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/EPAM-Landing-Page-Card-1.png" /&gt;&lt;/figure&gt;    &lt;p&gt;This is all happening amid escalating pressure to innovate faster and more cost-effectively to meet the demands of an AI-first future. As enterprises prepare for that inevitability, they are facing compute demands that are difficult, if not prohibitively expensive, to maintain exclusively on-premises.&lt;/p&gt;  &lt;p&gt;Download the full article.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/12/1124919/improving-vmware-migration-workflows-with-agentic-ai/</guid><pubDate>Wed, 12 Nov 2025 10:11:15 +0000</pubDate></item><item><title>[NEW] MMCTAgent: Enabling multimodal reasoning over large video and image collections (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/mmctagent-enabling-multimodal-reasoning-over-large-video-and-image-collections/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white icons on a blue-to-purple gradient background: the first icon shows an image/photo; the second icon depicts a computer monitor with vertical bars; the third icon displays three connected circles with user silhouettes." class="wp-image-1153930" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MMCTAgent-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Modern multimodal AI models can recognize objects, describe scenes, and answer questions about images and short video clips, but they struggle with long-form and large-scale visual data, where real-world reasoning requires moving beyond object recognition and short-clip analysis.&lt;/p&gt;



&lt;p&gt;Real-world reasoning increasingly involves analyzing long-form video content, where context spans minutes or hours, far beyond the context limits of most models. It also entails querying across massive multimodal libraries of videos, images, and transcripts, where finding and integrating relevant evidence requires more than retrieval, it requires strategic reasoning. Existing models typically perform single-pass inference, producing one-shot answers. This limits their ability to handle tasks that require temporal reasoning, cross-modal grounding, and iterative refinement.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="mmctagent"&gt;MMCTAgent&lt;/h2&gt;



&lt;p&gt;To meet these challenges, we developed the Multi-modal Critical Thinking Agent, or MMCTAgent, for structured reasoning over long-form video and image data, available on GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and featured on Azure AI Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;Built on AutoGen, Microsoft’s open-source multi-agent system, MMCTAgent provides multimodal question-answering with a Planner–Critic architecture. This design enables planning, reflection, and tool-based reasoning, bridging perception and deliberation in multimodal tasks. It links language, vision, and temporal understanding, transforming static multimodal tasks into dynamic reasoning workflows.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Unlike conventional models that produce one-shot answers, MMCTAgent has modality-specific agents, including ImageAgent and VideoAgent, which include tools like get_relevant_query_frames() or object_detection-tool(). These agents perform&amp;nbsp;deliberate, iterative reasoning—selecting the right tools for each modality, evaluating intermediate results, and refining conclusions through a Critic loop. This enables MMCTAgent to analyze complex queries across long videos and large image libraries with explainability, extensibility, and scalability.&lt;/p&gt;







	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Event Series&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Forum&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-forum"&gt;Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="how-mmctagent-works"&gt;How MMCTAgent works&lt;/h2&gt;



&lt;p&gt;MMCTAgent integrates two coordinated agents, Planner and Critic,orchestrated through AutoGen. The Planner agent decomposes a user query, identifies the appropriate reasoning tools, performs multimodal operations, and drafts a preliminary answer. The Critic agent reviews the Planner’s reasoning chain, validates evidence alignment, and refines or revises the response for factual accuracy and consistency.&lt;/p&gt;



&lt;p&gt;This iterative reasoning loop enables MMCTAgent to improve its answers through structured self-evaluation—bringing reflection into AI reasoning. A key strength of MMCTAgent lies in its modular extensibility. Developers can easily integrate new, domain-specific tools—such as medical image analyzers, industrial inspection models, or specialized retrieval modules—by adding them to ImageQnATools or VideoQnATools. This design makes MMCTAgent adaptable across domains.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="videoagent-from-ingestion-to-long-form-multimodal-reasoning"&gt;VideoAgent: From ingestion to long-form multimodal reasoning&lt;/h3&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="MMCTAgent’s Planner–Critic architecture enables multimodal reasoning over long-form video through structured ingestion, retrieval, and iterative feedback.&amp;nbsp;" class="wp-image-1155366" height="8455" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/MMCT_UPDATED_FINAL_FINAL.png" width="14353" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. MMCTAgent’s Planner–Critic architecture enables multimodal reasoning over long-form video through structured ingestion, retrieval, and iterative feedback.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The VideoAgent extends this architecture to long-form video reasoning. It operates in two connected phases: library creation (ingestion) and query-time reasoning.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h4 class="wp-block-heading" id="phase-1-video-ingestion-and-library-creation"&gt;Phase 1 – Video ingestion and library creation&lt;/h4&gt;



&lt;p&gt;Before reasoning, long-form videos undergo an ingestion pipeline that aligns multimodal information for retrieval and understanding:&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Transcription &lt;/strong&gt;and&lt;strong&gt; translation&lt;/strong&gt;: Converts audio to text and, if multilingual, translates transcripts into a consistent language&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Key-frame identification&lt;/strong&gt;: Extracts representative frames marking major visual or scene changes&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Semantic chunking &lt;/strong&gt;and&lt;strong&gt; chapter generation&lt;/strong&gt;: Combines transcript segments and visual summaries into coherent, semantically segmented chapters with associated key frames. Inspired by Microsoft’s Deep Video Discovery agentic search tool, this step also extracts detailed descriptions of objects, on-screen text, and characters present within each video segment, integrating these insights directly into the corresponding chapters.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Multimodal embedding creation&lt;/strong&gt;: Generates image embeddings for key frames, linking them to their corresponding transcript and chapter data&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;All structured metadata, including transcripts, visual summaries, chapters, and embeddings, is indexed in the Multimodal Knowledgebase using Azure AI Search&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which forms the foundation for scalable semantic retrieval and downstream reasoning.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h4 class="wp-block-heading" id="phase-2-video-question-answering-and-reasoning"&gt;Phase 2 – Video question answering and reasoning&lt;/h4&gt;



&lt;p&gt;When a user submits a query, the VideoAgent retrieves, analyzes, and reasons across the indexed video content using specialized planner and critic tools.&lt;/p&gt;



&lt;h5 class="wp-block-heading" id="planner-tools-1"&gt;Planner tools&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;get_video_analysis&lt;/strong&gt;: Finds the most relevant video, provides a summary, and lists detected objects&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;get_context&lt;/strong&gt;: Retrieves contextual information and relevant chapters from the Azure AI Search index&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;get_relevant_frames&lt;/strong&gt;: Selects key frames most relevant to the user query&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;query_frame&lt;/strong&gt;: Performs detailed visual and textual reasoning over selected frames&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;get_context&lt;/strong&gt; and &lt;strong&gt;get_relevant_frames&lt;/strong&gt; work in tandem to ensure that reasoning begins from the most semantically relevant evidence&lt;/li&gt;
&lt;/ul&gt;



&lt;h5 class="wp-block-heading" id="critic-tools-1"&gt;Critic tool&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;critic_tool&lt;/strong&gt;: Evaluates the reasoning output for temporal alignment, factual accuracy, and coherence between visual and textual modalities&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This two-phase design, which involves&amp;nbsp;structured ingestion followed by agentic reasoning, enables MMCTAgent to deliver accurate, interpretable insights for long information-dense videos.&amp;nbsp;&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="imageagent-structured-reasoning-for-static-visuals"&gt;ImageAgent: Structured reasoning for static visuals&lt;/h3&gt;



&lt;p&gt;While the VideoAgent handles temporal reasoning across long-form videos, the ImageAgent applies the same Planner–Critic paradigm to static visual analysis. It performs modular, tool-based reasoning over images, combining perception tools for recognition, detection, and optical character recognition with language-based reasoning for interpretation and explanation.&lt;/p&gt;



&lt;h5 class="wp-block-heading" id="planner-tools"&gt;Planner tools&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;vit_tool&lt;/strong&gt;: Leverages Vision Transformer (ViT) or Vision Languague Model (VLM) for high-level visual understanding and description&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;recog_tool&lt;/strong&gt;: Performs scene, face, and object recognition&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;object_detection_tool&lt;/strong&gt;: Localizes and labels entities within an image&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;ocr_tool&lt;/strong&gt;: Extracts embedded text from visual elements&lt;/li&gt;
&lt;/ul&gt;



&lt;h5 class="wp-block-heading" id="critic-tool"&gt;Critic tool&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;critic_tool&lt;/strong&gt;: Validates the Planner’s conclusions for factual alignment and consistency, refining the final response&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This lightweight ImageAgent provides fine-grained, explainable reasoning over image collections—supporting visual question answering, content inspection, and multimodal retrieval—while maintaining architectural symmetry with the VideoAgent.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="evaluation-results"&gt;Evaluation Results&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;To assess the effectiveness of MMCTAgent, we evaluated both the ImageAgent and VideoAgent with multiple base LLM models and a range of benchmark datasets and real-world scenarios. Some key results are presented here.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Image Datasets&lt;/th&gt;&lt;th&gt;GPT-4V&lt;/th&gt;&lt;th&gt;MMCT with GPT-4V&lt;/th&gt;&lt;th&gt;GPT4o&lt;/th&gt;&lt;th&gt;MMCT with GPT-4o&lt;/th&gt;&lt;th&gt;GPT-5&lt;/th&gt;&lt;th&gt;MMCT with GPT-5&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;MM-Vet [1]&lt;/td&gt;&lt;td&gt;60.20&lt;/td&gt;&lt;td&gt;74.24&lt;/td&gt;&lt;td&gt;77.98&lt;/td&gt;&lt;td&gt;79.36&lt;/td&gt;&lt;td&gt;80.51&lt;/td&gt;&lt;td&gt;81.65&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MMMU [2]&lt;/td&gt;&lt;td&gt;56.80&lt;/td&gt;&lt;td&gt;63.57&lt;/td&gt;&lt;td&gt;69.10&lt;/td&gt;&lt;td&gt;73.00&lt;/td&gt;&lt;td&gt;84.20&lt;/td&gt;&lt;td&gt;85.44&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Video Datasets&lt;/th&gt;&lt;th&gt;GPT4o&lt;/th&gt;&lt;th&gt;MMCT with GPT-4o&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;VideoMME [3]&lt;/td&gt;&lt;td&gt;72.10&lt;/td&gt;&lt;td&gt;76.70&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;p&gt;MMCTAgent enhances base model performance by augmenting their capabilities with appropriate tools such as object detection and optical character recognition (OCR) for weaker models, or domain-specific tools for stronger models, thereby leading to substantial improvements. For example, integrating these tools raised GPT-4V’s accuracy from 60.20% to 74.24% on MM-Vet dataset. Additionally, the configurable Critic agent provides additional validation, which is especially valuable in critical domains. The additional evaluation results are available here&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="takeaways-and-next-steps"&gt;Takeaways and next steps&lt;/h2&gt;



&lt;p&gt;MMCTAgent demonstrates a scalable agentic approach to multimodal reasoning with a Planner–Critic architecture. Its unified multimodal design supports both image and video pipelines, while the extensible toolchain enables rapid integration of domain-specific tools and capabilities. It provides Azure-native deployment and supports configurability within the broader open-source ecosystem.&lt;/p&gt;



&lt;p&gt;Looking ahead, we aim to improve efficiency and adaptability in retrieval and reasoning workflows, and to extend MMCTAgent’s applications beyond current agricultural evaluations, exploring new real-world domains through initiatives like Project Gecko to advance the creation of accessible, innovative multimodal applications for people around the globe.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;



&lt;p&gt;We would like to thank our team members for their valuable contributions to this work: Aman Patkar, Ogbemi Ekwejunor-Etchie, Somnath Kumar, Soumya De, and Yash Gadhia. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[1] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. “MM-VET: Evaluating large multimodal models for integrated capabilities”, 2023.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[2] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. “MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI”, 2023.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[3] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. “Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis”, 2024.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white icons on a blue-to-purple gradient background: the first icon shows an image/photo; the second icon depicts a computer monitor with vertical bars; the third icon displays three connected circles with user silhouettes." class="wp-image-1153930" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MMCTAgent-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Modern multimodal AI models can recognize objects, describe scenes, and answer questions about images and short video clips, but they struggle with long-form and large-scale visual data, where real-world reasoning requires moving beyond object recognition and short-clip analysis.&lt;/p&gt;



&lt;p&gt;Real-world reasoning increasingly involves analyzing long-form video content, where context spans minutes or hours, far beyond the context limits of most models. It also entails querying across massive multimodal libraries of videos, images, and transcripts, where finding and integrating relevant evidence requires more than retrieval, it requires strategic reasoning. Existing models typically perform single-pass inference, producing one-shot answers. This limits their ability to handle tasks that require temporal reasoning, cross-modal grounding, and iterative refinement.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="mmctagent"&gt;MMCTAgent&lt;/h2&gt;



&lt;p&gt;To meet these challenges, we developed the Multi-modal Critical Thinking Agent, or MMCTAgent, for structured reasoning over long-form video and image data, available on GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and featured on Azure AI Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;Built on AutoGen, Microsoft’s open-source multi-agent system, MMCTAgent provides multimodal question-answering with a Planner–Critic architecture. This design enables planning, reflection, and tool-based reasoning, bridging perception and deliberation in multimodal tasks. It links language, vision, and temporal understanding, transforming static multimodal tasks into dynamic reasoning workflows.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Unlike conventional models that produce one-shot answers, MMCTAgent has modality-specific agents, including ImageAgent and VideoAgent, which include tools like get_relevant_query_frames() or object_detection-tool(). These agents perform&amp;nbsp;deliberate, iterative reasoning—selecting the right tools for each modality, evaluating intermediate results, and refining conclusions through a Critic loop. This enables MMCTAgent to analyze complex queries across long videos and large image libraries with explainability, extensibility, and scalability.&lt;/p&gt;







	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Event Series&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Forum&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-forum"&gt;Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="how-mmctagent-works"&gt;How MMCTAgent works&lt;/h2&gt;



&lt;p&gt;MMCTAgent integrates two coordinated agents, Planner and Critic,orchestrated through AutoGen. The Planner agent decomposes a user query, identifies the appropriate reasoning tools, performs multimodal operations, and drafts a preliminary answer. The Critic agent reviews the Planner’s reasoning chain, validates evidence alignment, and refines or revises the response for factual accuracy and consistency.&lt;/p&gt;



&lt;p&gt;This iterative reasoning loop enables MMCTAgent to improve its answers through structured self-evaluation—bringing reflection into AI reasoning. A key strength of MMCTAgent lies in its modular extensibility. Developers can easily integrate new, domain-specific tools—such as medical image analyzers, industrial inspection models, or specialized retrieval modules—by adding them to ImageQnATools or VideoQnATools. This design makes MMCTAgent adaptable across domains.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="videoagent-from-ingestion-to-long-form-multimodal-reasoning"&gt;VideoAgent: From ingestion to long-form multimodal reasoning&lt;/h3&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="MMCTAgent’s Planner–Critic architecture enables multimodal reasoning over long-form video through structured ingestion, retrieval, and iterative feedback.&amp;nbsp;" class="wp-image-1155366" height="8455" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/MMCT_UPDATED_FINAL_FINAL.png" width="14353" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. MMCTAgent’s Planner–Critic architecture enables multimodal reasoning over long-form video through structured ingestion, retrieval, and iterative feedback.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The VideoAgent extends this architecture to long-form video reasoning. It operates in two connected phases: library creation (ingestion) and query-time reasoning.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h4 class="wp-block-heading" id="phase-1-video-ingestion-and-library-creation"&gt;Phase 1 – Video ingestion and library creation&lt;/h4&gt;



&lt;p&gt;Before reasoning, long-form videos undergo an ingestion pipeline that aligns multimodal information for retrieval and understanding:&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Transcription &lt;/strong&gt;and&lt;strong&gt; translation&lt;/strong&gt;: Converts audio to text and, if multilingual, translates transcripts into a consistent language&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Key-frame identification&lt;/strong&gt;: Extracts representative frames marking major visual or scene changes&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Semantic chunking &lt;/strong&gt;and&lt;strong&gt; chapter generation&lt;/strong&gt;: Combines transcript segments and visual summaries into coherent, semantically segmented chapters with associated key frames. Inspired by Microsoft’s Deep Video Discovery agentic search tool, this step also extracts detailed descriptions of objects, on-screen text, and characters present within each video segment, integrating these insights directly into the corresponding chapters.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Multimodal embedding creation&lt;/strong&gt;: Generates image embeddings for key frames, linking them to their corresponding transcript and chapter data&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;All structured metadata, including transcripts, visual summaries, chapters, and embeddings, is indexed in the Multimodal Knowledgebase using Azure AI Search&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which forms the foundation for scalable semantic retrieval and downstream reasoning.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h4 class="wp-block-heading" id="phase-2-video-question-answering-and-reasoning"&gt;Phase 2 – Video question answering and reasoning&lt;/h4&gt;



&lt;p&gt;When a user submits a query, the VideoAgent retrieves, analyzes, and reasons across the indexed video content using specialized planner and critic tools.&lt;/p&gt;



&lt;h5 class="wp-block-heading" id="planner-tools-1"&gt;Planner tools&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;get_video_analysis&lt;/strong&gt;: Finds the most relevant video, provides a summary, and lists detected objects&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;get_context&lt;/strong&gt;: Retrieves contextual information and relevant chapters from the Azure AI Search index&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;get_relevant_frames&lt;/strong&gt;: Selects key frames most relevant to the user query&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;query_frame&lt;/strong&gt;: Performs detailed visual and textual reasoning over selected frames&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;get_context&lt;/strong&gt; and &lt;strong&gt;get_relevant_frames&lt;/strong&gt; work in tandem to ensure that reasoning begins from the most semantically relevant evidence&lt;/li&gt;
&lt;/ul&gt;



&lt;h5 class="wp-block-heading" id="critic-tools-1"&gt;Critic tool&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;critic_tool&lt;/strong&gt;: Evaluates the reasoning output for temporal alignment, factual accuracy, and coherence between visual and textual modalities&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This two-phase design, which involves&amp;nbsp;structured ingestion followed by agentic reasoning, enables MMCTAgent to deliver accurate, interpretable insights for long information-dense videos.&amp;nbsp;&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="imageagent-structured-reasoning-for-static-visuals"&gt;ImageAgent: Structured reasoning for static visuals&lt;/h3&gt;



&lt;p&gt;While the VideoAgent handles temporal reasoning across long-form videos, the ImageAgent applies the same Planner–Critic paradigm to static visual analysis. It performs modular, tool-based reasoning over images, combining perception tools for recognition, detection, and optical character recognition with language-based reasoning for interpretation and explanation.&lt;/p&gt;



&lt;h5 class="wp-block-heading" id="planner-tools"&gt;Planner tools&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;vit_tool&lt;/strong&gt;: Leverages Vision Transformer (ViT) or Vision Languague Model (VLM) for high-level visual understanding and description&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;recog_tool&lt;/strong&gt;: Performs scene, face, and object recognition&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;object_detection_tool&lt;/strong&gt;: Localizes and labels entities within an image&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;ocr_tool&lt;/strong&gt;: Extracts embedded text from visual elements&lt;/li&gt;
&lt;/ul&gt;



&lt;h5 class="wp-block-heading" id="critic-tool"&gt;Critic tool&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;critic_tool&lt;/strong&gt;: Validates the Planner’s conclusions for factual alignment and consistency, refining the final response&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This lightweight ImageAgent provides fine-grained, explainable reasoning over image collections—supporting visual question answering, content inspection, and multimodal retrieval—while maintaining architectural symmetry with the VideoAgent.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="evaluation-results"&gt;Evaluation Results&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;To assess the effectiveness of MMCTAgent, we evaluated both the ImageAgent and VideoAgent with multiple base LLM models and a range of benchmark datasets and real-world scenarios. Some key results are presented here.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Image Datasets&lt;/th&gt;&lt;th&gt;GPT-4V&lt;/th&gt;&lt;th&gt;MMCT with GPT-4V&lt;/th&gt;&lt;th&gt;GPT4o&lt;/th&gt;&lt;th&gt;MMCT with GPT-4o&lt;/th&gt;&lt;th&gt;GPT-5&lt;/th&gt;&lt;th&gt;MMCT with GPT-5&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;MM-Vet [1]&lt;/td&gt;&lt;td&gt;60.20&lt;/td&gt;&lt;td&gt;74.24&lt;/td&gt;&lt;td&gt;77.98&lt;/td&gt;&lt;td&gt;79.36&lt;/td&gt;&lt;td&gt;80.51&lt;/td&gt;&lt;td&gt;81.65&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MMMU [2]&lt;/td&gt;&lt;td&gt;56.80&lt;/td&gt;&lt;td&gt;63.57&lt;/td&gt;&lt;td&gt;69.10&lt;/td&gt;&lt;td&gt;73.00&lt;/td&gt;&lt;td&gt;84.20&lt;/td&gt;&lt;td&gt;85.44&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Video Datasets&lt;/th&gt;&lt;th&gt;GPT4o&lt;/th&gt;&lt;th&gt;MMCT with GPT-4o&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;VideoMME [3]&lt;/td&gt;&lt;td&gt;72.10&lt;/td&gt;&lt;td&gt;76.70&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;p&gt;MMCTAgent enhances base model performance by augmenting their capabilities with appropriate tools such as object detection and optical character recognition (OCR) for weaker models, or domain-specific tools for stronger models, thereby leading to substantial improvements. For example, integrating these tools raised GPT-4V’s accuracy from 60.20% to 74.24% on MM-Vet dataset. Additionally, the configurable Critic agent provides additional validation, which is especially valuable in critical domains. The additional evaluation results are available here&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="takeaways-and-next-steps"&gt;Takeaways and next steps&lt;/h2&gt;



&lt;p&gt;MMCTAgent demonstrates a scalable agentic approach to multimodal reasoning with a Planner–Critic architecture. Its unified multimodal design supports both image and video pipelines, while the extensible toolchain enables rapid integration of domain-specific tools and capabilities. It provides Azure-native deployment and supports configurability within the broader open-source ecosystem.&lt;/p&gt;



&lt;p&gt;Looking ahead, we aim to improve efficiency and adaptability in retrieval and reasoning workflows, and to extend MMCTAgent’s applications beyond current agricultural evaluations, exploring new real-world domains through initiatives like Project Gecko to advance the creation of accessible, innovative multimodal applications for people around the globe.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;



&lt;p&gt;We would like to thank our team members for their valuable contributions to this work: Aman Patkar, Ogbemi Ekwejunor-Etchie, Somnath Kumar, Soumya De, and Yash Gadhia. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[1] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. “MM-VET: Evaluating large multimodal models for integrated capabilities”, 2023.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[2] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. “MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI”, 2023.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[3] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. “Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis”, 2024.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/mmctagent-enabling-multimodal-reasoning-over-large-video-and-image-collections/</guid><pubDate>Wed, 12 Nov 2025 12:00:20 +0000</pubDate></item></channel></rss>