<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 03 Nov 2025 12:46:51 +0000</lastBuildDate><item><title>Kevin Rose’s simple test for AI hardware — would you want to punch someone in the face who’s wearing it? (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/02/kevin-roses-simple-test-for-ai-hardware-would-you-want-to-punch-someone-in-the-face-whos-wearing-it/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/54889293139_d6dbc89df2_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Kevin Rose has a visceral rule for evaluating AI hardware investments: “If you feel like you should punch someone in the face for wearing it, you probably shouldn’t invest in it.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a typically candid assessment from the veteran investor, and one born from watching the current wave of AI hardware startups repeat mistakes he’s seen before. Rose, a general partner at True Ventures and early investor in Peloton, Ring, and Fitbit, has largely avoided the AI hardware gold rush that’s consumed Silicon Valley. While other VCs rush to fund the next smart glasses or AI pendant, Rose is taking a decidedly different approach.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“A lot of it is just like, ‘Let’s listen to the entire conversation,’” Rose says of the current crop of AI wearables. “And to me, that breaks a lot of these social constructs that we have with humans around privacy.”&lt;/p&gt;&lt;p&gt;Rose speaks from experience. He was on the board of Oura, which now commands 80% of the smart ring market, and he’s witnessed firsthand what separates successful wearables from failed ones. The difference isn’t just technical capability; it’s emotional resonance and social acceptability.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As an investor, you kind of have to not only say, okay, cool tech, sure, but emotionally, how does it make me feel? And how does it make others feel around me?” he explained on stage at TechCrunch Disrupt last week. “And for me, a lot of that is lost in all the AI stuff, where it’s just always on, always listening, trying to be the smartest person in the room. And it’s just not healthy.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He admits to trying various AI wearables himself, including the failed Humane AI pendant that briefly caught the world’s attention a year ago. But the breaking point came during an argument with his wife. “I was like, I know I didn’t say that. And I was trying to use it to actually win an argument,” he recalled. “That was the last time I wore that thing. You do not want to win a battle by going back and looking at the logs of your AI pin. That doesn’t fly.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tourist use case — asking your glasses what monument you’re looking at — isn’t good enough, Rose said. “We tend to bolt AI onto everything and it’s ruining the world,” he said, pointing to features like photo apps that let you erase people from the background. “I had a friend who erased a gate from behind him to make the picture look better. I’m like, ‘That’s your yard! Your kids are gonna look at that and be like, ‘Didn’t we have a gate there?’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rose worries we’re in an “early days of social media” moment with AI — making decisions that seem harmless now but will haunt us later. “We’re gonna look back and be like, ‘Wow, that was weird. We just slapped AI on everything, and thought it was a good idea,’ similar to what happened in the early days of social. We look back a decade or two later, and you’re like, ‘I wish I would have done that differently.’”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;He’s experiencing these tensions firsthand with his young children. Using OpenAI’s video generation tool Sora to create videos of tiny Labradoodles, his kids asked where they could get those puppies. “I’m like, that’s not really Dad there. How do you have that conversation? Very awkward,” he says. His solution, he said, is treating AI like movie magic, explaining that just as actors aren’t really flying on screen, Dad’s puppies aren’t real either.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Rose isn’t a Luddite. He’s deeply optimistic about how AI is transforming entrepreneurship itself, and by extension, the venture capital industry that funds it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The barriers to entry for entrepreneurs are just shrinking with every day that goes by,” Rose observed. He recounted a colleague who had never used AI coding tools before building and deploying a complete app during a drive from LA to San Francisco. Six months ago, the same task would have taken ten times as long and required navigating dozens of errors.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“In three months, when [Google’s] Gemini 3 hits the market, there’s going to be zero errors or next to it,” Rose predicted. “High school coding classes are no longer coding classes — they’re vibe coding classes, and they will build the next billion-dollar business launched out of some random high school. It will happen. It’s just a matter of time.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These developments completely change the VC equation, Rose said. Entrepreneurs can now delay fundraising until they absolutely need it, or potentially skip raising outside funding altogether. “It’s really going to change the world of VC, and I think for the better,” Rose said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Many venture firms have responded by hiring armies of engineers—Sequoia Capital, for instance, now employs as many developers as investors. But Rose doesn’t think that’s the answer. Instead, he believes the value proposition for VCs shifts to something more fundamental. “At the end of the day, the entrepreneur is going to have issues that are not technical,” he argued. “They’re very emotional problems. And so I think the VCs with the highest EQ that can show up best for the founders as their long term partner — that have been with firms and aren’t hopping around, that aren’t just fly-by-night VCs but have been around and seen these problems at scale — they’re going to be sought after.”&lt;/p&gt;&lt;p&gt;So what does Rose look for when making investments? He circles back to something Larry Page told him years ago when Rose was at Google Ventures, his first institutional investing job after co-founding the social news platform Digg and before joining True Ventures in 2017. “A healthy disregard for the impossible is what’s important to look for.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want founders that aren’t just sanding down the rough edges, but they’re really swinging for the fences with big, bold ideas that everyone else says, ‘That is a horrible idea. Why are you doing this?’” Rose said. “That’s what I’m drawn to. Because even if it doesn’t work, we love your mind. We love where you are, and we gladly back you the second time.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/54889293139_d6dbc89df2_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Kevin Rose has a visceral rule for evaluating AI hardware investments: “If you feel like you should punch someone in the face for wearing it, you probably shouldn’t invest in it.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a typically candid assessment from the veteran investor, and one born from watching the current wave of AI hardware startups repeat mistakes he’s seen before. Rose, a general partner at True Ventures and early investor in Peloton, Ring, and Fitbit, has largely avoided the AI hardware gold rush that’s consumed Silicon Valley. While other VCs rush to fund the next smart glasses or AI pendant, Rose is taking a decidedly different approach.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“A lot of it is just like, ‘Let’s listen to the entire conversation,’” Rose says of the current crop of AI wearables. “And to me, that breaks a lot of these social constructs that we have with humans around privacy.”&lt;/p&gt;&lt;p&gt;Rose speaks from experience. He was on the board of Oura, which now commands 80% of the smart ring market, and he’s witnessed firsthand what separates successful wearables from failed ones. The difference isn’t just technical capability; it’s emotional resonance and social acceptability.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As an investor, you kind of have to not only say, okay, cool tech, sure, but emotionally, how does it make me feel? And how does it make others feel around me?” he explained on stage at TechCrunch Disrupt last week. “And for me, a lot of that is lost in all the AI stuff, where it’s just always on, always listening, trying to be the smartest person in the room. And it’s just not healthy.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He admits to trying various AI wearables himself, including the failed Humane AI pendant that briefly caught the world’s attention a year ago. But the breaking point came during an argument with his wife. “I was like, I know I didn’t say that. And I was trying to use it to actually win an argument,” he recalled. “That was the last time I wore that thing. You do not want to win a battle by going back and looking at the logs of your AI pin. That doesn’t fly.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tourist use case — asking your glasses what monument you’re looking at — isn’t good enough, Rose said. “We tend to bolt AI onto everything and it’s ruining the world,” he said, pointing to features like photo apps that let you erase people from the background. “I had a friend who erased a gate from behind him to make the picture look better. I’m like, ‘That’s your yard! Your kids are gonna look at that and be like, ‘Didn’t we have a gate there?’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rose worries we’re in an “early days of social media” moment with AI — making decisions that seem harmless now but will haunt us later. “We’re gonna look back and be like, ‘Wow, that was weird. We just slapped AI on everything, and thought it was a good idea,’ similar to what happened in the early days of social. We look back a decade or two later, and you’re like, ‘I wish I would have done that differently.’”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;He’s experiencing these tensions firsthand with his young children. Using OpenAI’s video generation tool Sora to create videos of tiny Labradoodles, his kids asked where they could get those puppies. “I’m like, that’s not really Dad there. How do you have that conversation? Very awkward,” he says. His solution, he said, is treating AI like movie magic, explaining that just as actors aren’t really flying on screen, Dad’s puppies aren’t real either.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Rose isn’t a Luddite. He’s deeply optimistic about how AI is transforming entrepreneurship itself, and by extension, the venture capital industry that funds it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The barriers to entry for entrepreneurs are just shrinking with every day that goes by,” Rose observed. He recounted a colleague who had never used AI coding tools before building and deploying a complete app during a drive from LA to San Francisco. Six months ago, the same task would have taken ten times as long and required navigating dozens of errors.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“In three months, when [Google’s] Gemini 3 hits the market, there’s going to be zero errors or next to it,” Rose predicted. “High school coding classes are no longer coding classes — they’re vibe coding classes, and they will build the next billion-dollar business launched out of some random high school. It will happen. It’s just a matter of time.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These developments completely change the VC equation, Rose said. Entrepreneurs can now delay fundraising until they absolutely need it, or potentially skip raising outside funding altogether. “It’s really going to change the world of VC, and I think for the better,” Rose said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Many venture firms have responded by hiring armies of engineers—Sequoia Capital, for instance, now employs as many developers as investors. But Rose doesn’t think that’s the answer. Instead, he believes the value proposition for VCs shifts to something more fundamental. “At the end of the day, the entrepreneur is going to have issues that are not technical,” he argued. “They’re very emotional problems. And so I think the VCs with the highest EQ that can show up best for the founders as their long term partner — that have been with firms and aren’t hopping around, that aren’t just fly-by-night VCs but have been around and seen these problems at scale — they’re going to be sought after.”&lt;/p&gt;&lt;p&gt;So what does Rose look for when making investments? He circles back to something Larry Page told him years ago when Rose was at Google Ventures, his first institutional investing job after co-founding the social news platform Digg and before joining True Ventures in 2017. “A healthy disregard for the impossible is what’s important to look for.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want founders that aren’t just sanding down the rough edges, but they’re really swinging for the fences with big, bold ideas that everyone else says, ‘That is a horrible idea. Why are you doing this?’” Rose said. “That’s what I’m drawn to. Because even if it doesn’t work, we love your mind. We love where you are, and we gladly back you the second time.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/02/kevin-roses-simple-test-for-ai-hardware-would-you-want-to-punch-someone-in-the-face-whos-wearing-it/</guid><pubDate>Mon, 03 Nov 2025 01:54:17 +0000</pubDate></item><item><title>A faster problem-solving tool that guarantees feasibility (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/faster-problem-solving-tool-guarantees-feasibility-1103</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/MIT-Feasibility-Seeking-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Managing a power grid is like trying to solve an enormous puzzle.&lt;/p&gt;&lt;p&gt;Grid operators must ensure the proper amount of power is flowing to the right areas at the exact time when it is needed, and they must do this in a way that minimizes costs without overloading physical infrastructure. Even more, they must solve this complicated problem repeatedly, as rapidly as possible, to meet constantly changing demand.&lt;/p&gt;&lt;p&gt;To help crack this consistent conundrum, MIT researchers developed a problem-solving tool that finds the optimal solution much faster than traditional approaches while ensuring the solution doesn’t violate any of the system’s constraints. In a power grid, constraints could be things like generator and line capacity.&lt;/p&gt;&lt;p&gt;This new tool incorporates a feasibility-seeking step into a powerful machine-learning model trained to solve the problem. The feasibility-seeking step uses the model’s prediction as a starting point, iteratively refining the solution until it finds the best achievable answer.&lt;/p&gt;&lt;p&gt;The MIT system can unravel complex problems several times faster than traditional solvers, while providing strong guarantees of success. For some extremely complex problems, it could find better solutions than tried-and-true tools. The technique also outperformed pure machine learning approaches, which are fast but can’t always find feasible solutions.&lt;/p&gt;&lt;p&gt;In addition to helping schedule power production in an electric grid, this new tool could be applied to many types of complicated problems, such as designing new products, managing investment portfolios, or planning production to meet consumer demand.&lt;/p&gt;&lt;p&gt;“Solving these especially thorny problems well requires us to combine tools from machine learning, optimization, and electrical engineering to develop methods that hit the right tradeoffs in terms of providing value to the domain, while also meeting its requirements. You have to look at the needs of the application and design methods in a way that actually fulfills those needs,” says Priya Donti, the Silverman Family Career Development Professor in the Department of Electrical Engineering and Computer Science (EECS) and a principal investigator at the Laboratory for Information and Decision Systems (LIDS).&lt;/p&gt;&lt;p&gt;Donti, senior author of an open-access paper on this new tool, called FSNet, is joined by lead author Hoang Nguyen, an EECS graduate student. The paper will be presented at the Conference on Neural Information Processing Systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Combining approaches&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Ensuring optimal power flow in an electric grid is an extremely hard problem that is becoming more difficult for operators to solve quickly.&lt;/p&gt;&lt;p&gt;“As we try to integrate more renewables into the grid, operators must deal with the fact that the amount of power generation is going to vary moment to moment. At the same time, there are many more distributed devices to coordinate,” Donti explains.&lt;/p&gt;&lt;p&gt;Grid operators often rely on traditional solvers, which provide mathematical guarantees that the optimal solution doesn’t violate any problem constraints. But these tools can take hours or even days to arrive at that solution if the problem is especially convoluted.&lt;/p&gt;&lt;p&gt;On the other hand, deep-learning models can solve even very hard problems in a fraction of the time, but the solution might ignore some important constraints. For a power grid operator, this could result in issues like unsafe voltage levels or even grid outages.&lt;/p&gt;&lt;p&gt;“Machine-learning models struggle to satisfy all the constraints due to the many errors that occur during the training process,” Nguyen explains.&lt;/p&gt;&lt;p&gt;For FSNet, the researchers combined the best of both approaches into a two-step problem-solving framework.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Focusing on feasibility&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In the first step, a neural network predicts a solution to the optimization problem. Very loosely inspired by neurons in the human brain, neural networks are deep learning models that excel at recognizing patterns in data.&lt;/p&gt;&lt;p&gt;Next, a traditional solver that has been incorporated into FSNet performs a feasibility-seeking step. This optimization algorithm iteratively refines the initial prediction while ensuring the solution does not violate any constraints.&lt;/p&gt;&lt;p&gt;Because the feasibility-seeking step is based on a mathematical model of the problem, it can guarantee the solution is deployable.&lt;/p&gt;&lt;p&gt;“This step is very important. In FSNet, we can have the rigorous guarantees that we need in practice,” Hoang says.&lt;/p&gt;&lt;p&gt;The researchers designed FSNet to address both main types of constraints (equality and inequality) at the same time. This makes it easier to use than other approaches that may require customizing the neural network or solving for each type of constraint separately.&lt;/p&gt;&lt;p&gt;“Here, you can just plug and play with different optimization solvers,” Donti says.&lt;/p&gt;&lt;p&gt;By thinking differently about how the neural network solves complex optimization problems, the researchers were able to unlock a new technique that works better, she adds.&lt;/p&gt;&lt;p&gt;They compared FSNet to traditional solvers and pure machine-learning approaches on a range of challenging problems, including power grid optimization. Their system cut solving times by orders of magnitude compared to the baseline approaches, while respecting all problem constraints.&lt;/p&gt;&lt;p&gt;FSNet also found better solutions to some of the trickiest problems.&lt;/p&gt;&lt;p&gt;“While this was surprising to us, it does make sense. Our neural network can figure out by itself some additional structure in the data that the original optimization solver was not designed to exploit,” Donti explains.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to make FSNet less memory-intensive, incorporate more efficient optimization algorithms, and scale it up to tackle more realistic problems.&lt;/p&gt;&lt;p&gt;“Finding solutions to challenging optimization problems that are feasible is paramount to finding ones that are close to optimal. Especially for physical systems like power grids, close to optimal means nothing without feasibility. This work provides an important step toward ensuring that deep-learning models can produce predictions that satisfy constraints, with explicit guarantees on constraint enforcement,” says Kyri Baker, an associate professor at the University of Colorado Boulder, who was not involved with this work.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/MIT-Feasibility-Seeking-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Managing a power grid is like trying to solve an enormous puzzle.&lt;/p&gt;&lt;p&gt;Grid operators must ensure the proper amount of power is flowing to the right areas at the exact time when it is needed, and they must do this in a way that minimizes costs without overloading physical infrastructure. Even more, they must solve this complicated problem repeatedly, as rapidly as possible, to meet constantly changing demand.&lt;/p&gt;&lt;p&gt;To help crack this consistent conundrum, MIT researchers developed a problem-solving tool that finds the optimal solution much faster than traditional approaches while ensuring the solution doesn’t violate any of the system’s constraints. In a power grid, constraints could be things like generator and line capacity.&lt;/p&gt;&lt;p&gt;This new tool incorporates a feasibility-seeking step into a powerful machine-learning model trained to solve the problem. The feasibility-seeking step uses the model’s prediction as a starting point, iteratively refining the solution until it finds the best achievable answer.&lt;/p&gt;&lt;p&gt;The MIT system can unravel complex problems several times faster than traditional solvers, while providing strong guarantees of success. For some extremely complex problems, it could find better solutions than tried-and-true tools. The technique also outperformed pure machine learning approaches, which are fast but can’t always find feasible solutions.&lt;/p&gt;&lt;p&gt;In addition to helping schedule power production in an electric grid, this new tool could be applied to many types of complicated problems, such as designing new products, managing investment portfolios, or planning production to meet consumer demand.&lt;/p&gt;&lt;p&gt;“Solving these especially thorny problems well requires us to combine tools from machine learning, optimization, and electrical engineering to develop methods that hit the right tradeoffs in terms of providing value to the domain, while also meeting its requirements. You have to look at the needs of the application and design methods in a way that actually fulfills those needs,” says Priya Donti, the Silverman Family Career Development Professor in the Department of Electrical Engineering and Computer Science (EECS) and a principal investigator at the Laboratory for Information and Decision Systems (LIDS).&lt;/p&gt;&lt;p&gt;Donti, senior author of an open-access paper on this new tool, called FSNet, is joined by lead author Hoang Nguyen, an EECS graduate student. The paper will be presented at the Conference on Neural Information Processing Systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Combining approaches&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Ensuring optimal power flow in an electric grid is an extremely hard problem that is becoming more difficult for operators to solve quickly.&lt;/p&gt;&lt;p&gt;“As we try to integrate more renewables into the grid, operators must deal with the fact that the amount of power generation is going to vary moment to moment. At the same time, there are many more distributed devices to coordinate,” Donti explains.&lt;/p&gt;&lt;p&gt;Grid operators often rely on traditional solvers, which provide mathematical guarantees that the optimal solution doesn’t violate any problem constraints. But these tools can take hours or even days to arrive at that solution if the problem is especially convoluted.&lt;/p&gt;&lt;p&gt;On the other hand, deep-learning models can solve even very hard problems in a fraction of the time, but the solution might ignore some important constraints. For a power grid operator, this could result in issues like unsafe voltage levels or even grid outages.&lt;/p&gt;&lt;p&gt;“Machine-learning models struggle to satisfy all the constraints due to the many errors that occur during the training process,” Nguyen explains.&lt;/p&gt;&lt;p&gt;For FSNet, the researchers combined the best of both approaches into a two-step problem-solving framework.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Focusing on feasibility&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In the first step, a neural network predicts a solution to the optimization problem. Very loosely inspired by neurons in the human brain, neural networks are deep learning models that excel at recognizing patterns in data.&lt;/p&gt;&lt;p&gt;Next, a traditional solver that has been incorporated into FSNet performs a feasibility-seeking step. This optimization algorithm iteratively refines the initial prediction while ensuring the solution does not violate any constraints.&lt;/p&gt;&lt;p&gt;Because the feasibility-seeking step is based on a mathematical model of the problem, it can guarantee the solution is deployable.&lt;/p&gt;&lt;p&gt;“This step is very important. In FSNet, we can have the rigorous guarantees that we need in practice,” Hoang says.&lt;/p&gt;&lt;p&gt;The researchers designed FSNet to address both main types of constraints (equality and inequality) at the same time. This makes it easier to use than other approaches that may require customizing the neural network or solving for each type of constraint separately.&lt;/p&gt;&lt;p&gt;“Here, you can just plug and play with different optimization solvers,” Donti says.&lt;/p&gt;&lt;p&gt;By thinking differently about how the neural network solves complex optimization problems, the researchers were able to unlock a new technique that works better, she adds.&lt;/p&gt;&lt;p&gt;They compared FSNet to traditional solvers and pure machine-learning approaches on a range of challenging problems, including power grid optimization. Their system cut solving times by orders of magnitude compared to the baseline approaches, while respecting all problem constraints.&lt;/p&gt;&lt;p&gt;FSNet also found better solutions to some of the trickiest problems.&lt;/p&gt;&lt;p&gt;“While this was surprising to us, it does make sense. Our neural network can figure out by itself some additional structure in the data that the original optimization solver was not designed to exploit,” Donti explains.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to make FSNet less memory-intensive, incorporate more efficient optimization algorithms, and scale it up to tackle more realistic problems.&lt;/p&gt;&lt;p&gt;“Finding solutions to challenging optimization problems that are feasible is paramount to finding ones that are close to optimal. Especially for physical systems like power grids, close to optimal means nothing without feasibility. This work provides an important step toward ensuring that deep-learning models can produce predictions that satisfy constraints, with explicit guarantees on constraint enforcement,” says Kyri Baker, an associate professor at the University of Colorado Boulder, who was not involved with this work.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/faster-problem-solving-tool-guarantees-feasibility-1103</guid><pubDate>Mon, 03 Nov 2025 05:00:00 +0000</pubDate></item><item><title>[NEW] Qualcomm unveils AI data centre chips to crack the Inference market (AI News)</title><link>https://www.artificialintelligence-news.com/news/qualcomm-ai-data-centre-chips-ai200-ai250/</link><description>&lt;p&gt;The AI chip wars just got a new heavyweight contender. Qualcomm, the company that powers billions of smartphones worldwide, has made an audacious leap into AI data centre chips – a market where Nvidia has been minting money at an almost unfathomable rate and where fortunes rise and fall on promises of computational supremacy.&lt;/p&gt;&lt;p&gt;On October 28, 2025, Qualcomm threw down the gauntlet with its AI200 and AI250 solutions, rack-scale systems designed specifically for AI inference workloads. Wall Street’s reaction was immediate: Qualcomm’s stock price jumped approximately 11% as investors bet that even a modest slice of the exploding AI infrastructure market could transform the company’s trajectory.&lt;/p&gt;&lt;p&gt;The product launch could redefine Qualcomm’s identity. The San Diego chip giant has been synonymous with mobile technology, riding the smartphone wave to dominance. But with that market stagnating, CEO Cristiano Amon is placing a calculated wager on AI data centre chips, backed by a multi-billion-dollar partnership with a Saudi AI powerhouse that signals serious intent.&lt;/p&gt;&lt;h3&gt;Two chips, two different bets on the future&lt;/h3&gt;&lt;p&gt;Here’s where Qualcomm’s strategy gets interesting. Rather than releasing a single product and hoping for the best, the company is hedging its bets with two distinct AI data centre chip architectures, each targeting different market needs and timelines.&lt;/p&gt;&lt;p&gt;The AI200, arriving in 2026, takes the pragmatic approach. Think of it as Qualcomm’s foot in the door – a rack-scale system packing 768 GB of LPDDR memory per card.&lt;/p&gt;&lt;p&gt;That massive memory capacity is crucial for running today’s memory-hungry large language models and multimodal AI applications, and Qualcomm is betting that its lower-cost memory approach can undercut competitors on total cost of ownership while still delivering the performance enterprises demand.&lt;/p&gt;&lt;p&gt;But the AI250, slated for 2027, is where Qualcomm’s engineers have really been dreaming big. The solution introduces a near-memory computing architecture that promises to shatter conventional limitations with more than 10x higher effective memory bandwidth.&lt;/p&gt;&lt;p&gt;For AI data centre chips, memory bandwidth is often the bottleneck that determines whether your chatbot responds instantly or leaves users waiting. Qualcomm’s innovation here could be a genuine game-changer – assuming it can deliver on the promise.&lt;/p&gt;&lt;p&gt;“With Qualcomm AI200 and AI250, we’re redefining what’s possible for rack-scale AI inference,” said Durga Malladi, SVP and GM of technology planning, edge solutions &amp;amp; data centre at Qualcomm Technologies. “The innovative new AI infrastructure solutions empower customers to deploy AI at unprecedented TCO, while maintaining the flexibility and security modern data centres demand.”&lt;/p&gt;&lt;h3&gt;The real battle: Economics, not just performance&lt;/h3&gt;&lt;p&gt;In the AI infrastructure arms race, raw performance specs only tell half the story. The real war is fought on spreadsheets, where data centre operators calculate power bills, cooling costs, and hardware depreciation. Qualcomm knows this, and that’s why both AI data centre chip solutions obsess over total cost of ownership.&lt;/p&gt;&lt;p&gt;Each rack consumes 160 kW of power and employs direct liquid cooling – a necessity when you’re pushing this much computational power through silicon. The systems use PCIe for internal scaling and Ethernet for connecting multiple racks, providing deployment flexibility whether you’re running a modest AI service or building the next ChatGPT competitor.&lt;/p&gt;&lt;p&gt;Security hasn’t been an afterthought either; confidential computing capabilities are baked in, addressing the growing enterprise demand for protecting proprietary AI models and sensitive data.&lt;/p&gt;&lt;h3&gt;The Saudi connection: A billion-dollar validation&lt;/h3&gt;&lt;p&gt;Partnership announcements in tech can be vapour-thin, but Qualcomm’s deal with Humain carries some weight. The Saudi state-backed AI company has committed to deploying 200 megawatts of Qualcomm AI data centre chips – a figure that analyst Stacy Rasgon of Sanford C. Bernstein estimates translates to roughly $2 billion in revenue for Qualcomm.&lt;/p&gt;&lt;p&gt;Is $2 billion transformative? In the context of AMD’s $10 billion Humain deal announced the same year, it might seem modest. But for a company trying to prove it belongs in the AI infrastructure conversation, securing a major deployment commitment before your first product even ships is validation that money can’t buy.&lt;/p&gt;&lt;p&gt;“Together with Humain, we are laying the groundwork for transformative AI-driven innovation that will empower enterprises, government organisations and communities in the region and globally,” Amon declared in a statement that positions Qualcomm not just as a chip supplier, but as a strategic technology partner for emerging AI economies.&lt;/p&gt;&lt;p&gt;The collaboration, first announced in May 2025, transforms Qualcomm into a key infrastructure provider for Humain’s ambitious AI inferencing services – a role that could establish crucial reference designs and deployment patterns for future customers.&lt;/p&gt;&lt;h3&gt;Software stack and developer experience&lt;/h3&gt;&lt;p&gt;Beyond hardware specifications, Qualcomm is betting on developer-friendly software to accelerate adoption. The company’s AI software stack supports leading machine learning frameworks and promises “one-click deployment” of models from Hugging Face, a popular AI model repository.&lt;/p&gt;&lt;p&gt;The Qualcomm AI Inference Suite and Efficient Transformers Library aim to remove integration friction that has historically slowed enterprise AI deployments.&lt;/p&gt;&lt;h2&gt;David vs. Goliath (and another Goliath?)&lt;/h2&gt;&lt;p&gt;Let’s be honest about what Qualcomm is up against. Nvidia’s market capitalisation has soared past $4.5 trillion, a valuation that reflects years of AI dominance and an ecosystem so entrenched that many developers can’t imagine building on anything else.&lt;/p&gt;&lt;p&gt;AMD, once the scrappy challenger, has seen its shares more than double in value in 2025 as it successfully carved out its own piece of the AI pie.&lt;/p&gt;&lt;p&gt;Qualcomm’s late arrival to the AI data centre chips party means fighting an uphill battle against competitors who have battle-tested products, mature software stacks, and customers already running production workloads at scale.&lt;/p&gt;&lt;p&gt;The company’s smartphone focus, once its greatest strength, now looks like strategic tunnel vision that caused it to miss the initial AI infrastructure boom. Yet market analysts aren’t writing Qualcomm’s obituary. Timothy Arcuri of UBS captured the prevailing sentiment on a conference call: “The tide is rising so fast, and it will continue to rise so fast, it will lift all boats.” Translation: the AI market is expanding so rapidly that there’s room for multiple winners – even latecomers with compelling technology and competitive pricing.&lt;/p&gt;&lt;p&gt;Qualcomm is playing the long game, betting that sustained innovation in AI data centre chips can gradually win over customers looking for alternatives to the Nvidia-AMD duopoly. For enterprises evaluating AI infrastructure options, Qualcomm’s emphasis on inference optimisation, energy efficiency, and TCO presents an alternative worth watching – particularly as the AI200 approaches its 2026 launch date.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Qualcomm)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Migrating AI from Nvidia to Huawei: Opportunities and trade-offs&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-110157" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here&lt;/p&gt;</description><content:encoded>&lt;p&gt;The AI chip wars just got a new heavyweight contender. Qualcomm, the company that powers billions of smartphones worldwide, has made an audacious leap into AI data centre chips – a market where Nvidia has been minting money at an almost unfathomable rate and where fortunes rise and fall on promises of computational supremacy.&lt;/p&gt;&lt;p&gt;On October 28, 2025, Qualcomm threw down the gauntlet with its AI200 and AI250 solutions, rack-scale systems designed specifically for AI inference workloads. Wall Street’s reaction was immediate: Qualcomm’s stock price jumped approximately 11% as investors bet that even a modest slice of the exploding AI infrastructure market could transform the company’s trajectory.&lt;/p&gt;&lt;p&gt;The product launch could redefine Qualcomm’s identity. The San Diego chip giant has been synonymous with mobile technology, riding the smartphone wave to dominance. But with that market stagnating, CEO Cristiano Amon is placing a calculated wager on AI data centre chips, backed by a multi-billion-dollar partnership with a Saudi AI powerhouse that signals serious intent.&lt;/p&gt;&lt;h3&gt;Two chips, two different bets on the future&lt;/h3&gt;&lt;p&gt;Here’s where Qualcomm’s strategy gets interesting. Rather than releasing a single product and hoping for the best, the company is hedging its bets with two distinct AI data centre chip architectures, each targeting different market needs and timelines.&lt;/p&gt;&lt;p&gt;The AI200, arriving in 2026, takes the pragmatic approach. Think of it as Qualcomm’s foot in the door – a rack-scale system packing 768 GB of LPDDR memory per card.&lt;/p&gt;&lt;p&gt;That massive memory capacity is crucial for running today’s memory-hungry large language models and multimodal AI applications, and Qualcomm is betting that its lower-cost memory approach can undercut competitors on total cost of ownership while still delivering the performance enterprises demand.&lt;/p&gt;&lt;p&gt;But the AI250, slated for 2027, is where Qualcomm’s engineers have really been dreaming big. The solution introduces a near-memory computing architecture that promises to shatter conventional limitations with more than 10x higher effective memory bandwidth.&lt;/p&gt;&lt;p&gt;For AI data centre chips, memory bandwidth is often the bottleneck that determines whether your chatbot responds instantly or leaves users waiting. Qualcomm’s innovation here could be a genuine game-changer – assuming it can deliver on the promise.&lt;/p&gt;&lt;p&gt;“With Qualcomm AI200 and AI250, we’re redefining what’s possible for rack-scale AI inference,” said Durga Malladi, SVP and GM of technology planning, edge solutions &amp;amp; data centre at Qualcomm Technologies. “The innovative new AI infrastructure solutions empower customers to deploy AI at unprecedented TCO, while maintaining the flexibility and security modern data centres demand.”&lt;/p&gt;&lt;h3&gt;The real battle: Economics, not just performance&lt;/h3&gt;&lt;p&gt;In the AI infrastructure arms race, raw performance specs only tell half the story. The real war is fought on spreadsheets, where data centre operators calculate power bills, cooling costs, and hardware depreciation. Qualcomm knows this, and that’s why both AI data centre chip solutions obsess over total cost of ownership.&lt;/p&gt;&lt;p&gt;Each rack consumes 160 kW of power and employs direct liquid cooling – a necessity when you’re pushing this much computational power through silicon. The systems use PCIe for internal scaling and Ethernet for connecting multiple racks, providing deployment flexibility whether you’re running a modest AI service or building the next ChatGPT competitor.&lt;/p&gt;&lt;p&gt;Security hasn’t been an afterthought either; confidential computing capabilities are baked in, addressing the growing enterprise demand for protecting proprietary AI models and sensitive data.&lt;/p&gt;&lt;h3&gt;The Saudi connection: A billion-dollar validation&lt;/h3&gt;&lt;p&gt;Partnership announcements in tech can be vapour-thin, but Qualcomm’s deal with Humain carries some weight. The Saudi state-backed AI company has committed to deploying 200 megawatts of Qualcomm AI data centre chips – a figure that analyst Stacy Rasgon of Sanford C. Bernstein estimates translates to roughly $2 billion in revenue for Qualcomm.&lt;/p&gt;&lt;p&gt;Is $2 billion transformative? In the context of AMD’s $10 billion Humain deal announced the same year, it might seem modest. But for a company trying to prove it belongs in the AI infrastructure conversation, securing a major deployment commitment before your first product even ships is validation that money can’t buy.&lt;/p&gt;&lt;p&gt;“Together with Humain, we are laying the groundwork for transformative AI-driven innovation that will empower enterprises, government organisations and communities in the region and globally,” Amon declared in a statement that positions Qualcomm not just as a chip supplier, but as a strategic technology partner for emerging AI economies.&lt;/p&gt;&lt;p&gt;The collaboration, first announced in May 2025, transforms Qualcomm into a key infrastructure provider for Humain’s ambitious AI inferencing services – a role that could establish crucial reference designs and deployment patterns for future customers.&lt;/p&gt;&lt;h3&gt;Software stack and developer experience&lt;/h3&gt;&lt;p&gt;Beyond hardware specifications, Qualcomm is betting on developer-friendly software to accelerate adoption. The company’s AI software stack supports leading machine learning frameworks and promises “one-click deployment” of models from Hugging Face, a popular AI model repository.&lt;/p&gt;&lt;p&gt;The Qualcomm AI Inference Suite and Efficient Transformers Library aim to remove integration friction that has historically slowed enterprise AI deployments.&lt;/p&gt;&lt;h2&gt;David vs. Goliath (and another Goliath?)&lt;/h2&gt;&lt;p&gt;Let’s be honest about what Qualcomm is up against. Nvidia’s market capitalisation has soared past $4.5 trillion, a valuation that reflects years of AI dominance and an ecosystem so entrenched that many developers can’t imagine building on anything else.&lt;/p&gt;&lt;p&gt;AMD, once the scrappy challenger, has seen its shares more than double in value in 2025 as it successfully carved out its own piece of the AI pie.&lt;/p&gt;&lt;p&gt;Qualcomm’s late arrival to the AI data centre chips party means fighting an uphill battle against competitors who have battle-tested products, mature software stacks, and customers already running production workloads at scale.&lt;/p&gt;&lt;p&gt;The company’s smartphone focus, once its greatest strength, now looks like strategic tunnel vision that caused it to miss the initial AI infrastructure boom. Yet market analysts aren’t writing Qualcomm’s obituary. Timothy Arcuri of UBS captured the prevailing sentiment on a conference call: “The tide is rising so fast, and it will continue to rise so fast, it will lift all boats.” Translation: the AI market is expanding so rapidly that there’s room for multiple winners – even latecomers with compelling technology and competitive pricing.&lt;/p&gt;&lt;p&gt;Qualcomm is playing the long game, betting that sustained innovation in AI data centre chips can gradually win over customers looking for alternatives to the Nvidia-AMD duopoly. For enterprises evaluating AI infrastructure options, Qualcomm’s emphasis on inference optimisation, energy efficiency, and TCO presents an alternative worth watching – particularly as the AI200 approaches its 2026 launch date.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Qualcomm)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Migrating AI from Nvidia to Huawei: Opportunities and trade-offs&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-110157" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/qualcomm-ai-data-centre-chips-ai200-ai250/</guid><pubDate>Mon, 03 Nov 2025 09:00:00 +0000</pubDate></item><item><title>[NEW] NVIDIA and South Korea align on sovereign AI at APEC CEO Summit (AI News)</title><link>https://www.artificialintelligence-news.com/news/nvidia-and-south-korea-align-on-sovereign-ai-at-apec-summit/</link><description>&lt;p&gt;At the APEC CEO Summit, NVIDIA said it is working with public agencies and private companies to build sovereign AI infrastructure across South Korea. The plan includes hundreds of thousands of NVIDIA GPUs across sovereign clouds and AI factories for areas like automotive, manufacturing and telecommunications.&lt;/p&gt;&lt;p&gt;“Korea’s leadership in technology and manufacturing positions it at the heart of the AI industrial revolution — where accelerated computing infrastructure becomes as vital as power grids and broadband,” said Jensen Huang, founder and CEO of NVIDIA. “Just as Korea’s physical factories have inspired the world with sophisticated ships, cars, chips and electronics, the nation can now produce intelligence as a new export that will drive global transformation.”&lt;/p&gt;&lt;p&gt;“Now that AI has gone beyond mere innovation and become the foundation of future industries, South Korea stands at the threshold of transformation,” said Bae Kyung-hoon, Korea Deputy Prime Minister, and Minister of Science and Information and Communication Technologies.&lt;/p&gt;&lt;p&gt;The government plans to deploy up to 50,000 new NVIDIA GPUs to support sovereign AI programs for businesses and research groups. The first phase includes 13,000 NVIDIA Blackwell and other GPUs through providers such as NAVER Cloud, NHN Cloud and Kakao. The expansion includes efforts to build a National AI Computing Center. Startups, researchers and other organisations will be able to use this sovereign infrastructure to train models and build new applications.&lt;/p&gt;&lt;p&gt;NVIDIA is also working with Samsung, SK Telecom, ETRI, KT, LGU+ and Yonsei University on AI-RAN and 6G network research. The work focuses on shifting some computing tasks from devices to network base stations, which may reduce battery drain and lower computing costs across sovereign AI services.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-major-companies-build-sovereign-ai-factories"&gt;Major companies build sovereign AI factories&lt;/h3&gt;&lt;p&gt;Large corporations in Korea are investing in advanced AI infrastructure for chip production, network operations and digital manufacturing tools that support the country’s sovereign computing goals.&lt;/p&gt;&lt;p&gt;NVIDIA and Samsung plan to build a new AI factory that connects chip manufacturing with accelerated computing. The system will run more than 50,000 NVIDIA GPUs and support data-driven production methods, including predictive maintenance and process improvements across chip fabs.&lt;/p&gt;&lt;p&gt;“We are at the dawn of the AI industrial revolution — a new era that will redefine how the world designs, builds and manufactures,” said Jensen Huang. Jay Y. Lee, executive chairman of Samsung Electronics, added, “From Samsung’s DRAM for NVIDIA’s game-changing graphics card in 1995 to our new AI factory, we are thrilled to continue our longstanding journey with NVIDIA in leading this transformation.”&lt;/p&gt;&lt;p&gt;Samsung plans to use NVIDIA CUDA-X libraries, along with software from Synopsys, Cadence and Siemens, to speed circuit design and manufacturing workflows. It will also use NVIDIA Omniverse to create digital twins of factories and equipment for real-time simulation, testing and logistics planning — all supporting wider sovereign AI adoption.&lt;/p&gt;&lt;p&gt;NVIDIA’s cuLitho library is being integrated into Samsung’s computational lithography tools. The collaboration has led to major gains in performance, supporting faster scaling in chip production.&lt;/p&gt;&lt;p&gt;Samsung is also developing large language models that run across hundreds of millions of Samsung devices, supporting translation and other reasoning tasks. The company plans to expand into robotics using NVIDIA Isaac Sim, NVIDIA Cosmos and the Jetson Thor edge platform, which may strengthen its position in sovereign AI systems.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-sk-group-expands-ai-capacity"&gt;SK Group expands AI capacity&lt;/h3&gt;&lt;p&gt;SK Group is building an AI factory that will include more than 50,000 NVIDIA GPUs, with completion expected by late 2027. The facility will support SK subsidiaries and outside clients through GPU-as-a-service offerings that align with South Korea’s sovereign AI strategy. NVIDIA and SK are also working together on next-generation high-bandwidth memory for GPUs.&lt;/p&gt;&lt;p&gt;“SK Group is working with NVIDIA to make AI the engine of a profound transformation that will enable industries across Korea to transcend traditional limits of scale, speed and precision,” said Chey Tae-Won, chairman of SK Group.&lt;/p&gt;&lt;p&gt;SK Telecom plans to build an industrial AI cloud using NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs. The platform will support semiconductor manufacturing, digital twins and internal AI agents.&lt;/p&gt;&lt;p&gt;SK hynix is using NVIDIA PhysicsNeMo tools to support chip design simulations, aiming to improve accuracy and speed. It is also testing NVIDIA Blackwell GPUs with Synopsys software and building autonomous fab digital twins.&lt;/p&gt;&lt;p&gt;To support workers, SKT is developing a foundation model called A.X., built with NVIDIA NIM microservices and NVIDIA AI Enterprise. The model will power internal agents to assist thousands of employees across chip development and operations.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-hyundai-motor-group-plans-new-ai-factory"&gt;Hyundai Motor Group plans new AI factory&lt;/h3&gt;&lt;p&gt;NVIDIA and Hyundai Motor Group are expanding their partnership to support autonomous driving, factory automation and robotics. Hyundai plans to build an AI factory using NVIDIA Blackwell GPUs for integrated training, simulation and deployment.&lt;/p&gt;&lt;p&gt;“AI is revolutionising every facet of every industry, and in transportation alone — from vehicle design and manufacturing to robotics and autonomous driving — NVIDIA’s AI and computing platforms are transforming how the world moves,” said Jensen Huang.&lt;/p&gt;&lt;p&gt;The companies expect joint investment of about $3 billion to grow national physical AI capabilities. The plan includes an NVIDIA AI Technology Center, Hyundai’s Physical AI Application Center and new data centres. These programs aim to help train a new generation of AI talent.&lt;/p&gt;&lt;p&gt;Hyundai will use NVIDIA Omniverse Enterprise to build digital twins of factories, supporting virtual testing, robot integration and predictive maintenance. It will also use NVIDIA DRIVE AGX Thor for in-vehicle AI systems, including driver assistance and infotainment features.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-growth-of-sovereign-ai-models"&gt;Growth of sovereign AI models&lt;/h3&gt;&lt;p&gt;NAVER Cloud plans to deploy more than 60,000 GPUs for sovereign and physical AI work. The company will build industry-targeted models for areas such as shipbuilding and public safety.&lt;/p&gt;&lt;p&gt;The Ministry of Science and ICT is also leading a Sovereign AI Foundation Models project using NVIDIA NeMo and open Nemotron datasets. Partners include LG AI Research, NC AI, SK Telecom and Upstage. These models will support language and reasoning tasks.&lt;/p&gt;&lt;p&gt;LG is working with NVIDIA on physical AI research and will support startups and researchers using its EXAONE models, including healthcare applications.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-quantum-and-scientific-research"&gt;Quantum and scientific research&lt;/h3&gt;&lt;p&gt;KISTI plans to use NVIDIA accelerated computing in its sixth national supercomputer, HANGANG. The institute will support NVQLink, an open architecture for connecting quantum processors with GPU clusters. It will also develop scientific foundation models and explore physics-informed AI tools using NVIDIA PhysicsNeMo.&lt;/p&gt;&lt;p&gt;NVIDIA and local partners are forming a startup alliance through the NVIDIA Inception program. Members will gain access to accelerated computing resources from cloud partners like SK Telecom, along with support from venture firms. NVIDIA also plans to take part in the N-Up AI startup incubation program from the Ministry of SMEs and Startups.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Nvidia)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Migrating AI from Nvidia to Huawei: Opportunities and trade-offs&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-110157" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;At the APEC CEO Summit, NVIDIA said it is working with public agencies and private companies to build sovereign AI infrastructure across South Korea. The plan includes hundreds of thousands of NVIDIA GPUs across sovereign clouds and AI factories for areas like automotive, manufacturing and telecommunications.&lt;/p&gt;&lt;p&gt;“Korea’s leadership in technology and manufacturing positions it at the heart of the AI industrial revolution — where accelerated computing infrastructure becomes as vital as power grids and broadband,” said Jensen Huang, founder and CEO of NVIDIA. “Just as Korea’s physical factories have inspired the world with sophisticated ships, cars, chips and electronics, the nation can now produce intelligence as a new export that will drive global transformation.”&lt;/p&gt;&lt;p&gt;“Now that AI has gone beyond mere innovation and become the foundation of future industries, South Korea stands at the threshold of transformation,” said Bae Kyung-hoon, Korea Deputy Prime Minister, and Minister of Science and Information and Communication Technologies.&lt;/p&gt;&lt;p&gt;The government plans to deploy up to 50,000 new NVIDIA GPUs to support sovereign AI programs for businesses and research groups. The first phase includes 13,000 NVIDIA Blackwell and other GPUs through providers such as NAVER Cloud, NHN Cloud and Kakao. The expansion includes efforts to build a National AI Computing Center. Startups, researchers and other organisations will be able to use this sovereign infrastructure to train models and build new applications.&lt;/p&gt;&lt;p&gt;NVIDIA is also working with Samsung, SK Telecom, ETRI, KT, LGU+ and Yonsei University on AI-RAN and 6G network research. The work focuses on shifting some computing tasks from devices to network base stations, which may reduce battery drain and lower computing costs across sovereign AI services.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-major-companies-build-sovereign-ai-factories"&gt;Major companies build sovereign AI factories&lt;/h3&gt;&lt;p&gt;Large corporations in Korea are investing in advanced AI infrastructure for chip production, network operations and digital manufacturing tools that support the country’s sovereign computing goals.&lt;/p&gt;&lt;p&gt;NVIDIA and Samsung plan to build a new AI factory that connects chip manufacturing with accelerated computing. The system will run more than 50,000 NVIDIA GPUs and support data-driven production methods, including predictive maintenance and process improvements across chip fabs.&lt;/p&gt;&lt;p&gt;“We are at the dawn of the AI industrial revolution — a new era that will redefine how the world designs, builds and manufactures,” said Jensen Huang. Jay Y. Lee, executive chairman of Samsung Electronics, added, “From Samsung’s DRAM for NVIDIA’s game-changing graphics card in 1995 to our new AI factory, we are thrilled to continue our longstanding journey with NVIDIA in leading this transformation.”&lt;/p&gt;&lt;p&gt;Samsung plans to use NVIDIA CUDA-X libraries, along with software from Synopsys, Cadence and Siemens, to speed circuit design and manufacturing workflows. It will also use NVIDIA Omniverse to create digital twins of factories and equipment for real-time simulation, testing and logistics planning — all supporting wider sovereign AI adoption.&lt;/p&gt;&lt;p&gt;NVIDIA’s cuLitho library is being integrated into Samsung’s computational lithography tools. The collaboration has led to major gains in performance, supporting faster scaling in chip production.&lt;/p&gt;&lt;p&gt;Samsung is also developing large language models that run across hundreds of millions of Samsung devices, supporting translation and other reasoning tasks. The company plans to expand into robotics using NVIDIA Isaac Sim, NVIDIA Cosmos and the Jetson Thor edge platform, which may strengthen its position in sovereign AI systems.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-sk-group-expands-ai-capacity"&gt;SK Group expands AI capacity&lt;/h3&gt;&lt;p&gt;SK Group is building an AI factory that will include more than 50,000 NVIDIA GPUs, with completion expected by late 2027. The facility will support SK subsidiaries and outside clients through GPU-as-a-service offerings that align with South Korea’s sovereign AI strategy. NVIDIA and SK are also working together on next-generation high-bandwidth memory for GPUs.&lt;/p&gt;&lt;p&gt;“SK Group is working with NVIDIA to make AI the engine of a profound transformation that will enable industries across Korea to transcend traditional limits of scale, speed and precision,” said Chey Tae-Won, chairman of SK Group.&lt;/p&gt;&lt;p&gt;SK Telecom plans to build an industrial AI cloud using NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs. The platform will support semiconductor manufacturing, digital twins and internal AI agents.&lt;/p&gt;&lt;p&gt;SK hynix is using NVIDIA PhysicsNeMo tools to support chip design simulations, aiming to improve accuracy and speed. It is also testing NVIDIA Blackwell GPUs with Synopsys software and building autonomous fab digital twins.&lt;/p&gt;&lt;p&gt;To support workers, SKT is developing a foundation model called A.X., built with NVIDIA NIM microservices and NVIDIA AI Enterprise. The model will power internal agents to assist thousands of employees across chip development and operations.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-hyundai-motor-group-plans-new-ai-factory"&gt;Hyundai Motor Group plans new AI factory&lt;/h3&gt;&lt;p&gt;NVIDIA and Hyundai Motor Group are expanding their partnership to support autonomous driving, factory automation and robotics. Hyundai plans to build an AI factory using NVIDIA Blackwell GPUs for integrated training, simulation and deployment.&lt;/p&gt;&lt;p&gt;“AI is revolutionising every facet of every industry, and in transportation alone — from vehicle design and manufacturing to robotics and autonomous driving — NVIDIA’s AI and computing platforms are transforming how the world moves,” said Jensen Huang.&lt;/p&gt;&lt;p&gt;The companies expect joint investment of about $3 billion to grow national physical AI capabilities. The plan includes an NVIDIA AI Technology Center, Hyundai’s Physical AI Application Center and new data centres. These programs aim to help train a new generation of AI talent.&lt;/p&gt;&lt;p&gt;Hyundai will use NVIDIA Omniverse Enterprise to build digital twins of factories, supporting virtual testing, robot integration and predictive maintenance. It will also use NVIDIA DRIVE AGX Thor for in-vehicle AI systems, including driver assistance and infotainment features.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-growth-of-sovereign-ai-models"&gt;Growth of sovereign AI models&lt;/h3&gt;&lt;p&gt;NAVER Cloud plans to deploy more than 60,000 GPUs for sovereign and physical AI work. The company will build industry-targeted models for areas such as shipbuilding and public safety.&lt;/p&gt;&lt;p&gt;The Ministry of Science and ICT is also leading a Sovereign AI Foundation Models project using NVIDIA NeMo and open Nemotron datasets. Partners include LG AI Research, NC AI, SK Telecom and Upstage. These models will support language and reasoning tasks.&lt;/p&gt;&lt;p&gt;LG is working with NVIDIA on physical AI research and will support startups and researchers using its EXAONE models, including healthcare applications.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-quantum-and-scientific-research"&gt;Quantum and scientific research&lt;/h3&gt;&lt;p&gt;KISTI plans to use NVIDIA accelerated computing in its sixth national supercomputer, HANGANG. The institute will support NVQLink, an open architecture for connecting quantum processors with GPU clusters. It will also develop scientific foundation models and explore physics-informed AI tools using NVIDIA PhysicsNeMo.&lt;/p&gt;&lt;p&gt;NVIDIA and local partners are forming a startup alliance through the NVIDIA Inception program. Members will gain access to accelerated computing resources from cloud partners like SK Telecom, along with support from venture firms. NVIDIA also plans to take part in the N-Up AI startup incubation program from the Ministry of SMEs and Startups.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Nvidia)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Migrating AI from Nvidia to Huawei: Opportunities and trade-offs&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-110157" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/nvidia-and-south-korea-align-on-sovereign-ai-at-apec-summit/</guid><pubDate>Mon, 03 Nov 2025 10:00:00 +0000</pubDate></item><item><title>[NEW] DevOps for AI: Continuous deployment pipelines for machine learning systems (AI News)</title><link>https://www.artificialintelligence-news.com/news/devops-for-ai-continuous-deployment-pipelines-for-machine-learning-systems/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/Picture1.jpg" /&gt;&lt;/div&gt;&lt;p&gt;AI’s effects on continuous development and deployment pipelines are becoming difficult to ignore. However, decision-makers in software development functions need to consider a broad range of elements when considering the uses of the technology.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-challenges-of-deploying-ai-at-scale"&gt;The challenges of deploying AI at scale&lt;/h3&gt;&lt;p&gt;Deploying artificial intelligence isn’t the same as deploying, for example, a web app. Traditional software updates are usually deterministic: once code passes tests, everything works as it’s meant to. With AI and machine learning, outputs can vary because models depend on ever-changing data and complex statistical behaviour.&lt;/p&gt;&lt;p&gt;Some unique challenges you’ll face include:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Data drift: Your training data may not match real-world use, causing performance to decline.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Model versioning: Unlike simple code updates, you need to track both the model and the data it was trained on.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Long training times: Iterating on a new model can take hours or even days, slowing down releases.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Hardware needs: Training and inference often require GPUs or specialised infrastructure.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Monitoring complexity: Tracking performance in production means watching not just uptime but also accuracy, bias, and fairness.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The challenges mean you can’t treat AI like traditional software. You need machine learning pipelines built with automation and monitoring.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-applying-devops-principles-to-ai-systems"&gt;Applying DevOps principles to AI systems&lt;/h3&gt;&lt;p&gt;DevOps was designed to bring developers and operations closer by promoting automation, collaboration, and fast feedback loops. When you bring these principles to AI, so AI and DevOps, you create a foundation for scalable machine learning deployment pipelines.&lt;/p&gt;&lt;p&gt;Some DevOps best practices translate directly:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Automation: Automating training, testing, and deployment reduces manual errors and saves time.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Continuous integration: Code, data, and model updates should all be integrated and tested regularly.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Monitoring and observability: Just like server uptime, models need monitoring for drift and accuracy.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Collaboration: Data scientists, engineers, and operations teams need to work together in the same cycle.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The main difference between DevOps and MLOps lies in the focus. While DevOps centres on code, MLOps is about managing models and datasets alongside code. MLOps extends DevOps to address challenges specific to machine learning pipelines, like data validation, experiment tracking, and retraining strategies.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-designing-a-continuous-deployment-pipeline-for-machine-learning"&gt;Designing a continuous deployment pipeline for machine learning&lt;/h3&gt;&lt;p&gt;When building a continuous deployment system for ML, you need to think beyond just code. Gone are the days of just needing to know how to programme and code; now it’s about much more. Having an artificial intelligence development company that can implement these stages for you is crucial. A step-by-step framework could look like this:&lt;/p&gt;&lt;ol class="wp-block-list"&gt;&lt;li&gt;Data ingestion and validation: Collect data from multiple sources, validate it for quality, and ensure privacy compliance. For example, a healthcare company might verify that patient data is anonymised before use.&lt;/li&gt;&lt;li&gt;Model training and versioning: Train models in controlled environments and store them with a clear version history. Fintech companies often keep a strict record of which datasets and algorithms power models that impact credit scoring.&lt;/li&gt;&lt;li&gt;Automated testing: Validate accuracy, bias, and performance before models move forward. This prevents unreliable models from reaching production.&lt;/li&gt;&lt;li&gt;Deployment to staging: Push models to a staging environment first to test integration with real services.&lt;/li&gt;&lt;li&gt;Production deployment: Deploy with automation, often using containers and orchestration systems like Kubernetes.&lt;/li&gt;&lt;li&gt;Monitoring and feedback loops: Track performance in production, watch for drift, and trigger retraining when thresholds are met.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;By designing an ML pipeline this way, you minimise risks, comply with regulations, and ensure reliable performance in high-stakes industries like healthcare and finance.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-role-of-a-dedicated-development-team-in-mlops"&gt;The Role of a dedicated development team in MLOps&lt;/h3&gt;&lt;p&gt;You may wonder whether you need a dedicated software development team for MLOps or if hiring consultants is enough. The reality is that one-off consultants often provide short-term fixes, but machine learning pipelines require ongoing attention. Models degrade over time, new data becomes available, and deployment environments evolve.&lt;/p&gt;&lt;p&gt;A dedicated team provides long-term ownership, cross-functional expertise, faster iteration, and risk management. Having a dedicated software development team that knows what it’s doing, how it’s doing it, and can keep doing it for you in the long run is ideal and works a lot better than having one-off consultants.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-best-practices-for-successful-devops-in-ai"&gt;Best practices for successful DevOps in AI&lt;/h3&gt;&lt;p&gt;Even with the right tools and teams, success in DevOps for AI depends on following solid best practices.&lt;/p&gt;&lt;p&gt;These include:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Version everything: Code, data, and models should all have clear version control.&lt;/li&gt;&lt;li&gt;Test for more than accuracy: Include checks for fairness, bias, and explainability.&lt;/li&gt;&lt;li&gt;Use containers for consistency: Containerising ML pipelines ensures models run the same in every environment.&lt;/li&gt;&lt;li&gt;Automate retraining triggers: Set thresholds for data drift or performance declines that trigger retraining jobs automatically.&lt;/li&gt;&lt;li&gt;Integrate monitoring into pipelines: Collect metrics on latency, accuracy, and use in real time.&lt;/li&gt;&lt;li&gt;Collaborate in roles: Encourage shared responsibility between data scientists, engineers, and operations teams.&lt;/li&gt;&lt;li&gt;Plan for scalability: Build pipelines that can handle growing datasets and user demand without major rework.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These practices transform a machine learning pipeline from experimental systems into production-ready infrastructure.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-conclusion"&gt;Conclusion&lt;/h3&gt;&lt;p&gt;The future of artificial intelligence depends on a reliable and scalable machine learning deployment pipeline. As a business, it’s paramount to implement AI in highly-specific ways to create digital services and products.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/Picture1.jpg" /&gt;&lt;/div&gt;&lt;p&gt;AI’s effects on continuous development and deployment pipelines are becoming difficult to ignore. However, decision-makers in software development functions need to consider a broad range of elements when considering the uses of the technology.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-challenges-of-deploying-ai-at-scale"&gt;The challenges of deploying AI at scale&lt;/h3&gt;&lt;p&gt;Deploying artificial intelligence isn’t the same as deploying, for example, a web app. Traditional software updates are usually deterministic: once code passes tests, everything works as it’s meant to. With AI and machine learning, outputs can vary because models depend on ever-changing data and complex statistical behaviour.&lt;/p&gt;&lt;p&gt;Some unique challenges you’ll face include:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Data drift: Your training data may not match real-world use, causing performance to decline.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Model versioning: Unlike simple code updates, you need to track both the model and the data it was trained on.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Long training times: Iterating on a new model can take hours or even days, slowing down releases.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Hardware needs: Training and inference often require GPUs or specialised infrastructure.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Monitoring complexity: Tracking performance in production means watching not just uptime but also accuracy, bias, and fairness.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The challenges mean you can’t treat AI like traditional software. You need machine learning pipelines built with automation and monitoring.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-applying-devops-principles-to-ai-systems"&gt;Applying DevOps principles to AI systems&lt;/h3&gt;&lt;p&gt;DevOps was designed to bring developers and operations closer by promoting automation, collaboration, and fast feedback loops. When you bring these principles to AI, so AI and DevOps, you create a foundation for scalable machine learning deployment pipelines.&lt;/p&gt;&lt;p&gt;Some DevOps best practices translate directly:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Automation: Automating training, testing, and deployment reduces manual errors and saves time.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Continuous integration: Code, data, and model updates should all be integrated and tested regularly.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Monitoring and observability: Just like server uptime, models need monitoring for drift and accuracy.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Collaboration: Data scientists, engineers, and operations teams need to work together in the same cycle.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The main difference between DevOps and MLOps lies in the focus. While DevOps centres on code, MLOps is about managing models and datasets alongside code. MLOps extends DevOps to address challenges specific to machine learning pipelines, like data validation, experiment tracking, and retraining strategies.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-designing-a-continuous-deployment-pipeline-for-machine-learning"&gt;Designing a continuous deployment pipeline for machine learning&lt;/h3&gt;&lt;p&gt;When building a continuous deployment system for ML, you need to think beyond just code. Gone are the days of just needing to know how to programme and code; now it’s about much more. Having an artificial intelligence development company that can implement these stages for you is crucial. A step-by-step framework could look like this:&lt;/p&gt;&lt;ol class="wp-block-list"&gt;&lt;li&gt;Data ingestion and validation: Collect data from multiple sources, validate it for quality, and ensure privacy compliance. For example, a healthcare company might verify that patient data is anonymised before use.&lt;/li&gt;&lt;li&gt;Model training and versioning: Train models in controlled environments and store them with a clear version history. Fintech companies often keep a strict record of which datasets and algorithms power models that impact credit scoring.&lt;/li&gt;&lt;li&gt;Automated testing: Validate accuracy, bias, and performance before models move forward. This prevents unreliable models from reaching production.&lt;/li&gt;&lt;li&gt;Deployment to staging: Push models to a staging environment first to test integration with real services.&lt;/li&gt;&lt;li&gt;Production deployment: Deploy with automation, often using containers and orchestration systems like Kubernetes.&lt;/li&gt;&lt;li&gt;Monitoring and feedback loops: Track performance in production, watch for drift, and trigger retraining when thresholds are met.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;By designing an ML pipeline this way, you minimise risks, comply with regulations, and ensure reliable performance in high-stakes industries like healthcare and finance.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-role-of-a-dedicated-development-team-in-mlops"&gt;The Role of a dedicated development team in MLOps&lt;/h3&gt;&lt;p&gt;You may wonder whether you need a dedicated software development team for MLOps or if hiring consultants is enough. The reality is that one-off consultants often provide short-term fixes, but machine learning pipelines require ongoing attention. Models degrade over time, new data becomes available, and deployment environments evolve.&lt;/p&gt;&lt;p&gt;A dedicated team provides long-term ownership, cross-functional expertise, faster iteration, and risk management. Having a dedicated software development team that knows what it’s doing, how it’s doing it, and can keep doing it for you in the long run is ideal and works a lot better than having one-off consultants.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-best-practices-for-successful-devops-in-ai"&gt;Best practices for successful DevOps in AI&lt;/h3&gt;&lt;p&gt;Even with the right tools and teams, success in DevOps for AI depends on following solid best practices.&lt;/p&gt;&lt;p&gt;These include:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Version everything: Code, data, and models should all have clear version control.&lt;/li&gt;&lt;li&gt;Test for more than accuracy: Include checks for fairness, bias, and explainability.&lt;/li&gt;&lt;li&gt;Use containers for consistency: Containerising ML pipelines ensures models run the same in every environment.&lt;/li&gt;&lt;li&gt;Automate retraining triggers: Set thresholds for data drift or performance declines that trigger retraining jobs automatically.&lt;/li&gt;&lt;li&gt;Integrate monitoring into pipelines: Collect metrics on latency, accuracy, and use in real time.&lt;/li&gt;&lt;li&gt;Collaborate in roles: Encourage shared responsibility between data scientists, engineers, and operations teams.&lt;/li&gt;&lt;li&gt;Plan for scalability: Build pipelines that can handle growing datasets and user demand without major rework.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These practices transform a machine learning pipeline from experimental systems into production-ready infrastructure.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-conclusion"&gt;Conclusion&lt;/h3&gt;&lt;p&gt;The future of artificial intelligence depends on a reliable and scalable machine learning deployment pipeline. As a business, it’s paramount to implement AI in highly-specific ways to create digital services and products.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/devops-for-ai-continuous-deployment-pipelines-for-machine-learning-systems/</guid><pubDate>Mon, 03 Nov 2025 10:31:40 +0000</pubDate></item><item><title>[NEW] This startup wants to clean up the copper industry (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/03/1127474/copper-smelting-chemistry-clean/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Demand for copper is surging, as is pollution from its dirty production processes. The founders of one startup, Still Bright, think they have a better, cleaner way to generate the copper the world needs.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The company uses water-based reactions, based on battery chemistry technology, to purify copper in a process that could be less polluting than traditional smelting. The hope is that this alternative will also help ease growing strain on the copper supply chain.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;“We’re really focused on addressing the copper supply crisis that’s looming ahead of us,” says Randy Allen, Still Bright’s cofounder and CEO.&lt;/p&gt;  &lt;p&gt;Copper is a crucial ingredient in everything from electrical wiring to cookware today. And clean energy technologies like solar panels and electric vehicles are introducing even more demand for the metal. Global copper demand is expected to grow by 40% between now and 2040.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;As demand swells, so do the climate and environmental impacts of copper extraction, the process of refining ore into a pure metal. There’s also growing concern about the geographic concentration of the copper supply chain. Copper is mined all over the world, and historically, many of those mines had smelters on-site to process what they extracted. (Smelters form pure copper metal by essentially burning concentrated copper ore at high temperatures.) But today, the smelting industry has consolidated, with many mines shipping copper concentrates to smelters in Asia, particularly China.&lt;/p&gt;  &lt;p&gt;That’s partly because smelting uses a lot of energy and chemicals, and it can produce sulfur-containing emissions that can harm air quality. “They shipped the environmental and social problems elsewhere,” says Simon Jowitt, a professor at the University of Nevada, Reno, and director of the Nevada Bureau of Mines and Geology.&lt;/p&gt; 
 &lt;p&gt;It’s possible to scrub pollution out of a smelter’s emissions, and smelters are much cleaner than they used to be, Jowitt says. But overall, smelting centers aren’t exactly known for environmental responsibility.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So even countries like the US, which have plenty of copper reserves and operational mines, largely ship copper concentrates, which contain up to around 30% copper, to China or other countries for smelting. (There are just two operational ore smelters in the US today.)&lt;/p&gt;  &lt;p&gt;Still Bright avoids the pyrometallurgic process that smelters use in favor of a chemical approach, partially inspired by devices called vanadium flow batteries.&lt;/p&gt;  &lt;p&gt;In the startup’s reactor, vanadium reacts with the copper compounds in copper concentrates. The copper metal remains a solid, leaving many of the impurities behind in the liquid phase. The whole thing takes between 30 and 90 minutes. The solid, which contains roughly 70% copper after this reaction, can then be fed into another, established process in the mining industry, called solvent extraction and electrowinning, to make copper that’s over 99% pure.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt;&lt;p&gt;This is far from the first attempt to use a water-based, chemical approach to processing copper. Today, some copper ore is processed with acid, for example, and Ceibo, a startup based in Chile, is trying to use a version of that process on the type of copper that’s traditionally smelted. The difference here is the particular chemistry, particularly the choice to use vanadium.&lt;/p&gt;  &lt;p&gt;One of Still Bright’s founders, Jon Vardner, was researching copper reactions and vanadium flow batteries when he came up with the idea to marry a copper extraction reaction with an electrical charging step that could recycle the vanadium.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="worker in the lab" class="wp-image-1127471" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Copy-of-HAX_MeasuringConcentrate_Max.jpg?w=734" /&gt;&lt;div class="image-credit"&gt;COURTESY OF STILL BRIGHT&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;After the vanadium reacts with the copper, the liquid soup can be fed into an electrolyzer, which uses electricity to turn the vanadium back into a form that can react with copper again. It’s basically the same process that vanadium flow batteries use to charge up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;While other chemical processes for copper refining require high temperatures or extremely acidic conditions to get the copper into solution and force the reaction to proceed quickly and ensure all the copper gets reacted, Still Bright’s process can run at ambient temperatures.&lt;/p&gt; 

 &lt;p&gt;One of the major benefits to this approach is cutting the pollution from copper refining.&amp;nbsp; Traditional smelting heats the target material to over 1,200 °C (2,000 °F), forming sulfur-containing gases that are released into the atmosphere.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Still Bright’s process produces hydrogen sulfide gas as a by-product instead. It’s still a dangerous material, but one that can be effectively captured and converted into useful side products, Allen says.&lt;/p&gt;  &lt;p&gt;Another source of potential pollution is the sulfide minerals left over after the refining process, which can form sulfuric acid when exposed to air and water (this is called acid mine drainage, common in mining waste). Still Bright’s process will also produce that material, and the company plans to carefully track it, ensuring that it doesn’t leak into groundwater.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The company is currently testing its process in the lab in New Jersey and designing a pilot facility in Colorado, which will have the capacity to make about two tons of copper per year. Next will be a demonstration-scale reactor, which will have a 500-ton annual capacity and should come online in 2027 or 2028 at a mine site, Allen says. Still Bright recently raised an $18.7 million seed round to help with the scale-up process.&lt;/p&gt;  &lt;p&gt;How scale up goes will be a crucial test of the technology and whether the typically conservative mining industry will jump on board, UNR’s Jowitt says: “You want to see what happens on an industrial scale. And I think until that happens, people might be a little reluctant to get into this.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Demand for copper is surging, as is pollution from its dirty production processes. The founders of one startup, Still Bright, think they have a better, cleaner way to generate the copper the world needs.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The company uses water-based reactions, based on battery chemistry technology, to purify copper in a process that could be less polluting than traditional smelting. The hope is that this alternative will also help ease growing strain on the copper supply chain.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;“We’re really focused on addressing the copper supply crisis that’s looming ahead of us,” says Randy Allen, Still Bright’s cofounder and CEO.&lt;/p&gt;  &lt;p&gt;Copper is a crucial ingredient in everything from electrical wiring to cookware today. And clean energy technologies like solar panels and electric vehicles are introducing even more demand for the metal. Global copper demand is expected to grow by 40% between now and 2040.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;As demand swells, so do the climate and environmental impacts of copper extraction, the process of refining ore into a pure metal. There’s also growing concern about the geographic concentration of the copper supply chain. Copper is mined all over the world, and historically, many of those mines had smelters on-site to process what they extracted. (Smelters form pure copper metal by essentially burning concentrated copper ore at high temperatures.) But today, the smelting industry has consolidated, with many mines shipping copper concentrates to smelters in Asia, particularly China.&lt;/p&gt;  &lt;p&gt;That’s partly because smelting uses a lot of energy and chemicals, and it can produce sulfur-containing emissions that can harm air quality. “They shipped the environmental and social problems elsewhere,” says Simon Jowitt, a professor at the University of Nevada, Reno, and director of the Nevada Bureau of Mines and Geology.&lt;/p&gt; 
 &lt;p&gt;It’s possible to scrub pollution out of a smelter’s emissions, and smelters are much cleaner than they used to be, Jowitt says. But overall, smelting centers aren’t exactly known for environmental responsibility.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So even countries like the US, which have plenty of copper reserves and operational mines, largely ship copper concentrates, which contain up to around 30% copper, to China or other countries for smelting. (There are just two operational ore smelters in the US today.)&lt;/p&gt;  &lt;p&gt;Still Bright avoids the pyrometallurgic process that smelters use in favor of a chemical approach, partially inspired by devices called vanadium flow batteries.&lt;/p&gt;  &lt;p&gt;In the startup’s reactor, vanadium reacts with the copper compounds in copper concentrates. The copper metal remains a solid, leaving many of the impurities behind in the liquid phase. The whole thing takes between 30 and 90 minutes. The solid, which contains roughly 70% copper after this reaction, can then be fed into another, established process in the mining industry, called solvent extraction and electrowinning, to make copper that’s over 99% pure.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt;&lt;p&gt;This is far from the first attempt to use a water-based, chemical approach to processing copper. Today, some copper ore is processed with acid, for example, and Ceibo, a startup based in Chile, is trying to use a version of that process on the type of copper that’s traditionally smelted. The difference here is the particular chemistry, particularly the choice to use vanadium.&lt;/p&gt;  &lt;p&gt;One of Still Bright’s founders, Jon Vardner, was researching copper reactions and vanadium flow batteries when he came up with the idea to marry a copper extraction reaction with an electrical charging step that could recycle the vanadium.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="worker in the lab" class="wp-image-1127471" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Copy-of-HAX_MeasuringConcentrate_Max.jpg?w=734" /&gt;&lt;div class="image-credit"&gt;COURTESY OF STILL BRIGHT&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;After the vanadium reacts with the copper, the liquid soup can be fed into an electrolyzer, which uses electricity to turn the vanadium back into a form that can react with copper again. It’s basically the same process that vanadium flow batteries use to charge up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;While other chemical processes for copper refining require high temperatures or extremely acidic conditions to get the copper into solution and force the reaction to proceed quickly and ensure all the copper gets reacted, Still Bright’s process can run at ambient temperatures.&lt;/p&gt; 

 &lt;p&gt;One of the major benefits to this approach is cutting the pollution from copper refining.&amp;nbsp; Traditional smelting heats the target material to over 1,200 °C (2,000 °F), forming sulfur-containing gases that are released into the atmosphere.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Still Bright’s process produces hydrogen sulfide gas as a by-product instead. It’s still a dangerous material, but one that can be effectively captured and converted into useful side products, Allen says.&lt;/p&gt;  &lt;p&gt;Another source of potential pollution is the sulfide minerals left over after the refining process, which can form sulfuric acid when exposed to air and water (this is called acid mine drainage, common in mining waste). Still Bright’s process will also produce that material, and the company plans to carefully track it, ensuring that it doesn’t leak into groundwater.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The company is currently testing its process in the lab in New Jersey and designing a pilot facility in Colorado, which will have the capacity to make about two tons of copper per year. Next will be a demonstration-scale reactor, which will have a 500-ton annual capacity and should come online in 2027 or 2028 at a mine site, Allen says. Still Bright recently raised an $18.7 million seed round to help with the scale-up process.&lt;/p&gt;  &lt;p&gt;How scale up goes will be a crucial test of the technology and whether the typically conservative mining industry will jump on board, UNR’s Jowitt says: “You want to see what happens on an industrial scale. And I think until that happens, people might be a little reluctant to get into this.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/03/1127474/copper-smelting-chemistry-clean/</guid><pubDate>Mon, 03 Nov 2025 11:00:00 +0000</pubDate></item><item><title>[NEW] From ambition to accountability: Quantifying AI ROI in strategy (AI News)</title><link>https://www.artificialintelligence-news.com/news/quantifying-ai-roi-leading-resolutions/</link><description>&lt;p&gt;For many UK executives, AI investment has become a necessity, not an experiment in innovation. Boards now demand evidence of measurable impact – whether through efficiency gains, revenue growth, or reduced operational risk. Yet, as Pete Smyth, CEO of Leading Resolutions notes, many SMEs treat AI as an exploratory exercise, not a structured business strategy. The result is wasted investment and a lack of demonstrable return.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-business-impact"&gt;Business impact&lt;/h3&gt;&lt;p&gt;Enterprises implementing AI effectively are doing so with a focus on business outcomes. Instead of isolated pilots, they align initiatives with strategic goals – optimising operations and enhancing customer experience, for example. Leaders of organisations of any size can transform AI from a speculative technology into performance improvement by translating their ambitions into quantifiable metrics.&lt;/p&gt;&lt;p&gt;Smyth gives examples that include automating routine analysis to reduce manual workflows, applying predictive analytics for inventory optimisation, or using natural language models to streamline customer service. The impact is measurable, he says: improved margins, faster decisions, and business resilience.&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-110164" height="306" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/pete-smyth.webp" width="360" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Pete Smyth, Leading Resolutions&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-implementation-amp-challenges"&gt;Implementation &amp;amp; challenges&lt;/h3&gt;&lt;p&gt;According to Smyth’s Leading Resolutions, implementation success depends on priorities. The process begins with stakeholder engagement that identifies potential uses for AI in different departments. Each idea is evaluated for business value and readiness to implement; these processes produce a shortlist for potential pilot schemes.&lt;/p&gt;&lt;p&gt;Next comes structured value assessment, combining cost-benefit analysis with execution feasibility and risk tolerance. Leaders should agree on the metrics that would define success before any pilot begins. These might include tracking KPIs (cost reduction, customer retention, productivity gains, etc.). Once validated, AI’s use can be scaled carefully in discrete business units.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-strategic-takeaway"&gt;Strategic takeaway&lt;/h3&gt;&lt;p&gt;For data leaders and business decision-makers, measurable ROI requires a practically-based shift from experimentation to operational accountability. Focus should be on three principles, Smyth posits:&lt;/p&gt;&lt;ol class="wp-block-list"&gt;&lt;li&gt;Tie AI projects directly to business outcomes with pre-agreed KPIs.&lt;/li&gt;&lt;li&gt;Embed governance, risk controls, and explainability early.&lt;/li&gt;&lt;li&gt;Build an AI culture grounded in data quality, collaboration, and evidence-based decision-making.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;As enterprises navigate tighter regulation and rising AI expectations, success depends not on how much they invest, but how effectively they quantify and scale positive results. Moving from speculative ambition to measurable performance is the hallmark of credible AI implementation.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Main image source: “M4 AT Night” by Paulio Geordio is licensed under CC BY 2.0.)&lt;/em&gt;&lt;/p&gt;&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;For many UK executives, AI investment has become a necessity, not an experiment in innovation. Boards now demand evidence of measurable impact – whether through efficiency gains, revenue growth, or reduced operational risk. Yet, as Pete Smyth, CEO of Leading Resolutions notes, many SMEs treat AI as an exploratory exercise, not a structured business strategy. The result is wasted investment and a lack of demonstrable return.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-business-impact"&gt;Business impact&lt;/h3&gt;&lt;p&gt;Enterprises implementing AI effectively are doing so with a focus on business outcomes. Instead of isolated pilots, they align initiatives with strategic goals – optimising operations and enhancing customer experience, for example. Leaders of organisations of any size can transform AI from a speculative technology into performance improvement by translating their ambitions into quantifiable metrics.&lt;/p&gt;&lt;p&gt;Smyth gives examples that include automating routine analysis to reduce manual workflows, applying predictive analytics for inventory optimisation, or using natural language models to streamline customer service. The impact is measurable, he says: improved margins, faster decisions, and business resilience.&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-110164" height="306" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/pete-smyth.webp" width="360" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Pete Smyth, Leading Resolutions&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-implementation-amp-challenges"&gt;Implementation &amp;amp; challenges&lt;/h3&gt;&lt;p&gt;According to Smyth’s Leading Resolutions, implementation success depends on priorities. The process begins with stakeholder engagement that identifies potential uses for AI in different departments. Each idea is evaluated for business value and readiness to implement; these processes produce a shortlist for potential pilot schemes.&lt;/p&gt;&lt;p&gt;Next comes structured value assessment, combining cost-benefit analysis with execution feasibility and risk tolerance. Leaders should agree on the metrics that would define success before any pilot begins. These might include tracking KPIs (cost reduction, customer retention, productivity gains, etc.). Once validated, AI’s use can be scaled carefully in discrete business units.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-strategic-takeaway"&gt;Strategic takeaway&lt;/h3&gt;&lt;p&gt;For data leaders and business decision-makers, measurable ROI requires a practically-based shift from experimentation to operational accountability. Focus should be on three principles, Smyth posits:&lt;/p&gt;&lt;ol class="wp-block-list"&gt;&lt;li&gt;Tie AI projects directly to business outcomes with pre-agreed KPIs.&lt;/li&gt;&lt;li&gt;Embed governance, risk controls, and explainability early.&lt;/li&gt;&lt;li&gt;Build an AI culture grounded in data quality, collaboration, and evidence-based decision-making.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;As enterprises navigate tighter regulation and rising AI expectations, success depends not on how much they invest, but how effectively they quantify and scale positive results. Moving from speculative ambition to measurable performance is the hallmark of credible AI implementation.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Main image source: “M4 AT Night” by Paulio Geordio is licensed under CC BY 2.0.)&lt;/em&gt;&lt;/p&gt;&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/quantifying-ai-roi-leading-resolutions/</guid><pubDate>Mon, 03 Nov 2025 11:45:00 +0000</pubDate></item></channel></rss>