<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 17 Dec 2025 18:36:22 +0000</lastBuildDate><item><title>[NEW] AI is moving to the edge – and network security needs to catch up (AI | VentureBeat)</title><link>https://venturebeat.com/ai/ai-is-moving-to-the-edge-and-network-security-needs-to-catch-up</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by T-Mobile for Business&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;Small and mid-sized businesses are adopting AI at a pace that would have seemed unrealistic even a few years ago. Smart assistants that greet customers, predictive tools that flag inventory shortages before they happen, and on-site analytics that help staff make decisions faster — these used to be features of the enterprise. Now they’re being deployed in retail storefronts, regional medical clinics, branch offices, and remote operations hubs.&lt;/p&gt;&lt;p&gt;What’s changed is not just the AI itself, but where it runs. Increasingly, AI workloads are being pushed out of centralized data centers and into the real world — into the places where employees work and customers interact. This shift to the edge promises faster insights and more resilient operations, but it also transforms the demands placed on the network. Edge sites need consistent bandwidth, real-time data pathways, and the ability to process information locally rather than relying on the cloud for every decision.&lt;/p&gt;&lt;p&gt;The catch is that as companies race to connect these locations, security often lags behind. A store may adopt AI-enabled cameras or sensors long before it has the policies to manage them. A clinic may roll out mobile diagnostic devices without fully segmenting their traffic. A warehouse may rely on a mix of Wi-Fi, wired, and cellular connections that weren’t designed to support AI-driven operations. When connectivity scales faster than security, it creates cracks — unmonitored devices, inconsistent access controls, and unsegmented data flows that make it hard to see what’s happening, let alone protect it.&lt;/p&gt;&lt;p&gt;Edge AI only delivers its full value when connectivity and security evolve together.&lt;/p&gt;&lt;h3&gt;Why AI is moving to the edge — and what that breaks&lt;/h3&gt;&lt;p&gt;Businesses are shifting AI to the edge for three core reasons:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Real-time responsiveness: &lt;/b&gt;Some decisions can’t wait for a round trip to the cloud. Whether it’s identifying an item on a shelf, detecting an abnormal reading from a medical device, or recognizing a safety risk in a warehouse aisle, the delay introduced by centralized processing can mean missed opportunities or slow reactions.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Resilience and privacy: &lt;/b&gt;Keeping data and inference local makes operations less vulnerable to outages or latency spikes, and it reduces the flow of sensitive information across networks. This helps SMBs meet data sovereignty and compliance requirements without rewriting their entire infrastructure.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Mobility and deployment speed: &lt;/b&gt;Many SMBs operate across distributed footprints — remote workers, pop-up locations, seasonal operations, or mobile teams. Wireless-first connectivity, including 5G business lines, lets them deploy AI tools quickly without waiting for fixed circuits or expensive buildouts.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Technologies like Edge Control from &lt;a href="https://tmobilebusinessgroup.sjv.io/c/2562575/3301556/37552"&gt;T-Mobile for Business&lt;/a&gt; fit naturally into this model. By routing traffic directly along the paths it needs — keeping latency-sensitive workloads local and bypassing the bottlenecks that traditional VPNs introduce — businesses can adopt edge AI without dragging their network into constant contention.&lt;/p&gt;&lt;p&gt;Yet the shift introduces new risk. Every edge site becomes, in effect, its own small data center. A retail store may have cameras, sensors, POS systems, digital signage, and staff devices all sharing the same access point. A clinic may run diagnostic tools, tablets, wearables, and video consult systems side by side. A manufacturing floor might combine robotics, sensors, handheld scanners, and on-site analytics platforms.&lt;/p&gt;&lt;p&gt;This diversity increases the attack surface dramatically. Many SMBs roll out connectivity first, then add piecemeal security later — leaving the blind spots attackers rely on.&lt;/p&gt;&lt;h3&gt;Zero trust becomes essential at the edge&lt;/h3&gt;&lt;p&gt;When AI is distributed across dozens or hundreds of sites, the old idea of a single secure “inside” network breaks down. Every store, clinic, kiosk, or field location becomes its own micro-environment — and every device within it becomes its own potential entry point.&lt;/p&gt;&lt;p&gt;Zero trust offers a framework to make this manageable.&lt;/p&gt;&lt;p&gt;At the edge, zero trust means:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Verifying identity rather than location &lt;/b&gt;— access is granted because a user or device proves who it is, not because it sits behind a corporate firewall.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Continuous authentication&lt;/b&gt; — trust isn’t permanent; it’s re-evaluated throughout a session.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Segmentation that limits movement&lt;/b&gt; — if something goes wrong, attackers can’t jump freely from system to system.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This approach is especially critical given that many edge devices can’t run traditional security clients. SIM-based identity and secure mobile connectivity — areas where T-Mobile for Business brings significant strength — help verify IoT devices, 5G routers, and sensors that otherwise sit outside the visibility of IT teams.&lt;/p&gt;&lt;p&gt;This is why connectivity providers are increasingly combining networking and security into a single approach. T-Mobile for Business embeds segmentation, device visibility, and zero-trust safeguards directly into its wireless-first connectivity offerings, reducing the need for SMBs to stitch together multiple tools.&lt;/p&gt;&lt;h3&gt;Secure-by-default networks reshape the landscape&lt;/h3&gt;&lt;p&gt;A major architectural shift is underway: networks that assume every device, session, and workload must be authenticated, segmented, and monitored from the start. Instead of building security on top of connectivity, the two are fused.&lt;/p&gt;&lt;p&gt;&lt;a href="https://tmobilebusinessgroup.sjv.io/c/2562575/3301556/37552"&gt;T-Mobile for Business&lt;/a&gt; solutions shows how this is evolving. Its SASE platform, powered by Palo Alto Networks Prisma SASE 5G, blends secure access with connectivity into one cloud-delivered service. Private Access gives users the least-privileged access they need, nothing more. T-SIMsecure authenticates devices at the SIM layer, allowing IoT sensors and 5G routers to be verified automatically. Security Slice isolates sensitive SASE traffic on a dedicated portion of the 5G network, ensuring consistency even during heavy demand.&lt;/p&gt;&lt;p&gt;A unified dashboard like T-Platform brings it together, offering real-time visibility across SASE, IoT, business internet, and edge control — simplifying operations for SMBs with limited staff.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The future: AI that runs the edge and protects it&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;As AI models become more dynamic and autonomous, we’ll see the relationship flip: the edge won’t just support AI; AI will actively run and secure the edge — optimizing traffic paths, adjusting segmentation automatically, and spotting anomalies that matter to one specific store or site.&lt;/p&gt;&lt;p&gt;Self-healing networks and adaptive policy engines will move from experimental to expected.&lt;/p&gt;&lt;p&gt;For SMBs, this is a pivotal moment. The organizations that modernize their connectivity and security foundations now will be the ones best positioned to scale AI everywhere — safely, confidently, and without unnecessary complexity.&lt;/p&gt;&lt;p&gt;Partners like&lt;a href="https://tmobilebusinessgroup.sjv.io/c/2562575/3301556/37552"&gt; T-Mobile for Business&lt;/a&gt; are already moving in this direction, giving SMBs a way to deploy AI at the edge without sacrificing control or visibility.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by T-Mobile for Business&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;Small and mid-sized businesses are adopting AI at a pace that would have seemed unrealistic even a few years ago. Smart assistants that greet customers, predictive tools that flag inventory shortages before they happen, and on-site analytics that help staff make decisions faster — these used to be features of the enterprise. Now they’re being deployed in retail storefronts, regional medical clinics, branch offices, and remote operations hubs.&lt;/p&gt;&lt;p&gt;What’s changed is not just the AI itself, but where it runs. Increasingly, AI workloads are being pushed out of centralized data centers and into the real world — into the places where employees work and customers interact. This shift to the edge promises faster insights and more resilient operations, but it also transforms the demands placed on the network. Edge sites need consistent bandwidth, real-time data pathways, and the ability to process information locally rather than relying on the cloud for every decision.&lt;/p&gt;&lt;p&gt;The catch is that as companies race to connect these locations, security often lags behind. A store may adopt AI-enabled cameras or sensors long before it has the policies to manage them. A clinic may roll out mobile diagnostic devices without fully segmenting their traffic. A warehouse may rely on a mix of Wi-Fi, wired, and cellular connections that weren’t designed to support AI-driven operations. When connectivity scales faster than security, it creates cracks — unmonitored devices, inconsistent access controls, and unsegmented data flows that make it hard to see what’s happening, let alone protect it.&lt;/p&gt;&lt;p&gt;Edge AI only delivers its full value when connectivity and security evolve together.&lt;/p&gt;&lt;h3&gt;Why AI is moving to the edge — and what that breaks&lt;/h3&gt;&lt;p&gt;Businesses are shifting AI to the edge for three core reasons:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Real-time responsiveness: &lt;/b&gt;Some decisions can’t wait for a round trip to the cloud. Whether it’s identifying an item on a shelf, detecting an abnormal reading from a medical device, or recognizing a safety risk in a warehouse aisle, the delay introduced by centralized processing can mean missed opportunities or slow reactions.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Resilience and privacy: &lt;/b&gt;Keeping data and inference local makes operations less vulnerable to outages or latency spikes, and it reduces the flow of sensitive information across networks. This helps SMBs meet data sovereignty and compliance requirements without rewriting their entire infrastructure.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Mobility and deployment speed: &lt;/b&gt;Many SMBs operate across distributed footprints — remote workers, pop-up locations, seasonal operations, or mobile teams. Wireless-first connectivity, including 5G business lines, lets them deploy AI tools quickly without waiting for fixed circuits or expensive buildouts.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Technologies like Edge Control from &lt;a href="https://tmobilebusinessgroup.sjv.io/c/2562575/3301556/37552"&gt;T-Mobile for Business&lt;/a&gt; fit naturally into this model. By routing traffic directly along the paths it needs — keeping latency-sensitive workloads local and bypassing the bottlenecks that traditional VPNs introduce — businesses can adopt edge AI without dragging their network into constant contention.&lt;/p&gt;&lt;p&gt;Yet the shift introduces new risk. Every edge site becomes, in effect, its own small data center. A retail store may have cameras, sensors, POS systems, digital signage, and staff devices all sharing the same access point. A clinic may run diagnostic tools, tablets, wearables, and video consult systems side by side. A manufacturing floor might combine robotics, sensors, handheld scanners, and on-site analytics platforms.&lt;/p&gt;&lt;p&gt;This diversity increases the attack surface dramatically. Many SMBs roll out connectivity first, then add piecemeal security later — leaving the blind spots attackers rely on.&lt;/p&gt;&lt;h3&gt;Zero trust becomes essential at the edge&lt;/h3&gt;&lt;p&gt;When AI is distributed across dozens or hundreds of sites, the old idea of a single secure “inside” network breaks down. Every store, clinic, kiosk, or field location becomes its own micro-environment — and every device within it becomes its own potential entry point.&lt;/p&gt;&lt;p&gt;Zero trust offers a framework to make this manageable.&lt;/p&gt;&lt;p&gt;At the edge, zero trust means:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Verifying identity rather than location &lt;/b&gt;— access is granted because a user or device proves who it is, not because it sits behind a corporate firewall.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Continuous authentication&lt;/b&gt; — trust isn’t permanent; it’s re-evaluated throughout a session.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Segmentation that limits movement&lt;/b&gt; — if something goes wrong, attackers can’t jump freely from system to system.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This approach is especially critical given that many edge devices can’t run traditional security clients. SIM-based identity and secure mobile connectivity — areas where T-Mobile for Business brings significant strength — help verify IoT devices, 5G routers, and sensors that otherwise sit outside the visibility of IT teams.&lt;/p&gt;&lt;p&gt;This is why connectivity providers are increasingly combining networking and security into a single approach. T-Mobile for Business embeds segmentation, device visibility, and zero-trust safeguards directly into its wireless-first connectivity offerings, reducing the need for SMBs to stitch together multiple tools.&lt;/p&gt;&lt;h3&gt;Secure-by-default networks reshape the landscape&lt;/h3&gt;&lt;p&gt;A major architectural shift is underway: networks that assume every device, session, and workload must be authenticated, segmented, and monitored from the start. Instead of building security on top of connectivity, the two are fused.&lt;/p&gt;&lt;p&gt;&lt;a href="https://tmobilebusinessgroup.sjv.io/c/2562575/3301556/37552"&gt;T-Mobile for Business&lt;/a&gt; solutions shows how this is evolving. Its SASE platform, powered by Palo Alto Networks Prisma SASE 5G, blends secure access with connectivity into one cloud-delivered service. Private Access gives users the least-privileged access they need, nothing more. T-SIMsecure authenticates devices at the SIM layer, allowing IoT sensors and 5G routers to be verified automatically. Security Slice isolates sensitive SASE traffic on a dedicated portion of the 5G network, ensuring consistency even during heavy demand.&lt;/p&gt;&lt;p&gt;A unified dashboard like T-Platform brings it together, offering real-time visibility across SASE, IoT, business internet, and edge control — simplifying operations for SMBs with limited staff.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The future: AI that runs the edge and protects it&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;As AI models become more dynamic and autonomous, we’ll see the relationship flip: the edge won’t just support AI; AI will actively run and secure the edge — optimizing traffic paths, adjusting segmentation automatically, and spotting anomalies that matter to one specific store or site.&lt;/p&gt;&lt;p&gt;Self-healing networks and adaptive policy engines will move from experimental to expected.&lt;/p&gt;&lt;p&gt;For SMBs, this is a pivotal moment. The organizations that modernize their connectivity and security foundations now will be the ones best positioned to scale AI everywhere — safely, confidently, and without unnecessary complexity.&lt;/p&gt;&lt;p&gt;Partners like&lt;a href="https://tmobilebusinessgroup.sjv.io/c/2562575/3301556/37552"&gt; T-Mobile for Business&lt;/a&gt; are already moving in this direction, giving SMBs a way to deploy AI at the edge without sacrificing control or visibility.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/ai-is-moving-to-the-edge-and-network-security-needs-to-catch-up</guid><pubDate>Wed, 17 Dec 2025 08:00:00 +0000</pubDate></item><item><title>Roblox brings AI into the Studio to speed up game creation (AI News)</title><link>https://www.artificialintelligence-news.com/news/roblox-brings-ai-into-the-studio-to-speed-up-game-creation/</link><description>[unable to retrieve full-text content]&lt;p&gt;Roblox is often seen as a games platform, but its day-to-day reality looks closer to a production studio. Small teams release new experiences on a rolling basis and then monetise them at scale. That pace creates two persistent problems: time lost to repeatable production work, and friction when moving outputs between tools. Roblox’s 2025 updates [&amp;#8230;]&lt;/p&gt;
&lt;p&gt;The post &lt;a href="https://www.artificialintelligence-news.com/news/roblox-brings-ai-into-the-studio-to-speed-up-game-creation/"&gt;Roblox brings AI into the Studio to speed up game creation&lt;/a&gt; appeared first on &lt;a href="https://www.artificialintelligence-news.com"&gt;AI News&lt;/a&gt;.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Roblox is often seen as a games platform, but its day-to-day reality looks closer to a production studio. Small teams release new experiences on a rolling basis and then monetise them at scale. That pace creates two persistent problems: time lost to repeatable production work, and friction when moving outputs between tools. Roblox’s 2025 updates [&amp;#8230;]&lt;/p&gt;
&lt;p&gt;The post &lt;a href="https://www.artificialintelligence-news.com/news/roblox-brings-ai-into-the-studio-to-speed-up-game-creation/"&gt;Roblox brings AI into the Studio to speed up game creation&lt;/a&gt; appeared first on &lt;a href="https://www.artificialintelligence-news.com"&gt;AI News&lt;/a&gt;.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/roblox-brings-ai-into-the-studio-to-speed-up-game-creation/</guid><pubDate>Wed, 17 Dec 2025 10:00:00 +0000</pubDate></item><item><title>This Nobel Prize–winning chemist dreams of making water from thin air (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/17/1129259/omar-yaghi-chemist-nobel-prize-crystals-water-air/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Omar Yaghi was a quiet child, diligent, unlikely to roughhouse with his nine siblings. So when he was old enough, his parents tasked him with one of the family’s most vital chores: fetching water. Like most homes in his Palestinian neighborhood in Amman, Jordan, the Yaghis’ had no electricity or running water. At least once every two weeks, the city switched on local taps for a few hours so residents could fill their tanks. Young Omar helped top up the family supply. Decades later, he says he can’t remember once showing up late. The fear of leaving his parents, seven brothers, and two sisters parched kept him punctual.&lt;/p&gt;  &lt;p&gt;Yaghi proved so dependable that his father put him in charge of monitoring how much the cattle destined for the family butcher shop ate and drank. The best-­quality cuts came from well-fed, hydrated animals—a challenge given that they were raised in arid desert.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;Specially designed materials called metal-organic frameworks can pull water from the air like a sponge—and then give it back.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;But at 10 years old, Yaghi learned of a different occupation. Hoping to avoid a rambunctious crowd at recess, he found the library doors in his school unbolted and sneaked in. Thumbing through a chemistry textbook, he saw an image he didn’t understand: little balls connected by sticks in fascinating shapes. Molecules. The building blocks of everything.&lt;/p&gt;  &lt;p&gt;“I didn’t know what they were, but it captivated my attention,” Yaghi says. “I kept trying to figure out what they might be.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;That’s how he discovered chemistry—or maybe how chemistry discovered him. After coming to the United States and, eventually, a postdoctoral program at Harvard University, Yaghi devoted his career to finding ways to make entirely new and fascinating shapes for those little sticks and balls. In October 2025, he was one of three scientists who won a Nobel Prize in chemistry for identifying metal-­organic frameworks, or MOFs—metal ions tethered to organic molecules that form repeating structural landscapes. Today that work is the basis for a new project that sounds like science fiction, or a miracle: conjuring water out of thin air.&lt;/p&gt;  &lt;p&gt;When he first started working with MOFs, Yaghi thought they might be able to absorb climate-damaging carbon dioxide—or maybe hold hydrogen molecules, solving the thorny problem of storing that climate-friendly but hard-to-contain fuel. But then, in 2014, Yaghi’s team of researchers at UC Berkeley had an epiphany. The tiny pores in MOFs could be designed so the material would pull water molecules from the air around them, like a sponge—and then, with just a little heat, give back that water as if squeezed dry. Just one gram of a water-absorbing MOF has an internal surface area of roughly 7,000 square meters.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Yaghi wasn’t the first to try to pull potable water from the atmosphere. But his method could do it at lower levels of humidity than rivals—potentially shaking up a tiny, nascent industry that could be critical to humanity in the thirsty decades to come. Now the company he founded, called Atoco, is racing to demonstrate a pair of machines that Yaghi believes could produce clean, fresh, drinkable water virtually anywhere on Earth, without even hooking up to an energy supply.&lt;/p&gt;  &lt;p&gt;That’s the goal Yaghi has been working toward for more than a decade now, with the rigid determination that he learned while doing chores in his father’s butcher shop.&lt;/p&gt; 
 &lt;p&gt;“It was in that shop where I learned how to perfect things, how to have a work ethic,” he says. “I learned that a job is not done until it is well done. Don’t start a job unless you can finish it.”&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Most of Earth is covered in water, but just 3% of it is fresh, with no salt—the kind of water all terrestrial living things need. Today, desalination plants that take the salt out of seawater provide the bulk of potable water in technologically advanced desert nations like Israel and the United Arab Emirates, but at a high cost. Desalination facilities either heat water to distill out the drinkable stuff or filter it with membranes the salt doesn’t pass through; both methods require a lot of energy and leave behind concentrated brine. Typically desal pumps send that brine back into the ocean, with devastating ecological effects.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="hand holding a ball and stick model" class="wp-image-1129754" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/GettyImages-2239399344.jpg?w=878" /&gt;&lt;figcaption class="wp-element-caption"&gt;Heiner Linke, chair of the Nobel Committee for Chemistry, uses a model to explain how metalorganic frameworks (MOFs) can trap smaller molecules inside. In October 2025, Yaghi and two other scientists won the Nobel Prize in chemistry for identifying MOFs.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;JONATHAN NACKSTRAND/GETTY IMAGES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;I was talking to Atoco executives about carbon dioxide capture earlier this year when they mentioned the possibility of harvesting water from the atmosphere. Of course my mind immediately jumped to Star Wars, and Luke Skywalker working on his family’s moisture farm, using “vaporators” to pull water from the atmosphere of the arid planet Tatooine. (Other sci-fi fans’ minds might go to Dune, and the water-gathering technology of the Fremen.) Could this possibly be real?&lt;/p&gt;  &lt;p&gt;It turns out people have been doing it for millennia. Archaeological evidence of water harvesting from fog dates back as far as 5000 BCE. The ancient Greeks harvested dew, and 500 years ago so did the Inca, using mesh nets and buckets under trees.&lt;/p&gt;  &lt;p&gt;Today, harvesting water from the air is a business already worth billions of dollars, say industry analysts—and it’s on track to be worth billions more in the next five years. In part that’s because typical sources of fresh water are in crisis. Less snowfall in mountains during hotter winters means less meltwater in the spring, which means less water downstream. Droughts regularly break records. Rising seas seep into underground aquifers, already drained by farming and sprawling cities. Aging septic tanks leach bacteria into water, and cancer-causing “forever chemicals” are creating what the US Government Accountability Office last year said “may be the biggest water problem since lead.” That doesn’t even get to the emerging catastrophe from microplastics.&lt;/p&gt;  &lt;p&gt;So lots of places are turning to atmospheric water harvesting. Watergen, an Israel-based company working on the tech, initially planned on deploying in the arid, poorer parts of the world. Instead, buyers in Europe and the United States have approached the company as a way to ensure a clean supply of water. And one of Watergen’s biggest markets is the wealthy United Arab Emirates. “When you say ‘water crisis,’ it’s not just the lack of water—it’s access to good-quality water,” says Anna Chernyavsky, Watergen’s vice president of marketing.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;In other words, the technology “has evolved from lab prototypes to robust, field-deployable systems,” says Guihua Yu, a mechanical engineer at the University of Texas at Austin. “There is still room to improve productivity and energy efficiency in the whole-system level, but so much progress has been steady and encouraging.”&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;MOFs are just the latest approach to the idea. The first generation of commercial tech depended on compressors and refrigerant chemicals—large-scale versions of the machine that keeps food cold and fresh in your kitchen. Both use electricity and a clot of pipes and exchangers to make cold by phase-shifting a chemical from gas to liquid and back; refrigerators try to limit condensation, and water generators basically try to enhance it.&lt;/p&gt;  &lt;p&gt;That’s how Watergen’s tech works: using a compressor and a heat exchanger to wring water from air at humidity levels as low as 20%—Death Valley in the spring. “We’re talking about deserts,” Chernyavsky says. “Below 20%, you get nosebleeds.”&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="children in queue at a blue Watergen dispenser" class="wp-image-1129755" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/IMG_20190322_142704-1.jpg?w=1151" /&gt;&lt;figcaption class="wp-element-caption"&gt;A Watergen unit provides drinking water to students and staff at St. Joseph’s, a girls’ school in Freetown, Sierra Leone. “When you say ‘water crisis,’ it’s not just the lack of water— it’s access to good-quality water,” says Anna Chernyavsky, Watergen’s vice president of marketing.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF WATERGEN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;That still might not be good enough. “Refrigeration works pretty well when you are above a certain relative humidity,” says Sameer Rao, a mechanical engineer at the University of Utah who researches atmospheric water harvesting. “As the environment dries out, you go to lower relative humidities, and it becomes harder and harder. In some cases, it’s impossible for refrigeration-based systems to really work.”&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;So a second wave of technology has found a market. Companies like Source Global use desiccants—substances that absorb moisture from the air, like the silica packets found in vitamin bottles—to pull in moisture and then release it when heated. In theory, the benefit of desiccant-­based tech is that it could absorb water at lower humidity levels, and it uses less energy on the front end since it isn’t running a condenser system. Source Global claims its off-grid, solar-powered system is deployed in dozens of countries.&lt;/p&gt;  &lt;p&gt;But both technologies still require a lot of energy, either to run the heat exchangers or to generate sufficient heat to release water from the desiccants. MOFs, Yaghi hopes, do not. Now Atoco is trying to prove it. Instead of using heat exchangers to bring the air temperature to dew point or desiccants to attract water from the atmosphere, a system can rely on specially designed MOFs to attract water molecules. Atoco’s prototype version uses an MOF that looks like baby powder, stuck to a surface like glass. The pores in the MOF naturally draw in water molecules but remain open, making it theoretically easy to discharge the water with no more heat than what comes from direct sunlight. Atoco’s industrial-scale design uses electricity to speed up the process, but the company is working on a second design that can operate completely off grid, without any energy input.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;Yaghi’s Atoco isn’t the only contender seeking to use MOFs for water harvesting. A competitor, AirJoule, has introduced MOF-based atmospheric water generators in Texas and the UAE and is working with researchers at Arizona State University, planning to deploy more units in the coming months. The company started out trying to build more efficient air-­conditioning for electric buses operating on hot, humid city streets. But then founder Matt Jore heard about US government efforts to harvest water from air—and pivoted. The startup’s stock price has been a bit of a roller-­coaster, but Jore says the sheer size of the market should keep him in business. Take Maricopa County, encompassing Phoenix and its environs—it uses 1.2 billion gallons of water from its shrinking aquifer every day, and another 874 million gallons from surface sources like rivers.&lt;/p&gt;  &lt;p&gt;“So, a couple of billion gallons a day, right?” Jore tells me. “You know how much influx is in the atmosphere every day? Twenty-five billion gallons.”&lt;/p&gt; 
 &lt;p&gt;My eyebrows go up. “Globally?”&lt;/p&gt;  &lt;p&gt;“Just the greater Phoenix area gets influx of about 25 billion gallons of water in the air,” he says. “If you can tap into it, that’s your source. And it’s not going away. It’s all around the world. We view the atmosphere as the world’s free pipeline.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;Besides AirJoule’s head start on Atoco, the companies also differ on where they get their MOFs. AirJoule’s system relies on an off-the-shelf version the company buys from the chemical giant BASF; Atoco aims to use Yaghi’s skill with designing the novel material to create bespoke MOFs for different applications and locations.&lt;/p&gt;  &lt;p&gt;“Given the fact that we have the inventor of the whole class of materials, and we leverage the stuff that comes out of his lab at Berkeley—everything else equal, we have a good starting point to engineer maybe the best materials in the world,” says Magnus Bach, Atoco’s VP of business development.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt;&lt;p&gt;Yaghi envisions a two-pronged product line. Industrial-scale water generators that run on electricity would be capable of producing thousands of liters per day on one end, while units that run on passive systems could operate in remote locations without power, just harnessing energy from the sun and ambient temperatures. In theory, these units could someday replace desalination and even entire municipal water supplies. The next round of field tests is scheduled for early 2026, in the Mojave Desert—one of the hottest, driest places on Earth.&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“That’s my dream,” Yaghi says. “To give people water independence, so they’re not reliant on another party for their lives.”&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;Both Yaghi and Watergen’s Chernyavsky say they’re looking at more decentralized versions that could operate outside municipal utility systems. Home appliances, similar to rooftop solar panels and batteries, could allow households to generate their own water off grid.&lt;/p&gt; 
 &lt;p&gt;That could be tricky, though, without economies of scale to bring down prices. “You have to produce, you have to cool, you have to filter—all in one place,” Chernyavsky says. “So to make it small is very, very challenging.”&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Difficult as that may be, Yaghi’s childhood gave him a particular appreciation for the freedom to go off grid, to liberate the basic necessity of water from the whims of systems that dictate when and how people can access it.&lt;/p&gt; 
 &lt;p&gt;“That’s really my dream,” he says. “To give people independence, water independence, so that they’re not reliant on another party for their livelihood or lives.”&lt;/p&gt;  &lt;p&gt;Toward the end of one of our conversations, I asked Yaghi what he would tell the younger version of himself if he could. “Jordan is one of the worst countries in terms of the impact of water stress,” he said. “I would say, ‘Continue to be diligent and observant. It doesn’t really matter what you’re pursuing, as long as you’re passionate.’”&lt;/p&gt;  &lt;p&gt;I pressed him for something more specific: “What do you think he’d say when you described this technology to him?”&lt;/p&gt;  &lt;p&gt;Yaghi smiled: “I think young Omar would think you’re putting him on, that this is all fictitious and you’re trying to take something from him.” This reality, in other words, would be beyond young Omar’s wildest dreams.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Alexander C. Kaufman is a reporter who has covered energy, climate change, pollution, business, and geopolitics for more than a decade.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Omar Yaghi was a quiet child, diligent, unlikely to roughhouse with his nine siblings. So when he was old enough, his parents tasked him with one of the family’s most vital chores: fetching water. Like most homes in his Palestinian neighborhood in Amman, Jordan, the Yaghis’ had no electricity or running water. At least once every two weeks, the city switched on local taps for a few hours so residents could fill their tanks. Young Omar helped top up the family supply. Decades later, he says he can’t remember once showing up late. The fear of leaving his parents, seven brothers, and two sisters parched kept him punctual.&lt;/p&gt;  &lt;p&gt;Yaghi proved so dependable that his father put him in charge of monitoring how much the cattle destined for the family butcher shop ate and drank. The best-­quality cuts came from well-fed, hydrated animals—a challenge given that they were raised in arid desert.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;Specially designed materials called metal-organic frameworks can pull water from the air like a sponge—and then give it back.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;But at 10 years old, Yaghi learned of a different occupation. Hoping to avoid a rambunctious crowd at recess, he found the library doors in his school unbolted and sneaked in. Thumbing through a chemistry textbook, he saw an image he didn’t understand: little balls connected by sticks in fascinating shapes. Molecules. The building blocks of everything.&lt;/p&gt;  &lt;p&gt;“I didn’t know what they were, but it captivated my attention,” Yaghi says. “I kept trying to figure out what they might be.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;That’s how he discovered chemistry—or maybe how chemistry discovered him. After coming to the United States and, eventually, a postdoctoral program at Harvard University, Yaghi devoted his career to finding ways to make entirely new and fascinating shapes for those little sticks and balls. In October 2025, he was one of three scientists who won a Nobel Prize in chemistry for identifying metal-­organic frameworks, or MOFs—metal ions tethered to organic molecules that form repeating structural landscapes. Today that work is the basis for a new project that sounds like science fiction, or a miracle: conjuring water out of thin air.&lt;/p&gt;  &lt;p&gt;When he first started working with MOFs, Yaghi thought they might be able to absorb climate-damaging carbon dioxide—or maybe hold hydrogen molecules, solving the thorny problem of storing that climate-friendly but hard-to-contain fuel. But then, in 2014, Yaghi’s team of researchers at UC Berkeley had an epiphany. The tiny pores in MOFs could be designed so the material would pull water molecules from the air around them, like a sponge—and then, with just a little heat, give back that water as if squeezed dry. Just one gram of a water-absorbing MOF has an internal surface area of roughly 7,000 square meters.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Yaghi wasn’t the first to try to pull potable water from the atmosphere. But his method could do it at lower levels of humidity than rivals—potentially shaking up a tiny, nascent industry that could be critical to humanity in the thirsty decades to come. Now the company he founded, called Atoco, is racing to demonstrate a pair of machines that Yaghi believes could produce clean, fresh, drinkable water virtually anywhere on Earth, without even hooking up to an energy supply.&lt;/p&gt;  &lt;p&gt;That’s the goal Yaghi has been working toward for more than a decade now, with the rigid determination that he learned while doing chores in his father’s butcher shop.&lt;/p&gt; 
 &lt;p&gt;“It was in that shop where I learned how to perfect things, how to have a work ethic,” he says. “I learned that a job is not done until it is well done. Don’t start a job unless you can finish it.”&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Most of Earth is covered in water, but just 3% of it is fresh, with no salt—the kind of water all terrestrial living things need. Today, desalination plants that take the salt out of seawater provide the bulk of potable water in technologically advanced desert nations like Israel and the United Arab Emirates, but at a high cost. Desalination facilities either heat water to distill out the drinkable stuff or filter it with membranes the salt doesn’t pass through; both methods require a lot of energy and leave behind concentrated brine. Typically desal pumps send that brine back into the ocean, with devastating ecological effects.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="hand holding a ball and stick model" class="wp-image-1129754" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/GettyImages-2239399344.jpg?w=878" /&gt;&lt;figcaption class="wp-element-caption"&gt;Heiner Linke, chair of the Nobel Committee for Chemistry, uses a model to explain how metalorganic frameworks (MOFs) can trap smaller molecules inside. In October 2025, Yaghi and two other scientists won the Nobel Prize in chemistry for identifying MOFs.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;JONATHAN NACKSTRAND/GETTY IMAGES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;I was talking to Atoco executives about carbon dioxide capture earlier this year when they mentioned the possibility of harvesting water from the atmosphere. Of course my mind immediately jumped to Star Wars, and Luke Skywalker working on his family’s moisture farm, using “vaporators” to pull water from the atmosphere of the arid planet Tatooine. (Other sci-fi fans’ minds might go to Dune, and the water-gathering technology of the Fremen.) Could this possibly be real?&lt;/p&gt;  &lt;p&gt;It turns out people have been doing it for millennia. Archaeological evidence of water harvesting from fog dates back as far as 5000 BCE. The ancient Greeks harvested dew, and 500 years ago so did the Inca, using mesh nets and buckets under trees.&lt;/p&gt;  &lt;p&gt;Today, harvesting water from the air is a business already worth billions of dollars, say industry analysts—and it’s on track to be worth billions more in the next five years. In part that’s because typical sources of fresh water are in crisis. Less snowfall in mountains during hotter winters means less meltwater in the spring, which means less water downstream. Droughts regularly break records. Rising seas seep into underground aquifers, already drained by farming and sprawling cities. Aging septic tanks leach bacteria into water, and cancer-causing “forever chemicals” are creating what the US Government Accountability Office last year said “may be the biggest water problem since lead.” That doesn’t even get to the emerging catastrophe from microplastics.&lt;/p&gt;  &lt;p&gt;So lots of places are turning to atmospheric water harvesting. Watergen, an Israel-based company working on the tech, initially planned on deploying in the arid, poorer parts of the world. Instead, buyers in Europe and the United States have approached the company as a way to ensure a clean supply of water. And one of Watergen’s biggest markets is the wealthy United Arab Emirates. “When you say ‘water crisis,’ it’s not just the lack of water—it’s access to good-quality water,” says Anna Chernyavsky, Watergen’s vice president of marketing.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;In other words, the technology “has evolved from lab prototypes to robust, field-deployable systems,” says Guihua Yu, a mechanical engineer at the University of Texas at Austin. “There is still room to improve productivity and energy efficiency in the whole-system level, but so much progress has been steady and encouraging.”&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;MOFs are just the latest approach to the idea. The first generation of commercial tech depended on compressors and refrigerant chemicals—large-scale versions of the machine that keeps food cold and fresh in your kitchen. Both use electricity and a clot of pipes and exchangers to make cold by phase-shifting a chemical from gas to liquid and back; refrigerators try to limit condensation, and water generators basically try to enhance it.&lt;/p&gt;  &lt;p&gt;That’s how Watergen’s tech works: using a compressor and a heat exchanger to wring water from air at humidity levels as low as 20%—Death Valley in the spring. “We’re talking about deserts,” Chernyavsky says. “Below 20%, you get nosebleeds.”&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="children in queue at a blue Watergen dispenser" class="wp-image-1129755" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/IMG_20190322_142704-1.jpg?w=1151" /&gt;&lt;figcaption class="wp-element-caption"&gt;A Watergen unit provides drinking water to students and staff at St. Joseph’s, a girls’ school in Freetown, Sierra Leone. “When you say ‘water crisis,’ it’s not just the lack of water— it’s access to good-quality water,” says Anna Chernyavsky, Watergen’s vice president of marketing.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF WATERGEN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;That still might not be good enough. “Refrigeration works pretty well when you are above a certain relative humidity,” says Sameer Rao, a mechanical engineer at the University of Utah who researches atmospheric water harvesting. “As the environment dries out, you go to lower relative humidities, and it becomes harder and harder. In some cases, it’s impossible for refrigeration-based systems to really work.”&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;So a second wave of technology has found a market. Companies like Source Global use desiccants—substances that absorb moisture from the air, like the silica packets found in vitamin bottles—to pull in moisture and then release it when heated. In theory, the benefit of desiccant-­based tech is that it could absorb water at lower humidity levels, and it uses less energy on the front end since it isn’t running a condenser system. Source Global claims its off-grid, solar-powered system is deployed in dozens of countries.&lt;/p&gt;  &lt;p&gt;But both technologies still require a lot of energy, either to run the heat exchangers or to generate sufficient heat to release water from the desiccants. MOFs, Yaghi hopes, do not. Now Atoco is trying to prove it. Instead of using heat exchangers to bring the air temperature to dew point or desiccants to attract water from the atmosphere, a system can rely on specially designed MOFs to attract water molecules. Atoco’s prototype version uses an MOF that looks like baby powder, stuck to a surface like glass. The pores in the MOF naturally draw in water molecules but remain open, making it theoretically easy to discharge the water with no more heat than what comes from direct sunlight. Atoco’s industrial-scale design uses electricity to speed up the process, but the company is working on a second design that can operate completely off grid, without any energy input.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;Yaghi’s Atoco isn’t the only contender seeking to use MOFs for water harvesting. A competitor, AirJoule, has introduced MOF-based atmospheric water generators in Texas and the UAE and is working with researchers at Arizona State University, planning to deploy more units in the coming months. The company started out trying to build more efficient air-­conditioning for electric buses operating on hot, humid city streets. But then founder Matt Jore heard about US government efforts to harvest water from air—and pivoted. The startup’s stock price has been a bit of a roller-­coaster, but Jore says the sheer size of the market should keep him in business. Take Maricopa County, encompassing Phoenix and its environs—it uses 1.2 billion gallons of water from its shrinking aquifer every day, and another 874 million gallons from surface sources like rivers.&lt;/p&gt;  &lt;p&gt;“So, a couple of billion gallons a day, right?” Jore tells me. “You know how much influx is in the atmosphere every day? Twenty-five billion gallons.”&lt;/p&gt; 
 &lt;p&gt;My eyebrows go up. “Globally?”&lt;/p&gt;  &lt;p&gt;“Just the greater Phoenix area gets influx of about 25 billion gallons of water in the air,” he says. “If you can tap into it, that’s your source. And it’s not going away. It’s all around the world. We view the atmosphere as the world’s free pipeline.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;Besides AirJoule’s head start on Atoco, the companies also differ on where they get their MOFs. AirJoule’s system relies on an off-the-shelf version the company buys from the chemical giant BASF; Atoco aims to use Yaghi’s skill with designing the novel material to create bespoke MOFs for different applications and locations.&lt;/p&gt;  &lt;p&gt;“Given the fact that we have the inventor of the whole class of materials, and we leverage the stuff that comes out of his lab at Berkeley—everything else equal, we have a good starting point to engineer maybe the best materials in the world,” says Magnus Bach, Atoco’s VP of business development.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt;&lt;p&gt;Yaghi envisions a two-pronged product line. Industrial-scale water generators that run on electricity would be capable of producing thousands of liters per day on one end, while units that run on passive systems could operate in remote locations without power, just harnessing energy from the sun and ambient temperatures. In theory, these units could someday replace desalination and even entire municipal water supplies. The next round of field tests is scheduled for early 2026, in the Mojave Desert—one of the hottest, driest places on Earth.&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“That’s my dream,” Yaghi says. “To give people water independence, so they’re not reliant on another party for their lives.”&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;Both Yaghi and Watergen’s Chernyavsky say they’re looking at more decentralized versions that could operate outside municipal utility systems. Home appliances, similar to rooftop solar panels and batteries, could allow households to generate their own water off grid.&lt;/p&gt; 
 &lt;p&gt;That could be tricky, though, without economies of scale to bring down prices. “You have to produce, you have to cool, you have to filter—all in one place,” Chernyavsky says. “So to make it small is very, very challenging.”&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Difficult as that may be, Yaghi’s childhood gave him a particular appreciation for the freedom to go off grid, to liberate the basic necessity of water from the whims of systems that dictate when and how people can access it.&lt;/p&gt; 
 &lt;p&gt;“That’s really my dream,” he says. “To give people independence, water independence, so that they’re not reliant on another party for their livelihood or lives.”&lt;/p&gt;  &lt;p&gt;Toward the end of one of our conversations, I asked Yaghi what he would tell the younger version of himself if he could. “Jordan is one of the worst countries in terms of the impact of water stress,” he said. “I would say, ‘Continue to be diligent and observant. It doesn’t really matter what you’re pursuing, as long as you’re passionate.’”&lt;/p&gt;  &lt;p&gt;I pressed him for something more specific: “What do you think he’d say when you described this technology to him?”&lt;/p&gt;  &lt;p&gt;Yaghi smiled: “I think young Omar would think you’re putting him on, that this is all fictitious and you’re trying to take something from him.” This reality, in other words, would be beyond young Omar’s wildest dreams.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Alexander C. Kaufman is a reporter who has covered energy, climate change, pollution, business, and geopolitics for more than a decade.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/17/1129259/omar-yaghi-chemist-nobel-prize-crystals-water-air/</guid><pubDate>Wed, 17 Dec 2025 11:00:00 +0000</pubDate></item><item><title>[NEW] Gemini 3 Flash: frontier intelligence built for speed (Google DeepMind News)</title><link>https://deepmind.google/blog/gemini-3-flash-frontier-intelligence-built-for-speed/</link><description>&lt;div class="article-image-hero"&gt;
  &lt;div class="article-image-hero__container"&gt;
    &lt;figure class="article-image--full-aspect article-module"&gt;
      &lt;div class="aspect-ratio-image"&gt;
        &lt;div class="aspect-ratio-image__container"&gt;
          &lt;img alt="Gemini 3 Flash text" class="aspect-ratio-image__image uni-progressive-image--blur" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-3_flash_model_blog_header_.width-2200.format-webp.webp" width="360px" /&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      
    &lt;/figure&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;div class="uni-content uni-blog-article-container article-container__content
                      
                      "&gt;

            
  
    



















&lt;div class="audio-player-tts"&gt;
  &lt;audio class="audio-player-tts__player" title="Gemini 3 Flash: frontier intelligence built for speed"&gt;
      &lt;source src="https://ftr.bazqux.com/self.ttsaudio_set.first.tts_audio.url" type="self.ttsaudio_set.first.tts_audio.file.file.mime_type" /&gt;
      &lt;p&gt;Your browser does not support the audio element.&lt;/p&gt;
  &lt;/audio&gt;
  &lt;div class="audio-player-tts__container"&gt;
    &lt;div class="audio-player-tts__content"&gt;
      &lt;button class="audio-player-tts__preview-play"&gt;
        &lt;svg class="icon audio-player-tts__play-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__text-content"&gt;
        &lt;span class="audio-player-tts__text-content--title"&gt;
          Listen to article
          &lt;span class="audio-player-tts__disclaimer" tabindex="0"&gt;
            &lt;div class="audio-player-tts__disclaimer--copy uni-small-text"&gt;This content is generated by Google AI. Generative AI is experimental&lt;/div&gt;
            &lt;svg class="audio-player-tts__disclaimer--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

          &lt;/span&gt;
        &lt;/span&gt;
        &lt;div class="audio-player-tts__duration uni-small-text"&gt;[[duration]] minutes&lt;/div&gt;
      &lt;/div&gt;
      &lt;button class="audio-player-tts__pause"&gt;
        &lt;svg class="icon audio-player-tts__icon-play" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;svg class="icon audio-player-tts__icon-pause audio-player-tts__icon-pause--hidden" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__console"&gt;
        &lt;div class="audio-player-tts__time-bar"&gt;
          &lt;span class="audio-player-tts__current-time uni-small-text"&gt;&lt;/span&gt;
          &lt;div class="audio-player-tts__timeline-slider-container"&gt;
            &lt;input class="timeline__slider" max="100" step="5" tabindex="0" type="range" value="0" /&gt;
          &lt;/div&gt;
          &lt;span class="audio-player-tts__duration-time uni-small-text"&gt;&lt;/span&gt;
        &lt;/div&gt;
        &lt;button class="audio-player-tts__audio-settings"&gt;
          &lt;svg class="icon audio-player-tts__audio-settings--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
        &lt;div class="audio-player-tts__settings-container"&gt;
          &lt;div class="audio-player-tts__settings--main uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings--current-voice"&gt;
              &lt;span class="audio-player-tts__settings--current-voice-info"&gt;
                &lt;svg class="audio-player-tts__settings--current-voice-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;span&gt;Voice&lt;/span&gt;
              &lt;/span&gt;
              &lt;span class="audio-player-tts__settings--current-voice-next"&gt;
                &lt;span class="audio-player-tts__settings--current-voice-text uni-small-text"&gt;&lt;/span&gt;
                &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

              &lt;/span&gt;
            &lt;/button&gt;
            &lt;button class="audio-player-tts__settings--current-speed"&gt;
              &lt;span class="audio-player-tts__settings--current-speed-info"&gt;
                  &lt;svg class="audio-player-tts__settings--current-speed-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                  &lt;span&gt;Speed&lt;/span&gt;
                &lt;/span&gt;
                &lt;span class="audio-player-tts__settings--current-speed-next"&gt;
                  &lt;span class="audio-player-tts__settings--current-speed-text uni-small-text"&gt;&lt;/span&gt;
                  &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;/span&gt;
            &lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--voices uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Voice&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--speeds uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Speed&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;0.75X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option audio-player-tts__settings-option--selected"&gt;&lt;span&gt;1X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;1.5X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;2X&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

  





            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Today, we're expanding the Gemini 3 model family with the release of Gemini 3 Flash, which offers frontier intelligence built for speed at a fraction of the cost. With this release, we’re making Gemini 3’s next-generation intelligence accessible to everyone across Google products.&lt;/p&gt;&lt;p&gt;Last month, we kicked off Gemini 3 with Gemini 3 Pro and Gemini 3 Deep Think mode, and the response has been incredible. Since launch day, we have been processing over 1T tokens per day on our API. We’ve seen you use Gemini 3 to vibe code simulations to learn about complex topics, build and design interactive games and understand all types of multimodal content.&lt;/p&gt;&lt;p&gt;With Gemini 3, we introduced frontier performance across complex reasoning, multimodal and vision understanding and agentic and vibe coding tasks. Gemini 3 Flash retains this foundation, combining Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. It not only enables everyday tasks with improved reasoning, but also is our most impressive model for agentic workflows.&lt;/p&gt;&lt;p&gt;Starting today, Gemini 3 Flash is rolling out to millions of people globally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;For developers in the Gemini API in Google AI Studio, Gemini CLI and our new agentic development platform Google Antigravity&lt;/li&gt;&lt;li&gt;For everyone via the Gemini app and in AI Mode in Search&lt;/li&gt;&lt;li&gt;For enterprises in Vertex AI and Gemini Enterprise&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Gemini 3 Flash: frontier intelligence at scale&lt;/h2&gt;&lt;p&gt;Gemini 3 Flash demonstrates that speed and scale don’t have to come at the cost of intelligence. It delivers frontier performance on PhD-level reasoning and knowledge benchmarks like GPQA Diamond (90.4%) and Humanity’s Last Exam (33.7% without tools), rivaling larger frontier models, and significantly outperforming even the best 2.5 model, Gemini 2.5 Pro, across a number of benchmarks. It also reaches state-of-the-art performance with an impressive score of 81.2% on MMMU Pro, comparable to Gemini 3 Pro.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    















  
  
    &lt;div&gt;
      &lt;img alt="A benchmark comparison table showing performance scores and prices for several language models including Gemini 3 Flash, Gemini 3 Pro Thinking, Gemini 2.5 Flash Thinking, Gemini 2.5 Pro Thinking, Claude Sonnet 4.5, GPT-5.2 Extra high, and Grok 4.1 Fast, across various tasks like academic reasoning, scientific knowledge, math, multi-modal understanding, coding, and long context performance." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-3-flash_final_benchmark-ta.width-1000.format-webp.webp" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;In addition to its frontier-level reasoning and multimodal capabilities, Gemini 3 Flash was built to be highly efficient, pushing the Pareto frontier of quality vs. cost and speed. When processing at the highest thinking level, Gemini 3 Flash is able to modulate how much it thinks. It may think longer for more complex use cases, but it also uses 30% fewer tokens on average than 2.5 Pro, as measured on typical traffic, to accurately complete everyday tasks with higher performance.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    















  
    &lt;div&gt;
      &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash pushes the Pareto frontier on performance vs. cost and speed.&lt;/p&gt;&lt;/div&gt;
    &lt;/div&gt;
  
  
    &lt;div&gt;
      &lt;img alt="A scatter plot showing LMArena Elo Score versus Price per million tokens for various language models, with a line highlighting the Pareto frontier through 'gemini-3-pro', 'gemini-3-flash', and 'gemini-3-flash-lite'." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-3-flash_pareto_graph_dec17.width-1000.format-webp.webp" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash’s strength lies in its raw speed, building on the Flash series that developers and consumers already love. It outperforms 2.5 Pro while being 3x faster (based on Artificial Analysis benchmarking) at a fraction of the cost. Gemini 3 Flash is priced at $0.50/1M input tokens and $3/1M output tokens (audio input remains at $1/1M input tokens).&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    















  
    &lt;div&gt;
      &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash outperforms 2.5 Pro in speed and quality.&lt;/p&gt;&lt;/div&gt;
    &lt;/div&gt;
  
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;For developers: intelligence that keeps up&lt;/h2&gt;&lt;p&gt;Gemini 3 Flash is made for iterative development, offering Gemini 3’s Pro-grade coding performance with low latency — it’s able to reason and solve tasks quickly in high-frequency workflows. On SWE-bench Verified, a benchmark for evaluating coding agent capabilities, Gemini 3 Flash achieves a score of 78%, outperforming not only the 2.5 series, but also Gemini 3 Pro. It strikes an ideal balance for agentic coding, production-ready systems and responsive interactive applications.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash’s strong performance in reasoning, tool use and multimodal capabilities is ideal for developers looking to do more complex video analysis, data extraction and visual Q&amp;amp;A, which means it can enable more intelligent applications — like in-game assistants or A/B test experiments — that demand both quick answers and deep reasoning.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    


















  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash enables multimodal reasoning in a hand-tracked "ball launching puzzle game" game providing near real-time AI assistance.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash builds and A/B tests new loading spinner designs in near real-time, streamlining the design-to-code process.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash uses multimodal reasoning to analyze and caption an image with contextual UI overlays in near real-time, ultimately transforming a static image into an interactive experience.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash takes a single instruction prompt and codes three unique design variations.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;We’ve received a tremendous response from companies using Gemini 3 Flash. Companies like JetBrains, Bridgewater Associates, and Figma are already using it to transform their businesses, recognizing how its inference speed, efficiency and reasoning capabilities perform on par with larger models. Gemini 3 Flash is available today to enterprises via Vertex AI and Gemini Enterprise.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    


















  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;For everyone: Gemini 3 Flash is rolling out globally&lt;/h2&gt;&lt;p&gt;Gemini 3 Flash is now the default model in the Gemini app, replacing 2.5 Flash. That means all of our Gemini users globally will get access to the Gemini 3 experience at no cost, giving their everyday tasks a major upgrade.&lt;/p&gt;&lt;p&gt;Because of Gemini 3 Flash’s incredible multimodal reasoning capabilities, you can use it to help you see, hear and understand any type of information faster. For example, you can ask Gemini to understand your videos and images and turn that content into a helpful and actionable plan in just a few seconds.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    


















  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash in the Gemini app can analyze short video content and give you a plan, like how to improve your golf swing.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;As Gemini 3 Flash is optimized for speed, it can see and guess what you’re drawing while you’re still sketching it.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;You can upload an audio recording and Gemini 3 Flash will identify your knowledge gaps, create a custom quiz, and give you detailed explanations on the answers.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Or you can quickly build fun, useful apps from scratch using your voice without prior coding knowledge. Just dictate to Gemini on the go, and it can transform your unstructured thoughts into a functioning app in minutes.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash is also starting to roll out as the default model for AI Mode in Search with access to everyone around the world.&lt;/p&gt;&lt;p&gt;Building on the reasoning capabilities of Gemini 3 Pro, AI Mode with Gemini 3 Flash is more powerful at parsing the nuances of your question. It considers each aspect of your query to serve thoughtful, comprehensive responses that are visually digestible — pulling real-time local information and helpful links from across the web. The result effectively combines research with immediate action: you get an intelligently organized breakdown alongside specific recommendations — at the speed of Search.&lt;/p&gt;&lt;p&gt;This shines when tackling complex goals with multiple considerations like trying to plan a last-minute trip or learning complex educational concepts quickly.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Try Gemini 3 Flash today&lt;/h2&gt;&lt;p&gt;Gemini 3 Flash is available now in preview via the Gemini API in Google AI Studio, Google Antigravity, Vertex AI and Gemini Enterprise. You can also access it through other developer tools like Gemini CLI and Android Studio. It’s also starting to roll out to everyone in the Gemini app and AI Mode in Search, bringing fast access to next-generation intelligence at no cost.&lt;/p&gt;&lt;p&gt;We’re looking forward to seeing what you bring to life with this expanded family of models: Gemini 3 Pro, Gemini 3 Deep Think and now, Gemini 3 Flash.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


Gemini Models


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    
      &lt;li&gt;
        
        
        


  


Google DeepMind


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;

            
          &lt;/div&gt;</description><content:encoded>&lt;div class="article-image-hero"&gt;
  &lt;div class="article-image-hero__container"&gt;
    &lt;figure class="article-image--full-aspect article-module"&gt;
      &lt;div class="aspect-ratio-image"&gt;
        &lt;div class="aspect-ratio-image__container"&gt;
          &lt;img alt="Gemini 3 Flash text" class="aspect-ratio-image__image uni-progressive-image--blur" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-3_flash_model_blog_header_.width-2200.format-webp.webp" width="360px" /&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      
    &lt;/figure&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;div class="uni-content uni-blog-article-container article-container__content
                      
                      "&gt;

            
  
    



















&lt;div class="audio-player-tts"&gt;
  &lt;audio class="audio-player-tts__player" title="Gemini 3 Flash: frontier intelligence built for speed"&gt;
      &lt;source src="https://ftr.bazqux.com/self.ttsaudio_set.first.tts_audio.url" type="self.ttsaudio_set.first.tts_audio.file.file.mime_type" /&gt;
      &lt;p&gt;Your browser does not support the audio element.&lt;/p&gt;
  &lt;/audio&gt;
  &lt;div class="audio-player-tts__container"&gt;
    &lt;div class="audio-player-tts__content"&gt;
      &lt;button class="audio-player-tts__preview-play"&gt;
        &lt;svg class="icon audio-player-tts__play-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__text-content"&gt;
        &lt;span class="audio-player-tts__text-content--title"&gt;
          Listen to article
          &lt;span class="audio-player-tts__disclaimer" tabindex="0"&gt;
            &lt;div class="audio-player-tts__disclaimer--copy uni-small-text"&gt;This content is generated by Google AI. Generative AI is experimental&lt;/div&gt;
            &lt;svg class="audio-player-tts__disclaimer--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

          &lt;/span&gt;
        &lt;/span&gt;
        &lt;div class="audio-player-tts__duration uni-small-text"&gt;[[duration]] minutes&lt;/div&gt;
      &lt;/div&gt;
      &lt;button class="audio-player-tts__pause"&gt;
        &lt;svg class="icon audio-player-tts__icon-play" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;svg class="icon audio-player-tts__icon-pause audio-player-tts__icon-pause--hidden" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__console"&gt;
        &lt;div class="audio-player-tts__time-bar"&gt;
          &lt;span class="audio-player-tts__current-time uni-small-text"&gt;&lt;/span&gt;
          &lt;div class="audio-player-tts__timeline-slider-container"&gt;
            &lt;input class="timeline__slider" max="100" step="5" tabindex="0" type="range" value="0" /&gt;
          &lt;/div&gt;
          &lt;span class="audio-player-tts__duration-time uni-small-text"&gt;&lt;/span&gt;
        &lt;/div&gt;
        &lt;button class="audio-player-tts__audio-settings"&gt;
          &lt;svg class="icon audio-player-tts__audio-settings--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
        &lt;div class="audio-player-tts__settings-container"&gt;
          &lt;div class="audio-player-tts__settings--main uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings--current-voice"&gt;
              &lt;span class="audio-player-tts__settings--current-voice-info"&gt;
                &lt;svg class="audio-player-tts__settings--current-voice-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;span&gt;Voice&lt;/span&gt;
              &lt;/span&gt;
              &lt;span class="audio-player-tts__settings--current-voice-next"&gt;
                &lt;span class="audio-player-tts__settings--current-voice-text uni-small-text"&gt;&lt;/span&gt;
                &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

              &lt;/span&gt;
            &lt;/button&gt;
            &lt;button class="audio-player-tts__settings--current-speed"&gt;
              &lt;span class="audio-player-tts__settings--current-speed-info"&gt;
                  &lt;svg class="audio-player-tts__settings--current-speed-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                  &lt;span&gt;Speed&lt;/span&gt;
                &lt;/span&gt;
                &lt;span class="audio-player-tts__settings--current-speed-next"&gt;
                  &lt;span class="audio-player-tts__settings--current-speed-text uni-small-text"&gt;&lt;/span&gt;
                  &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;/span&gt;
            &lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--voices uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Voice&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--speeds uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Speed&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;0.75X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option audio-player-tts__settings-option--selected"&gt;&lt;span&gt;1X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;1.5X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;2X&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

  





            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Today, we're expanding the Gemini 3 model family with the release of Gemini 3 Flash, which offers frontier intelligence built for speed at a fraction of the cost. With this release, we’re making Gemini 3’s next-generation intelligence accessible to everyone across Google products.&lt;/p&gt;&lt;p&gt;Last month, we kicked off Gemini 3 with Gemini 3 Pro and Gemini 3 Deep Think mode, and the response has been incredible. Since launch day, we have been processing over 1T tokens per day on our API. We’ve seen you use Gemini 3 to vibe code simulations to learn about complex topics, build and design interactive games and understand all types of multimodal content.&lt;/p&gt;&lt;p&gt;With Gemini 3, we introduced frontier performance across complex reasoning, multimodal and vision understanding and agentic and vibe coding tasks. Gemini 3 Flash retains this foundation, combining Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. It not only enables everyday tasks with improved reasoning, but also is our most impressive model for agentic workflows.&lt;/p&gt;&lt;p&gt;Starting today, Gemini 3 Flash is rolling out to millions of people globally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;For developers in the Gemini API in Google AI Studio, Gemini CLI and our new agentic development platform Google Antigravity&lt;/li&gt;&lt;li&gt;For everyone via the Gemini app and in AI Mode in Search&lt;/li&gt;&lt;li&gt;For enterprises in Vertex AI and Gemini Enterprise&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Gemini 3 Flash: frontier intelligence at scale&lt;/h2&gt;&lt;p&gt;Gemini 3 Flash demonstrates that speed and scale don’t have to come at the cost of intelligence. It delivers frontier performance on PhD-level reasoning and knowledge benchmarks like GPQA Diamond (90.4%) and Humanity’s Last Exam (33.7% without tools), rivaling larger frontier models, and significantly outperforming even the best 2.5 model, Gemini 2.5 Pro, across a number of benchmarks. It also reaches state-of-the-art performance with an impressive score of 81.2% on MMMU Pro, comparable to Gemini 3 Pro.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    















  
  
    &lt;div&gt;
      &lt;img alt="A benchmark comparison table showing performance scores and prices for several language models including Gemini 3 Flash, Gemini 3 Pro Thinking, Gemini 2.5 Flash Thinking, Gemini 2.5 Pro Thinking, Claude Sonnet 4.5, GPT-5.2 Extra high, and Grok 4.1 Fast, across various tasks like academic reasoning, scientific knowledge, math, multi-modal understanding, coding, and long context performance." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-3-flash_final_benchmark-ta.width-1000.format-webp.webp" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;In addition to its frontier-level reasoning and multimodal capabilities, Gemini 3 Flash was built to be highly efficient, pushing the Pareto frontier of quality vs. cost and speed. When processing at the highest thinking level, Gemini 3 Flash is able to modulate how much it thinks. It may think longer for more complex use cases, but it also uses 30% fewer tokens on average than 2.5 Pro, as measured on typical traffic, to accurately complete everyday tasks with higher performance.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    















  
    &lt;div&gt;
      &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash pushes the Pareto frontier on performance vs. cost and speed.&lt;/p&gt;&lt;/div&gt;
    &lt;/div&gt;
  
  
    &lt;div&gt;
      &lt;img alt="A scatter plot showing LMArena Elo Score versus Price per million tokens for various language models, with a line highlighting the Pareto frontier through 'gemini-3-pro', 'gemini-3-flash', and 'gemini-3-flash-lite'." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-3-flash_pareto_graph_dec17.width-1000.format-webp.webp" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash’s strength lies in its raw speed, building on the Flash series that developers and consumers already love. It outperforms 2.5 Pro while being 3x faster (based on Artificial Analysis benchmarking) at a fraction of the cost. Gemini 3 Flash is priced at $0.50/1M input tokens and $3/1M output tokens (audio input remains at $1/1M input tokens).&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    















  
    &lt;div&gt;
      &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash outperforms 2.5 Pro in speed and quality.&lt;/p&gt;&lt;/div&gt;
    &lt;/div&gt;
  
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;For developers: intelligence that keeps up&lt;/h2&gt;&lt;p&gt;Gemini 3 Flash is made for iterative development, offering Gemini 3’s Pro-grade coding performance with low latency — it’s able to reason and solve tasks quickly in high-frequency workflows. On SWE-bench Verified, a benchmark for evaluating coding agent capabilities, Gemini 3 Flash achieves a score of 78%, outperforming not only the 2.5 series, but also Gemini 3 Pro. It strikes an ideal balance for agentic coding, production-ready systems and responsive interactive applications.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash’s strong performance in reasoning, tool use and multimodal capabilities is ideal for developers looking to do more complex video analysis, data extraction and visual Q&amp;amp;A, which means it can enable more intelligent applications — like in-game assistants or A/B test experiments — that demand both quick answers and deep reasoning.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    


















  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash enables multimodal reasoning in a hand-tracked "ball launching puzzle game" game providing near real-time AI assistance.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash builds and A/B tests new loading spinner designs in near real-time, streamlining the design-to-code process.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash uses multimodal reasoning to analyze and caption an image with contextual UI overlays in near real-time, ultimately transforming a static image into an interactive experience.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash takes a single instruction prompt and codes three unique design variations.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;We’ve received a tremendous response from companies using Gemini 3 Flash. Companies like JetBrains, Bridgewater Associates, and Figma are already using it to transform their businesses, recognizing how its inference speed, efficiency and reasoning capabilities perform on par with larger models. Gemini 3 Flash is available today to enterprises via Vertex AI and Gemini Enterprise.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    


















  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;For everyone: Gemini 3 Flash is rolling out globally&lt;/h2&gt;&lt;p&gt;Gemini 3 Flash is now the default model in the Gemini app, replacing 2.5 Flash. That means all of our Gemini users globally will get access to the Gemini 3 experience at no cost, giving their everyday tasks a major upgrade.&lt;/p&gt;&lt;p&gt;Because of Gemini 3 Flash’s incredible multimodal reasoning capabilities, you can use it to help you see, hear and understand any type of information faster. For example, you can ask Gemini to understand your videos and images and turn that content into a helpful and actionable plan in just a few seconds.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    


















  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash in the Gemini app can analyze short video content and give you a plan, like how to improve your golf swing.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;As Gemini 3 Flash is optimized for speed, it can see and guess what you’re drawing while you’re still sketching it.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;You can upload an audio recording and Gemini 3 Flash will identify your knowledge gaps, create a custom quiz, and give you detailed explanations on the answers.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Or you can quickly build fun, useful apps from scratch using your voice without prior coding knowledge. Just dictate to Gemini on the go, and it can transform your unstructured thoughts into a functioning app in minutes.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 3 Flash is also starting to roll out as the default model for AI Mode in Search with access to everyone around the world.&lt;/p&gt;&lt;p&gt;Building on the reasoning capabilities of Gemini 3 Pro, AI Mode with Gemini 3 Flash is more powerful at parsing the nuances of your question. It considers each aspect of your query to serve thoughtful, comprehensive responses that are visually digestible — pulling real-time local information and helpful links from across the web. The result effectively combines research with immediate action: you get an intelligently organized breakdown alongside specific recommendations — at the speed of Search.&lt;/p&gt;&lt;p&gt;This shines when tackling complex goals with multiple considerations like trying to plan a last-minute trip or learning complex educational concepts quickly.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Try Gemini 3 Flash today&lt;/h2&gt;&lt;p&gt;Gemini 3 Flash is available now in preview via the Gemini API in Google AI Studio, Google Antigravity, Vertex AI and Gemini Enterprise. You can also access it through other developer tools like Gemini CLI and Android Studio. It’s also starting to roll out to everyone in the Gemini app and AI Mode in Search, bringing fast access to next-generation intelligence at no cost.&lt;/p&gt;&lt;p&gt;We’re looking forward to seeing what you bring to life with this expanded family of models: Gemini 3 Pro, Gemini 3 Deep Think and now, Gemini 3 Flash.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


Gemini Models


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    
      &lt;li&gt;
        
        
        


  


Google DeepMind


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;

            
          &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/blog/gemini-3-flash-frontier-intelligence-built-for-speed/</guid><pubDate>Wed, 17 Dec 2025 11:58:17 +0000</pubDate></item><item><title>“A Band-Aid on a giant gash”: Trump’s attacks on science may ruin his AI moonshot (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/12/trump-spent-2025-attacking-science-that-could-set-back-his-genesis-mission/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 md:my-10 md:py-8 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Trump’s AI “Manhattan Project” will fail if DOGE cuts are kept, critics say.
    &lt;/p&gt;

          
    
    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/trump-project-genesis.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;By executive order last month, Donald Trump launched his so-called “Genesis Mission.”&lt;/p&gt;
&lt;p&gt;Described as a “historic national effort” to “invest in AI-enabled science to accelerate scientific advancement,” Trump claimed his mission would address key challenges to American energy dominance, innovation, and national security.&lt;/p&gt;
&lt;p&gt;This mission, Trump boasted, would be a game-changer to science akin to putting a man on the moon or firing the first nuclear weapons. By building “an integrated AI platform” trained on “the world’s largest collection” of federal scientific data sets, he promised, the government could set off cascades of scientific breakthroughs.&lt;/p&gt;
&lt;p&gt;Access to such a platform, Trump imagined, would supercharge top US labs, powering AI agents to do tasks like quickly test hypotheses and automate research workflows to speed up discoveries.&lt;/p&gt;
&lt;p&gt;However, the mission crucially depends on strengthening collaboration between public, private, and academic sectors. And Trump’s order is concerningly vague on how those partnerships will be structured and funded at a time when many scientists have been sidelined due to a flurry of Trump orders earlier this year that eliminated their funding or removed them from their labs.&lt;/p&gt;
&lt;p&gt;To critics, including scientists, policy experts, advocates, and historians, Trump’s order seems divorced from reality, given that he spent the past year attacking some of the very institutions the Genesis Mission would seem to depend on. Trump also seemed unclear about what can be achieved with AI and confused about how scientific progress is actually made, some critics suggested.&lt;/p&gt;
&lt;p&gt;Among the critics was Arati Prabhakar, who served as the director of the White House Office of Science and Technology Policy under the Biden administration. Prabhakar told Ars that Trump’s crippling cuts to government science agencies, research grant funding freezes, and attacks on universities must be repaired or his mission will fail.&lt;/p&gt;
&lt;p&gt;“After the Trump administration has inflicted so much damage to valuable datasets and publicly funded research, the new executive order is a Band-Aid on a giant gash,” Prabhakar said.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;US won’t lead in AI if Trump’s science attacks continue&lt;/h2&gt;
&lt;p&gt;Also skeptical of Trump’s plans is Kathryn Kelley, executive director for the Coalition for Academic Scientific Computation, an educational nonprofit representing more than 100 of “the nation’s most forward-thinking universities and computing centers,” CASC’s LinkedIn said.&lt;/p&gt;
&lt;p&gt;Her group is specifically dedicated to Genesis Mission-aligned goals, “advocating for the use of the most advanced computing technology to accelerate scientific discovery for national competitiveness, global security, and economic success.” And while Trump’s initiative could be considered “a step in the right direction,” Kelley told Ars that it will require alignment, stabilization, and follow-through to be executable at the scale envisioned, due to cuts earlier this year.&lt;/p&gt;
&lt;p&gt;“Many research institutions and national laboratories continue to experience funding uncertainty, program disruptions, and workforce instability stemming from earlier cuts,” Kelley told Ars.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Particular pain points considered critical to address for Genesis Mission to move forward include reversing “impacts on staffing, ongoing research, and student pipelines,” she suggested.&lt;/p&gt;
&lt;p&gt;In one prominent example, some Department of Government Efficiency (DOGE) cuts targeting workers at the National Science Foundation&amp;nbsp;&lt;span style="margin: 0px; padding: 0px;"&gt;hit hardest in the&amp;nbsp;branch designed to accelerate technology development across&lt;/span&gt; a wide range of research settings in the US. DOGE slashed workers there simply because it was the youngest directorate at NSF with the most workers in transition when Trump took office. As courts weighed legal challenges to cuts, whistleblowers warned that Trump was aiming to politicize and dismantle NSF.&lt;/p&gt;
&lt;p&gt;“Large-scale initiatives like Genesis rely on highly skilled personnel, robust infrastructure, and sustained program support—some of the very resources at NSF and other federal agencies that were disrupted,” Kelley told Ars. “Rebuilding trust, re-establishing lost programs, and stabilizing the research workforce will be essential to make this mission feasible.”&lt;/p&gt;
&lt;p&gt;Critics urged that Trump’s attacks on science also included messing with government datasets that scientists depend on. Since Trump’s second term started, scientists have watched valuable data get censored or scrubbed from government websites. Some researchers have rushed to recreate datasets independently&amp;nbsp;with the help of the Internet Archive.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Prabhakar pointed out that some “datasets that could improve health and prevent disasters are eroding or even disappearing due to this administration,” while universities training “the next generation of great researchers and innovators have reduced or even stopped graduate admissions because of Trump’s assault.”&lt;/p&gt;
&lt;p&gt;Without a massive undertaking to undo moves that critics have said undermined both US science and trust in it, Trump’s dreams of launching AI models that would propel a million moonshots could go down in history as merely hype.&lt;/p&gt;
&lt;p&gt;“Without robust data and research and without people’s trust, America won’t lead in AI,” Prabhakar said.&lt;/p&gt;
&lt;h2&gt;Genesis Mission shows Trump’s “tremendous ignorance”&lt;/h2&gt;
&lt;p&gt;For people in the science community, it’s hard to square Trump’s aggressive cuts from earlier this year with the broad ambition of Genesis Mission. Frustratingly, the president demands that scientists make discoveries on his timeline, without acknowledging AI’s limitations or how his attacks on science could be driving away talent that could help labs advance AI.&lt;/p&gt;
&lt;p&gt;In many fields, scientists are still exploring how AI can aid research. Trump’s order appears to politicize science by focusing on areas he favors—like critical materials, nuclear energy, biotechnology, and quantum computing—despite their limited AI applications or data-quality challenges. Meanwhile, critics noted that it overlooks areas where “supercharging” AI could perhaps be more impactful—but where Trump notably does not want to leave his mark—like climate science or vaccine research.&lt;/p&gt;
&lt;p&gt;It also lays out aggressive timelines for results, demanding that the Department of Energy Secretary, Chris Wright, “demonstrate an initial operating capability of the Platform for at least one of the national science and technology challenges” identified in less than a year (270 days). Ideally, Trump’s mission will have generated significant discoveries in key fields within the next three years before he leaves office, his order outlined.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Paul Josephson, a Colby College professor and expert in the history of 20th-century science and technology, told Ars that Genesis Mission deadlines differ from John F. Kennedy’s 10-year timeline to reach the moon.&lt;/p&gt;
&lt;p&gt;Trump’s order “shows tremendous ignorance of how science and technology work,” Josephson said. The White House is saying, “Tell me what your discoveries will be and how many there will be in three years,” Josephson said, expecting that “we can pick the places where we want discoveries and make them happen.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;“That’s not anything like how science works,” Josephson told Ars, reducing Genesis Mission to “a vision without policy” and “a hope without funding.”&lt;/p&gt;
&lt;p&gt;To Josephson, Trump’s order sounded “more like it came out of Silicon Valley” than out of talks with government scientists, seemingly rushing approvals of industry partnerships and incentives without mentioning what resources would be available to fund gutted labs or train the next generation of scientists. It’s perhaps notable that the order is DOE-centric and does not place the same emphasis on contributions from universities or national labs funded by NSF and the National Institutes of Health as it does on industry partners.&lt;/p&gt;
&lt;p&gt;Kelley told Ars that “many public datasets are already being used effectively in research and industry” in the ways that Trump intends his AI platform to amplify. However, “there are areas—such as advanced nuclear research or emerging energy technologies—where datasets are limited.” And Trump risks reducing Genesis Mission to bluster by claiming that an AI platform could drive breakthroughs to the major challenges he flagged in the short term.&lt;/p&gt;
&lt;p&gt;“There is a real risk that the EO’s ambitious framing could overpromise what AI can achieve in the near term without addressing foundational data gaps,” Kelley told Ars. “That said, even partial progress in these areas could provide valuable insights, but expectations need to be realistic.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;US on “thin ice” attracting scientific talent&lt;/h2&gt;
&lt;p&gt;Just as important as asking where Genesis Mission funding is coming from or who the funding is going to, Chris R. Glass asks: “Where’s the talent coming from?”&lt;/p&gt;
&lt;p&gt;Trump’s order does not forecast that, only vaguely referencing support for universities training scientists. This comes, of course, after the administration revoked an estimated $1.5 billion in federal grant money in 2025. Those grant cuts shrank the pipeline for PhD students at an “unprecedented rate,” Axios reported.&lt;/p&gt;
&lt;p&gt;A Boston College professor who researches global student mobility and the impact of emergent technology on learning, Glass told Ars that Trump has notably left international talent out of his AI plans, despite the prominent roles that both “domestic and international scientists play in our current leadership” in AI.&lt;/p&gt;
&lt;p&gt;In a recent Washington Post op-ed, Glass warned that “America is losing research scientists,” who are seeking more stable environments to set up their lives and conduct long-term studies.&lt;/p&gt;
&lt;p&gt;As the Trump administration has attacked immigrants, other governments like the European Union and China have benefited by offering friendlier visa systems to attract the best and brightest minds graduating from US universities. Of course, of the two, China is America’s bigger AI rival. Earlier this year, China began heavily recruiting American scientists spooked by Trump’s grant funding cuts, and Glass confirmed that China has continued those efforts with the AI race heating up. Meanwhile, Trump appears to be going the other direction, recently requiring a $100,000 payment for some skilled workers seeking non-immigrant visas.&lt;/p&gt;
&lt;p&gt;Throughout 2025, US universities’ ability to attract international students showed resilience, but “we’re on thin ice,” Glass told Ars, with that resilience “waning.”&lt;/p&gt;
&lt;p&gt;Currently, the US “is ranked the lowest among top destinations for its safety and welcoming and the lowest for its post-graduation visa policies,” Glass said, noting that doctoral students must affirm that they do not intend to immigrate, even though the majority of STEM PhD students stay in the US after graduating.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“They want to stay, and we want them to stay,” Glass told Ars.&lt;/p&gt;
&lt;p&gt;Another concerning outcome of Trump cuts that could hamper Genesis Mission: Entire research groups at many institutions were “displaced”—removed from their labs and left to work in cubicles without access to their equipment, Glass told Ars.&lt;/p&gt;
&lt;p&gt;“I think scientists want to go where the best sciences are being done, but eventually these kinds of friction points and these hostile policies make them redirect elsewhere, even temporarily redirect, earn their doctorate in Europe and hope that the policy environment in the US changes,” Glass said.&lt;/p&gt;
&lt;p&gt;To turn it around, Glass made several recommendations in his op-ed to help retain PhD graduates and create stable pathways for high-value talents. That includes suggesting that the Trump administration consider fast-tracking green cards for students in fields that Genesis Mission depends on, including AI and machine-learning researchers, quantum computing scientists, and semiconductor engineers.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;He also thinks the US should “unlock the O-1A visa for researchers and entrepreneurs” by redefining what makes someone an “extraordinary” talent and creating dedicated “founder tracks” for international talent, as Britain and Singapore do. That visa is “uncapped yet underused,” Glass said, only approving 4,500 STEM candidates in 2023.&lt;/p&gt;
&lt;p&gt;Without changes to the visa system, the US “risks redirecting those talent flows,” he said. “And like a river, once those talent flows get redirected, they are very difficult to reverse.”&lt;/p&gt;
&lt;p&gt;And it won’t just be international talents jumping ship, Glass suggested, but also possibly US scientists forced to continue navigating potentially more of Trump’s cuts and indirect costs in the coming years.&lt;/p&gt;
&lt;p&gt;“I think that’s the kind of thing that slowly eats away at someone’s desire to continue to do science in the United States,” Glass said.&lt;/p&gt;
&lt;p&gt;Glass told Ars that he expects the US to stay on a “downward trajectory,” driving away talent in 2026, which Josephson suggested “will damage science both for the short and long term.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Many universities figured out a one-year contingency plan, but reality will set in if funding continues to be cut,” Glass said.&lt;/p&gt;
&lt;h2&gt;Trump picking winners paints “a very troubling picture”&lt;/h2&gt;
&lt;p&gt;CASC’s Kelley told Ars that like university international student recruitment, “the US research ecosystem continues to be resilient, but the gap between ambitious goals and the current capacity must be carefully managed.”&lt;/p&gt;
&lt;p&gt;“While the Genesis Mission signals strong intentions to invest in science and technology, its success will depend on aligning resources, rebuilding workforce capacity, and thoughtfully integrating AI and data capabilities where they are most effective,” Kelley said.&lt;/p&gt;
&lt;p&gt;A scientist might be best positioned to understand the nuance that requires, but Josephson noted that Trump tasked his Science Advisor, Michael Kratsios, with leading the initiative. Unlike prior officials serving in that role, Kratsios is not a scientist and has no PhD, earning his BA in politics. Instead, Kratsios has strong industry ties, previously serving as chief of staff for venture capitalist Peter Thiel and managing director of a company called Scale AI.&lt;/p&gt;
&lt;p&gt;To Josephson, Kratsios as head of the mission—which “seems to be totally based on faith in AI and datasets to do everything”—makes the initiative seem more aligned with Silicon Valley ambitions than public good. That could be a problem since historically, it has never worked when governments attempt to “pick winners” or pass industrial policy with claims that “if we do this, we will come out on top.”&lt;/p&gt;
&lt;p&gt;“It’s a belief in AI as the cure or the panacea for all the world’s problems to ensure we’re a dominant technological power, but ignoring climate change, race, gender, anything that is important in daily life,” Josephson said.&lt;/p&gt;
&lt;p&gt;Josephson is also an expert in Russian and Soviet history, explaining that precedent shows there are “tremendous dangers” of governments controlling which sciences are funded. In some ways, he thinks Genesis Mission “smells of Putin,” he told Ars, warning that Trump’s attempts to hoard and censor science in 2025 have been “as damaging to science and technology in the world’s leading centers as totalitarian regimes have been.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“It reflects the general timbre of the Trump administration toward the scientific enterprise,” he suggested, saying that the president has embraced the “authoritarian view” that he “has the right to pick and choose which fields and which branches merit more support and which should not be funded at all.”&lt;/p&gt;
&lt;p&gt;Jules Barbati-Dajches, an analyst for the Center for Science and Democracy at the Union of Concerned Scientists (UCS), told Ars that in addition to cuts, Trump recently “weakened federal agency policies (called scientific integrity policies) that were specifically in place to protect federal agency science from political interference.” This further threatens scientific integrity, Barbati-Dajches warned in August.&lt;/p&gt;
&lt;p&gt;UCS has tracked “instances of science being sidelined, ignored, or misused by the federal government” across “multiple presidential administrations” for two decades, Barbati-Dajches told Ars. And although their methodology was recently updated, the current Trump administration stands out, as “the rate and impact of attacks on science we’ve observed over the past nine months far outpace anything UCS has tracked before,” Barbati-Dajches said.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Additionally, UCS has documented “cases of the administration using AI in their reports and research that raise concern” that AI initiatives like Genesis Mission may promote dubious claims to serve “politicized” outcomes, Barbati-Dajches said.&lt;/p&gt;
&lt;p&gt;“This altogether paints a very troubling picture,” Barbati-Dajches said. “Scientific innovation and discovery are exciting, important, and can help inform federal policy and guidance. But as history tells us (and recent history even more so), science in the federal government needs protective guardrails to keep it independent and free from undue influence.”&lt;/p&gt;
&lt;h2&gt;Genesis Mission overlooks AI for public good&lt;/h2&gt;
&lt;p&gt;With Trump pushing for rapid buildouts of AI data centers—sparking widespread backlash among Americans—Barbati-Dajches noted that UCS has documented his administration making “policy choices and decisions that benefit favored interests (including tech and fossil fuel companies) over the health and safety of the public and planet.” Genesis Mission appears to follow that trend, critics suggested, along with Trump’s most recent executive order threatening to block state AI laws, which many consider a gift to the tech industry.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“And meanwhile, most Americans are clear they don’t trust AI and want it regulated—but this administration has opposed even basic guardrails,” Prabhakar said.&lt;/p&gt;
&lt;p&gt;Planning to closely monitor Genesis Mission, UCS is keen to get answers to many questions, such as “who will have access to the platform that’s being created as part of this initiative” and “who will own or will benefit from the outputs of this type of program?”&lt;/p&gt;
&lt;p&gt;Josephson said that it’s unlikely Genesis Mission will advance much before the midterm elections. In the next steps, Congress will have to approve funding for the mission, as its broad ambitions, if supported, would surely require structure to continue across multiple administrations.&lt;/p&gt;
&lt;p&gt;To Barbati-Dajches, it’s critical that Genesis Mission is “viewed in the context of [the Trump administration’s] pattern of anti-science actions.”&lt;/p&gt;
&lt;p&gt;“One of my main concerns is that this type of mission is being funded in the name of science and innovation when the administration has continuously and methodically attacked federal scientific systems since Inauguration Day,” Barbati-Dajches said.&lt;/p&gt;
&lt;p&gt;It’s unclear whether Genesis Mission will amount to anything but hype. But Josephson noted that perhaps the most blustery part of Trump’s order was a claim that “from the founding of the Republic, scientific discovery and technological innovation have driven American progress and prosperity.”&lt;/p&gt;
&lt;p&gt;The US only began funding research in the back half of the 19th century, Josephson said, “but the amount of money coming from the federal government to the sciences was limited until the Manhattan Project.” After that, the US emerged as a “leading scientific power” in the Cold War, not by racing for “global technology dominance,” as Trump wants, but by embracing science as a “national good.”&lt;/p&gt;
&lt;p&gt;As it stands now, Genesis Mission’s biggest flaw might stem from Trump’s disdain for DEI, which fueled his attacks on science and universities all year, Josephson suggested.&lt;/p&gt;
&lt;p&gt;“Funding science and technology and allowing the scientific community through peer review to determine what is the best science and to give funding to encourage young people to enter the science pipeline and to ensure that there are more women and people of color in the scientific community, so that more and more brains are taking part—there’s none of that in the Genesis Mission,” Josephson said.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 md:my-10 md:py-8 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Trump’s AI “Manhattan Project” will fail if DOGE cuts are kept, critics say.
    &lt;/p&gt;

          
    
    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/trump-project-genesis.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;By executive order last month, Donald Trump launched his so-called “Genesis Mission.”&lt;/p&gt;
&lt;p&gt;Described as a “historic national effort” to “invest in AI-enabled science to accelerate scientific advancement,” Trump claimed his mission would address key challenges to American energy dominance, innovation, and national security.&lt;/p&gt;
&lt;p&gt;This mission, Trump boasted, would be a game-changer to science akin to putting a man on the moon or firing the first nuclear weapons. By building “an integrated AI platform” trained on “the world’s largest collection” of federal scientific data sets, he promised, the government could set off cascades of scientific breakthroughs.&lt;/p&gt;
&lt;p&gt;Access to such a platform, Trump imagined, would supercharge top US labs, powering AI agents to do tasks like quickly test hypotheses and automate research workflows to speed up discoveries.&lt;/p&gt;
&lt;p&gt;However, the mission crucially depends on strengthening collaboration between public, private, and academic sectors. And Trump’s order is concerningly vague on how those partnerships will be structured and funded at a time when many scientists have been sidelined due to a flurry of Trump orders earlier this year that eliminated their funding or removed them from their labs.&lt;/p&gt;
&lt;p&gt;To critics, including scientists, policy experts, advocates, and historians, Trump’s order seems divorced from reality, given that he spent the past year attacking some of the very institutions the Genesis Mission would seem to depend on. Trump also seemed unclear about what can be achieved with AI and confused about how scientific progress is actually made, some critics suggested.&lt;/p&gt;
&lt;p&gt;Among the critics was Arati Prabhakar, who served as the director of the White House Office of Science and Technology Policy under the Biden administration. Prabhakar told Ars that Trump’s crippling cuts to government science agencies, research grant funding freezes, and attacks on universities must be repaired or his mission will fail.&lt;/p&gt;
&lt;p&gt;“After the Trump administration has inflicted so much damage to valuable datasets and publicly funded research, the new executive order is a Band-Aid on a giant gash,” Prabhakar said.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;US won’t lead in AI if Trump’s science attacks continue&lt;/h2&gt;
&lt;p&gt;Also skeptical of Trump’s plans is Kathryn Kelley, executive director for the Coalition for Academic Scientific Computation, an educational nonprofit representing more than 100 of “the nation’s most forward-thinking universities and computing centers,” CASC’s LinkedIn said.&lt;/p&gt;
&lt;p&gt;Her group is specifically dedicated to Genesis Mission-aligned goals, “advocating for the use of the most advanced computing technology to accelerate scientific discovery for national competitiveness, global security, and economic success.” And while Trump’s initiative could be considered “a step in the right direction,” Kelley told Ars that it will require alignment, stabilization, and follow-through to be executable at the scale envisioned, due to cuts earlier this year.&lt;/p&gt;
&lt;p&gt;“Many research institutions and national laboratories continue to experience funding uncertainty, program disruptions, and workforce instability stemming from earlier cuts,” Kelley told Ars.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Particular pain points considered critical to address for Genesis Mission to move forward include reversing “impacts on staffing, ongoing research, and student pipelines,” she suggested.&lt;/p&gt;
&lt;p&gt;In one prominent example, some Department of Government Efficiency (DOGE) cuts targeting workers at the National Science Foundation&amp;nbsp;&lt;span style="margin: 0px; padding: 0px;"&gt;hit hardest in the&amp;nbsp;branch designed to accelerate technology development across&lt;/span&gt; a wide range of research settings in the US. DOGE slashed workers there simply because it was the youngest directorate at NSF with the most workers in transition when Trump took office. As courts weighed legal challenges to cuts, whistleblowers warned that Trump was aiming to politicize and dismantle NSF.&lt;/p&gt;
&lt;p&gt;“Large-scale initiatives like Genesis rely on highly skilled personnel, robust infrastructure, and sustained program support—some of the very resources at NSF and other federal agencies that were disrupted,” Kelley told Ars. “Rebuilding trust, re-establishing lost programs, and stabilizing the research workforce will be essential to make this mission feasible.”&lt;/p&gt;
&lt;p&gt;Critics urged that Trump’s attacks on science also included messing with government datasets that scientists depend on. Since Trump’s second term started, scientists have watched valuable data get censored or scrubbed from government websites. Some researchers have rushed to recreate datasets independently&amp;nbsp;with the help of the Internet Archive.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Prabhakar pointed out that some “datasets that could improve health and prevent disasters are eroding or even disappearing due to this administration,” while universities training “the next generation of great researchers and innovators have reduced or even stopped graduate admissions because of Trump’s assault.”&lt;/p&gt;
&lt;p&gt;Without a massive undertaking to undo moves that critics have said undermined both US science and trust in it, Trump’s dreams of launching AI models that would propel a million moonshots could go down in history as merely hype.&lt;/p&gt;
&lt;p&gt;“Without robust data and research and without people’s trust, America won’t lead in AI,” Prabhakar said.&lt;/p&gt;
&lt;h2&gt;Genesis Mission shows Trump’s “tremendous ignorance”&lt;/h2&gt;
&lt;p&gt;For people in the science community, it’s hard to square Trump’s aggressive cuts from earlier this year with the broad ambition of Genesis Mission. Frustratingly, the president demands that scientists make discoveries on his timeline, without acknowledging AI’s limitations or how his attacks on science could be driving away talent that could help labs advance AI.&lt;/p&gt;
&lt;p&gt;In many fields, scientists are still exploring how AI can aid research. Trump’s order appears to politicize science by focusing on areas he favors—like critical materials, nuclear energy, biotechnology, and quantum computing—despite their limited AI applications or data-quality challenges. Meanwhile, critics noted that it overlooks areas where “supercharging” AI could perhaps be more impactful—but where Trump notably does not want to leave his mark—like climate science or vaccine research.&lt;/p&gt;
&lt;p&gt;It also lays out aggressive timelines for results, demanding that the Department of Energy Secretary, Chris Wright, “demonstrate an initial operating capability of the Platform for at least one of the national science and technology challenges” identified in less than a year (270 days). Ideally, Trump’s mission will have generated significant discoveries in key fields within the next three years before he leaves office, his order outlined.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Paul Josephson, a Colby College professor and expert in the history of 20th-century science and technology, told Ars that Genesis Mission deadlines differ from John F. Kennedy’s 10-year timeline to reach the moon.&lt;/p&gt;
&lt;p&gt;Trump’s order “shows tremendous ignorance of how science and technology work,” Josephson said. The White House is saying, “Tell me what your discoveries will be and how many there will be in three years,” Josephson said, expecting that “we can pick the places where we want discoveries and make them happen.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;“That’s not anything like how science works,” Josephson told Ars, reducing Genesis Mission to “a vision without policy” and “a hope without funding.”&lt;/p&gt;
&lt;p&gt;To Josephson, Trump’s order sounded “more like it came out of Silicon Valley” than out of talks with government scientists, seemingly rushing approvals of industry partnerships and incentives without mentioning what resources would be available to fund gutted labs or train the next generation of scientists. It’s perhaps notable that the order is DOE-centric and does not place the same emphasis on contributions from universities or national labs funded by NSF and the National Institutes of Health as it does on industry partners.&lt;/p&gt;
&lt;p&gt;Kelley told Ars that “many public datasets are already being used effectively in research and industry” in the ways that Trump intends his AI platform to amplify. However, “there are areas—such as advanced nuclear research or emerging energy technologies—where datasets are limited.” And Trump risks reducing Genesis Mission to bluster by claiming that an AI platform could drive breakthroughs to the major challenges he flagged in the short term.&lt;/p&gt;
&lt;p&gt;“There is a real risk that the EO’s ambitious framing could overpromise what AI can achieve in the near term without addressing foundational data gaps,” Kelley told Ars. “That said, even partial progress in these areas could provide valuable insights, but expectations need to be realistic.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;US on “thin ice” attracting scientific talent&lt;/h2&gt;
&lt;p&gt;Just as important as asking where Genesis Mission funding is coming from or who the funding is going to, Chris R. Glass asks: “Where’s the talent coming from?”&lt;/p&gt;
&lt;p&gt;Trump’s order does not forecast that, only vaguely referencing support for universities training scientists. This comes, of course, after the administration revoked an estimated $1.5 billion in federal grant money in 2025. Those grant cuts shrank the pipeline for PhD students at an “unprecedented rate,” Axios reported.&lt;/p&gt;
&lt;p&gt;A Boston College professor who researches global student mobility and the impact of emergent technology on learning, Glass told Ars that Trump has notably left international talent out of his AI plans, despite the prominent roles that both “domestic and international scientists play in our current leadership” in AI.&lt;/p&gt;
&lt;p&gt;In a recent Washington Post op-ed, Glass warned that “America is losing research scientists,” who are seeking more stable environments to set up their lives and conduct long-term studies.&lt;/p&gt;
&lt;p&gt;As the Trump administration has attacked immigrants, other governments like the European Union and China have benefited by offering friendlier visa systems to attract the best and brightest minds graduating from US universities. Of course, of the two, China is America’s bigger AI rival. Earlier this year, China began heavily recruiting American scientists spooked by Trump’s grant funding cuts, and Glass confirmed that China has continued those efforts with the AI race heating up. Meanwhile, Trump appears to be going the other direction, recently requiring a $100,000 payment for some skilled workers seeking non-immigrant visas.&lt;/p&gt;
&lt;p&gt;Throughout 2025, US universities’ ability to attract international students showed resilience, but “we’re on thin ice,” Glass told Ars, with that resilience “waning.”&lt;/p&gt;
&lt;p&gt;Currently, the US “is ranked the lowest among top destinations for its safety and welcoming and the lowest for its post-graduation visa policies,” Glass said, noting that doctoral students must affirm that they do not intend to immigrate, even though the majority of STEM PhD students stay in the US after graduating.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“They want to stay, and we want them to stay,” Glass told Ars.&lt;/p&gt;
&lt;p&gt;Another concerning outcome of Trump cuts that could hamper Genesis Mission: Entire research groups at many institutions were “displaced”—removed from their labs and left to work in cubicles without access to their equipment, Glass told Ars.&lt;/p&gt;
&lt;p&gt;“I think scientists want to go where the best sciences are being done, but eventually these kinds of friction points and these hostile policies make them redirect elsewhere, even temporarily redirect, earn their doctorate in Europe and hope that the policy environment in the US changes,” Glass said.&lt;/p&gt;
&lt;p&gt;To turn it around, Glass made several recommendations in his op-ed to help retain PhD graduates and create stable pathways for high-value talents. That includes suggesting that the Trump administration consider fast-tracking green cards for students in fields that Genesis Mission depends on, including AI and machine-learning researchers, quantum computing scientists, and semiconductor engineers.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;He also thinks the US should “unlock the O-1A visa for researchers and entrepreneurs” by redefining what makes someone an “extraordinary” talent and creating dedicated “founder tracks” for international talent, as Britain and Singapore do. That visa is “uncapped yet underused,” Glass said, only approving 4,500 STEM candidates in 2023.&lt;/p&gt;
&lt;p&gt;Without changes to the visa system, the US “risks redirecting those talent flows,” he said. “And like a river, once those talent flows get redirected, they are very difficult to reverse.”&lt;/p&gt;
&lt;p&gt;And it won’t just be international talents jumping ship, Glass suggested, but also possibly US scientists forced to continue navigating potentially more of Trump’s cuts and indirect costs in the coming years.&lt;/p&gt;
&lt;p&gt;“I think that’s the kind of thing that slowly eats away at someone’s desire to continue to do science in the United States,” Glass said.&lt;/p&gt;
&lt;p&gt;Glass told Ars that he expects the US to stay on a “downward trajectory,” driving away talent in 2026, which Josephson suggested “will damage science both for the short and long term.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Many universities figured out a one-year contingency plan, but reality will set in if funding continues to be cut,” Glass said.&lt;/p&gt;
&lt;h2&gt;Trump picking winners paints “a very troubling picture”&lt;/h2&gt;
&lt;p&gt;CASC’s Kelley told Ars that like university international student recruitment, “the US research ecosystem continues to be resilient, but the gap between ambitious goals and the current capacity must be carefully managed.”&lt;/p&gt;
&lt;p&gt;“While the Genesis Mission signals strong intentions to invest in science and technology, its success will depend on aligning resources, rebuilding workforce capacity, and thoughtfully integrating AI and data capabilities where they are most effective,” Kelley said.&lt;/p&gt;
&lt;p&gt;A scientist might be best positioned to understand the nuance that requires, but Josephson noted that Trump tasked his Science Advisor, Michael Kratsios, with leading the initiative. Unlike prior officials serving in that role, Kratsios is not a scientist and has no PhD, earning his BA in politics. Instead, Kratsios has strong industry ties, previously serving as chief of staff for venture capitalist Peter Thiel and managing director of a company called Scale AI.&lt;/p&gt;
&lt;p&gt;To Josephson, Kratsios as head of the mission—which “seems to be totally based on faith in AI and datasets to do everything”—makes the initiative seem more aligned with Silicon Valley ambitions than public good. That could be a problem since historically, it has never worked when governments attempt to “pick winners” or pass industrial policy with claims that “if we do this, we will come out on top.”&lt;/p&gt;
&lt;p&gt;“It’s a belief in AI as the cure or the panacea for all the world’s problems to ensure we’re a dominant technological power, but ignoring climate change, race, gender, anything that is important in daily life,” Josephson said.&lt;/p&gt;
&lt;p&gt;Josephson is also an expert in Russian and Soviet history, explaining that precedent shows there are “tremendous dangers” of governments controlling which sciences are funded. In some ways, he thinks Genesis Mission “smells of Putin,” he told Ars, warning that Trump’s attempts to hoard and censor science in 2025 have been “as damaging to science and technology in the world’s leading centers as totalitarian regimes have been.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“It reflects the general timbre of the Trump administration toward the scientific enterprise,” he suggested, saying that the president has embraced the “authoritarian view” that he “has the right to pick and choose which fields and which branches merit more support and which should not be funded at all.”&lt;/p&gt;
&lt;p&gt;Jules Barbati-Dajches, an analyst for the Center for Science and Democracy at the Union of Concerned Scientists (UCS), told Ars that in addition to cuts, Trump recently “weakened federal agency policies (called scientific integrity policies) that were specifically in place to protect federal agency science from political interference.” This further threatens scientific integrity, Barbati-Dajches warned in August.&lt;/p&gt;
&lt;p&gt;UCS has tracked “instances of science being sidelined, ignored, or misused by the federal government” across “multiple presidential administrations” for two decades, Barbati-Dajches told Ars. And although their methodology was recently updated, the current Trump administration stands out, as “the rate and impact of attacks on science we’ve observed over the past nine months far outpace anything UCS has tracked before,” Barbati-Dajches said.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Additionally, UCS has documented “cases of the administration using AI in their reports and research that raise concern” that AI initiatives like Genesis Mission may promote dubious claims to serve “politicized” outcomes, Barbati-Dajches said.&lt;/p&gt;
&lt;p&gt;“This altogether paints a very troubling picture,” Barbati-Dajches said. “Scientific innovation and discovery are exciting, important, and can help inform federal policy and guidance. But as history tells us (and recent history even more so), science in the federal government needs protective guardrails to keep it independent and free from undue influence.”&lt;/p&gt;
&lt;h2&gt;Genesis Mission overlooks AI for public good&lt;/h2&gt;
&lt;p&gt;With Trump pushing for rapid buildouts of AI data centers—sparking widespread backlash among Americans—Barbati-Dajches noted that UCS has documented his administration making “policy choices and decisions that benefit favored interests (including tech and fossil fuel companies) over the health and safety of the public and planet.” Genesis Mission appears to follow that trend, critics suggested, along with Trump’s most recent executive order threatening to block state AI laws, which many consider a gift to the tech industry.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“And meanwhile, most Americans are clear they don’t trust AI and want it regulated—but this administration has opposed even basic guardrails,” Prabhakar said.&lt;/p&gt;
&lt;p&gt;Planning to closely monitor Genesis Mission, UCS is keen to get answers to many questions, such as “who will have access to the platform that’s being created as part of this initiative” and “who will own or will benefit from the outputs of this type of program?”&lt;/p&gt;
&lt;p&gt;Josephson said that it’s unlikely Genesis Mission will advance much before the midterm elections. In the next steps, Congress will have to approve funding for the mission, as its broad ambitions, if supported, would surely require structure to continue across multiple administrations.&lt;/p&gt;
&lt;p&gt;To Barbati-Dajches, it’s critical that Genesis Mission is “viewed in the context of [the Trump administration’s] pattern of anti-science actions.”&lt;/p&gt;
&lt;p&gt;“One of my main concerns is that this type of mission is being funded in the name of science and innovation when the administration has continuously and methodically attacked federal scientific systems since Inauguration Day,” Barbati-Dajches said.&lt;/p&gt;
&lt;p&gt;It’s unclear whether Genesis Mission will amount to anything but hype. But Josephson noted that perhaps the most blustery part of Trump’s order was a claim that “from the founding of the Republic, scientific discovery and technological innovation have driven American progress and prosperity.”&lt;/p&gt;
&lt;p&gt;The US only began funding research in the back half of the 19th century, Josephson said, “but the amount of money coming from the federal government to the sciences was limited until the Manhattan Project.” After that, the US emerged as a “leading scientific power” in the Cold War, not by racing for “global technology dominance,” as Trump wants, but by embracing science as a “national good.”&lt;/p&gt;
&lt;p&gt;As it stands now, Genesis Mission’s biggest flaw might stem from Trump’s disdain for DEI, which fueled his attacks on science and universities all year, Josephson suggested.&lt;/p&gt;
&lt;p&gt;“Funding science and technology and allowing the scientific community through peer review to determine what is the best science and to give funding to encourage young people to enter the science pipeline and to ensure that there are more women and people of color in the scientific community, so that more and more brains are taking part—there’s none of that in the Genesis Mission,” Josephson said.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/12/trump-spent-2025-attacking-science-that-could-set-back-his-genesis-mission/</guid><pubDate>Wed, 17 Dec 2025 12:00:13 +0000</pubDate></item><item><title>Amazon reportedly in talks to invest $10B in OpenAI as circular deals stay popular (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/17/amazon-reportedly-in-talks-to-invest-10b-in-openai-as-circular-deals-stay-popular/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/GettyImages-2214107176.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon is in early discussions to invest as much as $10 billion in OpenAI in a deal that would see the AI lab using the e-commerce giant’s AI chips, CNBC reported. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If it materializes, the deal would value OpenAI at more than $500 billion, Bloomberg reported, citing an anonymous source. The Information was first to report on the story. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Amazon has been looking to diversify its bets in the AI race, which has so far seen it partner up and invest $8 billion in Anthropic, a rival to OpenAI. The e-commerce giant earlier this month also unveiled the latest iteration in its Trainium series of chips, and outlined the development of the next installment of those chips, complementing its cloud computing offerings via Amazon Web Services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;News of the deal comes a couple months after OpenAI completed its transition to a for-profit model, which gives it more freedom to strike deals with investors other than Microsoft, one of the company’s earliest backers with a stake of 27%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon investing in OpenAI would mark the latest in a series of circular deals in the AI space — major hardware manufacturers and cloud providers strike deals with young AI companies to use their products, while the upstarts commit to using their data centers and chips for training their AI models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This past March, OpenAI invested $350 million of equity into CoreWeave, which used the funds to buy chips from its backer Nvidia. Those same chips provide compute to OpenAI, which increases CoreWeave’s revenue and in the end makes OpenAI’s stake more valuable. Then in October, OpenAI signed a deal to pick up a 10% stake in AMD and committed to using the chipmaker’s AI GPUs, and also signed a chip usage agreement with Broadcom that month. And in November, the ChatGPT maker signed a $38 billion cloud computing deal with Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI and Amazon did not immediately respond to requests for comment.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/GettyImages-2214107176.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon is in early discussions to invest as much as $10 billion in OpenAI in a deal that would see the AI lab using the e-commerce giant’s AI chips, CNBC reported. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If it materializes, the deal would value OpenAI at more than $500 billion, Bloomberg reported, citing an anonymous source. The Information was first to report on the story. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Amazon has been looking to diversify its bets in the AI race, which has so far seen it partner up and invest $8 billion in Anthropic, a rival to OpenAI. The e-commerce giant earlier this month also unveiled the latest iteration in its Trainium series of chips, and outlined the development of the next installment of those chips, complementing its cloud computing offerings via Amazon Web Services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;News of the deal comes a couple months after OpenAI completed its transition to a for-profit model, which gives it more freedom to strike deals with investors other than Microsoft, one of the company’s earliest backers with a stake of 27%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon investing in OpenAI would mark the latest in a series of circular deals in the AI space — major hardware manufacturers and cloud providers strike deals with young AI companies to use their products, while the upstarts commit to using their data centers and chips for training their AI models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This past March, OpenAI invested $350 million of equity into CoreWeave, which used the funds to buy chips from its backer Nvidia. Those same chips provide compute to OpenAI, which increases CoreWeave’s revenue and in the end makes OpenAI’s stake more valuable. Then in October, OpenAI signed a deal to pick up a 10% stake in AMD and committed to using the chipmaker’s AI GPUs, and also signed a chip usage agreement with Broadcom that month. And in November, the ChatGPT maker signed a $38 billion cloud computing deal with Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI and Amazon did not immediately respond to requests for comment.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/17/amazon-reportedly-in-talks-to-invest-10b-in-openai-as-circular-deals-stay-popular/</guid><pubDate>Wed, 17 Dec 2025 12:03:36 +0000</pubDate></item><item><title>[NEW] The Open Evaluation Standard: Benchmarking NVIDIA Nemotron 3 Nano with NeMo Evaluator (Hugging Face - Blog)</title><link>https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe</link><description>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Seph Mard's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://cdn-avatars.huggingface.co/v1/production/uploads/688cf7e6026af0cf8ac969dd/zYPjGkU8LznN4CWbzaIO-.png" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
It has become increasingly challenging to assess whether a model’s
reported improvements reflect genuine advances or variations in
evaluation conditions, dataset composition, or training data that
mirrors benchmark tasks. The NVIDIA Nemotron approach to openness
addresses this by publishing transparent and reproducible evaluation
recipes that make results independently verifiable.
&lt;p&gt;NVIDIA released &lt;u&gt;Nemotron 3 Nano 30B
A3B&lt;/u&gt;
with an explicitly open evaluation approach to make that distinction
clear. Alongside the model card, we are publishing the complete
evaluation recipe used to generate the results, built with the
&lt;u&gt;NVIDIA NeMo
Evaluator&lt;/u&gt; library, so
anyone can rerun the evaluation pipeline, inspect the artifacts, and
analyze the outcomes independently.&lt;/p&gt;
&lt;p&gt;We believe that open innovation is the foundation of AI progress. This
level of transparency matters because most model evaluations omit
critical details. Configs, prompts, harness versions, runtime settings,
and logs are often missing or underspecified, and even small differences
in these parameters can materially change results. Without a complete
recipe, it’s nearly impossible to tell whether a model is genuinely
more intelligent or simply optimized for a benchmark.&lt;/p&gt;
&lt;p&gt;This blog shows developers exactly how to reproduce the evaluation
behind &lt;u&gt;Nemotron 3 Nano 30B
A3B&lt;/u&gt;
using fully open tools, configurations, and artifacts. You’ll learn how
the evaluation was run, why the methodology matters, and how to execute
the same end-to-end workflow using the NeMo Evaluator library so you can
verify results, compare models consistently, and build transparent
evaluation pipelines of your own.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Building a consistent and transparent evaluation workflow with NeMo Evaluator
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		A single, consistent evaluation system
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Developers and researchers need evaluation workflows they can rely on,
not one-off scripts that behave differently from model to model. NeMo
Evaluator provides a unified way to define benchmarks, prompts,
configuration, and runtime behavior once, then reuse that methodology
across models and releases. This avoids the common scenario where the
evaluation setup quietly changes between runs, making comparisons over
time difficult or misleading.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Methodology independent of inference setup
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Model outputs can vary by inference backend and configuration, so
evaluation tools should never be tied to a single inference solution.
Locking an evaluation tool to one inference solution would limit its
usefulness. NeMo Evaluator avoids this by separating the evaluation
pipeline from the inference backend, allowing the same configuration to
run against hosted endpoints, local deployments, or third-party
providers. This separation enables meaningful comparisons even when you
change infrastructure or inference engines.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Built to scale beyond one-off experiments
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Many evaluation pipelines work once and then break down as the scope
expands. NeMo Evaluator is designed to scale from quick,
single-benchmark validation to full model card suites and repeated
evaluations across multiple models. The launcher, artifact layout, and
configuration model support ongoing workflows, not just isolated
experiments, so teams can maintain consistent evaluation practices over
time.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Auditability with structured artifacts and logs
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Transparent evaluation requires more than final scores. Each evaluation
run produces structured results and logs by default, making it easy to
inspect how scores were computed, understand score calculations, debug
unexpected behavior, and conduct deeper analysis. Each component of the
evaluation is captured and reproducible.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		A shared evaluation standard
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;By releasing &lt;u&gt;Nemotron 3 Nano 30B
A3B&lt;/u&gt;
with its &lt;u&gt;full evaluation
recipe&lt;/u&gt;,
NVIDIA is providing a reference methodology that the community can run,
inspect, and build upon. Using the same configuration and tools brings
consistency to how benchmarks are selected, executed, and interpreted,
enabling more reliable comparisons across models, providers, and
releases.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Open evaluation for Nemotron 3 Nano
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Open evaluation means publishing not just the final results, but the
full methodology behind them, so benchmarks are run consistently, and
results can be compared meaningfully over time. For &lt;u&gt;Nemotron 3 Nano
30B
A3B&lt;/u&gt;,
this includes open‑source tooling, transparent configurations, and
reproducible artifacts that anyone can run end‑to‑end.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Open-source model evaluation tooling
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;u&gt;NeMo
Evaluator&lt;/u&gt; is an
open-source library designed for robust, reproducible, and scalable
evaluation of generative models. Instead of introducing yet another
standalone benchmark runner, it acts as a unifying orchestration layer
that brings multiple evaluation harnesses under a single, consistent
interface.&lt;/p&gt;
&lt;p&gt;Under this architecture, NeMo Evaluator integrates and coordinates
hundreds of benchmarks from many widely used evaluation harnesses,
including &lt;u&gt;NeMo
Skills&lt;/u&gt;
for Nemotron instruction-following, tool use, and agentic evaluations,
as well as the &lt;u&gt;LM Evaluation
Harness&lt;/u&gt;
for base model and pre-training benchmarks, and many more (&lt;u&gt;full
benchmark
catalog&lt;/u&gt;).
Each harness retains its native logic, datasets, and scoring semantics,
while NeMo Evaluator standardizes how they are configured, executed, and
logged.&lt;/p&gt;
&lt;p&gt;This provides two practical advantages: teams can run diverse benchmark
categories using a single configuration without rewriting custom
evaluation scripts, and results from different harnesses are stored and
inspected in a consistent, predictable way, even when the underlying
tasks differ. The same orchestration framework used internally by
NVIDIA’s Nemotron research and model‑evaluation teams is now available
to the community, enabling developers to run heterogeneous,
multi‑harness evaluations through a shared, auditable workflow.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Open configurations
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We published the exact YAML configuration used for the &lt;u&gt;Nemotron 3
Nano 30B A3B model
card&lt;/u&gt;
evaluation with NeMo Evaluator. This includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;model inference and deployment settings&lt;/li&gt;
&lt;li&gt;benchmark and task selection&lt;/li&gt;
&lt;li&gt;benchmark-specific parameters such as sampling, repeats, and prompt
templates&lt;/li&gt;
&lt;li&gt;runtime controls including parallelism, timeouts, and retries&lt;/li&gt;
&lt;li&gt;output paths and artifact layout&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the same configuration means running the same evaluation
methodology.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Open logs and artifacts
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Each evaluation run produces structured, inspectable outputs, including
per‑task &lt;code&gt;results.json&lt;/code&gt; files, execution logs for debugging and
auditability, and artifacts organized by task for easy comparison. This
structure makes it possible to understand not only the final scores, but
also how those scores were produced and to perform deeper analysis of
model behavior.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		The reproducibility workflow
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Reproducing &lt;u&gt;Nemotron 3 Nano 30B A3B model
card&lt;/u&gt;
results follows a simple loop:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start from the released model checkpoint or hosted endpoint&lt;/li&gt;
&lt;li&gt;Use the &lt;u&gt;published NeMo Evaluator
config&lt;/u&gt;&lt;/li&gt;
&lt;li&gt;Execute the evaluation with a single CLI command&lt;/li&gt;
&lt;li&gt;Inspect logs and artifacts, and compare results to the model card&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The same workflow applies to any model you evaluate using NeMo
Evaluator. You can point the evaluation at a hosted endpoint or a local
deployment, including common inference providers such as
&lt;u&gt;HuggingFace&lt;/u&gt;,
&lt;u&gt;build.nvidia.com&lt;/u&gt;,
and
&lt;u&gt;OpenRouter&lt;/u&gt;.
The key requirement is access to the model, either as weights you can
serve or as an endpoint you can call. For this tutorial, we use the
hosted endpoint on
&lt;u&gt;build.nvidia.com&lt;/u&gt;.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Reproducing Nemotron 3 Nano benchmark results
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img alt="nano-3-nemotron" src="https://cdn-uploads.huggingface.co/production/uploads/688cf7e6026af0cf8ac969dd/k_VBb7Duq1_96Y8mxWOdT.png" /&gt;&lt;/p&gt;
&lt;p&gt;This tutorial reproduces the evaluation results for &lt;u&gt;NVIDIA Nemotron
3 Nano 30B
A3B&lt;/u&gt;
using NeMo Evaluator. The step-by-step tutorial, including the
&lt;u&gt;published configs used for the model card
evaluation&lt;/u&gt;,
is available on GitHub. Although we have focused this tutorial on the
Nemotron 3 Nano 30B A3B, we also published &lt;u&gt;recipes for the base
model
evaluation&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;This walkthrough runs a comprehensive evaluation suite of the &lt;u&gt;published configs used for the model card
evaluation&lt;/u&gt; for &lt;u&gt;NVIDIA Nemotron
3 Nano 30B A3B&lt;/u&gt; using the following benchmarks:&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Benchmark&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;BFCL v4&lt;/td&gt;
&lt;td&gt;53.8&lt;/td&gt;
&lt;td&gt;Function Calling&lt;/td&gt;
&lt;td&gt;Berkeley Function Calling Leaderboard v4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LiveCodeBench (v6 2025-08–2025-05)&lt;/td&gt;
&lt;td&gt;68.3&lt;/td&gt;
&lt;td&gt;Coding&lt;/td&gt;
&lt;td&gt;Real-world coding problems evaluation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MMLU-Pro&lt;/td&gt;
&lt;td&gt;78.3&lt;/td&gt;
&lt;td&gt;Knowledge&lt;/td&gt;
&lt;td&gt;Multi-task language understanding (10-choice)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPQA&lt;/td&gt;
&lt;td&gt;73.0&lt;/td&gt;
&lt;td&gt;Science&lt;/td&gt;
&lt;td&gt;Graduate-level science questions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AIME 2025&lt;/td&gt;
&lt;td&gt;89.1&lt;/td&gt;
&lt;td&gt;Mathematics&lt;/td&gt;
&lt;td&gt;American Invitational Mathematics Exam&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SciCode&lt;/td&gt;
&lt;td&gt;33.3&lt;/td&gt;
&lt;td&gt;Scientific Coding&lt;/td&gt;
&lt;td&gt;Scientific programming challenges&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IFBench&lt;/td&gt;
&lt;td&gt;71.5&lt;/td&gt;
&lt;td&gt;Instruction Following&lt;/td&gt;
&lt;td&gt;Instruction following benchmark&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HLE&lt;/td&gt;
&lt;td&gt;10.6&lt;/td&gt;
&lt;td&gt;Humanity's Last Exam&lt;/td&gt;
&lt;td&gt;Expert-level questions across domains&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;For Model Card details, see the &lt;u&gt;NVIDIA Nemotron
3 Nano 30B A3B Model Card&lt;/u&gt;. For a deep dive into the architecture, datasets, and benchmarks, read the full &lt;u&gt;Nemotron 3 Nano Technical Report&lt;/u&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		1. Install NeMo Evaluator Launcher
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;pip install nemo-evaluator-launcher&lt;/code&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		2. Set required environment variables
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;# NVIDIA endpoint access
export NGC_API_KEY="your-ngc-api-key"

# Hugging Face access
export HF_TOKEN="your-huggingface-token"

# Required only for judge-based benchmarks such as HLE
export JUDGE_API_KEY="your-judge-api-key"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Optional but recommended for faster reruns:&lt;/em&gt;
&lt;code&gt;export HF_HOME="/path/to/your/huggingface/cache"&lt;/code&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		3. Model endpoint
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The evaluation uses the NVIDIA API endpoint hosted on
&lt;u&gt;build.nvidia.com&lt;/u&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;target:
  api_endpoint:
    model_id: nvidia/nemotron-nano-3-30b-a3b
    url: https://integrate.api.nvidia.com/v1/chat/completions
    api_key_name: NGC_API_KEY
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evaluations can be run against common inference providers such as
&lt;u&gt;HuggingFace&lt;/u&gt;,
&lt;u&gt;build.nvidia.com&lt;/u&gt;,
or
&lt;u&gt;OpenRouter&lt;/u&gt;,
or anywhere that the model has an available endpoint.
&lt;span class="mark"&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If you're hosting the model locally or using a
different endpoint:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nemo-evaluator-launcher run \
  --config local_nvidia_nemotron_3_nano_30b_a3b.yaml \
  -o target.api_endpoint.url=http://localhost:8000/v1/chat/completions
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		4. Run the full evaluation suite
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Preview the run without executing using &lt;code&gt;--dry-run&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nemo-evaluator-launcher run \
  --config local_nvidia_nemotron_3_nano_30b_a3b.yaml \
  --dry-run
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the examples directory, run the evaluation using the YAML
configuration provided:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nemo-evaluator-launcher run \
  --config /path/to/examples/nemotron/local_nvidia_nemotron_3_nano_30b_a3b.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class="mark"&gt;Note that for quick testing, you can limit the number
of samples by setting&lt;/span&gt; &lt;code&gt;limit_samples&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nemo-evaluator-launcher run \
  --config local_nvidia_nemotron_3_nano_30b_a3b.yaml \
  -o evaluation.nemo_evaluator_config.config.params.limit_samples=10
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		5. Running an individual benchmark
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;span class="mark"&gt;You can run specific benchmarks using the&lt;/span&gt; &lt;code&gt;-t&lt;/code&gt;
&lt;span class="mark"&gt;flag (from the&lt;/span&gt; examples/nemotron
&lt;span class="mark"&gt;directory):&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Run only MMLU-Pro
nemo-evaluator-launcher run --config local_nvidia_nemotron_3_nano_30b_a3b.yaml -t ns_mmlu_pro

# Run only coding benchmarks
nemo-evaluator-launcher run --config local_nvidia_nemotron_3_nano_30b_a3b.yaml -t ns_livecodebench

# Run multiple specific benchmarks
nemo-evaluator-launcher run --config local_nvidia_nemotron_3_nano_30b_a3b.yaml -t ns_gpqa -t ns_aime2025
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		6. Monitor execution and inspect results
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;# Check status of a specific job
nemo-evaluator-launcher status
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Stream logs for a specific job
nemo-evaluator-launcher logs &amp;lt;job-id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results are written to the defined output directory:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-tree"&gt;results_nvidia_nemotron_3_nano_30b_a3b/
├── artifacts/
│   └── &amp;lt;task_name&amp;gt;/
│       └── results.json
└── logs/
    └── stdout.log
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Interpreting results
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;When reproducing evaluations, you may observe small differences in final
scores across runs. This variance reflects the probabilistic nature of
LLMs rather than an issue with the evaluation pipeline. Modern
evaluation introduces several sources of non‑determinism: decoding
settings, repeated trials, judge‑based scoring, parallel execution, and
differences in serving infrastructure. All of which can lead to slight
fluctuations.&lt;/p&gt;
&lt;p&gt;The purpose of open evaluation is not to force bit-wise identical
outputs, but to deliver &lt;strong&gt;methodological consistency&lt;/strong&gt; with clear
provenance of evaluation results. To ensure your evaluation aligns with
the reference standard, verify the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Configuration&lt;/strong&gt;: use the published NeMo Evaluator YAML without
modification, or document any changes explicitly&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Benchmark selection&lt;/strong&gt;: run the intended tasks, task versions, and
prompt templates&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inference target&lt;/strong&gt;: verify you are evaluating the intended model and
endpoint, including chat template behavior and reasoning settings when
relevant&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Execution settings&lt;/strong&gt;: keep runtime parameters consistent, including
repeats, parallelism, timeouts, and retry behavior&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outputs&lt;/strong&gt;: confirm artifacts and logs are complete and follow the
expected structure for each task&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When these elements are consistent, your results represent a valid
reproduction of the methodology, even if individual runs differ
slightly. NeMo Evaluator simplifies this process, tying benchmark
definitions, prompts, runtime settings, and inference configuration into
a single auditable workflow to minimize inconsistencies.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion: A more transparent standard for open models
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The evaluation recipe released alongside Nemotron 3 Nano represents a
meaningful step toward a more transparent and reliable approach to
open-model evaluation. We are moving away from evaluation as a
collection of bespoke, "black box" scripts, and towards a defined system
where benchmark selection, prompts, and execution semantics are encoded
into a transparent workflow.&lt;/p&gt;
&lt;p&gt;For developers and researchers, this transparency changes what it means
to share results. A score is only as trustworthy as the methodology
behind it and making that methodology public is what enables the
community to verify claims, compare models fairly, and continue building
on shared foundations. With open evaluation configurations, open
artifacts, and open tooling, Nemotron 3 Nano demonstrates what that
commitment to openness looks like in practice.&lt;/p&gt;
&lt;p&gt;NeMo Evaluator supports this shift by providing a consistent
benchmarking methodology across models, releases, and inference
environments. The objective isn’t identical numbers on every run; it’s
confidence in an evaluation methodology that is explicit, inspectable,
and repeatable. And for organizations that need automated or large‑scale
evaluation pipelines, a separate microservice offering provides an
enterprise‑ready &lt;u&gt;NeMo Evaluator
microservice&lt;/u&gt; built on
the same evaluation principles.&lt;/p&gt;
&lt;p&gt; Use the published &lt;u&gt;NeMo Evaluator
    evaluation configuration&lt;/u&gt; for an end-to-end walkthrough of the evaluation recipe. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Join the Community!&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;&lt;u&gt;NeMo
Evaluator&lt;/u&gt; is fully open
source, and community input is essential to shaping the future of open
evaluation. If there’s a benchmark you’d like us to support or an
improvement you want to propose, open an issue, or contribute directly
on GitHub. Your contributions help strengthen the ecosystem and advance
a shared, transparent standard for evaluating generative models.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Seph Mard's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://cdn-avatars.huggingface.co/v1/production/uploads/688cf7e6026af0cf8ac969dd/zYPjGkU8LznN4CWbzaIO-.png" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
It has become increasingly challenging to assess whether a model’s
reported improvements reflect genuine advances or variations in
evaluation conditions, dataset composition, or training data that
mirrors benchmark tasks. The NVIDIA Nemotron approach to openness
addresses this by publishing transparent and reproducible evaluation
recipes that make results independently verifiable.
&lt;p&gt;NVIDIA released &lt;u&gt;Nemotron 3 Nano 30B
A3B&lt;/u&gt;
with an explicitly open evaluation approach to make that distinction
clear. Alongside the model card, we are publishing the complete
evaluation recipe used to generate the results, built with the
&lt;u&gt;NVIDIA NeMo
Evaluator&lt;/u&gt; library, so
anyone can rerun the evaluation pipeline, inspect the artifacts, and
analyze the outcomes independently.&lt;/p&gt;
&lt;p&gt;We believe that open innovation is the foundation of AI progress. This
level of transparency matters because most model evaluations omit
critical details. Configs, prompts, harness versions, runtime settings,
and logs are often missing or underspecified, and even small differences
in these parameters can materially change results. Without a complete
recipe, it’s nearly impossible to tell whether a model is genuinely
more intelligent or simply optimized for a benchmark.&lt;/p&gt;
&lt;p&gt;This blog shows developers exactly how to reproduce the evaluation
behind &lt;u&gt;Nemotron 3 Nano 30B
A3B&lt;/u&gt;
using fully open tools, configurations, and artifacts. You’ll learn how
the evaluation was run, why the methodology matters, and how to execute
the same end-to-end workflow using the NeMo Evaluator library so you can
verify results, compare models consistently, and build transparent
evaluation pipelines of your own.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Building a consistent and transparent evaluation workflow with NeMo Evaluator
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		A single, consistent evaluation system
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Developers and researchers need evaluation workflows they can rely on,
not one-off scripts that behave differently from model to model. NeMo
Evaluator provides a unified way to define benchmarks, prompts,
configuration, and runtime behavior once, then reuse that methodology
across models and releases. This avoids the common scenario where the
evaluation setup quietly changes between runs, making comparisons over
time difficult or misleading.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Methodology independent of inference setup
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Model outputs can vary by inference backend and configuration, so
evaluation tools should never be tied to a single inference solution.
Locking an evaluation tool to one inference solution would limit its
usefulness. NeMo Evaluator avoids this by separating the evaluation
pipeline from the inference backend, allowing the same configuration to
run against hosted endpoints, local deployments, or third-party
providers. This separation enables meaningful comparisons even when you
change infrastructure or inference engines.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Built to scale beyond one-off experiments
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Many evaluation pipelines work once and then break down as the scope
expands. NeMo Evaluator is designed to scale from quick,
single-benchmark validation to full model card suites and repeated
evaluations across multiple models. The launcher, artifact layout, and
configuration model support ongoing workflows, not just isolated
experiments, so teams can maintain consistent evaluation practices over
time.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Auditability with structured artifacts and logs
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Transparent evaluation requires more than final scores. Each evaluation
run produces structured results and logs by default, making it easy to
inspect how scores were computed, understand score calculations, debug
unexpected behavior, and conduct deeper analysis. Each component of the
evaluation is captured and reproducible.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		A shared evaluation standard
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;By releasing &lt;u&gt;Nemotron 3 Nano 30B
A3B&lt;/u&gt;
with its &lt;u&gt;full evaluation
recipe&lt;/u&gt;,
NVIDIA is providing a reference methodology that the community can run,
inspect, and build upon. Using the same configuration and tools brings
consistency to how benchmarks are selected, executed, and interpreted,
enabling more reliable comparisons across models, providers, and
releases.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Open evaluation for Nemotron 3 Nano
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Open evaluation means publishing not just the final results, but the
full methodology behind them, so benchmarks are run consistently, and
results can be compared meaningfully over time. For &lt;u&gt;Nemotron 3 Nano
30B
A3B&lt;/u&gt;,
this includes open‑source tooling, transparent configurations, and
reproducible artifacts that anyone can run end‑to‑end.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Open-source model evaluation tooling
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;u&gt;NeMo
Evaluator&lt;/u&gt; is an
open-source library designed for robust, reproducible, and scalable
evaluation of generative models. Instead of introducing yet another
standalone benchmark runner, it acts as a unifying orchestration layer
that brings multiple evaluation harnesses under a single, consistent
interface.&lt;/p&gt;
&lt;p&gt;Under this architecture, NeMo Evaluator integrates and coordinates
hundreds of benchmarks from many widely used evaluation harnesses,
including &lt;u&gt;NeMo
Skills&lt;/u&gt;
for Nemotron instruction-following, tool use, and agentic evaluations,
as well as the &lt;u&gt;LM Evaluation
Harness&lt;/u&gt;
for base model and pre-training benchmarks, and many more (&lt;u&gt;full
benchmark
catalog&lt;/u&gt;).
Each harness retains its native logic, datasets, and scoring semantics,
while NeMo Evaluator standardizes how they are configured, executed, and
logged.&lt;/p&gt;
&lt;p&gt;This provides two practical advantages: teams can run diverse benchmark
categories using a single configuration without rewriting custom
evaluation scripts, and results from different harnesses are stored and
inspected in a consistent, predictable way, even when the underlying
tasks differ. The same orchestration framework used internally by
NVIDIA’s Nemotron research and model‑evaluation teams is now available
to the community, enabling developers to run heterogeneous,
multi‑harness evaluations through a shared, auditable workflow.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Open configurations
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We published the exact YAML configuration used for the &lt;u&gt;Nemotron 3
Nano 30B A3B model
card&lt;/u&gt;
evaluation with NeMo Evaluator. This includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;model inference and deployment settings&lt;/li&gt;
&lt;li&gt;benchmark and task selection&lt;/li&gt;
&lt;li&gt;benchmark-specific parameters such as sampling, repeats, and prompt
templates&lt;/li&gt;
&lt;li&gt;runtime controls including parallelism, timeouts, and retries&lt;/li&gt;
&lt;li&gt;output paths and artifact layout&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the same configuration means running the same evaluation
methodology.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Open logs and artifacts
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Each evaluation run produces structured, inspectable outputs, including
per‑task &lt;code&gt;results.json&lt;/code&gt; files, execution logs for debugging and
auditability, and artifacts organized by task for easy comparison. This
structure makes it possible to understand not only the final scores, but
also how those scores were produced and to perform deeper analysis of
model behavior.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		The reproducibility workflow
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Reproducing &lt;u&gt;Nemotron 3 Nano 30B A3B model
card&lt;/u&gt;
results follows a simple loop:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start from the released model checkpoint or hosted endpoint&lt;/li&gt;
&lt;li&gt;Use the &lt;u&gt;published NeMo Evaluator
config&lt;/u&gt;&lt;/li&gt;
&lt;li&gt;Execute the evaluation with a single CLI command&lt;/li&gt;
&lt;li&gt;Inspect logs and artifacts, and compare results to the model card&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The same workflow applies to any model you evaluate using NeMo
Evaluator. You can point the evaluation at a hosted endpoint or a local
deployment, including common inference providers such as
&lt;u&gt;HuggingFace&lt;/u&gt;,
&lt;u&gt;build.nvidia.com&lt;/u&gt;,
and
&lt;u&gt;OpenRouter&lt;/u&gt;.
The key requirement is access to the model, either as weights you can
serve or as an endpoint you can call. For this tutorial, we use the
hosted endpoint on
&lt;u&gt;build.nvidia.com&lt;/u&gt;.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Reproducing Nemotron 3 Nano benchmark results
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img alt="nano-3-nemotron" src="https://cdn-uploads.huggingface.co/production/uploads/688cf7e6026af0cf8ac969dd/k_VBb7Duq1_96Y8mxWOdT.png" /&gt;&lt;/p&gt;
&lt;p&gt;This tutorial reproduces the evaluation results for &lt;u&gt;NVIDIA Nemotron
3 Nano 30B
A3B&lt;/u&gt;
using NeMo Evaluator. The step-by-step tutorial, including the
&lt;u&gt;published configs used for the model card
evaluation&lt;/u&gt;,
is available on GitHub. Although we have focused this tutorial on the
Nemotron 3 Nano 30B A3B, we also published &lt;u&gt;recipes for the base
model
evaluation&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;This walkthrough runs a comprehensive evaluation suite of the &lt;u&gt;published configs used for the model card
evaluation&lt;/u&gt; for &lt;u&gt;NVIDIA Nemotron
3 Nano 30B A3B&lt;/u&gt; using the following benchmarks:&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Benchmark&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;BFCL v4&lt;/td&gt;
&lt;td&gt;53.8&lt;/td&gt;
&lt;td&gt;Function Calling&lt;/td&gt;
&lt;td&gt;Berkeley Function Calling Leaderboard v4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LiveCodeBench (v6 2025-08–2025-05)&lt;/td&gt;
&lt;td&gt;68.3&lt;/td&gt;
&lt;td&gt;Coding&lt;/td&gt;
&lt;td&gt;Real-world coding problems evaluation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MMLU-Pro&lt;/td&gt;
&lt;td&gt;78.3&lt;/td&gt;
&lt;td&gt;Knowledge&lt;/td&gt;
&lt;td&gt;Multi-task language understanding (10-choice)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPQA&lt;/td&gt;
&lt;td&gt;73.0&lt;/td&gt;
&lt;td&gt;Science&lt;/td&gt;
&lt;td&gt;Graduate-level science questions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AIME 2025&lt;/td&gt;
&lt;td&gt;89.1&lt;/td&gt;
&lt;td&gt;Mathematics&lt;/td&gt;
&lt;td&gt;American Invitational Mathematics Exam&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SciCode&lt;/td&gt;
&lt;td&gt;33.3&lt;/td&gt;
&lt;td&gt;Scientific Coding&lt;/td&gt;
&lt;td&gt;Scientific programming challenges&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IFBench&lt;/td&gt;
&lt;td&gt;71.5&lt;/td&gt;
&lt;td&gt;Instruction Following&lt;/td&gt;
&lt;td&gt;Instruction following benchmark&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HLE&lt;/td&gt;
&lt;td&gt;10.6&lt;/td&gt;
&lt;td&gt;Humanity's Last Exam&lt;/td&gt;
&lt;td&gt;Expert-level questions across domains&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;For Model Card details, see the &lt;u&gt;NVIDIA Nemotron
3 Nano 30B A3B Model Card&lt;/u&gt;. For a deep dive into the architecture, datasets, and benchmarks, read the full &lt;u&gt;Nemotron 3 Nano Technical Report&lt;/u&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		1. Install NeMo Evaluator Launcher
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;pip install nemo-evaluator-launcher&lt;/code&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		2. Set required environment variables
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;# NVIDIA endpoint access
export NGC_API_KEY="your-ngc-api-key"

# Hugging Face access
export HF_TOKEN="your-huggingface-token"

# Required only for judge-based benchmarks such as HLE
export JUDGE_API_KEY="your-judge-api-key"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Optional but recommended for faster reruns:&lt;/em&gt;
&lt;code&gt;export HF_HOME="/path/to/your/huggingface/cache"&lt;/code&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		3. Model endpoint
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The evaluation uses the NVIDIA API endpoint hosted on
&lt;u&gt;build.nvidia.com&lt;/u&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;target:
  api_endpoint:
    model_id: nvidia/nemotron-nano-3-30b-a3b
    url: https://integrate.api.nvidia.com/v1/chat/completions
    api_key_name: NGC_API_KEY
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evaluations can be run against common inference providers such as
&lt;u&gt;HuggingFace&lt;/u&gt;,
&lt;u&gt;build.nvidia.com&lt;/u&gt;,
or
&lt;u&gt;OpenRouter&lt;/u&gt;,
or anywhere that the model has an available endpoint.
&lt;span class="mark"&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If you're hosting the model locally or using a
different endpoint:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nemo-evaluator-launcher run \
  --config local_nvidia_nemotron_3_nano_30b_a3b.yaml \
  -o target.api_endpoint.url=http://localhost:8000/v1/chat/completions
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		4. Run the full evaluation suite
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Preview the run without executing using &lt;code&gt;--dry-run&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nemo-evaluator-launcher run \
  --config local_nvidia_nemotron_3_nano_30b_a3b.yaml \
  --dry-run
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the examples directory, run the evaluation using the YAML
configuration provided:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nemo-evaluator-launcher run \
  --config /path/to/examples/nemotron/local_nvidia_nemotron_3_nano_30b_a3b.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class="mark"&gt;Note that for quick testing, you can limit the number
of samples by setting&lt;/span&gt; &lt;code&gt;limit_samples&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nemo-evaluator-launcher run \
  --config local_nvidia_nemotron_3_nano_30b_a3b.yaml \
  -o evaluation.nemo_evaluator_config.config.params.limit_samples=10
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		5. Running an individual benchmark
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;span class="mark"&gt;You can run specific benchmarks using the&lt;/span&gt; &lt;code&gt;-t&lt;/code&gt;
&lt;span class="mark"&gt;flag (from the&lt;/span&gt; examples/nemotron
&lt;span class="mark"&gt;directory):&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Run only MMLU-Pro
nemo-evaluator-launcher run --config local_nvidia_nemotron_3_nano_30b_a3b.yaml -t ns_mmlu_pro

# Run only coding benchmarks
nemo-evaluator-launcher run --config local_nvidia_nemotron_3_nano_30b_a3b.yaml -t ns_livecodebench

# Run multiple specific benchmarks
nemo-evaluator-launcher run --config local_nvidia_nemotron_3_nano_30b_a3b.yaml -t ns_gpqa -t ns_aime2025
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		6. Monitor execution and inspect results
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;# Check status of a specific job
nemo-evaluator-launcher status
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Stream logs for a specific job
nemo-evaluator-launcher logs &amp;lt;job-id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results are written to the defined output directory:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-tree"&gt;results_nvidia_nemotron_3_nano_30b_a3b/
├── artifacts/
│   └── &amp;lt;task_name&amp;gt;/
│       └── results.json
└── logs/
    └── stdout.log
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Interpreting results
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;When reproducing evaluations, you may observe small differences in final
scores across runs. This variance reflects the probabilistic nature of
LLMs rather than an issue with the evaluation pipeline. Modern
evaluation introduces several sources of non‑determinism: decoding
settings, repeated trials, judge‑based scoring, parallel execution, and
differences in serving infrastructure. All of which can lead to slight
fluctuations.&lt;/p&gt;
&lt;p&gt;The purpose of open evaluation is not to force bit-wise identical
outputs, but to deliver &lt;strong&gt;methodological consistency&lt;/strong&gt; with clear
provenance of evaluation results. To ensure your evaluation aligns with
the reference standard, verify the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Configuration&lt;/strong&gt;: use the published NeMo Evaluator YAML without
modification, or document any changes explicitly&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Benchmark selection&lt;/strong&gt;: run the intended tasks, task versions, and
prompt templates&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inference target&lt;/strong&gt;: verify you are evaluating the intended model and
endpoint, including chat template behavior and reasoning settings when
relevant&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Execution settings&lt;/strong&gt;: keep runtime parameters consistent, including
repeats, parallelism, timeouts, and retry behavior&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outputs&lt;/strong&gt;: confirm artifacts and logs are complete and follow the
expected structure for each task&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When these elements are consistent, your results represent a valid
reproduction of the methodology, even if individual runs differ
slightly. NeMo Evaluator simplifies this process, tying benchmark
definitions, prompts, runtime settings, and inference configuration into
a single auditable workflow to minimize inconsistencies.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion: A more transparent standard for open models
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The evaluation recipe released alongside Nemotron 3 Nano represents a
meaningful step toward a more transparent and reliable approach to
open-model evaluation. We are moving away from evaluation as a
collection of bespoke, "black box" scripts, and towards a defined system
where benchmark selection, prompts, and execution semantics are encoded
into a transparent workflow.&lt;/p&gt;
&lt;p&gt;For developers and researchers, this transparency changes what it means
to share results. A score is only as trustworthy as the methodology
behind it and making that methodology public is what enables the
community to verify claims, compare models fairly, and continue building
on shared foundations. With open evaluation configurations, open
artifacts, and open tooling, Nemotron 3 Nano demonstrates what that
commitment to openness looks like in practice.&lt;/p&gt;
&lt;p&gt;NeMo Evaluator supports this shift by providing a consistent
benchmarking methodology across models, releases, and inference
environments. The objective isn’t identical numbers on every run; it’s
confidence in an evaluation methodology that is explicit, inspectable,
and repeatable. And for organizations that need automated or large‑scale
evaluation pipelines, a separate microservice offering provides an
enterprise‑ready &lt;u&gt;NeMo Evaluator
microservice&lt;/u&gt; built on
the same evaluation principles.&lt;/p&gt;
&lt;p&gt; Use the published &lt;u&gt;NeMo Evaluator
    evaluation configuration&lt;/u&gt; for an end-to-end walkthrough of the evaluation recipe. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Join the Community!&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;&lt;u&gt;NeMo
Evaluator&lt;/u&gt; is fully open
source, and community input is essential to shaping the future of open
evaluation. If there’s a benchmark you’d like us to support or an
improvement you want to propose, open an issue, or contribute directly
on GitHub. Your contributions help strengthen the ecosystem and advance
a shared, transparent standard for evaluating generative models.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe</guid><pubDate>Wed, 17 Dec 2025 13:22:18 +0000</pubDate></item><item><title>[NEW] Skana Robotics helps fleets of underwater robots communicate with each other (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/17/skana-robotics-helps-fleets-of-underwater-robots-communicate-with-each-other/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Skana.jpg?resize=1200,1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Underwater autonomous vessels and robots could play a substantial role in defense operations, but submersibles have historically had trouble communicating across large distances unless they rose to the surface. But coming up to transmit poses the very obvious risk of being exposed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Skana Robotics thinks it’s made a breakthrough with underwater communications using AI — but not the large language models the industry touts today.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Tel Aviv-based Skana has developed a new capability for its fleet management software system, SeaSphere, that allows groups of vessels to communicate with each other underwater across long distances using AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The system allows vessels to share data and react to what they hear from other robots. This, Skana says, gives individual units the ability to autonomously adapt to the information they receive and change their course or task while still working toward the same general mission as the fleet. The startup says its software can also be used to secure underwater infrastructure and supply chains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Communication between vessels is one of the main challenges during the deployment of multi-domain, multi-vessel operations,” Idan Levy, the co-founder and CEO of Skana Robotics, told TechCrunch. “The problem that we tackle is how you can deploy hundreds of unmanned vessels in an operation, share data, communicate on the surface level and under the water.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Teddy Lazebnik, an AI scientist and professor at the University of Haifa in Israel, led the research to develop this new capability. Lazebnik told TechCrunch that to build this decision-making algorithm, they couldn’t turn to the latest AI technology, but had to use AI algorithms that are a bit older and more mathematically driven.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The new algorithms have two properties: they are more powerful, but as a result, are less predictable,” Lazebnik said. “Hypothetically, you’re paying in the performance or the ‘wow effect’ of the of this algorithm, but the older ones, you gain explainability, predictability, and actually generality.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Skana Robotics was founded in 2024 and exited stealth mode earlier this year. The company is currently focused on selling to governments and companies in Europe, as maritime threat levels increase due to the war between Russia and Ukraine.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Levy said the company is in talks for a sizable government contract that it hopes to close by the end of the year. In 2026, Skana hopes to release the commercial version of its product and start proving its tech out in the wild.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want to show we can use this in scale,” Lazebnik said. “We argue that our software can handle complex maneuvers, etc. We want to show it. We claim we know how to manage an operation. We want admirals from EU and in EU countries to actually check this argument and see by themselves that we actually get results.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Skana.jpg?resize=1200,1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Underwater autonomous vessels and robots could play a substantial role in defense operations, but submersibles have historically had trouble communicating across large distances unless they rose to the surface. But coming up to transmit poses the very obvious risk of being exposed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Skana Robotics thinks it’s made a breakthrough with underwater communications using AI — but not the large language models the industry touts today.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Tel Aviv-based Skana has developed a new capability for its fleet management software system, SeaSphere, that allows groups of vessels to communicate with each other underwater across long distances using AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The system allows vessels to share data and react to what they hear from other robots. This, Skana says, gives individual units the ability to autonomously adapt to the information they receive and change their course or task while still working toward the same general mission as the fleet. The startup says its software can also be used to secure underwater infrastructure and supply chains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Communication between vessels is one of the main challenges during the deployment of multi-domain, multi-vessel operations,” Idan Levy, the co-founder and CEO of Skana Robotics, told TechCrunch. “The problem that we tackle is how you can deploy hundreds of unmanned vessels in an operation, share data, communicate on the surface level and under the water.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Teddy Lazebnik, an AI scientist and professor at the University of Haifa in Israel, led the research to develop this new capability. Lazebnik told TechCrunch that to build this decision-making algorithm, they couldn’t turn to the latest AI technology, but had to use AI algorithms that are a bit older and more mathematically driven.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The new algorithms have two properties: they are more powerful, but as a result, are less predictable,” Lazebnik said. “Hypothetically, you’re paying in the performance or the ‘wow effect’ of the of this algorithm, but the older ones, you gain explainability, predictability, and actually generality.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Skana Robotics was founded in 2024 and exited stealth mode earlier this year. The company is currently focused on selling to governments and companies in Europe, as maritime threat levels increase due to the war between Russia and Ukraine.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Levy said the company is in talks for a sizable government contract that it hopes to close by the end of the year. In 2026, Skana hopes to release the commercial version of its product and start proving its tech out in the wild.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want to show we can use this in scale,” Lazebnik said. “We argue that our software can handle complex maneuvers, etc. We want to show it. We claim we know how to manage an operation. We want admirals from EU and in EU countries to actually check this argument and see by themselves that we actually get results.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/17/skana-robotics-helps-fleets-of-underwater-robots-communicate-with-each-other/</guid><pubDate>Wed, 17 Dec 2025 14:05:00 +0000</pubDate></item><item><title>[NEW] Google’s vibe-coding tool Opal comes to Gemini (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/17/googles-vibe-coding-tool-opal-comes-to-gemini/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Google-Opal.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s vibe-coding tool, Opal, is making its way to Gemini. The company on Wednesday said it is integrating the tool, which lets you build AI-powered mini apps, inside the Gemini web app, allowing users to create their own custom apps, which Google calls Gems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced in 2024, Gems are customized versions of Gemini designed for specific tasks or scenarios. For instance, some of Google’s pre-made Gems include a learning coach, a brainstorming assistant, a career guide, a coding partner, and an editor.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Opal, meanwhile, focuses on helping users create mini-apps or mix existing apps. To use the feature, users describe in natural language the app they want to make, and the tool will use the different Gemini models to create it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, Opal is directly available from Gemini on the web, where it’s found in the Gems manager. The tool has a visual editor that lays out the steps required to create an application. From the editor, users can rearrange steps and link them together, without writing code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google notes that the visual editor also includes a new view in Gemini that will take the user’s written prompts and turn them into a list of steps. This makes it even easier to build apps and see how they work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For more advanced customization options, users can move from Gemini to the Advanced Editor at opal.google.com. The mini apps can be reused after they’re created.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Known as “vibe-coding,” using AI to program and make apps has skyrocketed in popularity over the past couple of years. The market now has apps from startups like&amp;nbsp;Lovable&amp;nbsp;and&amp;nbsp;Cursor, as well as offerings from AI providers like Anthropic and OpenAI. There are also tools focused more directly on consumers, like those from AI-powered app-building startup Wabi.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini’s web app is available at gemini.google.com.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Google-Opal.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s vibe-coding tool, Opal, is making its way to Gemini. The company on Wednesday said it is integrating the tool, which lets you build AI-powered mini apps, inside the Gemini web app, allowing users to create their own custom apps, which Google calls Gems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced in 2024, Gems are customized versions of Gemini designed for specific tasks or scenarios. For instance, some of Google’s pre-made Gems include a learning coach, a brainstorming assistant, a career guide, a coding partner, and an editor.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Opal, meanwhile, focuses on helping users create mini-apps or mix existing apps. To use the feature, users describe in natural language the app they want to make, and the tool will use the different Gemini models to create it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, Opal is directly available from Gemini on the web, where it’s found in the Gems manager. The tool has a visual editor that lays out the steps required to create an application. From the editor, users can rearrange steps and link them together, without writing code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google notes that the visual editor also includes a new view in Gemini that will take the user’s written prompts and turn them into a list of steps. This makes it even easier to build apps and see how they work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For more advanced customization options, users can move from Gemini to the Advanced Editor at opal.google.com. The mini apps can be reused after they’re created.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Known as “vibe-coding,” using AI to program and make apps has skyrocketed in popularity over the past couple of years. The market now has apps from startups like&amp;nbsp;Lovable&amp;nbsp;and&amp;nbsp;Cursor, as well as offerings from AI providers like Anthropic and OpenAI. There are also tools focused more directly on consumers, like those from AI-powered app-building startup Wabi.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini’s web app is available at gemini.google.com.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/17/googles-vibe-coding-tool-opal-comes-to-gemini/</guid><pubDate>Wed, 17 Dec 2025 15:16:42 +0000</pubDate></item><item><title>[NEW] Mozilla’s new CEO says AI is coming to Firefox, but will remain a choice (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/17/mozillas-new-ceo-says-ai-is-coming-to-firefox-but-will-remain-a-choice/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/08/GettyImages-956152050.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Mozilla has appointed Anthony Enzor-DeMeo as its CEO as the Firefox browser maker scrambles to adapt in a rapidly changing browser market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The appointment comes at a time when web browsers are seeing a revitalization of sorts as AI changes how people use the internet. After more than a decade of dominating the market, incumbents like Firefox, Google Chrome, and Apple’s Safari are facing a fresh challenge from companies like Perplexity, Arc, OpenAI, and Opera, which are focused on baking AI models and agents into their browsers to bring AI to users at the first point of contact with the internet: the web browser.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These changes don’t seem to be lost on Mozilla, which consists of several organizations, one of which is the Mozilla Corporation, which develops Firefox and other technologies, and another of which is its nonprofit and tax-exempt Mozilla Foundation, which oversees Mozilla’s corporate governance structure and sets the browser maker’s policies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has had a tough time lately: It’s gone through a restructuring, and last year laid off 30% of its employees and dropped its advocacy and global programs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the potential to make a comeback amid the modern browser wars doesn’t seem to be lost on the company. Mozilla will be investing in AI and will add AI features to Firefox, Enzor-DeMeo said in a blog post announcing his appointment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, Mozilla seems intent on not infuriating users who’ve chosen Firefox for its lack of AI features: Enzor-DeMeo said the company will make AI features optional within Firefox and its other products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI should always be a choice — something people can easily turn off. People should know why a feature works the way it does and what value they get from it,” he wrote.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company will also be investing in diversifying its revenue beyond search (in exchange for having Google as its default search engine, Mozilla makes a significant portion of its revenue from the search giant), and Enzor-DeMeo said Mozilla plans to flesh Firefox out into “a broader ecosystem of trusted software.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Currently, the company also develops the Thunderbird email client, a VPN, and last year launched an AI-powered website creator aimed at small businesses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before this appointment, Enzor-DeMeo was general manager of Firefox, and is now taking over from interim CEO, Laura Chambers, who was in the role for the past couple of years. Enzor-DeMeo previously held product roles at Roofstock, Better, and Wayfair. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/08/GettyImages-956152050.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Mozilla has appointed Anthony Enzor-DeMeo as its CEO as the Firefox browser maker scrambles to adapt in a rapidly changing browser market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The appointment comes at a time when web browsers are seeing a revitalization of sorts as AI changes how people use the internet. After more than a decade of dominating the market, incumbents like Firefox, Google Chrome, and Apple’s Safari are facing a fresh challenge from companies like Perplexity, Arc, OpenAI, and Opera, which are focused on baking AI models and agents into their browsers to bring AI to users at the first point of contact with the internet: the web browser.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These changes don’t seem to be lost on Mozilla, which consists of several organizations, one of which is the Mozilla Corporation, which develops Firefox and other technologies, and another of which is its nonprofit and tax-exempt Mozilla Foundation, which oversees Mozilla’s corporate governance structure and sets the browser maker’s policies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has had a tough time lately: It’s gone through a restructuring, and last year laid off 30% of its employees and dropped its advocacy and global programs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the potential to make a comeback amid the modern browser wars doesn’t seem to be lost on the company. Mozilla will be investing in AI and will add AI features to Firefox, Enzor-DeMeo said in a blog post announcing his appointment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, Mozilla seems intent on not infuriating users who’ve chosen Firefox for its lack of AI features: Enzor-DeMeo said the company will make AI features optional within Firefox and its other products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI should always be a choice — something people can easily turn off. People should know why a feature works the way it does and what value they get from it,” he wrote.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company will also be investing in diversifying its revenue beyond search (in exchange for having Google as its default search engine, Mozilla makes a significant portion of its revenue from the search giant), and Enzor-DeMeo said Mozilla plans to flesh Firefox out into “a broader ecosystem of trusted software.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Currently, the company also develops the Thunderbird email client, a VPN, and last year launched an AI-powered website creator aimed at small businesses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before this appointment, Enzor-DeMeo was general manager of Firefox, and is now taking over from interim CEO, Laura Chambers, who was in the role for the past couple of years. Enzor-DeMeo previously held product roles at Roofstock, Better, and Wayfair. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/17/mozillas-new-ceo-says-ai-is-coming-to-firefox-but-will-remain-a-choice/</guid><pubDate>Wed, 17 Dec 2025 15:17:52 +0000</pubDate></item><item><title>[NEW] Browser extensions with 8 million users collect extended AI conversations (AI – Ars Technica)</title><link>https://arstechnica.com/security/2025/12/browser-extensions-with-8-million-users-collect-extended-ai-conversations/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The extensions, available for Chromium browsers, harvest full AI conversations over months.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="448" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/electronic-privacy-invasion-640x448.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/electronic-privacy-invasion-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Browser extensions with more than 8 million installs are harvesting complete and extended conversations from users’ AI conversations and selling them for marketing purposes, according to data collected from the Google and Microsoft pages hosting them.&lt;/p&gt;
&lt;p&gt;Security firm Koi discovered the eight extensions, which as of late Tuesday night remained available in both Google’s and Microsoft’s extension stores. Seven of them carry “Featured” badges, which are endorsements meant to signal that the companies have determined the extensions meet their quality standards. The free extensions provide functions such as VPN routing to safeguard online privacy and ad blocking for ad-free browsing. All provide assurances that user data remains anonymous and isn’t shared for purposes other than their described use.&lt;/p&gt;
&lt;h2&gt;A gold mine for marketers and data brokers&lt;/h2&gt;
&lt;p&gt;An examination of the extensions’ underlying code tells a much more complicated story. Each contains eight of what Koi calls “executor” scripts, with each being unique for ChatGPT, Claude, Gemini, and five other leading AI chat platforms. The scripts are injected into webpages anytime the user visits one of these platforms. From there, the scripts override browsers’ built-in functions for making network requests and receiving responses.&lt;/p&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="ars-gallery-1-up my-5"&gt;
  &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/executor-script.png" width="1064" /&gt;
  
  &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1262" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/extension-ai-platforms.png" width="844" /&gt;
  
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/extension-flags.png" width="578" /&gt;
  
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
              &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;As a result, all interaction between the browser and the AI bots is routed not by the legitimate browser APIs—in this case fetch() and HttpRequest—but through the executor script. The extensions eventually compress the data and send it to endpoints belonging to the extension maker.&lt;/p&gt;
&lt;p&gt;“By overriding the [browser APIs], the extension inserts itself into that flow and captures a copy of everything before the page even displays it,” Koi CTO Idan Dardikman wrote in an email. “The consequence: The extension sees your complete conversation in raw form—your prompts, the AI’s responses, timestamps, everything—and sends a copy to their servers.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Besides ChatGPT, Claude, and Gemini, the extensions harvest all conversations from Copilot, Perplexity, DeepSeek, Grok, and Meta AI. Koi said the full description of the data captured includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Every prompt a user sends to the AI&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Every response received&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Conversation identifiers and timestamps&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Session metadata&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;The specific AI platform and model used&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The executor script runs independently from the VPN networking, ad blocking, or other core functionality. That means that even when a user toggles off VPN networking, AI protection, ad blocking, or other functions, the conversation collection continues. The only way to stop the harvesting is to disable the extension in the browser settings or to uninstall it.&lt;/p&gt;
&lt;p&gt;Koi said it first discovered the conversation harvesting in Urban VPN Proxy, a VPN routing extension that lists “AI protection” as one of its benefits. The data collection began in early July with the release of version 5.5.0.&lt;/p&gt;
&lt;p&gt;“Anyone who used ChatGPT, Claude, Gemini, or the other targeted platforms while Urban VPN was installed after July 9, 2025 should assume those conversations are now on Urban VPN’s servers and have been shared with third parties,” the company said. “Medical questions, financial details, proprietary code, personal dilemmas—all of it, sold for ‘marketing analytics purposes.'”&lt;/p&gt;
&lt;p&gt;Following that discovery, the security firm uncovered seven additional extensions with identical AI harvesting functionality. Four of the extensions are available in the Chrome Web Store. The other four are on the Edge add-ons page. Collectively, they have been installed more than 8 million times.&lt;/p&gt;
&lt;p&gt;They are:&lt;/p&gt;
&lt;p&gt;Chrome Store&lt;/p&gt;
&lt;ul&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban VPN Proxy: 6 million users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;1ClickVPN Proxy: 600,000 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Browser Guard: 40,000 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Ad Blocker: 10,000 users&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Edge Add-ons:&lt;/p&gt;
&lt;ul&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban VPN Proxy: 1,32 million users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;1ClickVPN Proxy: 36,459 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Browser Guard – 12,624 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Ad Blocker – 6,476 users&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Read the fine print&lt;/h2&gt;
&lt;p&gt;The extensions come with conflicting messages about how they handle bot conversations, which often contain deeply personal information about users’ physical and mental health, finances, personal relationships, and other sensitive information that could be a gold mine for marketers and data brokers. The Urban VPN Proxy in the Chrome Web Store, for instance, lists “AI protection” as a benefit. It goes on to say:&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;blockquote&gt;&lt;p&gt;Our VPN provides added security features to help shield your browsing experience from phishing attempts, malware, intrusive ads and AI protection which checks prompts for personal data (like an email or phone number), checks AI chat responses for suspicious or unsafe links and displays a warning before click or submit your prompt.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;On the privacy policy for the extension, Google says the developer has declared that user data isn’t sold to third parties outside of approved use cases and won’t be “used or transferred for purposes that are unrelated to the item’s core functionality.” The page goes on to list the personal data handled as location, web history, and website content.&lt;/p&gt;
&lt;p&gt;Koi said that a consent prompt that the extensions display during setup notifies the user that they process “ChatAI communication,” “pages you visit,” and “security signals.” The notification goes on to say that the data is processed to “provide these protections,” which presumably means the core functions such as VPN routing or ad blocking.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132319 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1262" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/consent-prompt.png" width="642" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Koi

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The only explicit mention of AI conversations being harvested is in legalese buried in the privacy policy, such as this 6,000-word one for Urban VPN Proxy, posted on each extension website. There, it says that the extension will “collect the prompts and outputs queried by the End-User or generated by the AI chat provider, as applicable.” It goes on to say that the extension developer will “disclose the AI prompts for marketing analytics purposes.”&lt;/p&gt;
&lt;p&gt;All eight extensions and the privacy policies covering them are developed and written by Urban Cyber Security, a company that says its apps and extensions are used by 100 million people. The policies say the extensions share “Web Browsing Data” with “our affiliated company,” which is listed as both BiScience and B.I Science. The affiliated company “uses this raw data and creates insights which are commercially used and shared with Business Partners.” The policy goes on to refer users to the BiScience privacy policy. BiScience, whose privacy practices have been scrutinized before, says its services “transform enormous volumes of digital signals into clear, actionable market intelligence.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It’s hard to fathom how both Google and Microsoft would allow such extensions onto their platforms at all, let alone go out of their way to endorse seven of them with a featured badge. Neither company returned emails asking how they decide which extensions qualify for such a distinction, if they have plans to stop making them available to Chrome and Edge users, or why the privacy policies are so unclear to normal users.&lt;/p&gt;
&lt;p&gt;Messages sent to both individual extension developers and Urban Cyber Security went unanswered. BiScience provides no email. A call to the company’s New York office was answered by someone who said they were in Israel and to call back during normal business hours in that country.&lt;/p&gt;
&lt;p&gt;Koi’s discovery is the latest cautionary tale illustrating the growing perils of being online. It’s questionable in the first place whether people should trust their most intimate secrets and sensitive business information to AI chatbots, which come with no HIPAA assurances, attorney-client privilege, or expectations of privacy. Yet increasingly, that’s exactly what AI companies are encouraging, and users, it seems, are more than willing to comply.&lt;/p&gt;
&lt;p&gt;Compounding the risk is the rush to install free apps and extensions—particularly those from little-known developers and providing at best minimal benefits—on devices storing and transmitting these chats. Taken together, they’re a recipe for disaster, and that’s exactly what we have here.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The extensions, available for Chromium browsers, harvest full AI conversations over months.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="448" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/electronic-privacy-invasion-640x448.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/electronic-privacy-invasion-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Browser extensions with more than 8 million installs are harvesting complete and extended conversations from users’ AI conversations and selling them for marketing purposes, according to data collected from the Google and Microsoft pages hosting them.&lt;/p&gt;
&lt;p&gt;Security firm Koi discovered the eight extensions, which as of late Tuesday night remained available in both Google’s and Microsoft’s extension stores. Seven of them carry “Featured” badges, which are endorsements meant to signal that the companies have determined the extensions meet their quality standards. The free extensions provide functions such as VPN routing to safeguard online privacy and ad blocking for ad-free browsing. All provide assurances that user data remains anonymous and isn’t shared for purposes other than their described use.&lt;/p&gt;
&lt;h2&gt;A gold mine for marketers and data brokers&lt;/h2&gt;
&lt;p&gt;An examination of the extensions’ underlying code tells a much more complicated story. Each contains eight of what Koi calls “executor” scripts, with each being unique for ChatGPT, Claude, Gemini, and five other leading AI chat platforms. The scripts are injected into webpages anytime the user visits one of these platforms. From there, the scripts override browsers’ built-in functions for making network requests and receiving responses.&lt;/p&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="ars-gallery-1-up my-5"&gt;
  &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/executor-script.png" width="1064" /&gt;
  
  &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1262" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/extension-ai-platforms.png" width="844" /&gt;
  
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/extension-flags.png" width="578" /&gt;
  
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
              &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;As a result, all interaction between the browser and the AI bots is routed not by the legitimate browser APIs—in this case fetch() and HttpRequest—but through the executor script. The extensions eventually compress the data and send it to endpoints belonging to the extension maker.&lt;/p&gt;
&lt;p&gt;“By overriding the [browser APIs], the extension inserts itself into that flow and captures a copy of everything before the page even displays it,” Koi CTO Idan Dardikman wrote in an email. “The consequence: The extension sees your complete conversation in raw form—your prompts, the AI’s responses, timestamps, everything—and sends a copy to their servers.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Besides ChatGPT, Claude, and Gemini, the extensions harvest all conversations from Copilot, Perplexity, DeepSeek, Grok, and Meta AI. Koi said the full description of the data captured includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Every prompt a user sends to the AI&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Every response received&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Conversation identifiers and timestamps&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Session metadata&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;The specific AI platform and model used&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The executor script runs independently from the VPN networking, ad blocking, or other core functionality. That means that even when a user toggles off VPN networking, AI protection, ad blocking, or other functions, the conversation collection continues. The only way to stop the harvesting is to disable the extension in the browser settings or to uninstall it.&lt;/p&gt;
&lt;p&gt;Koi said it first discovered the conversation harvesting in Urban VPN Proxy, a VPN routing extension that lists “AI protection” as one of its benefits. The data collection began in early July with the release of version 5.5.0.&lt;/p&gt;
&lt;p&gt;“Anyone who used ChatGPT, Claude, Gemini, or the other targeted platforms while Urban VPN was installed after July 9, 2025 should assume those conversations are now on Urban VPN’s servers and have been shared with third parties,” the company said. “Medical questions, financial details, proprietary code, personal dilemmas—all of it, sold for ‘marketing analytics purposes.'”&lt;/p&gt;
&lt;p&gt;Following that discovery, the security firm uncovered seven additional extensions with identical AI harvesting functionality. Four of the extensions are available in the Chrome Web Store. The other four are on the Edge add-ons page. Collectively, they have been installed more than 8 million times.&lt;/p&gt;
&lt;p&gt;They are:&lt;/p&gt;
&lt;p&gt;Chrome Store&lt;/p&gt;
&lt;ul&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban VPN Proxy: 6 million users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;1ClickVPN Proxy: 600,000 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Browser Guard: 40,000 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Ad Blocker: 10,000 users&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Edge Add-ons:&lt;/p&gt;
&lt;ul&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban VPN Proxy: 1,32 million users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;1ClickVPN Proxy: 36,459 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Browser Guard – 12,624 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Ad Blocker – 6,476 users&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Read the fine print&lt;/h2&gt;
&lt;p&gt;The extensions come with conflicting messages about how they handle bot conversations, which often contain deeply personal information about users’ physical and mental health, finances, personal relationships, and other sensitive information that could be a gold mine for marketers and data brokers. The Urban VPN Proxy in the Chrome Web Store, for instance, lists “AI protection” as a benefit. It goes on to say:&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;blockquote&gt;&lt;p&gt;Our VPN provides added security features to help shield your browsing experience from phishing attempts, malware, intrusive ads and AI protection which checks prompts for personal data (like an email or phone number), checks AI chat responses for suspicious or unsafe links and displays a warning before click or submit your prompt.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;On the privacy policy for the extension, Google says the developer has declared that user data isn’t sold to third parties outside of approved use cases and won’t be “used or transferred for purposes that are unrelated to the item’s core functionality.” The page goes on to list the personal data handled as location, web history, and website content.&lt;/p&gt;
&lt;p&gt;Koi said that a consent prompt that the extensions display during setup notifies the user that they process “ChatAI communication,” “pages you visit,” and “security signals.” The notification goes on to say that the data is processed to “provide these protections,” which presumably means the core functions such as VPN routing or ad blocking.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132319 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1262" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/consent-prompt.png" width="642" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Koi

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The only explicit mention of AI conversations being harvested is in legalese buried in the privacy policy, such as this 6,000-word one for Urban VPN Proxy, posted on each extension website. There, it says that the extension will “collect the prompts and outputs queried by the End-User or generated by the AI chat provider, as applicable.” It goes on to say that the extension developer will “disclose the AI prompts for marketing analytics purposes.”&lt;/p&gt;
&lt;p&gt;All eight extensions and the privacy policies covering them are developed and written by Urban Cyber Security, a company that says its apps and extensions are used by 100 million people. The policies say the extensions share “Web Browsing Data” with “our affiliated company,” which is listed as both BiScience and B.I Science. The affiliated company “uses this raw data and creates insights which are commercially used and shared with Business Partners.” The policy goes on to refer users to the BiScience privacy policy. BiScience, whose privacy practices have been scrutinized before, says its services “transform enormous volumes of digital signals into clear, actionable market intelligence.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It’s hard to fathom how both Google and Microsoft would allow such extensions onto their platforms at all, let alone go out of their way to endorse seven of them with a featured badge. Neither company returned emails asking how they decide which extensions qualify for such a distinction, if they have plans to stop making them available to Chrome and Edge users, or why the privacy policies are so unclear to normal users.&lt;/p&gt;
&lt;p&gt;Messages sent to both individual extension developers and Urban Cyber Security went unanswered. BiScience provides no email. A call to the company’s New York office was answered by someone who said they were in Israel and to call back during normal business hours in that country.&lt;/p&gt;
&lt;p&gt;Koi’s discovery is the latest cautionary tale illustrating the growing perils of being online. It’s questionable in the first place whether people should trust their most intimate secrets and sensitive business information to AI chatbots, which come with no HIPAA assurances, attorney-client privilege, or expectations of privacy. Yet increasingly, that’s exactly what AI companies are encouraging, and users, it seems, are more than willing to comply.&lt;/p&gt;
&lt;p&gt;Compounding the risk is the rush to install free apps and extensions—particularly those from little-known developers and providing at best minimal benefits—on devices storing and transmitting these chats. Taken together, they’re a recipe for disaster, and that’s exactly what we have here.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/security/2025/12/browser-extensions-with-8-million-users-collect-extended-ai-conversations/</guid><pubDate>Wed, 17 Dec 2025 15:25:25 +0000</pubDate></item><item><title>[NEW] Google launches Gemini 3 Flash, makes it the default model in the Gemini app (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/17/google-launches-gemini-3-flash-makes-it-the-default-model-in-the-gemini-app/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google today released its fast and cheap Gemini 3 Flash model, based on the Gemini 3 released last month, looking to steal OpenAI’s thunder. The company is also making this the default model in the Gemini app and AI mode in search.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new Flash model arrives six months after Google announced the Gemini 2.5 Flash model, offering significant improvements. On the benchmark, the Gemini 3 Flash model outperforms its predecessor by a significant margin and matches the performance of other frontier models, like Gemini 3 Pro and GPT 5.2, in some measures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, it scored 33.7% without tool use on Humanity’s Last Exam benchmark, which is designed to test expertise across different domains. In comparison, Gemini 3 Pro scored 37.5%, Gemini 2.5 Flash scored 11%, and the newly released GPT-5.2 scored 34.5%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the multimodality and reasoning benchmark MMMU-Pro, the new model outscored all competitors with an 81.2% score. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-consumer-rollout"&gt;Consumer rollout&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Google is making Gemini 3 Flash the default model in the Gemini app globally, replacing Gemini 2.5 Flash. Users can still choose the Pro model from the model picker for math and coding questions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says the new model is good at identifying multimodal content and giving you an answer based on that. For instance, you can upload your pickleball short video and ask for tips; you can try drawing a sketch and have the model guess what you are drawing; or you can upload an audio recording to get analysis or generate a quiz.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also said the model better understands the intent of users’ queries and can generate more visual answers with elements like images and tables.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;You can also use the new model to create app prototypes in the Gemini app using prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Gemini 3 Pro is now available to everyone in the U.S. for search and more people in the U.S. can access the Nano Banana Pro image model in search, as well.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-enterprise-and-developer-availability"&gt;Enterprise and developer availability&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Google noted that companies like JetBrains, Figma, Cursor, Harvey, and Latitude are already using the Gemini 3 Flash model, which is available through Vertex AI and Gemini Enterprise.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For developers, the company is making the model available in a preview model through the API and in Antigravity, Google’s new coding tool released last month. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said the Gemini 3 Pro scores 78% on the SWE-bench verified coding benchmark, only outperformed by GPT-5.2. It added that the model is ideal for video analysis, data extraction, and visual Q&amp;amp;A, and because of its speed, it is suited for quick and repeatable workflows.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3076854" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/gemini-3-flash-swe-bench.jpeg?w=544" width="544" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Model pricing is $0.50 per 1 million input tokens and $3.00 per 1 million output tokens. This is slightly more expensive than $0.30 per 1 million input tokens and $2.50 per 1 million output tokens of Gemini Flash 2.5. But Google claims that the new model outperforms the Gemini 2.5 Pro model while being three times faster. And, for thinking tasks, it uses 30% fewer tokens on average than 2.5 Pro. That means overall, you might save on the number of tokens for certain tasks.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3076855" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/gemini-3-flash-token-efficiency.jpeg?w=544" width="544" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“We really position flash as more of your workhorse model. So if you look at, for example, even the input and output prices at the top of this table, Flash is just a much cheaper offering from an input and output price perspective. And so it actually allows for, for many companies, bulk tasks,” Tulsee Doshi, senior director &amp;amp; head of Product for Gemini Models, told TechCrunch in a briefing&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since it released Gemini 3, Google has processed over 1 trillion tokens per day on its API, amid its fierce release and performance war with OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this month, Sam Altman reportedly sent an internal “Code Red” memo to the OpenAI team after ChatGPT’s traffic dipped as Google’s market share in consumers rose. Post that, OpenAI has released GPT-5.2 and a new image generation model. OpenAI also boasted about its growing enterprise use and said the ChatGPT messages volume has grown 8x since November 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Google didn’t directly address the competition with OpenAI, it said that the release of new models is challenging all companies to be active.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Just about what’s happening across the industry is like all of these models are continuing to be awesome, challenge each other, push the frontier. And I think what’s also awesome is as companies are releasing these models,” Doshi said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re also introducing new benchmarks and new ways of evaluating these models. And so that’s also encouraging us.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google today released its fast and cheap Gemini 3 Flash model, based on the Gemini 3 released last month, looking to steal OpenAI’s thunder. The company is also making this the default model in the Gemini app and AI mode in search.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new Flash model arrives six months after Google announced the Gemini 2.5 Flash model, offering significant improvements. On the benchmark, the Gemini 3 Flash model outperforms its predecessor by a significant margin and matches the performance of other frontier models, like Gemini 3 Pro and GPT 5.2, in some measures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, it scored 33.7% without tool use on Humanity’s Last Exam benchmark, which is designed to test expertise across different domains. In comparison, Gemini 3 Pro scored 37.5%, Gemini 2.5 Flash scored 11%, and the newly released GPT-5.2 scored 34.5%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the multimodality and reasoning benchmark MMMU-Pro, the new model outscored all competitors with an 81.2% score. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-consumer-rollout"&gt;Consumer rollout&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Google is making Gemini 3 Flash the default model in the Gemini app globally, replacing Gemini 2.5 Flash. Users can still choose the Pro model from the model picker for math and coding questions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says the new model is good at identifying multimodal content and giving you an answer based on that. For instance, you can upload your pickleball short video and ask for tips; you can try drawing a sketch and have the model guess what you are drawing; or you can upload an audio recording to get analysis or generate a quiz.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also said the model better understands the intent of users’ queries and can generate more visual answers with elements like images and tables.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;You can also use the new model to create app prototypes in the Gemini app using prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Gemini 3 Pro is now available to everyone in the U.S. for search and more people in the U.S. can access the Nano Banana Pro image model in search, as well.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-enterprise-and-developer-availability"&gt;Enterprise and developer availability&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Google noted that companies like JetBrains, Figma, Cursor, Harvey, and Latitude are already using the Gemini 3 Flash model, which is available through Vertex AI and Gemini Enterprise.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For developers, the company is making the model available in a preview model through the API and in Antigravity, Google’s new coding tool released last month. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said the Gemini 3 Pro scores 78% on the SWE-bench verified coding benchmark, only outperformed by GPT-5.2. It added that the model is ideal for video analysis, data extraction, and visual Q&amp;amp;A, and because of its speed, it is suited for quick and repeatable workflows.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3076854" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/gemini-3-flash-swe-bench.jpeg?w=544" width="544" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Model pricing is $0.50 per 1 million input tokens and $3.00 per 1 million output tokens. This is slightly more expensive than $0.30 per 1 million input tokens and $2.50 per 1 million output tokens of Gemini Flash 2.5. But Google claims that the new model outperforms the Gemini 2.5 Pro model while being three times faster. And, for thinking tasks, it uses 30% fewer tokens on average than 2.5 Pro. That means overall, you might save on the number of tokens for certain tasks.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3076855" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/gemini-3-flash-token-efficiency.jpeg?w=544" width="544" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“We really position flash as more of your workhorse model. So if you look at, for example, even the input and output prices at the top of this table, Flash is just a much cheaper offering from an input and output price perspective. And so it actually allows for, for many companies, bulk tasks,” Tulsee Doshi, senior director &amp;amp; head of Product for Gemini Models, told TechCrunch in a briefing&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since it released Gemini 3, Google has processed over 1 trillion tokens per day on its API, amid its fierce release and performance war with OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this month, Sam Altman reportedly sent an internal “Code Red” memo to the OpenAI team after ChatGPT’s traffic dipped as Google’s market share in consumers rose. Post that, OpenAI has released GPT-5.2 and a new image generation model. OpenAI also boasted about its growing enterprise use and said the ChatGPT messages volume has grown 8x since November 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Google didn’t directly address the competition with OpenAI, it said that the release of new models is challenging all companies to be active.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Just about what’s happening across the industry is like all of these models are continuing to be awesome, challenge each other, push the frontier. And I think what’s also awesome is as companies are releasing these models,” Doshi said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re also introducing new benchmarks and new ways of evaluating these models. And so that’s also encouraging us.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/17/google-launches-gemini-3-flash-makes-it-the-default-model-in-the-gemini-app/</guid><pubDate>Wed, 17 Dec 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] UC San Diego Lab Advances Generative AI Research With NVIDIA DGX B200 System (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ucsd-generative-ai-research-dgx-b200/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The Hao AI Lab research team at the University of California San Diego &amp;nbsp;— at the forefront of pioneering AI model innovation — recently received an NVIDIA DGX B200 system to elevate their critical work in large language model inference.&lt;/p&gt;
&lt;p&gt;Many LLM inference platforms in production today, such as NVIDIA Dynamo, use research concepts that originated in the Hao AI Lab, including DistServe.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Is Hao AI Lab Using the DGX B200? &lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption alignnone" id="attachment_88356"&gt;&lt;img alt="Researchers standing around the DGX B200 system inside the San Diego Supercomputing Center. " class="size-large wp-image-88356" height="1120" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/UCSD--1680x1120.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88356"&gt;Members of the Hao AI Lab standing with the NVIDIA DGX B200 system.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;With the DGX B200 now fully accessible to the Hao AI Lab and broader UC San Diego community at the School of Computing, Information and Data Sciences’ San Diego Supercomputer Center, the research opportunities are boundless.&lt;/p&gt;
&lt;p&gt;“DGX B200 is one of the most powerful AI systems from NVIDIA to date, which means that its performance is among the best in the world,” said Hao Zhang, assistant professor in the Halıcıoğlu Data Science Institute and department of computer science and engineering at UC San Diego. “It enables us to prototype and experiment much faster than using previous-generation hardware.”&lt;/p&gt;
&lt;p&gt;Two Hao AI Lab projects the DGX B200 is accelerating are FastVideo and the Lmgame benchmark.&lt;/p&gt;
&lt;p&gt;FastVideo focuses on training a family of video generation models to produce a five-second video based on a given text prompt — in just five seconds.&lt;/p&gt;
&lt;p&gt;The research phase of FastVideo taps into NVIDIA H200 GPUs in addition to the DGX B200 system.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Lmgame-bench is a benchmarking suite that puts LLMs to the test using popular online games including &lt;i&gt;Tetris&lt;/i&gt; and &lt;i&gt;Super Mario Bros&lt;/i&gt;. Users can test one model at a time or put two models up against each other to measure their performance.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88360"&gt;&lt;img alt="Illustrated image of Lmgame-Bench workflow. " class="size-large wp-image-88360" height="579" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Cute-Robot-UCSD-1680x579.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88360"&gt;The illustrated workflow of Hao AI Lab’s Lmgame-Bench project.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Other ongoing projects at Hao AI Labs explore new ways to achieve low-latency LLM serving, pushing large language models toward real-time responsiveness.&lt;/p&gt;
&lt;p&gt;“Our current research uses the DGX B200 to explore the next frontier of low-latency LLM-serving on the awesome hardware specs the system gives us,” said Junda Chen, a doctoral candidate in computer science at UC San Diego.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How DistServe Influenced Disaggregated Serving&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Disaggregated inference is a way to ensure large-scale LLM-serving engines can achieve the optimal aggregate system throughput while maintaining acceptably low latency for user requests.&lt;/p&gt;
&lt;p&gt;The benefit of disaggregated inference lies in optimizing what DistServe calls “goodput” instead of “throughput” in the LLM-serving engine.&lt;/p&gt;
&lt;p&gt;Here’s the difference:&lt;/p&gt;
&lt;p&gt;Throughput is measured by the number of tokens per second that the entire system can generate. Higher throughput means lower cost to generate each token to serve the user. For a long time, throughput was the only metric used by LLM-serving engines to measure their performance against one another.&lt;/p&gt;
&lt;p&gt;While throughput measures the aggregate performance of the system, it doesn’t directly correlate to the latency that a user perceives. If a user demands lower latency to generate the tokens, the system has to sacrifice throughput.&lt;/p&gt;
&lt;p&gt;This natural trade-off between throughput and latency is what led the DistServe team to propose a new metric, “goodput”: the measure of throughput while satisfying the user-specified latency objectives, usually called service-level objectives. In other words, goodput represents the overall health of a system while satisfying user experience.&lt;/p&gt;
&lt;p&gt;DistServe shows that goodput is a much better metric for LLM-serving systems, as it factors in both cost and service quality. Goodput leads to optimal efficiency and ideal output from a model.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Can Developers Achieve Optimal Goodput? &amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;When a user makes a request in an LLM system, the system takes the user input and generates the first token, known as prefill. Then, the system creates numerous output tokens, one after another, predicting each token’s future behavior based on past requests’ outcomes. This process is known as decode.&lt;/p&gt;

&lt;p&gt;Prefill and decode have historically run on the same GPU, but the researchers behind DistServe found that splitting them onto different GPUs maximizes goodput.&lt;/p&gt;
&lt;p&gt;“Previously, if you put these two jobs on a GPU, they would compete with each other for resources, which could make it slow from a user perspective,” Chen said. “Now, if I split the jobs onto two different sets of GPUs — one doing prefill, which is compute intensive, and the other doing decode, which is more memory intensive — we can fundamentally eliminate the interference between the two jobs, making both jobs run faster.&lt;/p&gt;
&lt;p&gt;This process is called prefill/decode disaggregation, or separating the prefill from decode to get greater goodput.&lt;/p&gt;
&lt;p&gt;Increasing goodput and using the disaggregated inference method enables the continuous scaling of workloads without compromising on low-latency or high-quality model responses.&lt;/p&gt;
&lt;p&gt;NVIDIA Dynamo — an open-source framework designed to accelerate and scale generative AI models at the highest efficiency levels with the lowest cost — enables scaling disaggregated inference.&lt;/p&gt;
&lt;p&gt;In addition to these projects, cross-departmental collaborations, such as in healthcare and biology, are underway at UC San Diego to further optimize an array of research projects using the NVIDIA DGX B200, as researchers continue exploring how AI platforms can accelerate innovation.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about the &lt;/i&gt;&lt;i&gt;NVIDIA DGX B200&lt;/i&gt;&lt;i&gt; system.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The Hao AI Lab research team at the University of California San Diego &amp;nbsp;— at the forefront of pioneering AI model innovation — recently received an NVIDIA DGX B200 system to elevate their critical work in large language model inference.&lt;/p&gt;
&lt;p&gt;Many LLM inference platforms in production today, such as NVIDIA Dynamo, use research concepts that originated in the Hao AI Lab, including DistServe.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Is Hao AI Lab Using the DGX B200? &lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption alignnone" id="attachment_88356"&gt;&lt;img alt="Researchers standing around the DGX B200 system inside the San Diego Supercomputing Center. " class="size-large wp-image-88356" height="1120" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/UCSD--1680x1120.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88356"&gt;Members of the Hao AI Lab standing with the NVIDIA DGX B200 system.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;With the DGX B200 now fully accessible to the Hao AI Lab and broader UC San Diego community at the School of Computing, Information and Data Sciences’ San Diego Supercomputer Center, the research opportunities are boundless.&lt;/p&gt;
&lt;p&gt;“DGX B200 is one of the most powerful AI systems from NVIDIA to date, which means that its performance is among the best in the world,” said Hao Zhang, assistant professor in the Halıcıoğlu Data Science Institute and department of computer science and engineering at UC San Diego. “It enables us to prototype and experiment much faster than using previous-generation hardware.”&lt;/p&gt;
&lt;p&gt;Two Hao AI Lab projects the DGX B200 is accelerating are FastVideo and the Lmgame benchmark.&lt;/p&gt;
&lt;p&gt;FastVideo focuses on training a family of video generation models to produce a five-second video based on a given text prompt — in just five seconds.&lt;/p&gt;
&lt;p&gt;The research phase of FastVideo taps into NVIDIA H200 GPUs in addition to the DGX B200 system.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Lmgame-bench is a benchmarking suite that puts LLMs to the test using popular online games including &lt;i&gt;Tetris&lt;/i&gt; and &lt;i&gt;Super Mario Bros&lt;/i&gt;. Users can test one model at a time or put two models up against each other to measure their performance.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88360"&gt;&lt;img alt="Illustrated image of Lmgame-Bench workflow. " class="size-large wp-image-88360" height="579" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Cute-Robot-UCSD-1680x579.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88360"&gt;The illustrated workflow of Hao AI Lab’s Lmgame-Bench project.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Other ongoing projects at Hao AI Labs explore new ways to achieve low-latency LLM serving, pushing large language models toward real-time responsiveness.&lt;/p&gt;
&lt;p&gt;“Our current research uses the DGX B200 to explore the next frontier of low-latency LLM-serving on the awesome hardware specs the system gives us,” said Junda Chen, a doctoral candidate in computer science at UC San Diego.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How DistServe Influenced Disaggregated Serving&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Disaggregated inference is a way to ensure large-scale LLM-serving engines can achieve the optimal aggregate system throughput while maintaining acceptably low latency for user requests.&lt;/p&gt;
&lt;p&gt;The benefit of disaggregated inference lies in optimizing what DistServe calls “goodput” instead of “throughput” in the LLM-serving engine.&lt;/p&gt;
&lt;p&gt;Here’s the difference:&lt;/p&gt;
&lt;p&gt;Throughput is measured by the number of tokens per second that the entire system can generate. Higher throughput means lower cost to generate each token to serve the user. For a long time, throughput was the only metric used by LLM-serving engines to measure their performance against one another.&lt;/p&gt;
&lt;p&gt;While throughput measures the aggregate performance of the system, it doesn’t directly correlate to the latency that a user perceives. If a user demands lower latency to generate the tokens, the system has to sacrifice throughput.&lt;/p&gt;
&lt;p&gt;This natural trade-off between throughput and latency is what led the DistServe team to propose a new metric, “goodput”: the measure of throughput while satisfying the user-specified latency objectives, usually called service-level objectives. In other words, goodput represents the overall health of a system while satisfying user experience.&lt;/p&gt;
&lt;p&gt;DistServe shows that goodput is a much better metric for LLM-serving systems, as it factors in both cost and service quality. Goodput leads to optimal efficiency and ideal output from a model.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Can Developers Achieve Optimal Goodput? &amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;When a user makes a request in an LLM system, the system takes the user input and generates the first token, known as prefill. Then, the system creates numerous output tokens, one after another, predicting each token’s future behavior based on past requests’ outcomes. This process is known as decode.&lt;/p&gt;

&lt;p&gt;Prefill and decode have historically run on the same GPU, but the researchers behind DistServe found that splitting them onto different GPUs maximizes goodput.&lt;/p&gt;
&lt;p&gt;“Previously, if you put these two jobs on a GPU, they would compete with each other for resources, which could make it slow from a user perspective,” Chen said. “Now, if I split the jobs onto two different sets of GPUs — one doing prefill, which is compute intensive, and the other doing decode, which is more memory intensive — we can fundamentally eliminate the interference between the two jobs, making both jobs run faster.&lt;/p&gt;
&lt;p&gt;This process is called prefill/decode disaggregation, or separating the prefill from decode to get greater goodput.&lt;/p&gt;
&lt;p&gt;Increasing goodput and using the disaggregated inference method enables the continuous scaling of workloads without compromising on low-latency or high-quality model responses.&lt;/p&gt;
&lt;p&gt;NVIDIA Dynamo — an open-source framework designed to accelerate and scale generative AI models at the highest efficiency levels with the lowest cost — enables scaling disaggregated inference.&lt;/p&gt;
&lt;p&gt;In addition to these projects, cross-departmental collaborations, such as in healthcare and biology, are underway at UC San Diego to further optimize an array of research projects using the NVIDIA DGX B200, as researchers continue exploring how AI platforms can accelerate innovation.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about the &lt;/i&gt;&lt;i&gt;NVIDIA DGX B200&lt;/i&gt;&lt;i&gt; system.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ucsd-generative-ai-research-dgx-b200/</guid><pubDate>Wed, 17 Dec 2025 16:00:15 +0000</pubDate></item><item><title>[NEW] Google releases Gemini 3 Flash, promising improved intelligence and efficiency (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/12/google-releases-gemini-3-flash-promising-improved-intelligence-and-efficiency/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google’s Gemini 3 family is now complete with release of Gemini 3 Flash.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3_flash_model_blog_header_dark_bleed_2096x1182-640x361.png" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3_flash_model_blog_header_dark_bleed_2096x1182-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Add a lightning bolt because it's fast. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google began its transition to Gemini 3 a few weeks ago with the launch of the Pro model, and the arrival of Gemini 3 Flash kicks it into high gear. The new, faster Gemini 3 model is coming to the Gemini app and search, and developers will be able to access it immediately via the Gemini API, Vertex AI, AI Studio, and Antigravity. Google’s bigger gen AI model is also picking up steam, with both Gemini 3 Pro and its image component (Nano Banana Pro) expanding in search.&lt;/p&gt;
&lt;p&gt;This may come as a shock, but Google says Gemini 3 Flash is faster and more capable than its previous base model. As usual, Google has a raft of benchmark numbers that show modest improvements for the new model. It bests the old 2.5 Flash in basic academic and reasoning tests like GPQA Diamond and MMMU Pro (where it even beats 3 Pro). It gets a larger boost in Humanity’s Last Exam (HLE), which tests advanced domain-specific knowledge. Gemini 3 Flash has tripled the old models’ score in HLE, landing at 33.7 percent without tool use. That’s just a few points behind the Gemini 3 Pro model.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132361 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Gemini HLE test" class="fullwidth full" height="1350" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3-flash-hle.png" width="1080" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google is talking up Gemini 3 Flash’s coding skills, and the provided benchmarks seem to back that talk up. Over the past year, Google has mostly pushed its Pro models as the best for generating code, but 3 Flash has done a lot of catching up. In the popular SWE-Bench Verified test, Gemini 3 Flash has gained almost 20 points on the 2.5 branch.&lt;/p&gt;
&lt;p&gt;The new model is also a lot less likely to get general-knowledge questions wrong. In the Simple QA Verified test, Gemini 3 Flash scored 68.7 percent, which is only a little below Gemini 3 Pro. The last Flash model scored just 28.1 percent on that test. At least as far as the evaluation scores go, Gemini 3 Flash performs much closer to Google’s Pro model versus the older 2.5 family. At the same time, it’s considerably more efficient, according to Google.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="930" id="video-2132353-1" preload="metadata" width="736"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Gemini-3-Flash-App-opt.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;One of Gemini 3 Pro’s defining advances was its ability to generate interactive simulations and multimodal content. Gemini 3 Flash reportedly retains that underlying capability. Gemini 3 Flash offers better performance than Gemini 2.5 Pro did, but it runs workloads three times faster. It’s also a lot cheaper than the Pro models if you’re paying per token. One million input tokens for 3 Flash will run devs $0.50, and a million output tokens will cost $3. However, that’s an increase compared to Gemini 2.5 Flash input and output at $0.30 and $2.50, respectively. The Pro model’s tokens are $2 (1M input) and $12 (1M output).&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Simplified model selection&lt;/h2&gt;
&lt;p&gt;Google’s rapid-fire release of new AI models and tools has occasionally made the Gemini app a bit confusing. Over recent weeks, the settings have been pared down and rearranged. With the release of Gemini 3 Flash, that will become the new default model in the Gemini app and web interface—that’s the Fast setting in the app, as well as the one labeled Thinking, which uses simulated reasoning for better outputs.&lt;/p&gt;
&lt;p&gt;Gemini 3 Pro will continue to be available under the Pro option. That’s still a bit misleading, though, as both versions of Gemini 3 can use the reasoning process that Google likes to call “thinking” to generate answers. Whichever one you choose in the app, you can then select tools like image generation, canvas, and Deep Research.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132356 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Gemini model picker new" class="fullwidth full" height="298" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Gemini-3-Flash-debut.png" width="786" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Fast and Thinking are both Gemini 3 Flash; the Pro option invokes Gemini 3 Pro, which always “thinks.”

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;In addition to its debut in the Gemini app, the new Flash model will be coming to search immediately. When Google says “search” in this context, it mostly means AI Mode. Gemini 3 Flash will be the default model in AI Mode going forward. That means free users will see a notable improvement when using the Gemini app.&lt;/p&gt;
&lt;p&gt;There are no specific changes to AI Overviews. Google says AI Overviews will continue to use the best model for the job. Due to its place at the top of organic search results, though, you’ll probably see it lean on less capable (but faster) models. Gemini 3 Flash could show up there—even Gemini 3 Pro could power some complex queries in AI Overviews for paying subscribers.&lt;/p&gt;
&lt;p&gt;Gemini 3 Pro is also expanding in AI Mode for all US-based users. Likewise, Gemini 3 Pro Image (Nano Banana Pro) will also arrive in AI mode for all. There will be limits on free access to these models, but Google hasn’t specified what those are. It does say that Pro and Ultra subscribers will enjoy much higher usage limits.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google’s Gemini 3 family is now complete with release of Gemini 3 Flash.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3_flash_model_blog_header_dark_bleed_2096x1182-640x361.png" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3_flash_model_blog_header_dark_bleed_2096x1182-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Add a lightning bolt because it's fast. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google began its transition to Gemini 3 a few weeks ago with the launch of the Pro model, and the arrival of Gemini 3 Flash kicks it into high gear. The new, faster Gemini 3 model is coming to the Gemini app and search, and developers will be able to access it immediately via the Gemini API, Vertex AI, AI Studio, and Antigravity. Google’s bigger gen AI model is also picking up steam, with both Gemini 3 Pro and its image component (Nano Banana Pro) expanding in search.&lt;/p&gt;
&lt;p&gt;This may come as a shock, but Google says Gemini 3 Flash is faster and more capable than its previous base model. As usual, Google has a raft of benchmark numbers that show modest improvements for the new model. It bests the old 2.5 Flash in basic academic and reasoning tests like GPQA Diamond and MMMU Pro (where it even beats 3 Pro). It gets a larger boost in Humanity’s Last Exam (HLE), which tests advanced domain-specific knowledge. Gemini 3 Flash has tripled the old models’ score in HLE, landing at 33.7 percent without tool use. That’s just a few points behind the Gemini 3 Pro model.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132361 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Gemini HLE test" class="fullwidth full" height="1350" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3-flash-hle.png" width="1080" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google is talking up Gemini 3 Flash’s coding skills, and the provided benchmarks seem to back that talk up. Over the past year, Google has mostly pushed its Pro models as the best for generating code, but 3 Flash has done a lot of catching up. In the popular SWE-Bench Verified test, Gemini 3 Flash has gained almost 20 points on the 2.5 branch.&lt;/p&gt;
&lt;p&gt;The new model is also a lot less likely to get general-knowledge questions wrong. In the Simple QA Verified test, Gemini 3 Flash scored 68.7 percent, which is only a little below Gemini 3 Pro. The last Flash model scored just 28.1 percent on that test. At least as far as the evaluation scores go, Gemini 3 Flash performs much closer to Google’s Pro model versus the older 2.5 family. At the same time, it’s considerably more efficient, according to Google.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="930" id="video-2132353-1" preload="metadata" width="736"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Gemini-3-Flash-App-opt.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;One of Gemini 3 Pro’s defining advances was its ability to generate interactive simulations and multimodal content. Gemini 3 Flash reportedly retains that underlying capability. Gemini 3 Flash offers better performance than Gemini 2.5 Pro did, but it runs workloads three times faster. It’s also a lot cheaper than the Pro models if you’re paying per token. One million input tokens for 3 Flash will run devs $0.50, and a million output tokens will cost $3. However, that’s an increase compared to Gemini 2.5 Flash input and output at $0.30 and $2.50, respectively. The Pro model’s tokens are $2 (1M input) and $12 (1M output).&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Simplified model selection&lt;/h2&gt;
&lt;p&gt;Google’s rapid-fire release of new AI models and tools has occasionally made the Gemini app a bit confusing. Over recent weeks, the settings have been pared down and rearranged. With the release of Gemini 3 Flash, that will become the new default model in the Gemini app and web interface—that’s the Fast setting in the app, as well as the one labeled Thinking, which uses simulated reasoning for better outputs.&lt;/p&gt;
&lt;p&gt;Gemini 3 Pro will continue to be available under the Pro option. That’s still a bit misleading, though, as both versions of Gemini 3 can use the reasoning process that Google likes to call “thinking” to generate answers. Whichever one you choose in the app, you can then select tools like image generation, canvas, and Deep Research.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132356 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Gemini model picker new" class="fullwidth full" height="298" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Gemini-3-Flash-debut.png" width="786" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Fast and Thinking are both Gemini 3 Flash; the Pro option invokes Gemini 3 Pro, which always “thinks.”

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;In addition to its debut in the Gemini app, the new Flash model will be coming to search immediately. When Google says “search” in this context, it mostly means AI Mode. Gemini 3 Flash will be the default model in AI Mode going forward. That means free users will see a notable improvement when using the Gemini app.&lt;/p&gt;
&lt;p&gt;There are no specific changes to AI Overviews. Google says AI Overviews will continue to use the best model for the job. Due to its place at the top of organic search results, though, you’ll probably see it lean on less capable (but faster) models. Gemini 3 Flash could show up there—even Gemini 3 Pro could power some complex queries in AI Overviews for paying subscribers.&lt;/p&gt;
&lt;p&gt;Gemini 3 Pro is also expanding in AI Mode for all US-based users. Likewise, Gemini 3 Pro Image (Nano Banana Pro) will also arrive in AI mode for all. There will be limits on free access to these models, but Google hasn’t specified what those are. It does say that Pro and Ultra subscribers will enjoy much higher usage limits.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/12/google-releases-gemini-3-flash-promising-improved-intelligence-and-efficiency/</guid><pubDate>Wed, 17 Dec 2025 16:00:25 +0000</pubDate></item><item><title>[NEW] Into the Omniverse: OpenUSD and NVIDIA Halos Accelerate Safety for Robotaxis, Physical AI Systems (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/openusd-halos-safety-robotaxi-physical-ai/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of &lt;/i&gt;&lt;i&gt;Into the Omniverse&lt;/i&gt;&lt;i&gt;, a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advancements in &lt;/i&gt;&lt;i&gt;OpenUSD&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;NVIDIA Omniverse&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Physical AI is moving from research labs into the real world, powering intelligent robots and autonomous vehicles (AVs) — such as robotaxis — that must reliably sense, reason and act amid unpredictable conditions.&lt;/p&gt;
&lt;p&gt;To safely scale these systems, developers need workflows that connect real-world data, high-fidelity simulation and robust AI models atop the common foundation provided by the OpenUSD framework.&lt;/p&gt;
&lt;p&gt;The recently published OpenUSD Core Specification 1.0, OpenUSD — aka Universal Scene Description — now defines standard data types, file formats and composition behaviors, giving developers predictable, interoperable USD pipelines as they scale autonomous systems.&lt;/p&gt;
&lt;p&gt;Powered by OpenUSD, NVIDIA Omniverse libraries combine NVIDIA RTX rendering, physics simulation and efficient runtimes to create digital twins and simulation-ready (SimReady) assets that accurately reflect real-world environments for synthetic data generation and testing.&lt;/p&gt;
&lt;p&gt;NVIDIA Cosmos world foundation models can run on top of these simulations to amplify data variation, generating new weather, lighting and terrain conditions from the same scenes so teams can safely cover rare and challenging edge cases.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more by watching the OpenUSD livestream today at 11 a.m. PT or in replay, part of the NVIDIA Omniverse OpenUSD Insiders series:&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;In addition, advancements in synthetic data generation, multimodal datasets and SimReady workflows are now converging with the NVIDIA Halos framework for AV safety, creating a standards-based path to safer, faster, more cost-effective deployment of next-generation autonomous machines.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Building the Foundation for Safe Physical AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;b&gt;Open Standards and SimReady Assets&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The OpenUSD Core Specification 1.0 establishes the standard data models and behaviors that underpin SimReady assets, enabling developers to build interoperable simulation pipelines for AI factories and robotics on OpenUSD.&lt;/p&gt;
&lt;p&gt;Built on this foundation, SimReady 3D assets can be reused across tools and teams and loaded directly into NVIDIA Isaac Sim, where USDPhysics colliders, rigid body dynamics and composition-arc–based variants let teams test robots in virtual facilities that closely mirror real operations.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Open-Source Learning&amp;nbsp;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The Learn OpenUSD curriculum is now open source and available on GitHub, enabling contributors to localize and adapt templates, exercises and content for different audiences, languages and use cases. This gives educators a ready-made foundation to onboard new teams into OpenUSD-centric simulation workflows.​&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Generative Worlds as Safety Multiplier&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Gaussian splatting — a technique that uses editable 3D elements to render environments quickly and with high fidelity — and world models are accelerating simulation pipelines for safe robotics testing and validation.&lt;/p&gt;
&lt;p&gt;At SIGGRAPH Asia, the NVIDIA Research team introduced Play4D, a streaming pipeline that enables 4D Gaussian splatting to accurately render dynamic scenes and improve realism.&lt;/p&gt;
&lt;p&gt;Spatial intelligence company World Labs is using its Marble generative world model with NVIDIA Isaac Sim and Omniverse NuRec so researchers can turn text prompts and sample images into photorealistic, Gaussian-based physics-ready 3D environments in hours instead of weeks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-full wp-image-88411" height="338" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/WorldLabs_IsaacSim_Clip.gif" width="600" /&gt;&lt;/p&gt;
&lt;p&gt;Those worlds can then be used for physical AI training, testing and sim-to-real transfer. This high-fidelity simulation workflow expands the range of scenarios robots can practice in while keeping experimentation safely in simulation.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Lightwheel Helps Teams Scale Robot Training With SimReady Assets&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Powered by OpenUSD, Lightwheel’s SimReady asset library includes a common scene description layer, making it easy to assemble high-fidelity digital twins for robots. The SimReady assets are embedded with precise geometry, materials and validated physical properties, which can be loaded directly into NVIDIA Isaac Sim and Isaac Lab for robot training. This allows robots to experience realistic contacts, dynamics and sensor feedback as they learn.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;End-to-End Autonomous Vehicle Safety&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;End-to-end autonomous vehicle safety advancements are accelerating with new research, open frameworks and inspection services that make validation more rigorous and scalable.&lt;/p&gt;
&lt;p&gt;NVIDIA researchers, with collaborators at Harvard University and Stanford University, recently introduced the Sim2Val framework to statistically combine real-world and simulated test results, reducing AV developers’ need for costly physical mileage while demonstrating how robotaxis and AVs can behave safely across rare and safety-critical scenarios.&lt;/p&gt;
&lt;p&gt;Learn more by watching NVIDIA’s “Safety in the Loop” livestream:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;These innovations are complemented by a new, open-source NVIDIA Omniverse NuRec Fixer, a Cosmos-based model trained on AV data that removes artifacts in neural reconstructions to produce higher-quality SimReady assets.&lt;/p&gt;
&lt;p&gt;To align these advances with rigorous global standards, the NVIDIA Halos AI Systems Inspection Lab — accredited by ANAB — provides impartial inspection and certification of Halos elements across robotaxi fleets, AV stacks, sensors and manufacturer platforms through the Halos Certification Program.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AV Ecosystem Leaders Putting Physical AI Safety to Work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bosch, Nuro and Wayve are among the first participants in the NVIDIA Halos AI Systems Inspection Lab, which aims to accelerate the safe, large-scale deployment of robotaxi fleets. Onsemi, which makes sensor systems for AVs, industrial automation and medical applications, has recently become the first company to pass inspection for the NVIDIA Halos AI Systems Inspection Lab.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;The open-source CARLA simulator integrates NVIDIA NuRec and Cosmos Transfer to generate reconstructed drives and diverse scenario variations, while Voxel51’s FiftyOne engine, linked to Cosmos Dataset Search, NuRec and Cosmos Transfer, helps teams curate, annotate and evaluate multimodal datasets across the AV pipeline.​&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Mcity at the University of Michigan is enhancing the digital twin of its 32-acre AV test facility using Omniverse libraries and technologies. The team is integrating the NVIDIA Blueprint for AV simulation and Omniverse Sensor RTX application programming interfaces to create physics-based models of camera, lidar, radar and ultrasonic sensors.&lt;/p&gt;
&lt;p&gt;By aligning real sensor recordings with high-fidelity simulated data and sharing assets openly, Mcity enables safe, repeatable testing of rare and hazardous driving scenarios before vehicles operate on public roads.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Plugged Into the World of OpenUSD and Physical AI Safety&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Learn more about OpenUSD, NVIDIA Halos and physical AI safety by exploring these resources:&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Stay up to date by subscribing to&lt;/i&gt; &lt;i&gt;NVIDIA news&lt;/i&gt;&lt;i&gt;, joining the &lt;/i&gt;&lt;i&gt;community&lt;/i&gt;&lt;i&gt; and following NVIDIA Omniverse on &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Medium&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of &lt;/i&gt;&lt;i&gt;Into the Omniverse&lt;/i&gt;&lt;i&gt;, a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advancements in &lt;/i&gt;&lt;i&gt;OpenUSD&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;NVIDIA Omniverse&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Physical AI is moving from research labs into the real world, powering intelligent robots and autonomous vehicles (AVs) — such as robotaxis — that must reliably sense, reason and act amid unpredictable conditions.&lt;/p&gt;
&lt;p&gt;To safely scale these systems, developers need workflows that connect real-world data, high-fidelity simulation and robust AI models atop the common foundation provided by the OpenUSD framework.&lt;/p&gt;
&lt;p&gt;The recently published OpenUSD Core Specification 1.0, OpenUSD — aka Universal Scene Description — now defines standard data types, file formats and composition behaviors, giving developers predictable, interoperable USD pipelines as they scale autonomous systems.&lt;/p&gt;
&lt;p&gt;Powered by OpenUSD, NVIDIA Omniverse libraries combine NVIDIA RTX rendering, physics simulation and efficient runtimes to create digital twins and simulation-ready (SimReady) assets that accurately reflect real-world environments for synthetic data generation and testing.&lt;/p&gt;
&lt;p&gt;NVIDIA Cosmos world foundation models can run on top of these simulations to amplify data variation, generating new weather, lighting and terrain conditions from the same scenes so teams can safely cover rare and challenging edge cases.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more by watching the OpenUSD livestream today at 11 a.m. PT or in replay, part of the NVIDIA Omniverse OpenUSD Insiders series:&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;In addition, advancements in synthetic data generation, multimodal datasets and SimReady workflows are now converging with the NVIDIA Halos framework for AV safety, creating a standards-based path to safer, faster, more cost-effective deployment of next-generation autonomous machines.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Building the Foundation for Safe Physical AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;b&gt;Open Standards and SimReady Assets&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The OpenUSD Core Specification 1.0 establishes the standard data models and behaviors that underpin SimReady assets, enabling developers to build interoperable simulation pipelines for AI factories and robotics on OpenUSD.&lt;/p&gt;
&lt;p&gt;Built on this foundation, SimReady 3D assets can be reused across tools and teams and loaded directly into NVIDIA Isaac Sim, where USDPhysics colliders, rigid body dynamics and composition-arc–based variants let teams test robots in virtual facilities that closely mirror real operations.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Open-Source Learning&amp;nbsp;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The Learn OpenUSD curriculum is now open source and available on GitHub, enabling contributors to localize and adapt templates, exercises and content for different audiences, languages and use cases. This gives educators a ready-made foundation to onboard new teams into OpenUSD-centric simulation workflows.​&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Generative Worlds as Safety Multiplier&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Gaussian splatting — a technique that uses editable 3D elements to render environments quickly and with high fidelity — and world models are accelerating simulation pipelines for safe robotics testing and validation.&lt;/p&gt;
&lt;p&gt;At SIGGRAPH Asia, the NVIDIA Research team introduced Play4D, a streaming pipeline that enables 4D Gaussian splatting to accurately render dynamic scenes and improve realism.&lt;/p&gt;
&lt;p&gt;Spatial intelligence company World Labs is using its Marble generative world model with NVIDIA Isaac Sim and Omniverse NuRec so researchers can turn text prompts and sample images into photorealistic, Gaussian-based physics-ready 3D environments in hours instead of weeks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-full wp-image-88411" height="338" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/WorldLabs_IsaacSim_Clip.gif" width="600" /&gt;&lt;/p&gt;
&lt;p&gt;Those worlds can then be used for physical AI training, testing and sim-to-real transfer. This high-fidelity simulation workflow expands the range of scenarios robots can practice in while keeping experimentation safely in simulation.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Lightwheel Helps Teams Scale Robot Training With SimReady Assets&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Powered by OpenUSD, Lightwheel’s SimReady asset library includes a common scene description layer, making it easy to assemble high-fidelity digital twins for robots. The SimReady assets are embedded with precise geometry, materials and validated physical properties, which can be loaded directly into NVIDIA Isaac Sim and Isaac Lab for robot training. This allows robots to experience realistic contacts, dynamics and sensor feedback as they learn.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;End-to-End Autonomous Vehicle Safety&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;End-to-end autonomous vehicle safety advancements are accelerating with new research, open frameworks and inspection services that make validation more rigorous and scalable.&lt;/p&gt;
&lt;p&gt;NVIDIA researchers, with collaborators at Harvard University and Stanford University, recently introduced the Sim2Val framework to statistically combine real-world and simulated test results, reducing AV developers’ need for costly physical mileage while demonstrating how robotaxis and AVs can behave safely across rare and safety-critical scenarios.&lt;/p&gt;
&lt;p&gt;Learn more by watching NVIDIA’s “Safety in the Loop” livestream:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;These innovations are complemented by a new, open-source NVIDIA Omniverse NuRec Fixer, a Cosmos-based model trained on AV data that removes artifacts in neural reconstructions to produce higher-quality SimReady assets.&lt;/p&gt;
&lt;p&gt;To align these advances with rigorous global standards, the NVIDIA Halos AI Systems Inspection Lab — accredited by ANAB — provides impartial inspection and certification of Halos elements across robotaxi fleets, AV stacks, sensors and manufacturer platforms through the Halos Certification Program.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AV Ecosystem Leaders Putting Physical AI Safety to Work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bosch, Nuro and Wayve are among the first participants in the NVIDIA Halos AI Systems Inspection Lab, which aims to accelerate the safe, large-scale deployment of robotaxi fleets. Onsemi, which makes sensor systems for AVs, industrial automation and medical applications, has recently become the first company to pass inspection for the NVIDIA Halos AI Systems Inspection Lab.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;The open-source CARLA simulator integrates NVIDIA NuRec and Cosmos Transfer to generate reconstructed drives and diverse scenario variations, while Voxel51’s FiftyOne engine, linked to Cosmos Dataset Search, NuRec and Cosmos Transfer, helps teams curate, annotate and evaluate multimodal datasets across the AV pipeline.​&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Mcity at the University of Michigan is enhancing the digital twin of its 32-acre AV test facility using Omniverse libraries and technologies. The team is integrating the NVIDIA Blueprint for AV simulation and Omniverse Sensor RTX application programming interfaces to create physics-based models of camera, lidar, radar and ultrasonic sensors.&lt;/p&gt;
&lt;p&gt;By aligning real sensor recordings with high-fidelity simulated data and sharing assets openly, Mcity enables safe, repeatable testing of rare and hazardous driving scenarios before vehicles operate on public roads.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Plugged Into the World of OpenUSD and Physical AI Safety&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Learn more about OpenUSD, NVIDIA Halos and physical AI safety by exploring these resources:&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Stay up to date by subscribing to&lt;/i&gt; &lt;i&gt;NVIDIA news&lt;/i&gt;&lt;i&gt;, joining the &lt;/i&gt;&lt;i&gt;community&lt;/i&gt;&lt;i&gt; and following NVIDIA Omniverse on &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Medium&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/openusd-halos-safety-robotaxi-physical-ai/</guid><pubDate>Wed, 17 Dec 2025 17:00:49 +0000</pubDate></item><item><title>[NEW] Amazon appoints longtime AWS exec Peter DeSantis to lead new AI org (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/17/amazon-appoints-longtime-aws-exec-peter-desantis-to-lead-new-ai-org/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/amazon-data-center.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon CEO Andy Jassy announced in a message to staff on Wednesday that longtime AWS executive Peter DeSantis will lead a new AI-focused organization within the company. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This organization will be responsible for Amazon’s AI models like Nova, as well as silicon development and quantum computing, which help make AI tools faster and more efficient.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;DeSantis has spent 27 years at Amazon, including eight years as an SVP for AWS, the cloud provider that powers about one-third of the internet. At AWS’s recent re:Invent event, Amazon hammered home its commitment to AI for enterprise use, so it makes sense that the company is spinning out a new team from AWS leadership.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With our Nova 2 models just launched at re:Invent, our custom silicon growing rapidly, and the advantages of optimizing across models, chips, and cloud software and infrastructure, we wanted to free Peter up to focus his energy, invention cycles, and leadership on these new areas,” Jassy wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon’s increasing emphasis on AI comes at a time when the company is eager to strengthen its foothold in the AI race, perhaps more through investments than its own innovations. Last month, AWS announced a $50 billion investment in the U.S. government’s AI infrastructure. Amazon is also reportedly in talks to invest $10 billion in OpenAI, and has already invested $8 billion in OpenAI rival Anthropic.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/amazon-data-center.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon CEO Andy Jassy announced in a message to staff on Wednesday that longtime AWS executive Peter DeSantis will lead a new AI-focused organization within the company. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This organization will be responsible for Amazon’s AI models like Nova, as well as silicon development and quantum computing, which help make AI tools faster and more efficient.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;DeSantis has spent 27 years at Amazon, including eight years as an SVP for AWS, the cloud provider that powers about one-third of the internet. At AWS’s recent re:Invent event, Amazon hammered home its commitment to AI for enterprise use, so it makes sense that the company is spinning out a new team from AWS leadership.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With our Nova 2 models just launched at re:Invent, our custom silicon growing rapidly, and the advantages of optimizing across models, chips, and cloud software and infrastructure, we wanted to free Peter up to focus his energy, invention cycles, and leadership on these new areas,” Jassy wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon’s increasing emphasis on AI comes at a time when the company is eager to strengthen its foothold in the AI race, perhaps more through investments than its own innovations. Last month, AWS announced a $50 billion investment in the U.S. government’s AI infrastructure. Amazon is also reportedly in talks to invest $10 billion in OpenAI, and has already invested $8 billion in OpenAI rival Anthropic.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/17/amazon-appoints-longtime-aws-exec-peter-desantis-to-lead-new-ai-org/</guid><pubDate>Wed, 17 Dec 2025 18:06:29 +0000</pubDate></item><item><title>[NEW] A new way to increase the capabilities of large language models (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/new-way-to-increase-large-language-model-capabilities-1217</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/mit-watson-cats-in-a-box.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Most languages use word position and sentence structure to extract meaning. For example, “The cat sat on the box,” is not the same as “The box was on the cat.” Over a long text, like a financial document or a novel, the syntax of these words likely evolves.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Similarly, a person might be tracking variables in a piece of code or following instructions that have conditional actions. These are examples of state changes and sequential reasoning that we expect state-of-the-art artificial intelligence systems to excel at; however, the existing, cutting-edge attention mechanism within transformers — the primarily architecture used in large language models (LLMs) for determining the importance of words — has theoretical and empirical limitations when it comes to such capabilities.&lt;/p&gt;&lt;p&gt;An attention mechanism allows an LLM to look back at earlier parts of a query or document and, based on its training, determine which details and words matter most; however, this mechanism alone does not understand word order. It “sees” all of the input words, a.k.a. tokens, at the same time and handles them in the order that they’re presented, so researchers have developed techniques to encode position information. This is key for domains that are highly structured, like language. But the predominant position-encoding method, called rotary position encoding (RoPE), only takes into account the relative distance between tokens in a sequence and is independent of the input data. This means that, for example, words that are four positions apart, like “cat” and “box” in the example above, will all receive the same fixed mathematical rotation specific to that relative distance.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Now research led by MIT and the MIT-IBM Watson AI Lab has produced an encoding technique known as “PaTH Attention” that makes positional information adaptive and context-aware rather than static, as with RoPE.&lt;/p&gt;&lt;p&gt;“Transformers enable accurate and scalable modeling of many domains, but they have these limitations vis-a-vis state tracking, a class of phenomena that is thought to underlie important capabilities that we want in our AI systems. So, the important question is: How can we maintain the scalability and efficiency of transformers, while enabling state tracking?” says the paper’s senior author Yoon Kim, an associate professor in the Department of Electrical Engineering and Computer Science (EECS), a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL), and a researcher with the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;A new paper on this work was presented earlier this month at the Conference on Neural Information Processing Systems (NeurIPS). Kim’s co-authors include lead author Songlin Yang, an EECS graduate student and former MIT-IBM Watson AI Lab Summer Program intern; Kaiyue Wen of Stanford University; Liliang Ren of Microsoft; and Yikang Shen, Shawn Tan, Mayank Mishra, and Rameswar Panda of IBM Research and the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Path to understanding&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Instead of assigning every word a fixed rotation based on relative distance between tokens, as RoPE does, PaTH Attention is flexible, treating the in-between words as a path made up of small, data-dependent transformations. Each transformation, based on a mathematical operation called a Householder reflection, acts like a tiny mirror that adjusts depending on the content of each token it passes. Each step in a sequence can influence how the model interprets information later on. The cumulative effect lets the system model how the meaning changes along the path between words, not just how far apart they are. This approach allows transformers to keep track of how entities and relationships change over time, giving it a sense of “positional memory.” Think of this as walking a path while experiencing your environment and how it affects you. Further, the team also developed a hardware-efficient algorithm to more efficiently compute attention scores between every pair of tokens so that the cumulative mathematical transformation from PaTH Attention is compressed and broken down into smaller computations so that it’s compatible with fast processing on GPUs.&lt;/p&gt;&lt;p&gt;The MIT-IBM researchers then explored PaTH Attention’s performance on synthetic and real-world tasks, including reasoning, long-context benchmarks, and full LLM training to see whether it improved a model’s ability to track information over time. The team tested its ability to follow the most recent “write” command despite many distracting steps and multi-step recall tests, tasks that are difficult for standard positional encoding methods like RoPE. The researchers also trained mid-size LLMs and compared them against other methods. PaTH Attention improved perplexity and outcompeted other methods on reasoning benchmarks it wasn’t trained on. They also evaluated retrieval, reasoning, and stability with inputs of tens of thousands of tokens. PaTH Attention consistently proved capable of content-awareness.&lt;/p&gt;&lt;p&gt;“We found that both on diagnostic tasks that are designed to test the limitations of transformers and on real-world language modeling tasks, our new approach was able to outperform existing attention mechanisms, while maintaining their efficiency,” says Kim. Further, “I’d be excited to see whether these types of data-dependent position encodings, like PATH, improve the performance of transformers on structured domains like biology, in [analyzing] proteins or DNA.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Thinking bigger and more efficiently&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The researchers then investigated how the PaTH Attention mechanism would perform if it more similarly mimicked human cognition, where we ignore old or less-relevant information when making decisions. To do this, they combined PaTH Attention with another position encoding scheme known as the Forgetting Transformer (FoX), which allows models to selectively “forget.” The resulting PaTH-FoX system adds a way to down-weight information in a data-dependent way, achieving strong results across reasoning, long-context understanding, and language modeling benchmarks. In this way, PaTH Attention extends the expressive power of transformer architectures.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kim says research like this is part of a broader effort to develop the “next big thing” in AI. He explains that a major driver of both the deep learning and generative AI revolutions has been the creation of “general-purpose building blocks that can be applied to wide domains,” such as “convolution layers, RNN [recurrent neural network] layers,” and, most recently, transformers. Looking ahead, Kim notes that considerations like accuracy, expressivity, flexibility, and hardware scalability have been and will be essential. As he puts it, “the core enterprise of modern architecture research is trying to come up with these new primitives that maintain or improve the expressivity, while also being scalable.”&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the MIT-IBM Watson AI Lab and the AI2050 program at Schmidt Sciences.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/mit-watson-cats-in-a-box.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Most languages use word position and sentence structure to extract meaning. For example, “The cat sat on the box,” is not the same as “The box was on the cat.” Over a long text, like a financial document or a novel, the syntax of these words likely evolves.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Similarly, a person might be tracking variables in a piece of code or following instructions that have conditional actions. These are examples of state changes and sequential reasoning that we expect state-of-the-art artificial intelligence systems to excel at; however, the existing, cutting-edge attention mechanism within transformers — the primarily architecture used in large language models (LLMs) for determining the importance of words — has theoretical and empirical limitations when it comes to such capabilities.&lt;/p&gt;&lt;p&gt;An attention mechanism allows an LLM to look back at earlier parts of a query or document and, based on its training, determine which details and words matter most; however, this mechanism alone does not understand word order. It “sees” all of the input words, a.k.a. tokens, at the same time and handles them in the order that they’re presented, so researchers have developed techniques to encode position information. This is key for domains that are highly structured, like language. But the predominant position-encoding method, called rotary position encoding (RoPE), only takes into account the relative distance between tokens in a sequence and is independent of the input data. This means that, for example, words that are four positions apart, like “cat” and “box” in the example above, will all receive the same fixed mathematical rotation specific to that relative distance.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Now research led by MIT and the MIT-IBM Watson AI Lab has produced an encoding technique known as “PaTH Attention” that makes positional information adaptive and context-aware rather than static, as with RoPE.&lt;/p&gt;&lt;p&gt;“Transformers enable accurate and scalable modeling of many domains, but they have these limitations vis-a-vis state tracking, a class of phenomena that is thought to underlie important capabilities that we want in our AI systems. So, the important question is: How can we maintain the scalability and efficiency of transformers, while enabling state tracking?” says the paper’s senior author Yoon Kim, an associate professor in the Department of Electrical Engineering and Computer Science (EECS), a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL), and a researcher with the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;A new paper on this work was presented earlier this month at the Conference on Neural Information Processing Systems (NeurIPS). Kim’s co-authors include lead author Songlin Yang, an EECS graduate student and former MIT-IBM Watson AI Lab Summer Program intern; Kaiyue Wen of Stanford University; Liliang Ren of Microsoft; and Yikang Shen, Shawn Tan, Mayank Mishra, and Rameswar Panda of IBM Research and the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Path to understanding&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Instead of assigning every word a fixed rotation based on relative distance between tokens, as RoPE does, PaTH Attention is flexible, treating the in-between words as a path made up of small, data-dependent transformations. Each transformation, based on a mathematical operation called a Householder reflection, acts like a tiny mirror that adjusts depending on the content of each token it passes. Each step in a sequence can influence how the model interprets information later on. The cumulative effect lets the system model how the meaning changes along the path between words, not just how far apart they are. This approach allows transformers to keep track of how entities and relationships change over time, giving it a sense of “positional memory.” Think of this as walking a path while experiencing your environment and how it affects you. Further, the team also developed a hardware-efficient algorithm to more efficiently compute attention scores between every pair of tokens so that the cumulative mathematical transformation from PaTH Attention is compressed and broken down into smaller computations so that it’s compatible with fast processing on GPUs.&lt;/p&gt;&lt;p&gt;The MIT-IBM researchers then explored PaTH Attention’s performance on synthetic and real-world tasks, including reasoning, long-context benchmarks, and full LLM training to see whether it improved a model’s ability to track information over time. The team tested its ability to follow the most recent “write” command despite many distracting steps and multi-step recall tests, tasks that are difficult for standard positional encoding methods like RoPE. The researchers also trained mid-size LLMs and compared them against other methods. PaTH Attention improved perplexity and outcompeted other methods on reasoning benchmarks it wasn’t trained on. They also evaluated retrieval, reasoning, and stability with inputs of tens of thousands of tokens. PaTH Attention consistently proved capable of content-awareness.&lt;/p&gt;&lt;p&gt;“We found that both on diagnostic tasks that are designed to test the limitations of transformers and on real-world language modeling tasks, our new approach was able to outperform existing attention mechanisms, while maintaining their efficiency,” says Kim. Further, “I’d be excited to see whether these types of data-dependent position encodings, like PATH, improve the performance of transformers on structured domains like biology, in [analyzing] proteins or DNA.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Thinking bigger and more efficiently&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The researchers then investigated how the PaTH Attention mechanism would perform if it more similarly mimicked human cognition, where we ignore old or less-relevant information when making decisions. To do this, they combined PaTH Attention with another position encoding scheme known as the Forgetting Transformer (FoX), which allows models to selectively “forget.” The resulting PaTH-FoX system adds a way to down-weight information in a data-dependent way, achieving strong results across reasoning, long-context understanding, and language modeling benchmarks. In this way, PaTH Attention extends the expressive power of transformer architectures.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kim says research like this is part of a broader effort to develop the “next big thing” in AI. He explains that a major driver of both the deep learning and generative AI revolutions has been the creation of “general-purpose building blocks that can be applied to wide domains,” such as “convolution layers, RNN [recurrent neural network] layers,” and, most recently, transformers. Looking ahead, Kim notes that considerations like accuracy, expressivity, flexibility, and hardware scalability have been and will be essential. As he puts it, “the core enterprise of modern architecture research is trying to come up with these new primitives that maintain or improve the expressivity, while also being scalable.”&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the MIT-IBM Watson AI Lab and the AI2050 program at Schmidt Sciences.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/new-way-to-increase-large-language-model-capabilities-1217</guid><pubDate>Thu, 18 Dec 2025 04:10:00 +0000</pubDate></item></channel></rss>