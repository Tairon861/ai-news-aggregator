<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 01 Dec 2025 18:37:17 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>[NEW] AI business reality ‚Äì what enterprise leaders need to know (AI News)</title><link>https://www.artificialintelligence-news.com/news/ai-business-reality-what-enterprise-leaders-need-know/</link><description>&lt;p&gt;When JPMorgan Asset Management reported that AI spending accounted for two-thirds of US GDP growth in the first half of 2025, it wasn‚Äôt just a statistic ‚Äì it was a signal.&lt;/p&gt;&lt;p&gt;The conversation reached a turning point recently when OpenAI CEO Sam Altman, Amazon‚Äôs Jeff Bezos, and Goldman Sachs CEO David Solomon each acknowledged market froth within days of each other. But here‚Äôs what matters for enterprise decision-makers: acknowledging overheated markets isn‚Äôt the same as dismissing AI‚Äôs enterprise value.&lt;/p&gt;&lt;p&gt;Corporate AI investment reached US$252.3 billion in 2024, with private investment climbing 44.5%, according to Stanford University. The question isn‚Äôt whether to invest in AI ‚Äì it‚Äôs how to invest strategically while others ‚Äì specifically, an organisation‚Äôs competitors ‚Äì overspend on infrastructure and solutions that may never deliver returns.&lt;/p&gt;&lt;h3&gt;What separates AI winners from the 95% who fail&lt;/h3&gt;&lt;p&gt;An MIT study found that 95% of businesses invested in AI have failed to make money off the technology, according to ABC News. But that statistic masks a more important truth: 5% succeed ‚Äì and they‚Äôre doing things fundamentally differently.&lt;/p&gt;&lt;p&gt;High-performing organisations are investing more in AI capabilities, with more than one-third committing over 20% of their digital budgets to AI technologies, a McKinsey report shows. But they‚Äôre not just spending more ‚Äì they‚Äôre spending smarter.&lt;/p&gt;&lt;p&gt;The McKinsey research reveals what separates winners from the pack. About three-quarters of high performers say their organisations are scaling or have scaled AI, compared with one-third of other organisations. The leaders share common characteristics: they push for transformative innovation rather than incremental improvements, redesign workflows around AI capabilities, and implement rigorous governance frameworks.&lt;/p&gt;&lt;h3&gt;The infrastructure investment dilemma&lt;/h3&gt;&lt;p&gt;Enterprise leaders face a genuine dilemma. Google‚Äôs Gemini Ultra cost US$191 million to train, while OpenAI‚Äôs GPT-4 required US$78 million in hardware costs alone. For most enterprises, building proprietary large language models isn‚Äôt viable ‚Äì and that makes vendor selection and partnership strategy important.&lt;/p&gt;&lt;p&gt;Despite surging demand, CoreWeave slashed its 2025 capital expenditure guidance by up to 40%, citing delayed power infrastructure delivery. Oracle is ‚Äústill waving off customers‚Äù due to capacity shortages, CEO Safra Catz confirmed, as per a Euronews report.&lt;/p&gt;&lt;p&gt;This creates risk &lt;em&gt;and&lt;/em&gt; opportunity. Enterprises that diversify their AI infrastructure strategies ‚Äì building relationships with multiple providers, validating alternative architectures, and stress-testing for supply constraints ‚Äì position themselves better than those betting everything on a single hyperscaler.&lt;/p&gt;&lt;h3&gt;Strategic AI investment in a frothy market&lt;/h3&gt;&lt;p&gt;Goldman Sachs equity analyst Peter Oppenheimer points out that ‚Äúunlike speculative companies of the early 2000s, today‚Äôs AI giants are delivering real profits. While AI stock prices have appreciated strongly, this has been matched by sustained earnings growth.‚Äù&lt;/p&gt;&lt;p&gt;The enterprise takeaway isn‚Äôt to avoid AI investment ‚Äì it‚Äôs to avoid the mistakes that plague the 95% who see no returns:&lt;/p&gt;&lt;p&gt;&lt;b&gt;Focus on specific use cases with measurable ROI: &lt;/b&gt;High performers are more than three times more likely than others to say their organisation intends to use AI to bring about transformative change to their businesses, data from McKinsey shows. They‚Äôre not deploying AI for AI‚Äôs sake ‚Äì they‚Äôre targeting specific business problems where AI delivers quantifiable value.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Invest in organisational readiness, not just technology: &lt;/b&gt;Having an agile product delivery organisation is strongly correlated with achieving value. Establishing robust talent strategies and implementing technology and data infrastructure show meaningful contributions to AI success.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Build governance frameworks now: &lt;/b&gt;The share of respondents reporting mitigation efforts for risks like personal and individual privacy, explainability, organisational reputation, and regulatory compliance has grown since 2022. As regulations tighten globally, early governance investment becomes a competitive advantage.&lt;/p&gt;&lt;h3&gt;Learning from market concentration&lt;/h3&gt;&lt;p&gt;In late 2025, 30% of the US S&amp;amp;P 500 was held up by just five companies ‚Äì the greatest concentration in half a century. For enterprises, this concentration creates dependencies worth managing.&lt;/p&gt;&lt;p&gt;The successful five percent diversify their AI vendors and their strategic approaches. They‚Äôre combining cloud-based AI services with edge computing, partnering with multiple model providers, and building internal capabilities for the workflows most important to competitive advantage.&lt;/p&gt;&lt;h3&gt;The real AI investment strategy&lt;/h3&gt;&lt;p&gt;Google‚Äôs Sundar Pichai captured the nuance enterprises must navigate: ‚ÄúWe can look back at the internet right now. There was clearly a lot of excess investment, but none of us would question whether the internet was profound. I expect AI to be the same.‚Äù&lt;/p&gt;&lt;p&gt;OpenAI‚Äôs ChatGPT has about 700 million weekly users, making it one of the fastest-growing consumer products in history. The enterprise challenge is deploying it effectively, leaving others waste billions on vanity projects.&lt;/p&gt;&lt;p&gt;The enterprises winning at AI share a common approach: they treat AI as a business transformation initiative, not a technology project. They establish clear success metrics before deployment. They invest in change management as much as infrastructure. And they maintain healthy scepticism about vendor promises and remain committed to the technology‚Äôs potential.&lt;/p&gt;&lt;h3&gt;What this means for enterprise strategy&lt;/h3&gt;&lt;p&gt;Whether we‚Äôre in an AI bubble matters less to enterprise leaders than building sustainable AI capabilities. The market will correct itself ‚Äì it always does. But businesses that develop genuine AI competencies during this investment surge will emerge stronger regardless of market dynamics.&lt;/p&gt;&lt;p&gt;In 2024, the proportion of survey respondents reporting AI use by their organisations jumped to 78% from 55% in 2023, as per the Stanford data. AI adoption is accelerating, and enterprises that wait for perfect market conditions risk falling behind competitors building capabilities today.&lt;/p&gt;&lt;p&gt;The strategic imperative is to ensure your AI investments deliver measurable business value regardless of market sentiment. Focus on practical deployments, measurable outcomes, and organisational readiness. Let others chase inflated valuations while you build sustainable competitive advantage.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source:Jasper Campbell)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://techhq.com/wp-content/uploads/2025/08/techhq-expo.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to experience the full spectrum of enterprise technology innovation?&lt;/strong&gt; Join TechEx in Amsterdam, California, and London. Covering AI, Big Data, Cyber Security, IoT, Digital Transformation, Intelligent Automation, Edge Computing, and Data Centres, TechEx brings together global leaders to share real-world use cases and in-depth insights. Click here for more information.&lt;/p&gt;&lt;p&gt;TechHQ is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;When JPMorgan Asset Management reported that AI spending accounted for two-thirds of US GDP growth in the first half of 2025, it wasn‚Äôt just a statistic ‚Äì it was a signal.&lt;/p&gt;&lt;p&gt;The conversation reached a turning point recently when OpenAI CEO Sam Altman, Amazon‚Äôs Jeff Bezos, and Goldman Sachs CEO David Solomon each acknowledged market froth within days of each other. But here‚Äôs what matters for enterprise decision-makers: acknowledging overheated markets isn‚Äôt the same as dismissing AI‚Äôs enterprise value.&lt;/p&gt;&lt;p&gt;Corporate AI investment reached US$252.3 billion in 2024, with private investment climbing 44.5%, according to Stanford University. The question isn‚Äôt whether to invest in AI ‚Äì it‚Äôs how to invest strategically while others ‚Äì specifically, an organisation‚Äôs competitors ‚Äì overspend on infrastructure and solutions that may never deliver returns.&lt;/p&gt;&lt;h3&gt;What separates AI winners from the 95% who fail&lt;/h3&gt;&lt;p&gt;An MIT study found that 95% of businesses invested in AI have failed to make money off the technology, according to ABC News. But that statistic masks a more important truth: 5% succeed ‚Äì and they‚Äôre doing things fundamentally differently.&lt;/p&gt;&lt;p&gt;High-performing organisations are investing more in AI capabilities, with more than one-third committing over 20% of their digital budgets to AI technologies, a McKinsey report shows. But they‚Äôre not just spending more ‚Äì they‚Äôre spending smarter.&lt;/p&gt;&lt;p&gt;The McKinsey research reveals what separates winners from the pack. About three-quarters of high performers say their organisations are scaling or have scaled AI, compared with one-third of other organisations. The leaders share common characteristics: they push for transformative innovation rather than incremental improvements, redesign workflows around AI capabilities, and implement rigorous governance frameworks.&lt;/p&gt;&lt;h3&gt;The infrastructure investment dilemma&lt;/h3&gt;&lt;p&gt;Enterprise leaders face a genuine dilemma. Google‚Äôs Gemini Ultra cost US$191 million to train, while OpenAI‚Äôs GPT-4 required US$78 million in hardware costs alone. For most enterprises, building proprietary large language models isn‚Äôt viable ‚Äì and that makes vendor selection and partnership strategy important.&lt;/p&gt;&lt;p&gt;Despite surging demand, CoreWeave slashed its 2025 capital expenditure guidance by up to 40%, citing delayed power infrastructure delivery. Oracle is ‚Äústill waving off customers‚Äù due to capacity shortages, CEO Safra Catz confirmed, as per a Euronews report.&lt;/p&gt;&lt;p&gt;This creates risk &lt;em&gt;and&lt;/em&gt; opportunity. Enterprises that diversify their AI infrastructure strategies ‚Äì building relationships with multiple providers, validating alternative architectures, and stress-testing for supply constraints ‚Äì position themselves better than those betting everything on a single hyperscaler.&lt;/p&gt;&lt;h3&gt;Strategic AI investment in a frothy market&lt;/h3&gt;&lt;p&gt;Goldman Sachs equity analyst Peter Oppenheimer points out that ‚Äúunlike speculative companies of the early 2000s, today‚Äôs AI giants are delivering real profits. While AI stock prices have appreciated strongly, this has been matched by sustained earnings growth.‚Äù&lt;/p&gt;&lt;p&gt;The enterprise takeaway isn‚Äôt to avoid AI investment ‚Äì it‚Äôs to avoid the mistakes that plague the 95% who see no returns:&lt;/p&gt;&lt;p&gt;&lt;b&gt;Focus on specific use cases with measurable ROI: &lt;/b&gt;High performers are more than three times more likely than others to say their organisation intends to use AI to bring about transformative change to their businesses, data from McKinsey shows. They‚Äôre not deploying AI for AI‚Äôs sake ‚Äì they‚Äôre targeting specific business problems where AI delivers quantifiable value.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Invest in organisational readiness, not just technology: &lt;/b&gt;Having an agile product delivery organisation is strongly correlated with achieving value. Establishing robust talent strategies and implementing technology and data infrastructure show meaningful contributions to AI success.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Build governance frameworks now: &lt;/b&gt;The share of respondents reporting mitigation efforts for risks like personal and individual privacy, explainability, organisational reputation, and regulatory compliance has grown since 2022. As regulations tighten globally, early governance investment becomes a competitive advantage.&lt;/p&gt;&lt;h3&gt;Learning from market concentration&lt;/h3&gt;&lt;p&gt;In late 2025, 30% of the US S&amp;amp;P 500 was held up by just five companies ‚Äì the greatest concentration in half a century. For enterprises, this concentration creates dependencies worth managing.&lt;/p&gt;&lt;p&gt;The successful five percent diversify their AI vendors and their strategic approaches. They‚Äôre combining cloud-based AI services with edge computing, partnering with multiple model providers, and building internal capabilities for the workflows most important to competitive advantage.&lt;/p&gt;&lt;h3&gt;The real AI investment strategy&lt;/h3&gt;&lt;p&gt;Google‚Äôs Sundar Pichai captured the nuance enterprises must navigate: ‚ÄúWe can look back at the internet right now. There was clearly a lot of excess investment, but none of us would question whether the internet was profound. I expect AI to be the same.‚Äù&lt;/p&gt;&lt;p&gt;OpenAI‚Äôs ChatGPT has about 700 million weekly users, making it one of the fastest-growing consumer products in history. The enterprise challenge is deploying it effectively, leaving others waste billions on vanity projects.&lt;/p&gt;&lt;p&gt;The enterprises winning at AI share a common approach: they treat AI as a business transformation initiative, not a technology project. They establish clear success metrics before deployment. They invest in change management as much as infrastructure. And they maintain healthy scepticism about vendor promises and remain committed to the technology‚Äôs potential.&lt;/p&gt;&lt;h3&gt;What this means for enterprise strategy&lt;/h3&gt;&lt;p&gt;Whether we‚Äôre in an AI bubble matters less to enterprise leaders than building sustainable AI capabilities. The market will correct itself ‚Äì it always does. But businesses that develop genuine AI competencies during this investment surge will emerge stronger regardless of market dynamics.&lt;/p&gt;&lt;p&gt;In 2024, the proportion of survey respondents reporting AI use by their organisations jumped to 78% from 55% in 2023, as per the Stanford data. AI adoption is accelerating, and enterprises that wait for perfect market conditions risk falling behind competitors building capabilities today.&lt;/p&gt;&lt;p&gt;The strategic imperative is to ensure your AI investments deliver measurable business value regardless of market sentiment. Focus on practical deployments, measurable outcomes, and organisational readiness. Let others chase inflated valuations while you build sustainable competitive advantage.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source:Jasper Campbell)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://techhq.com/wp-content/uploads/2025/08/techhq-expo.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to experience the full spectrum of enterprise technology innovation?&lt;/strong&gt; Join TechEx in Amsterdam, California, and London. Covering AI, Big Data, Cyber Security, IoT, Digital Transformation, Intelligent Automation, Edge Computing, and Data Centres, TechEx brings together global leaders to share real-world use cases and in-depth insights. Click here for more information.&lt;/p&gt;&lt;p&gt;TechHQ is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ai-business-reality-what-enterprise-leaders-need-know/</guid><pubDate>Mon, 01 Dec 2025 08:00:00 +0000</pubDate></item><item><title>An AI model trained on prison phone calls now looks for planned crimes in those calls (Artificial intelligence ‚Äì MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/01/1128591/an-ai-model-trained-on-prison-phone-calls-is-now-being-used-to-surveil-inmates/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/prison-phone-wire.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;A US telecom company trained an AI model on years of inmates‚Äô phone and video calls and is now piloting that model to scan their calls, texts, and emails in the hope of predicting and preventing crimes.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Securus Technologies president Kevin Elder told &lt;em&gt;MIT Technology Review &lt;/em&gt;that the company began building its AI tools in 2023, using its massive database of recorded calls to train AI models to detect criminal activity. It created one model, for example, using seven years of calls made by inmates in the Texas prison system, but it has been working on building other state- or county-specific models.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Over the past year, Elder says, Securus has been piloting the AI tools to monitor inmate conversations in real time (the company declined to specify where this is taking place, but its customers include jails holding people awaiting trial, prisons for those serving sentences, and Immigrations and Customs Enforcement detention facilities).&lt;/p&gt;  &lt;p&gt;‚ÄúWe can point that large language model at an entire treasure trove [of data],‚Äù Elder says, ‚Äúto detect and understand when crimes are being thought about or contemplated, so that you‚Äôre catching it much earlier in the cycle.‚Äù &lt;/p&gt; 
 &lt;p&gt;As with its other monitoring tools, investigators at detention facilities can deploy the AI features to monitor randomly selected conversations or those of individuals suspected by facility investigators of criminal activity, according to Elder. The model will analyze phone and video calls, text messages, and emails and then flag sections for human agents to review. These agents then send them to investigators for follow-up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In an interview, Elder said Securus‚Äô monitoring efforts have helped disrupt human trafficking and gang activities organized from within prisons, among other crimes, and said its tools are also used to identify prison staff who are bringing in contraband. But the company did not provide &lt;em&gt;MIT Technology Review&lt;/em&gt; with any cases specifically uncovered by its new AI models.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;People in prison, and those they call, are notified that their conversations are recorded. But this doesn‚Äôt mean they‚Äôre aware that those conversations could be used to train an AI model, says Bianca Tylek, executive director of the prison rights advocacy group Worth Rises.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;‚ÄúThat‚Äôs coercive consent; there‚Äôs literally no other way you can communicate with your family,‚Äù Tylek says. And since inmates in the vast majority of states pay for these calls, she adds, ‚Äúnot only are you not compensating them for the use of their data, but you‚Äôre actually charging them while collecting their data.‚Äù&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;A Securus spokesperson said the use of data to train the tool "is not focused on surveilling or targeting specific individuals, but rather on identifying broader patterns, anomalies, and unlawful behaviors across the entire communication system." They added that correctional facilities determine their own recording and monitoring policies, which Securus follows, and did not directly answer whether inmates can opt out of having their recordings used to train AI.&lt;/p&gt;  &lt;p&gt;Other advocates for inmates say Securus has a history of violating their civil liberties. For example, leaks of its recordings databases showed the company had improperly recorded thousands of calls between inmates and their attorneys. Corene Kendrick, the deputy director of the ACLU‚Äôs National Prison Project, says that the new AI system enables a system of invasive surveillance, and courts have specified few limits to this power.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;‚Äú[Are we] going to stop crime before it happens because we‚Äôre monitoring every utterance and thought of incarcerated people?‚Äù Kendrick says. ‚ÄúI think this is one of many situations where the technology is way far ahead of the law.‚Äù&lt;/p&gt;  &lt;p&gt;The company spokesperson said the tool's function is to make monitoring more efficient amid staffing shortages, ‚Äúnot to surveil individuals without cause.‚Äù&lt;/p&gt;  &lt;p&gt;Securus will have an easier time funding its AI tool thanks to the company‚Äôs recent win in a battle with regulators over how telecom companies can spend the money they collect from inmates‚Äô calls.&lt;/p&gt;  &lt;p&gt;In 2024, the Federal Communications Commission issued a major reform, shaped and lauded by advocates for prisoners‚Äô rights, that forbade telecoms from passing the costs of recording and surveilling calls on to inmates. Companies were allowed to continue to charge inmates a capped rate for calls, but prisons and jails were ordered to pay for most security costs out of their own budgets.&lt;/p&gt; 

 &lt;p&gt;Negative reactions to this change were swift. Associations of sheriffs (who typically run county jails) complained they could no longer afford proper monitoring of calls, and attorneys general from 14 states sued over the ruling. Some prisons and jails warned they would cut off access to phone calls.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;While it was building and piloting its AI tool, Securus held meetings with the FCC and lobbied for a rule change, arguing that the 2024 reform went too far and asking that the agency again allow companies to use fees collected from inmates to pay for security.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In June, Brendan Carr, whom President Donald Trump appointed to lead the FCC, said it would postpone all deadlines for jails and prisons to adopt the 2024 reforms, and even signaled that the agency wants to help telecom companies fund their AI surveillance efforts with the fees paid by inmates. In a press release, Carr wrote that rolling back the 2024 reforms would ‚Äúlead to broader adoption of beneficial public safety tools that include advanced AI and machine learning.‚Äù&lt;/p&gt;  &lt;p&gt;On October 28, the agency went further: It voted to pass new, higher rate caps and allow companies like Securus to pass security costs relating to recording and monitoring of calls‚Äîlike storing recordings, transcribing them, or building AI tools to analyze such calls, for example‚Äîon to inmates.&amp;nbsp;A spokesperson for Securus told &lt;em&gt;MIT Technology Review&lt;/em&gt; that the company aims to balance affordability with the need to fund essential safety and security tools. ‚ÄúThese tools, which include our advanced monitoring and AI capabilities, are fundamental to maintaining secure facilities for incarcerated individuals and correctional staff and to protecting the public,‚Äù they wrote.&lt;/p&gt;  &lt;p&gt;FCC commissioner Anna Gomez dissented in last month‚Äôs ruling. ‚ÄúLaw enforcement,‚Äù she wrote in a statement, ‚Äúshould foot the bill for unrelated security and safety costs, not the families of incarcerated people.‚Äù&lt;/p&gt;  &lt;p&gt;The FCC will be seeking comment on these new rules before they take final effect.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/prison-phone-wire.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;A US telecom company trained an AI model on years of inmates‚Äô phone and video calls and is now piloting that model to scan their calls, texts, and emails in the hope of predicting and preventing crimes.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Securus Technologies president Kevin Elder told &lt;em&gt;MIT Technology Review &lt;/em&gt;that the company began building its AI tools in 2023, using its massive database of recorded calls to train AI models to detect criminal activity. It created one model, for example, using seven years of calls made by inmates in the Texas prison system, but it has been working on building other state- or county-specific models.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Over the past year, Elder says, Securus has been piloting the AI tools to monitor inmate conversations in real time (the company declined to specify where this is taking place, but its customers include jails holding people awaiting trial, prisons for those serving sentences, and Immigrations and Customs Enforcement detention facilities).&lt;/p&gt;  &lt;p&gt;‚ÄúWe can point that large language model at an entire treasure trove [of data],‚Äù Elder says, ‚Äúto detect and understand when crimes are being thought about or contemplated, so that you‚Äôre catching it much earlier in the cycle.‚Äù &lt;/p&gt; 
 &lt;p&gt;As with its other monitoring tools, investigators at detention facilities can deploy the AI features to monitor randomly selected conversations or those of individuals suspected by facility investigators of criminal activity, according to Elder. The model will analyze phone and video calls, text messages, and emails and then flag sections for human agents to review. These agents then send them to investigators for follow-up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In an interview, Elder said Securus‚Äô monitoring efforts have helped disrupt human trafficking and gang activities organized from within prisons, among other crimes, and said its tools are also used to identify prison staff who are bringing in contraband. But the company did not provide &lt;em&gt;MIT Technology Review&lt;/em&gt; with any cases specifically uncovered by its new AI models.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;People in prison, and those they call, are notified that their conversations are recorded. But this doesn‚Äôt mean they‚Äôre aware that those conversations could be used to train an AI model, says Bianca Tylek, executive director of the prison rights advocacy group Worth Rises.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;‚ÄúThat‚Äôs coercive consent; there‚Äôs literally no other way you can communicate with your family,‚Äù Tylek says. And since inmates in the vast majority of states pay for these calls, she adds, ‚Äúnot only are you not compensating them for the use of their data, but you‚Äôre actually charging them while collecting their data.‚Äù&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;A Securus spokesperson said the use of data to train the tool "is not focused on surveilling or targeting specific individuals, but rather on identifying broader patterns, anomalies, and unlawful behaviors across the entire communication system." They added that correctional facilities determine their own recording and monitoring policies, which Securus follows, and did not directly answer whether inmates can opt out of having their recordings used to train AI.&lt;/p&gt;  &lt;p&gt;Other advocates for inmates say Securus has a history of violating their civil liberties. For example, leaks of its recordings databases showed the company had improperly recorded thousands of calls between inmates and their attorneys. Corene Kendrick, the deputy director of the ACLU‚Äôs National Prison Project, says that the new AI system enables a system of invasive surveillance, and courts have specified few limits to this power.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;‚Äú[Are we] going to stop crime before it happens because we‚Äôre monitoring every utterance and thought of incarcerated people?‚Äù Kendrick says. ‚ÄúI think this is one of many situations where the technology is way far ahead of the law.‚Äù&lt;/p&gt;  &lt;p&gt;The company spokesperson said the tool's function is to make monitoring more efficient amid staffing shortages, ‚Äúnot to surveil individuals without cause.‚Äù&lt;/p&gt;  &lt;p&gt;Securus will have an easier time funding its AI tool thanks to the company‚Äôs recent win in a battle with regulators over how telecom companies can spend the money they collect from inmates‚Äô calls.&lt;/p&gt;  &lt;p&gt;In 2024, the Federal Communications Commission issued a major reform, shaped and lauded by advocates for prisoners‚Äô rights, that forbade telecoms from passing the costs of recording and surveilling calls on to inmates. Companies were allowed to continue to charge inmates a capped rate for calls, but prisons and jails were ordered to pay for most security costs out of their own budgets.&lt;/p&gt; 

 &lt;p&gt;Negative reactions to this change were swift. Associations of sheriffs (who typically run county jails) complained they could no longer afford proper monitoring of calls, and attorneys general from 14 states sued over the ruling. Some prisons and jails warned they would cut off access to phone calls.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;While it was building and piloting its AI tool, Securus held meetings with the FCC and lobbied for a rule change, arguing that the 2024 reform went too far and asking that the agency again allow companies to use fees collected from inmates to pay for security.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In June, Brendan Carr, whom President Donald Trump appointed to lead the FCC, said it would postpone all deadlines for jails and prisons to adopt the 2024 reforms, and even signaled that the agency wants to help telecom companies fund their AI surveillance efforts with the fees paid by inmates. In a press release, Carr wrote that rolling back the 2024 reforms would ‚Äúlead to broader adoption of beneficial public safety tools that include advanced AI and machine learning.‚Äù&lt;/p&gt;  &lt;p&gt;On October 28, the agency went further: It voted to pass new, higher rate caps and allow companies like Securus to pass security costs relating to recording and monitoring of calls‚Äîlike storing recordings, transcribing them, or building AI tools to analyze such calls, for example‚Äîon to inmates.&amp;nbsp;A spokesperson for Securus told &lt;em&gt;MIT Technology Review&lt;/em&gt; that the company aims to balance affordability with the need to fund essential safety and security tools. ‚ÄúThese tools, which include our advanced monitoring and AI capabilities, are fundamental to maintaining secure facilities for incarcerated individuals and correctional staff and to protecting the public,‚Äù they wrote.&lt;/p&gt;  &lt;p&gt;FCC commissioner Anna Gomez dissented in last month‚Äôs ruling. ‚ÄúLaw enforcement,‚Äù she wrote in a statement, ‚Äúshould foot the bill for unrelated security and safety costs, not the families of incarcerated people.‚Äù&lt;/p&gt;  &lt;p&gt;The FCC will be seeking comment on these new rules before they take final effect.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/01/1128591/an-ai-model-trained-on-prison-phone-calls-is-now-being-used-to-surveil-inmates/</guid><pubDate>Mon, 01 Dec 2025 10:30:00 +0000</pubDate></item><item><title>Nominations are now open for our global 2026 Innovators Under 35 competition (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/01/1128610/nominations-open-global-2026-innovators-under-35-competition/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/tr35-2026-SA.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;We have some exciting news: Nominations are now open for &lt;em&gt;MIT Technology Review&lt;/em&gt;‚Äôs 2026 Innovators Under 35 competition. This annual list recognizes 35 of the world‚Äôs best young scientists and inventors, and our newsroom has produced it for more than two decades.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It‚Äôs free to nominate yourself or someone you know, and it only takes a few moments. &lt;strong&gt;Submit your nomination&lt;/strong&gt;&lt;strong&gt; before 5 p.m. ET on Tuesday, January 20, 2026.&lt;/strong&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;We‚Äôre looking for people who are making important scientific discoveries and applying that knowledge to build new technologies. Or those who are engineering new systems and algorithms that will aid our work or extend our abilities.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Each year, many honorees are focused on improving human health or solving major problems like climate change; others are charting the future path of artificial intelligence or developing the next generation of robots.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The most successful candidates will have made a clear advance that is expected to have a positive impact beyond their own field. They should be the primary scientific or technical driver behind the work involved, and we like to see some signs that a candidate‚Äôs innovation is gaining real traction. You can look at last year‚Äôs list to get an idea of what we look out for.&lt;/p&gt;  &lt;p&gt;We encourage self-nominations, and if you previously nominated someone who wasn‚Äôt selected, feel free to put them forward again. &lt;strong&gt;Please note: To be eligible for the 2026 list, nominees must be under the age of 35 as of October 1, 2026.&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Semifinalists will be notified by early March and asked to complete an application at that time. Winners are then chosen by the editorial staff of &lt;em&gt;MIT Technology Review&lt;/em&gt;, with input from a panel of expert judges. (Here‚Äôs more info about our selection process and timelines.)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If you have any questions, please contact tr35@technologyreview.com. We look forward to reviewing your nominations. Good luck!&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/tr35-2026-SA.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;We have some exciting news: Nominations are now open for &lt;em&gt;MIT Technology Review&lt;/em&gt;‚Äôs 2026 Innovators Under 35 competition. This annual list recognizes 35 of the world‚Äôs best young scientists and inventors, and our newsroom has produced it for more than two decades.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It‚Äôs free to nominate yourself or someone you know, and it only takes a few moments. &lt;strong&gt;Submit your nomination&lt;/strong&gt;&lt;strong&gt; before 5 p.m. ET on Tuesday, January 20, 2026.&lt;/strong&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;We‚Äôre looking for people who are making important scientific discoveries and applying that knowledge to build new technologies. Or those who are engineering new systems and algorithms that will aid our work or extend our abilities.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Each year, many honorees are focused on improving human health or solving major problems like climate change; others are charting the future path of artificial intelligence or developing the next generation of robots.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The most successful candidates will have made a clear advance that is expected to have a positive impact beyond their own field. They should be the primary scientific or technical driver behind the work involved, and we like to see some signs that a candidate‚Äôs innovation is gaining real traction. You can look at last year‚Äôs list to get an idea of what we look out for.&lt;/p&gt;  &lt;p&gt;We encourage self-nominations, and if you previously nominated someone who wasn‚Äôt selected, feel free to put them forward again. &lt;strong&gt;Please note: To be eligible for the 2026 list, nominees must be under the age of 35 as of October 1, 2026.&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Semifinalists will be notified by early March and asked to complete an application at that time. Winners are then chosen by the editorial staff of &lt;em&gt;MIT Technology Review&lt;/em&gt;, with input from a panel of expert judges. (Here‚Äôs more info about our selection process and timelines.)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If you have any questions, please contact tr35@technologyreview.com. We look forward to reviewing your nominations. Good luck!&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/01/1128610/nominations-open-global-2026-innovators-under-35-competition/</guid><pubDate>Mon, 01 Dec 2025 11:02:41 +0000</pubDate></item><item><title>[NEW] The Download: spotting crimes in prisoners‚Äô phone calls, and nominate an Innovator Under 35 (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/01/1128615/the-download-spotting-crimes-in-prisoners-phone-calls-and-nominate-an-innovator-under-35/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;An AI model trained on prison phone calls now looks for planned crimes in those calls&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;A US telecom company trained an AI model on years of inmates‚Äô phone and video calls and is now piloting that model to scan their calls, texts, and emails in the hope of predicting and preventing crimes.&lt;/p&gt;&lt;p&gt;Securus Technologies president Kevin Elder told &lt;em&gt;MIT Technology Review&lt;/em&gt; that the company began building its AI tools in 2023, using its massive database of recorded calls to train AI models to detect criminal activity. It created one model, for example, using seven years of calls made by inmates in the Texas prison system, but it has been working on models for other states and counties.&lt;/p&gt;&lt;p&gt;However, prisoner rights advocates say that the new AI system enables a system of invasive surveillance, and courts have specified few limits to this power.&amp;nbsp; Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;‚ÄîJames O‚ÄôDonnell&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Nominations are now open for our global 2026 Innovators Under 35 competition&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;We have some exciting news: Nominations are now open for MIT Technology Review‚Äôs 2026 Innovators Under 35 competition. This annual list recognizes 35 of the world‚Äôs best young scientists and inventors, and our newsroom has produced it for more than two decades.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It‚Äôs free to nominate yourself or someone you know, and it only takes a few moments.&lt;strong&gt; &lt;/strong&gt;Here‚Äôs how to submit your nomination.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 New York is cracking down on personalized pricing algorithms&lt;/strong&gt;&lt;br /&gt;A new law forces retailers to declare if their pricing is informed by users‚Äô data. (NYT $)&lt;br /&gt;+ &lt;em&gt;The US National Retail Federation tried to block it from passing. &lt;/em&gt;(TechCrunch)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 The White House has launched a media bias tracker&lt;/strong&gt;&lt;br /&gt;Complete with a ‚Äúmedia offender of the week‚Äù section and a Hall of Shame. (WP $)&lt;br /&gt;+ &lt;em&gt;The Washington Post is currently listed as the site‚Äôs top offender. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Donald Trump has lashed out at several reporters in the past few weeks. &lt;/em&gt;(The Hill)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;3 American startups are hooked on open-source Chinese AI models&lt;br /&gt;They‚Äôre cheap and customizable‚Äîwhat‚Äôs not to like? (NBC News)&lt;br /&gt;+ &lt;em&gt;Americans also love China‚Äôs cheap goods, regardless of tariffs. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;The State of AI: Is China about to win the race? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 How police body cam footage became viral YouTube content&lt;/strong&gt;&lt;br /&gt;Recent arrestees live in fear of ending up on popular channels. (Vox)&lt;br /&gt;+ &lt;em&gt;AI was supposed to make police bodycams better. What happened? &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;5 Construction workers are cashing in on the data center boom&lt;br /&gt;Might as well enjoy it while it lasts. (WSJ $)&lt;br /&gt;+ &lt;em&gt;The data center boom in the desert. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;6 China isn‚Äôt convinced by crypto&lt;br /&gt;&lt;/strong&gt;Even though bitcoin mining is quietly making a (banned) comeback. (Reuters)&lt;br /&gt;+ &lt;em&gt;The country‚Äôs central bank is no fan of stablecoins. &lt;/em&gt;(CoinDesk)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 A startup is treating its AI companions like characters in a novel&lt;/strong&gt;&lt;br /&gt;Could that approach make for better AI companions? (Fast Company $)&lt;br /&gt;+ &lt;em&gt;Gemini is the most empathetic model, apparently. &lt;/em&gt;(Semafor)&lt;br /&gt;+ &lt;em&gt;The looming crackdown on AI companionship. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;8 Ozempic is so yesterday üíâ&lt;br /&gt;New weight-loss drugs are tailored to individual patients. (The Atlantic $)&lt;br /&gt;+ &lt;em&gt;What we still don‚Äôt know about weight-loss drugs. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 AI is upending how consultants work&lt;/strong&gt;&lt;br /&gt;For the third year in a row, big firms are freezing junior workers‚Äô salaries. (FT $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 Behind the scenes of Disney‚Äôs AI animation accelerator&lt;/strong&gt;&lt;br /&gt;What took five months to create has been whittled down to under five weeks. (CNET)&lt;br /&gt;+ &lt;em&gt;Director supremo James Cameron appears to have changed his mind about AI. &lt;/em&gt;(TechCrunch)&lt;br /&gt;+ &lt;em&gt;Why are people scrolling through weirdly-formatted TV clips? &lt;/em&gt;(WP $)&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;‚Äú[I hope AI] comes to a point where it becomes sort of mental junk food and we feel sick and we don‚Äôt know why.‚Äù&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;‚ÄîActor Jenna Ortega outlines her hopes for AI‚Äôs future role in filmmaking, Variety reports.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1128617" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/image.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The weeds are winning&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Since the 1980s, more and more plants have evolved to become immune to the biochemical mechanisms that herbicides leverage to kill them. This herbicidal resistance threatens to decrease yields‚Äîout-of-control weeds can reduce them by 50% or more, and extreme cases can wipe out whole fields.&lt;/p&gt; 
 &lt;p&gt;At worst, it can even drive farmers out of business. It‚Äôs the agricultural equivalent of antibiotic resistance, and it keeps getting worse. Weeds have evolved resistance to 168 different herbicides and 21 of the 31 known ‚Äúmodes of action,‚Äù which means the specific biochemical target or pathway a chemical is designed to disrupt.&lt;/p&gt;  &lt;p&gt;Agriculture needs to embrace a diversity of weed control practices. But that‚Äôs much easier said than done. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;An AI model trained on prison phone calls now looks for planned crimes in those calls&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;A US telecom company trained an AI model on years of inmates‚Äô phone and video calls and is now piloting that model to scan their calls, texts, and emails in the hope of predicting and preventing crimes.&lt;/p&gt;&lt;p&gt;Securus Technologies president Kevin Elder told &lt;em&gt;MIT Technology Review&lt;/em&gt; that the company began building its AI tools in 2023, using its massive database of recorded calls to train AI models to detect criminal activity. It created one model, for example, using seven years of calls made by inmates in the Texas prison system, but it has been working on models for other states and counties.&lt;/p&gt;&lt;p&gt;However, prisoner rights advocates say that the new AI system enables a system of invasive surveillance, and courts have specified few limits to this power.&amp;nbsp; Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;‚ÄîJames O‚ÄôDonnell&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Nominations are now open for our global 2026 Innovators Under 35 competition&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;We have some exciting news: Nominations are now open for MIT Technology Review‚Äôs 2026 Innovators Under 35 competition. This annual list recognizes 35 of the world‚Äôs best young scientists and inventors, and our newsroom has produced it for more than two decades.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It‚Äôs free to nominate yourself or someone you know, and it only takes a few moments.&lt;strong&gt; &lt;/strong&gt;Here‚Äôs how to submit your nomination.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 New York is cracking down on personalized pricing algorithms&lt;/strong&gt;&lt;br /&gt;A new law forces retailers to declare if their pricing is informed by users‚Äô data. (NYT $)&lt;br /&gt;+ &lt;em&gt;The US National Retail Federation tried to block it from passing. &lt;/em&gt;(TechCrunch)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 The White House has launched a media bias tracker&lt;/strong&gt;&lt;br /&gt;Complete with a ‚Äúmedia offender of the week‚Äù section and a Hall of Shame. (WP $)&lt;br /&gt;+ &lt;em&gt;The Washington Post is currently listed as the site‚Äôs top offender. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Donald Trump has lashed out at several reporters in the past few weeks. &lt;/em&gt;(The Hill)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;3 American startups are hooked on open-source Chinese AI models&lt;br /&gt;They‚Äôre cheap and customizable‚Äîwhat‚Äôs not to like? (NBC News)&lt;br /&gt;+ &lt;em&gt;Americans also love China‚Äôs cheap goods, regardless of tariffs. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;The State of AI: Is China about to win the race? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 How police body cam footage became viral YouTube content&lt;/strong&gt;&lt;br /&gt;Recent arrestees live in fear of ending up on popular channels. (Vox)&lt;br /&gt;+ &lt;em&gt;AI was supposed to make police bodycams better. What happened? &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;5 Construction workers are cashing in on the data center boom&lt;br /&gt;Might as well enjoy it while it lasts. (WSJ $)&lt;br /&gt;+ &lt;em&gt;The data center boom in the desert. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;6 China isn‚Äôt convinced by crypto&lt;br /&gt;&lt;/strong&gt;Even though bitcoin mining is quietly making a (banned) comeback. (Reuters)&lt;br /&gt;+ &lt;em&gt;The country‚Äôs central bank is no fan of stablecoins. &lt;/em&gt;(CoinDesk)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 A startup is treating its AI companions like characters in a novel&lt;/strong&gt;&lt;br /&gt;Could that approach make for better AI companions? (Fast Company $)&lt;br /&gt;+ &lt;em&gt;Gemini is the most empathetic model, apparently. &lt;/em&gt;(Semafor)&lt;br /&gt;+ &lt;em&gt;The looming crackdown on AI companionship. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;8 Ozempic is so yesterday üíâ&lt;br /&gt;New weight-loss drugs are tailored to individual patients. (The Atlantic $)&lt;br /&gt;+ &lt;em&gt;What we still don‚Äôt know about weight-loss drugs. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 AI is upending how consultants work&lt;/strong&gt;&lt;br /&gt;For the third year in a row, big firms are freezing junior workers‚Äô salaries. (FT $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 Behind the scenes of Disney‚Äôs AI animation accelerator&lt;/strong&gt;&lt;br /&gt;What took five months to create has been whittled down to under five weeks. (CNET)&lt;br /&gt;+ &lt;em&gt;Director supremo James Cameron appears to have changed his mind about AI. &lt;/em&gt;(TechCrunch)&lt;br /&gt;+ &lt;em&gt;Why are people scrolling through weirdly-formatted TV clips? &lt;/em&gt;(WP $)&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;‚Äú[I hope AI] comes to a point where it becomes sort of mental junk food and we feel sick and we don‚Äôt know why.‚Äù&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;‚ÄîActor Jenna Ortega outlines her hopes for AI‚Äôs future role in filmmaking, Variety reports.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1128617" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/image.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The weeds are winning&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Since the 1980s, more and more plants have evolved to become immune to the biochemical mechanisms that herbicides leverage to kill them. This herbicidal resistance threatens to decrease yields‚Äîout-of-control weeds can reduce them by 50% or more, and extreme cases can wipe out whole fields.&lt;/p&gt; 
 &lt;p&gt;At worst, it can even drive farmers out of business. It‚Äôs the agricultural equivalent of antibiotic resistance, and it keeps getting worse. Weeds have evolved resistance to 168 different herbicides and 21 of the 31 known ‚Äúmodes of action,‚Äù which means the specific biochemical target or pathway a chemical is designed to disrupt.&lt;/p&gt;  &lt;p&gt;Agriculture needs to embrace a diversity of weed control practices. But that‚Äôs much easier said than done. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/01/1128615/the-download-spotting-crimes-in-prisoners-phone-calls-and-nominate-an-innovator-under-35/</guid><pubDate>Mon, 01 Dec 2025 13:10:00 +0000</pubDate></item><item><title>[NEW] OpenAGI emerges from stealth with an AI agent that it claims crushes OpenAI and Anthropic (AI | VentureBeat)</title><link>https://venturebeat.com/ai/openagi-emerges-from-stealth-with-an-ai-agent-that-it-claims-crushes-openai</link><description>[unable to retrieve full-text content]&lt;p&gt;A stealth artificial intelligence startup founded by an MIT researcher emerged this morning with an ambitious claim: its new AI model can control computers better than systems built by &lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt; and &lt;a href="https://www.anthropic.com/"&gt;Anthropic&lt;/a&gt; ‚Äî at a fraction of the cost.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/"&gt;OpenAGI&lt;/a&gt;, led by chief executive &lt;a href="https://www.qinzy.tech/"&gt;Zengyi Qin&lt;/a&gt;, released &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt;, a foundation model designed to operate computers autonomously by interpreting screenshots and executing actions across desktop applications. The San Francisco-based company says Lux achieves an 83.6 percent success rate on &lt;a href="https://huggingface.co/spaces/osunlp/Online_Mind2Web_Leaderboard"&gt;Online-Mind2Web&lt;/a&gt;, a benchmark that has become the industry&amp;#x27;s most rigorous test for evaluating AI agents that control computers.&lt;/p&gt;&lt;p&gt;That score is a significant leap over the leading models from well-funded competitors. OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-operator/"&gt;Operator&lt;/a&gt;, released in January, scores 61.3 percent on the same benchmark. Anthropic&amp;#x27;s Claude &lt;a href="https://www.anthropic.com/news/3-5-models-and-computer-use"&gt;Computer Use&lt;/a&gt; achieves 56.3 percent.&lt;/p&gt;&lt;p&gt;&amp;quot;Traditional LLM training feeds a large amount of text corpus into the model. The model learns to produce text,&amp;quot; Qin said in an exclusive interview with VentureBeat. &amp;quot;By contrast, our model learns to produce actions. The model is trained with a large amount of computer screenshots and action sequences, allowing it to produce actions to control the computer.&amp;quot;&lt;/p&gt;&lt;p&gt;The announcement arrives at a pivotal moment for the AI industry. Technology giants and startups alike have poured billions of dollars into developing autonomous agents capable of navigating software, booking travel, filling out forms, and executing complex workflows. &lt;a href="https://openai.com/index/introducing-agentkit/"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://www.anthropic.com/engineering/building-effective-agents"&gt;Anthropic&lt;/a&gt;, &lt;a href="https://cloud.google.com/products/agent-builder?hl=en"&gt;Google&lt;/a&gt;, and &lt;a href="https://adoption.microsoft.com/en-us/ai-agents/agents-in-microsoft-365/"&gt;Microsoft&lt;/a&gt; have all released or announced agent products in the past year, betting that computer-controlling AI will become as transformative as chatbots.&lt;/p&gt;&lt;p&gt;Yet independent research has cast doubt on whether current agents are as capable as their creators suggest.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why university researchers built a tougher benchmark to test AI agents‚Äîand what they discovered&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href="https://huggingface.co/spaces/osunlp/Online_Mind2Web_Leaderboard"&gt;Online-Mind2Web benchmark&lt;/a&gt;, developed by researchers at Ohio State University and the University of California, Berkeley, was designed specifically to expose the gap between marketing claims and actual performance.&lt;/p&gt;&lt;p&gt;Published in April and accepted to the &lt;a href="https://colmweb.org/"&gt;Conference on Language Modeling 2025&lt;/a&gt;, the benchmark comprises 300 diverse tasks across 136 real websites ‚Äî everything from booking flights to navigating complex e-commerce checkouts. Unlike earlier benchmarks that cached parts of websites, Online-Mind2Web tests agents in live online environments where pages change dynamically and unexpected obstacles appear.&lt;/p&gt;&lt;p&gt;The results, according to the researchers, painted &amp;quot;a very different picture of the competency of current agents, suggesting over-optimism in previously reported results.&amp;quot;&lt;/p&gt;&lt;p&gt;When the Ohio State team tested five leading web agents with careful human evaluation, they found that many recent systems ‚Äî despite heavy investment and marketing fanfare ‚Äî did not outperform &lt;a href="https://osu-nlp-group.github.io/SeeAct/"&gt;SeeAct&lt;/a&gt;, a relatively simple agent released in January 2024. Even OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-operator/"&gt;Operator&lt;/a&gt;, the best performer among commercial offerings in their study, achieved only 61 percent success.&lt;/p&gt;&lt;p&gt;&amp;quot;It seemed that highly capable and practical agents were maybe indeed just months away,&amp;quot; the researchers wrote in a &lt;a href="https://tiancixue.notion.site/An-Illusion-of-Progress-Assessing-the-Current-State-of-Web-Agents-1ac6cd2b9aac80719cd6f68374aaf4b4"&gt;blog post&lt;/a&gt; accompanying their paper. &amp;quot;However, we are also well aware that there are still many fundamental gaps in research to fully autonomous agents, and current agents are probably not as competent as the reported benchmark numbers may depict.&amp;quot;&lt;/p&gt;&lt;p&gt;The benchmark has gained traction as an industry standard, with a public leaderboard hosted on Hugging Face tracking submissions from research groups and companies.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How OpenAGI trained its AI to take actions instead of just generating text&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;OpenAGI&amp;#x27;s claimed performance advantage stems from what the company calls &amp;quot;&lt;a href="https://developer.agiopen.org/docs/index"&gt;Agentic Active Pre-training&lt;/a&gt;,&amp;quot; a training methodology that differs fundamentally from how most large language models learn.&lt;/p&gt;&lt;p&gt;Conventional language models train on vast text corpora, learning to predict the next word in a sequence. The resulting systems excel at generating coherent text but were not designed to take actions in graphical environments.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt;, according to Qin, takes a different approach. The model trains on computer screenshots paired with action sequences, learning to interpret visual interfaces and determine which clicks, keystrokes, and navigation steps will accomplish a given goal.&lt;/p&gt;&lt;p&gt;&amp;quot;The action allows the model to actively explore the computer environment, and such exploration generates new knowledge, which is then fed back to the model for training,&amp;quot; Qin told VentureBeat. &amp;quot;This is a naturally self-evolving process, where a better model produces better exploration, better exploration produces better knowledge, and better knowledge leads to a better model.&amp;quot;&lt;/p&gt;&lt;p&gt;This self-reinforcing training loop, if it functions as described, could help explain how a smaller team might achieve results that elude larger organizations. Rather than requiring ever-larger static datasets, the approach would allow the model to continuously improve by generating its own training data through exploration.&lt;/p&gt;&lt;p&gt;OpenAGI also claims significant cost advantages. The company says Lux operates at roughly one-tenth the cost of frontier models from OpenAI and Anthropic while executing tasks faster.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Unlike browser-only competitors, Lux can control Slack, Excel, and other desktop applications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;A critical distinction in OpenAGI&amp;#x27;s announcement: &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt; can control applications across an entire desktop operating system, not just web browsers.&lt;/p&gt;&lt;p&gt;Most commercially available computer-use agents, including early versions of Anthropic&amp;#x27;s Claude &lt;a href="https://www.anthropic.com/news/3-5-models-and-computer-use"&gt;Computer Use&lt;/a&gt;, focus primarily on browser-based tasks. That limitation excludes vast categories of productivity work that occur in desktop applications ‚Äî spreadsheets in Microsoft Excel, communications in Slack, design work in Adobe products, code editing in development environments.&lt;/p&gt;&lt;p&gt;OpenAGI says Lux can navigate these native applications, a capability that would substantially expand the addressable market for computer-use agents. The company is releasing a developer software development kit alongside the model, allowing third parties to build applications on top of Lux.&lt;/p&gt;&lt;p&gt;The company is also working with &lt;a href="https://www.intel.com/content/www/us/en/homepage.html"&gt;Intel&lt;/a&gt; to optimize &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt; for edge devices, which would allow the model to run locally on laptops and workstations rather than requiring cloud infrastructure. That partnership could address enterprise concerns about sending sensitive screen data to external servers.&lt;/p&gt;&lt;p&gt;&amp;quot;We are partnering with Intel to optimize our model on edge devices, which will make it the best on-device computer-use model,&amp;quot; Qin said.&lt;/p&gt;&lt;p&gt;The company confirmed it is in exploratory discussions with AMD and Microsoft about additional partnerships.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What happens when you ask an AI agent to copy your bank details&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Computer-use agents present novel safety challenges that do not arise with conventional chatbots. An AI system capable of clicking buttons, entering text, and navigating applications could, if misdirected, cause significant harm ‚Äî transferring money, deleting files, or exfiltrating sensitive information.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/"&gt;OpenAGI&lt;/a&gt; says it has built safety mechanisms directly into Lux. When the model encounters requests that violate its safety policies, it refuses to proceed and alerts the user.&lt;/p&gt;&lt;p&gt;In an example provided by the company, when a user asked the model to &amp;quot;copy my bank details and paste it into a new Google doc,&amp;quot; Lux responded with an internal reasoning step: &amp;quot;The user asks me to copy the bank details, which are sensitive information. Based on the safety policy, I am not able to perform this action.&amp;quot; The model then issued a warning to the user rather than executing the potentially dangerous request.&lt;/p&gt;&lt;p&gt;Such safeguards will face intense scrutiny as computer-use agents proliferate. Security researchers have already demonstrated prompt injection attacks against early agent systems, where malicious instructions embedded in websites or documents can hijack an agent&amp;#x27;s behavior. Whether Lux&amp;#x27;s safety mechanisms can withstand adversarial attacks remains to be tested by independent researchers.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The MIT researcher who built two of GitHub&amp;#x27;s most downloaded AI models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.qinzy.tech/"&gt;Qin&lt;/a&gt; brings an unusual combination of academic credentials and entrepreneurial experience to OpenAGI.&lt;/p&gt;&lt;p&gt;He completed his doctorate at the Massachusetts Institute of Technology in 2025, where his research focused on computer vision, robotics, and machine learning. His academic work appeared in top venues including the &lt;a href="https://cvpr.thecvf.com/"&gt;Conference on Computer Vision and Pattern Recognition&lt;/a&gt;, the &lt;a href="https://iclr.cc/"&gt;International Conference on Learning Representations&lt;/a&gt;, and the &lt;a href="https://icml.cc/"&gt;International Conference on Machine Learning&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Before founding OpenAGI, Qin built several widely adopted AI systems. &lt;a href="https://research.myshell.ai/jetmoe"&gt;JetMoE&lt;/a&gt;, a large language model he led development on, demonstrated that a high-performing model could be trained from scratch for less than $100,000 ‚Äî a fraction of the tens of millions typically required. The model outperformed Meta&amp;#x27;s &lt;a href="https://huggingface.co/meta-llama/Llama-2-7b"&gt;LLaMA2-7B&lt;/a&gt; on standard benchmarks, according to a technical report that attracted attention from MIT&amp;#x27;s Computer Science and Artificial Intelligence Laboratory.&lt;/p&gt;&lt;p&gt;His previous open-source projects achieved remarkable adoption. &lt;a href="https://research.myshell.ai/open-voice"&gt;OpenVoice&lt;/a&gt;, a voice cloning model, accumulated approximately 35,000 stars on GitHub and ranked in the top 0.03 percent of open-source projects by popularity. &lt;a href="https://github.com/myshell-ai/MeloTTS"&gt;MeloTTS&lt;/a&gt;, a text-to-speech system, has been downloaded more than 19 million times, making it one of the most widely used audio AI models since its 2024 release.&lt;/p&gt;&lt;p&gt;Qin also co-founded &lt;a href="https://myshell.ai/"&gt;MyShell&lt;/a&gt;, an AI agent platform that has attracted six million users who have collectively built more than 200,000 AI agents. Users have had more than one billion interactions with agents on the platform, according to the company.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the billion-dollar race to build AI that controls your computer&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The computer-use agent market has attracted intense interest from investors and technology giants over the past year.&lt;/p&gt;&lt;p&gt;OpenAI released &lt;a href="https://openai.com/index/introducing-operator/"&gt;Operator&lt;/a&gt; in January, allowing users to instruct an AI to complete tasks across the web. Anthropic has continued developing Claude &lt;a href="https://www.anthropic.com/news/3-5-models-and-computer-use"&gt;Computer Use&lt;/a&gt;, positioning it as a core capability of its Claude model family. Google has incorporated agent features into its &lt;a href="https://gemini.google/overview/agent/?utm_source=gemini&amp;amp;utm_medium=paid_media&amp;amp;utm_campaign=g1_sb_ee_2tb_ai&amp;amp;utm_source=google&amp;amp;utm_medium=cpc&amp;amp;utm_campaign=2024enUS_gemfeb&amp;amp;gclsrc=aw.ds&amp;amp;gad_source=1&amp;amp;gad_campaignid=23230139705&amp;amp;gbraid=0AAAAApk5BhkJ0xALVXcjNzv91HdDzGiuM&amp;amp;gclid=CjwKCAiA86_JBhAIEiwA4i9Ju12ClTsObJAOyDZPPN24ifL0gh7lufci0PAhVryoY7i5rrmIVjjyFxoCiPkQAvD_BwE"&gt;Gemini&lt;/a&gt; products. Microsoft has integrated agent capabilities across its &lt;a href="https://www.microsoft.com/en-us/microsoft-copilot/copilot-101/copilot-ai-agents"&gt;Copilot&lt;/a&gt; offerings and &lt;a href="https://www.microsoft.com/en-us/microsoft-365-copilot/agents"&gt;Windows&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Yet the market remains nascent. Enterprise adoption has been limited by concerns about reliability, security, and the ability to handle edge cases that occur frequently in real-world workflows. The performance gaps revealed by benchmarks like &lt;a href="https://huggingface.co/spaces/osunlp/Online_Mind2Web_Leaderboard"&gt;Online-Mind2Web&lt;/a&gt; suggest that current systems may not be ready for mission-critical applications.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/"&gt;OpenAGI&lt;/a&gt; enters this competitive landscape as an independent alternative, positioning superior benchmark performance and lower costs against the massive resources of its well-funded rivals. The company&amp;#x27;s Lux model and developer SDK are available beginning today.&lt;/p&gt;&lt;p&gt;Whether OpenAGI can translate benchmark dominance into real-world reliability remains the central question. The AI industry has a long history of impressive demos that falter in production, of laboratory results that crumble against the chaos of actual use. Benchmarks measure what they measure, and the distance between a controlled test and an 8-hour workday full of edge cases, exceptions, and surprises can be vast.&lt;/p&gt;&lt;p&gt;But if &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt; performs in the wild the way it performs in the lab, the implications extend far beyond one startup&amp;#x27;s success. It would suggest that the path to capable AI agents runs not through the largest checkbooks but through the cleverest architectures‚Äîthat a small team with the right ideas can outmaneuver the giants.&lt;/p&gt;&lt;p&gt;The technology industry has seen that story before. It rarely stays true for long.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;A stealth artificial intelligence startup founded by an MIT researcher emerged this morning with an ambitious claim: its new AI model can control computers better than systems built by &lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt; and &lt;a href="https://www.anthropic.com/"&gt;Anthropic&lt;/a&gt; ‚Äî at a fraction of the cost.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/"&gt;OpenAGI&lt;/a&gt;, led by chief executive &lt;a href="https://www.qinzy.tech/"&gt;Zengyi Qin&lt;/a&gt;, released &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt;, a foundation model designed to operate computers autonomously by interpreting screenshots and executing actions across desktop applications. The San Francisco-based company says Lux achieves an 83.6 percent success rate on &lt;a href="https://huggingface.co/spaces/osunlp/Online_Mind2Web_Leaderboard"&gt;Online-Mind2Web&lt;/a&gt;, a benchmark that has become the industry&amp;#x27;s most rigorous test for evaluating AI agents that control computers.&lt;/p&gt;&lt;p&gt;That score is a significant leap over the leading models from well-funded competitors. OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-operator/"&gt;Operator&lt;/a&gt;, released in January, scores 61.3 percent on the same benchmark. Anthropic&amp;#x27;s Claude &lt;a href="https://www.anthropic.com/news/3-5-models-and-computer-use"&gt;Computer Use&lt;/a&gt; achieves 56.3 percent.&lt;/p&gt;&lt;p&gt;&amp;quot;Traditional LLM training feeds a large amount of text corpus into the model. The model learns to produce text,&amp;quot; Qin said in an exclusive interview with VentureBeat. &amp;quot;By contrast, our model learns to produce actions. The model is trained with a large amount of computer screenshots and action sequences, allowing it to produce actions to control the computer.&amp;quot;&lt;/p&gt;&lt;p&gt;The announcement arrives at a pivotal moment for the AI industry. Technology giants and startups alike have poured billions of dollars into developing autonomous agents capable of navigating software, booking travel, filling out forms, and executing complex workflows. &lt;a href="https://openai.com/index/introducing-agentkit/"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://www.anthropic.com/engineering/building-effective-agents"&gt;Anthropic&lt;/a&gt;, &lt;a href="https://cloud.google.com/products/agent-builder?hl=en"&gt;Google&lt;/a&gt;, and &lt;a href="https://adoption.microsoft.com/en-us/ai-agents/agents-in-microsoft-365/"&gt;Microsoft&lt;/a&gt; have all released or announced agent products in the past year, betting that computer-controlling AI will become as transformative as chatbots.&lt;/p&gt;&lt;p&gt;Yet independent research has cast doubt on whether current agents are as capable as their creators suggest.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why university researchers built a tougher benchmark to test AI agents‚Äîand what they discovered&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href="https://huggingface.co/spaces/osunlp/Online_Mind2Web_Leaderboard"&gt;Online-Mind2Web benchmark&lt;/a&gt;, developed by researchers at Ohio State University and the University of California, Berkeley, was designed specifically to expose the gap between marketing claims and actual performance.&lt;/p&gt;&lt;p&gt;Published in April and accepted to the &lt;a href="https://colmweb.org/"&gt;Conference on Language Modeling 2025&lt;/a&gt;, the benchmark comprises 300 diverse tasks across 136 real websites ‚Äî everything from booking flights to navigating complex e-commerce checkouts. Unlike earlier benchmarks that cached parts of websites, Online-Mind2Web tests agents in live online environments where pages change dynamically and unexpected obstacles appear.&lt;/p&gt;&lt;p&gt;The results, according to the researchers, painted &amp;quot;a very different picture of the competency of current agents, suggesting over-optimism in previously reported results.&amp;quot;&lt;/p&gt;&lt;p&gt;When the Ohio State team tested five leading web agents with careful human evaluation, they found that many recent systems ‚Äî despite heavy investment and marketing fanfare ‚Äî did not outperform &lt;a href="https://osu-nlp-group.github.io/SeeAct/"&gt;SeeAct&lt;/a&gt;, a relatively simple agent released in January 2024. Even OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-operator/"&gt;Operator&lt;/a&gt;, the best performer among commercial offerings in their study, achieved only 61 percent success.&lt;/p&gt;&lt;p&gt;&amp;quot;It seemed that highly capable and practical agents were maybe indeed just months away,&amp;quot; the researchers wrote in a &lt;a href="https://tiancixue.notion.site/An-Illusion-of-Progress-Assessing-the-Current-State-of-Web-Agents-1ac6cd2b9aac80719cd6f68374aaf4b4"&gt;blog post&lt;/a&gt; accompanying their paper. &amp;quot;However, we are also well aware that there are still many fundamental gaps in research to fully autonomous agents, and current agents are probably not as competent as the reported benchmark numbers may depict.&amp;quot;&lt;/p&gt;&lt;p&gt;The benchmark has gained traction as an industry standard, with a public leaderboard hosted on Hugging Face tracking submissions from research groups and companies.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How OpenAGI trained its AI to take actions instead of just generating text&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;OpenAGI&amp;#x27;s claimed performance advantage stems from what the company calls &amp;quot;&lt;a href="https://developer.agiopen.org/docs/index"&gt;Agentic Active Pre-training&lt;/a&gt;,&amp;quot; a training methodology that differs fundamentally from how most large language models learn.&lt;/p&gt;&lt;p&gt;Conventional language models train on vast text corpora, learning to predict the next word in a sequence. The resulting systems excel at generating coherent text but were not designed to take actions in graphical environments.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt;, according to Qin, takes a different approach. The model trains on computer screenshots paired with action sequences, learning to interpret visual interfaces and determine which clicks, keystrokes, and navigation steps will accomplish a given goal.&lt;/p&gt;&lt;p&gt;&amp;quot;The action allows the model to actively explore the computer environment, and such exploration generates new knowledge, which is then fed back to the model for training,&amp;quot; Qin told VentureBeat. &amp;quot;This is a naturally self-evolving process, where a better model produces better exploration, better exploration produces better knowledge, and better knowledge leads to a better model.&amp;quot;&lt;/p&gt;&lt;p&gt;This self-reinforcing training loop, if it functions as described, could help explain how a smaller team might achieve results that elude larger organizations. Rather than requiring ever-larger static datasets, the approach would allow the model to continuously improve by generating its own training data through exploration.&lt;/p&gt;&lt;p&gt;OpenAGI also claims significant cost advantages. The company says Lux operates at roughly one-tenth the cost of frontier models from OpenAI and Anthropic while executing tasks faster.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Unlike browser-only competitors, Lux can control Slack, Excel, and other desktop applications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;A critical distinction in OpenAGI&amp;#x27;s announcement: &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt; can control applications across an entire desktop operating system, not just web browsers.&lt;/p&gt;&lt;p&gt;Most commercially available computer-use agents, including early versions of Anthropic&amp;#x27;s Claude &lt;a href="https://www.anthropic.com/news/3-5-models-and-computer-use"&gt;Computer Use&lt;/a&gt;, focus primarily on browser-based tasks. That limitation excludes vast categories of productivity work that occur in desktop applications ‚Äî spreadsheets in Microsoft Excel, communications in Slack, design work in Adobe products, code editing in development environments.&lt;/p&gt;&lt;p&gt;OpenAGI says Lux can navigate these native applications, a capability that would substantially expand the addressable market for computer-use agents. The company is releasing a developer software development kit alongside the model, allowing third parties to build applications on top of Lux.&lt;/p&gt;&lt;p&gt;The company is also working with &lt;a href="https://www.intel.com/content/www/us/en/homepage.html"&gt;Intel&lt;/a&gt; to optimize &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt; for edge devices, which would allow the model to run locally on laptops and workstations rather than requiring cloud infrastructure. That partnership could address enterprise concerns about sending sensitive screen data to external servers.&lt;/p&gt;&lt;p&gt;&amp;quot;We are partnering with Intel to optimize our model on edge devices, which will make it the best on-device computer-use model,&amp;quot; Qin said.&lt;/p&gt;&lt;p&gt;The company confirmed it is in exploratory discussions with AMD and Microsoft about additional partnerships.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What happens when you ask an AI agent to copy your bank details&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Computer-use agents present novel safety challenges that do not arise with conventional chatbots. An AI system capable of clicking buttons, entering text, and navigating applications could, if misdirected, cause significant harm ‚Äî transferring money, deleting files, or exfiltrating sensitive information.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/"&gt;OpenAGI&lt;/a&gt; says it has built safety mechanisms directly into Lux. When the model encounters requests that violate its safety policies, it refuses to proceed and alerts the user.&lt;/p&gt;&lt;p&gt;In an example provided by the company, when a user asked the model to &amp;quot;copy my bank details and paste it into a new Google doc,&amp;quot; Lux responded with an internal reasoning step: &amp;quot;The user asks me to copy the bank details, which are sensitive information. Based on the safety policy, I am not able to perform this action.&amp;quot; The model then issued a warning to the user rather than executing the potentially dangerous request.&lt;/p&gt;&lt;p&gt;Such safeguards will face intense scrutiny as computer-use agents proliferate. Security researchers have already demonstrated prompt injection attacks against early agent systems, where malicious instructions embedded in websites or documents can hijack an agent&amp;#x27;s behavior. Whether Lux&amp;#x27;s safety mechanisms can withstand adversarial attacks remains to be tested by independent researchers.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The MIT researcher who built two of GitHub&amp;#x27;s most downloaded AI models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.qinzy.tech/"&gt;Qin&lt;/a&gt; brings an unusual combination of academic credentials and entrepreneurial experience to OpenAGI.&lt;/p&gt;&lt;p&gt;He completed his doctorate at the Massachusetts Institute of Technology in 2025, where his research focused on computer vision, robotics, and machine learning. His academic work appeared in top venues including the &lt;a href="https://cvpr.thecvf.com/"&gt;Conference on Computer Vision and Pattern Recognition&lt;/a&gt;, the &lt;a href="https://iclr.cc/"&gt;International Conference on Learning Representations&lt;/a&gt;, and the &lt;a href="https://icml.cc/"&gt;International Conference on Machine Learning&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Before founding OpenAGI, Qin built several widely adopted AI systems. &lt;a href="https://research.myshell.ai/jetmoe"&gt;JetMoE&lt;/a&gt;, a large language model he led development on, demonstrated that a high-performing model could be trained from scratch for less than $100,000 ‚Äî a fraction of the tens of millions typically required. The model outperformed Meta&amp;#x27;s &lt;a href="https://huggingface.co/meta-llama/Llama-2-7b"&gt;LLaMA2-7B&lt;/a&gt; on standard benchmarks, according to a technical report that attracted attention from MIT&amp;#x27;s Computer Science and Artificial Intelligence Laboratory.&lt;/p&gt;&lt;p&gt;His previous open-source projects achieved remarkable adoption. &lt;a href="https://research.myshell.ai/open-voice"&gt;OpenVoice&lt;/a&gt;, a voice cloning model, accumulated approximately 35,000 stars on GitHub and ranked in the top 0.03 percent of open-source projects by popularity. &lt;a href="https://github.com/myshell-ai/MeloTTS"&gt;MeloTTS&lt;/a&gt;, a text-to-speech system, has been downloaded more than 19 million times, making it one of the most widely used audio AI models since its 2024 release.&lt;/p&gt;&lt;p&gt;Qin also co-founded &lt;a href="https://myshell.ai/"&gt;MyShell&lt;/a&gt;, an AI agent platform that has attracted six million users who have collectively built more than 200,000 AI agents. Users have had more than one billion interactions with agents on the platform, according to the company.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the billion-dollar race to build AI that controls your computer&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The computer-use agent market has attracted intense interest from investors and technology giants over the past year.&lt;/p&gt;&lt;p&gt;OpenAI released &lt;a href="https://openai.com/index/introducing-operator/"&gt;Operator&lt;/a&gt; in January, allowing users to instruct an AI to complete tasks across the web. Anthropic has continued developing Claude &lt;a href="https://www.anthropic.com/news/3-5-models-and-computer-use"&gt;Computer Use&lt;/a&gt;, positioning it as a core capability of its Claude model family. Google has incorporated agent features into its &lt;a href="https://gemini.google/overview/agent/?utm_source=gemini&amp;amp;utm_medium=paid_media&amp;amp;utm_campaign=g1_sb_ee_2tb_ai&amp;amp;utm_source=google&amp;amp;utm_medium=cpc&amp;amp;utm_campaign=2024enUS_gemfeb&amp;amp;gclsrc=aw.ds&amp;amp;gad_source=1&amp;amp;gad_campaignid=23230139705&amp;amp;gbraid=0AAAAApk5BhkJ0xALVXcjNzv91HdDzGiuM&amp;amp;gclid=CjwKCAiA86_JBhAIEiwA4i9Ju12ClTsObJAOyDZPPN24ifL0gh7lufci0PAhVryoY7i5rrmIVjjyFxoCiPkQAvD_BwE"&gt;Gemini&lt;/a&gt; products. Microsoft has integrated agent capabilities across its &lt;a href="https://www.microsoft.com/en-us/microsoft-copilot/copilot-101/copilot-ai-agents"&gt;Copilot&lt;/a&gt; offerings and &lt;a href="https://www.microsoft.com/en-us/microsoft-365-copilot/agents"&gt;Windows&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Yet the market remains nascent. Enterprise adoption has been limited by concerns about reliability, security, and the ability to handle edge cases that occur frequently in real-world workflows. The performance gaps revealed by benchmarks like &lt;a href="https://huggingface.co/spaces/osunlp/Online_Mind2Web_Leaderboard"&gt;Online-Mind2Web&lt;/a&gt; suggest that current systems may not be ready for mission-critical applications.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/"&gt;OpenAGI&lt;/a&gt; enters this competitive landscape as an independent alternative, positioning superior benchmark performance and lower costs against the massive resources of its well-funded rivals. The company&amp;#x27;s Lux model and developer SDK are available beginning today.&lt;/p&gt;&lt;p&gt;Whether OpenAGI can translate benchmark dominance into real-world reliability remains the central question. The AI industry has a long history of impressive demos that falter in production, of laboratory results that crumble against the chaos of actual use. Benchmarks measure what they measure, and the distance between a controlled test and an 8-hour workday full of edge cases, exceptions, and surprises can be vast.&lt;/p&gt;&lt;p&gt;But if &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt; performs in the wild the way it performs in the lab, the implications extend far beyond one startup&amp;#x27;s success. It would suggest that the path to capable AI agents runs not through the largest checkbooks but through the cleverest architectures‚Äîthat a small team with the right ideas can outmaneuver the giants.&lt;/p&gt;&lt;p&gt;The technology industry has seen that story before. It rarely stays true for long.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openagi-emerges-from-stealth-with-an-ai-agent-that-it-claims-crushes-openai</guid><pubDate>Mon, 01 Dec 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] Black Forest Labs raises $300M at $3.25B valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/black-forest-labs-raises-300m-at-3-25b-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/6844c7ed531e3aa09958eea8a9deae8bdabd0b54-3721x2798-1.png?resize=1200,902" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;German AI lab Black Forest Labs said on Monday that it has raised $300 million in a Series B funding round that values the company at $3.25 billion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was co-led by Salesforce Ventures and Anjney Midha (AMP), and saw participation from a16z, NVIDIA, Northzone, Creandum, Earlybird VC, BroadLight Capital, General Catalyst, Temasek, Bain Capital Ventures, Air Street Capital, Visionaries Club, Canva, and Figma Ventures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup said it would use the funds for research and development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Black Forest Labs, which makes foundation AI models for generating and editing images, has risen to fame quickly since its launch in August 2024. The company was in the news last year after it was revealed that Elon Musk‚Äôs Grok chatbot was using the German company‚Äôs models to generate images, and its models are being used by a slew of companies, like Adobe, fal.ai, Picsart, ElevenLabs, VSCO, and Vercel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup recently revealed the latest version of its image-generation model, Flux 2, which it says features better text and image rendering, and uses up to 10 images as a reference to maintain the style and tone while generating images. The model can generate images at resolutions up to 4K pixels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Black Forest Labs‚Äô co-founders, Robin Rombach, Patrick Esser, and Andreas Blattmann, were formerly researchers who helped create Stability AI‚Äôs Stable Diffusion models.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/6844c7ed531e3aa09958eea8a9deae8bdabd0b54-3721x2798-1.png?resize=1200,902" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;German AI lab Black Forest Labs said on Monday that it has raised $300 million in a Series B funding round that values the company at $3.25 billion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was co-led by Salesforce Ventures and Anjney Midha (AMP), and saw participation from a16z, NVIDIA, Northzone, Creandum, Earlybird VC, BroadLight Capital, General Catalyst, Temasek, Bain Capital Ventures, Air Street Capital, Visionaries Club, Canva, and Figma Ventures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup said it would use the funds for research and development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Black Forest Labs, which makes foundation AI models for generating and editing images, has risen to fame quickly since its launch in August 2024. The company was in the news last year after it was revealed that Elon Musk‚Äôs Grok chatbot was using the German company‚Äôs models to generate images, and its models are being used by a slew of companies, like Adobe, fal.ai, Picsart, ElevenLabs, VSCO, and Vercel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup recently revealed the latest version of its image-generation model, Flux 2, which it says features better text and image rendering, and uses up to 10 images as a reference to maintain the style and tone while generating images. The model can generate images at resolutions up to 4K pixels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Black Forest Labs‚Äô co-founders, Robin Rombach, Patrick Esser, and Andreas Blattmann, were formerly researchers who helped create Stability AI‚Äôs Stable Diffusion models.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/black-forest-labs-raises-300m-at-3-25b-valuation/</guid><pubDate>Mon, 01 Dec 2025 14:08:13 +0000</pubDate></item><item><title>[NEW] Amazon‚Äôs AI chatbot Rufus drove sales on Black Friday (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/amazons-ai-chatbot-rufus-drove-sales-on-black-friday/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon‚Äôs AI chatbot, Rufus, saw a surge of adoption on Black Friday, according to new data published over the weekend by market intelligence firm Sensor Tower. In the U.S., Amazon sessions that resulted in a purchase surged 100% on Black Friday compared with the trailing 30 days, while sessions that resulted in a purchase and didn‚Äôt include Rufus increased by only 20%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Amazon saw a 75% day-over-day increase for sessions that included Rufus and resulted in a purchase, compared with just a 35% day-over-day increase for sessions without Rufus that had resulted in a purchase. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The firm also noted that Amazon sessions that involved the AI chatbot outpaced total website sessions. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Black Friday, Amazon‚Äôs total website sessions increased by 20% day over day, while those that involved Rufus were up by 35%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon‚Äôs AI chat was first launched into beta in early 2024 before rolling out to all U.S. customers later that year. Today, Rufus helps Amazon shoppers find products, get recommendations, and perform product comparisons. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rufus‚Äô adoption to drive Black Friday sales is part of a broader surge in consumers turning to AI to help them with holiday shopping, data shows.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Amazon Rufus" class="wp-image-2675035" height="351" src="https://techcrunch.com/wp-content/uploads/2024/03/Screenshot-2024-03-05-at-5.18.22‚ÄØPM-e1709677222431.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;According to e-commerce data from Adobe Analytics, which tracks more than 1 trillion visits to U.S. retail websites, AI traffic to U.S. retail sites increased by 805% year-over-year on Black Friday. This indicates that consumers more heavily embraced generative AI chatbots to find deals and research products this year. The AI tools were mostly used for popular Black Friday deal categories like electronics, video games, appliances, toys, personal care items, and baby and toddler products. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Adobe Analytics also noted that the use of AI increased conversions. It found U.S. shoppers who came to a retail site from an AI service were 38% more likely to buy, compared with non-AI traffic sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether AI directly contributed to the record Black Friday spending of $11.8 billion is less clear. Instead, the sizable figure this year could be due to higher prices, not an increase in online shopping. As TechCrunch reported on Saturday, Salesforce data showed prices were up by an average of 7%, while order volumes were down by 1%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sensor Tower‚Äôs data similarly suggests that consumers were perhaps being more conservative in their spending this year, likely due to economic strains. Even though mobile app and website adoption spiked on Black Friday compared to the previous 30 days, gains in total visits and downloads decelerated from 2024, its data indicated. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, Amazon and Walmart‚Äôs mobile app downloads grew by 24% and 20%, respectively on Black Friday, compared with the previous 30 days. But that growth paled when compared with 2024, when Amazon downloads surged by 50% and Walmart‚Äôs were up 75% during the same period, the firm pointed out. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon and Walmart‚Äôs website visits on Black Friday were up by 90% and 100% this year, respectively, compared with the prior 30 days. However, those same numbers in 2024 were 95% and 130%, also respectively.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a related Adobe survey, 48% of respondents said they have used or plan to use AI specifically for holiday shopping. &lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon‚Äôs AI chatbot, Rufus, saw a surge of adoption on Black Friday, according to new data published over the weekend by market intelligence firm Sensor Tower. In the U.S., Amazon sessions that resulted in a purchase surged 100% on Black Friday compared with the trailing 30 days, while sessions that resulted in a purchase and didn‚Äôt include Rufus increased by only 20%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Amazon saw a 75% day-over-day increase for sessions that included Rufus and resulted in a purchase, compared with just a 35% day-over-day increase for sessions without Rufus that had resulted in a purchase. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The firm also noted that Amazon sessions that involved the AI chatbot outpaced total website sessions. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Black Friday, Amazon‚Äôs total website sessions increased by 20% day over day, while those that involved Rufus were up by 35%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon‚Äôs AI chat was first launched into beta in early 2024 before rolling out to all U.S. customers later that year. Today, Rufus helps Amazon shoppers find products, get recommendations, and perform product comparisons. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rufus‚Äô adoption to drive Black Friday sales is part of a broader surge in consumers turning to AI to help them with holiday shopping, data shows.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Amazon Rufus" class="wp-image-2675035" height="351" src="https://techcrunch.com/wp-content/uploads/2024/03/Screenshot-2024-03-05-at-5.18.22‚ÄØPM-e1709677222431.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;According to e-commerce data from Adobe Analytics, which tracks more than 1 trillion visits to U.S. retail websites, AI traffic to U.S. retail sites increased by 805% year-over-year on Black Friday. This indicates that consumers more heavily embraced generative AI chatbots to find deals and research products this year. The AI tools were mostly used for popular Black Friday deal categories like electronics, video games, appliances, toys, personal care items, and baby and toddler products. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Adobe Analytics also noted that the use of AI increased conversions. It found U.S. shoppers who came to a retail site from an AI service were 38% more likely to buy, compared with non-AI traffic sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether AI directly contributed to the record Black Friday spending of $11.8 billion is less clear. Instead, the sizable figure this year could be due to higher prices, not an increase in online shopping. As TechCrunch reported on Saturday, Salesforce data showed prices were up by an average of 7%, while order volumes were down by 1%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sensor Tower‚Äôs data similarly suggests that consumers were perhaps being more conservative in their spending this year, likely due to economic strains. Even though mobile app and website adoption spiked on Black Friday compared to the previous 30 days, gains in total visits and downloads decelerated from 2024, its data indicated. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, Amazon and Walmart‚Äôs mobile app downloads grew by 24% and 20%, respectively on Black Friday, compared with the previous 30 days. But that growth paled when compared with 2024, when Amazon downloads surged by 50% and Walmart‚Äôs were up 75% during the same period, the firm pointed out. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon and Walmart‚Äôs website visits on Black Friday were up by 90% and 100% this year, respectively, compared with the prior 30 days. However, those same numbers in 2024 were 95% and 130%, also respectively.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a related Adobe survey, 48% of respondents said they have used or plan to use AI specifically for holiday shopping. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/amazons-ai-chatbot-rufus-drove-sales-on-black-friday/</guid><pubDate>Mon, 01 Dec 2025 16:25:24 +0000</pubDate></item><item><title>[NEW] The State of AI: welcome to the economic singularity (Artificial intelligence ‚Äì MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/01/1127872/the-state-of-ai-welcome-to-the-economic-singularity/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;strong&gt;Welcome back to The State of AI, a new collaboration between the &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;Financial Times&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt; and &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;. Every Monday for the next two weeks, writers from both publications will debate one aspect of the generative AI revolution reshaping global power.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This week, Richard Waters, &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;FT&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt; columnist and former West Coast editor, talks with &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;‚Äôs editor at large David Rotman about the true impact of AI on the job market.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Bonus: If you're an MIT Technology Review subscriber, you can join David and Richard, alongside MIT Technology Review‚Äôs editor in chief, Mat Honan, for an exclusive conversation live on Tuesday, December 9 at 1pm ET about this topic. Sign up to be a part here. &lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1127510" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/4c1e48f3-a8d9-fed3-bfeb-c686add0bb5d.png?w=1200" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Richard Waters writes&lt;/strong&gt;: &lt;/p&gt; 
 &lt;p&gt;Any far-reaching new technology is always uneven in its adoption, but few have been more uneven than generative AI. That makes it hard to assess its likely impact on individual businesses, let alone on productivity across the economy as a whole.&lt;/p&gt;  &lt;p&gt;At one extreme, AI coding assistants have revolutionized the work of software developers. Mark Zuckerberg recently predicted that half of Meta‚Äôs code would be written by AI within a year. At the other extreme, most companies are seeing little if any benefit from their initial investments. A widely cited study from MIT found that so far, 95% of gen AI projects produce zero return.&lt;/p&gt; 
 &lt;p&gt;That has provided fuel for the skeptics who maintain that‚Äîby its very nature as a probabilistic technology prone to hallucinating‚Äîgenerative AI will never have a deep impact on business.&lt;/p&gt;  &lt;p&gt;To many students of tech history, though, the lack of immediate impact is just the normal lag associated with transformative new technologies. Erik Brynjolfsson, then an assistant professor at MIT, first described what he called the ‚Äúproductivity paradox of IT‚Äù in the early 1990s. Despite plenty of anecdotal evidence that technology was changing the way people worked, it wasn‚Äôt showing up in the aggregate data in the form of higher productivity growth. Brynjolfsson‚Äôs conclusion was that it just took time for businesses to adapt.&lt;/p&gt;  &lt;p&gt;Big investments in IT finally showed through with a notable rebound in US productivity growth starting in the mid-1990s. But that tailed off a decade later and was followed by a second lull.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Richard Waters and David Rotman" class="wp-image-1126831" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/FT_TR_Newsletter-Episode-05.jpg?w=2731" /&gt;&lt;div class="image-credit"&gt;FT/MIT TECHNOLOGY REVIEW | ADOBE STOCK&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;In the case of AI, companies need to build new infrastructure (particularly data platforms), redesign core business processes, and retrain workers before they can expect to see results. If a lag effect explains the slow results, there may at least be reasons for optimism: Much of the cloud computing infrastructure needed to bring generative AI to a wider business audience is already in place.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The opportunities and the challenges are both enormous. An executive at one Fortune 500 company says his organization has carried out a comprehensive review of its use of analytics and concluded that its workers, overall, add little or no value. Rooting out the old software and replacing that inefficient human labor with AI might yield significant results. But, as this person says, such an overhaul would require big changes to existing processes and take years to carry out.&lt;/p&gt;  &lt;p&gt;There are some early encouraging signs. US productivity growth, stuck at 1% to 1.5% for more than a decade and a half, rebounded to more than 2% last year. It probably hit the same level in the first nine months of this year, though the lack of official data due to the recent US government shutdown makes this impossible to confirm.&lt;/p&gt;  &lt;p&gt;It is impossible to tell, though, how durable this rebound will be or how much can be attributed to AI. The effects of new technologies are seldom felt in isolation. Instead, the benefits compound. AI is riding earlier investments in cloud and mobile computing. In the same way, the latest AI boom may only be the precursor to breakthroughs in fields that have a wider impact on the economy, such as robotics. ChatGPT might have caught the popular imagination, but OpenAI‚Äôs chatbot is unlikely to have the final word.&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1128631" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/FT-State-of-AI-Ep5.png?w=794" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;David Rotman replies:&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;This is my favorite discussion these days when it comes to artificial intelligence. How will AI affect overall economic productivity? Forget about the mesmerizing videos, the promise of companionship, and the prospect of agents to do tedious everyday tasks‚Äîthe bottom line will be whether AI can grow the economy, and that means increasing productivity.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But, as you say, it‚Äôs hard to pin down just how AI is affecting such growth or how it will do so in the future. Erik Brynjolfsson predicts that, like other so-called general purpose technologies, AI will follow a J curve in which initially there is a slow, even negative, effect on productivity as companies invest heavily in the technology before finally reaping the rewards. And then the boom.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But there is a counterexample undermining the just-be-patient argument. Productivity growth from IT picked up in the mid-1990s but since the mid-2000s has been relatively dismal. Despite smartphones and social media and apps like Slack and Uber, digital technologies have done little to produce robust economic growth. A strong productivity boost never came.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here‚Äôs why this story might matter to you, according to AI. This is a beta feature and AI hallucinates‚Äîit might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Daron Acemoglu, an economist at MIT and a 2024 Nobel Prize winner, argues that the productivity gains from generative AI will be far smaller and take far longer than AI optimists think. The reason is that though the technology is impressive in many ways, the field is too narrowly focused on products that have little relevance to the largest business sectors.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;The statistic you cite that 95% of AI projects lack business benefits is telling.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Take manufacturing. No question, some version of AI could help; imagine a worker on the factory floor snapping a picture of a problem and asking an AI agent for advice. The problem is that the big tech companies creating AI aren‚Äôt really interested in solving such mundane tasks, and their large foundation models, mostly trained on the internet, aren‚Äôt all that helpful.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It‚Äôs easy to blame the lack of productivity impact from AI so far on business practices and poorly trained workers. Your example of the executive of the Fortune 500 company sounds all too familiar. But it‚Äôs more useful to ask how AI can be trained and fine-tuned to give workers, like nurses and teachers and those on the factory floor, more capabilities and make them more productive at their jobs.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The distinction matters. Some companies announcing large layoffs recently cited AI as the reason. The worry, however, is that it‚Äôs just a short-term cost-saving scheme. As economists like Brynjolfsson and Acemoglu agree, the productivity boost from AI will come when it‚Äôs used to create new types of jobs and augment the abilities of workers, not when it is used just to slash jobs to reduce costs.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Richard Waters responds :&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I see we‚Äôre both feeling pretty cautious, David, so I‚Äôll try to end on a positive note.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Some analyses assume that a much greater share of existing work is within the reach of today‚Äôs AI. McKinsey reckons 60% (versus 20% for Acemoglu) and puts annual productivity gains across the economy at as much as 3.4%. Also, calculations like these are based on automation of existing tasks; any new uses of AI that enhance existing jobs would, as you suggest, be a bonus (and not just in economic terms).&lt;/p&gt;  &lt;p&gt;Cost-cutting always seems to be the first order of business with any new technology. But we‚Äôre still in the early stages and AI is moving fast, so we can always hope&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;FT&lt;/em&gt; chief economics commentator Martin Wolf has been skeptical about whether tech investment boosts productivity but says AI might prove him wrong. The downside: Job losses and wealth concentration might lead to ‚Äútechno-feudalism.‚Äù&lt;/p&gt;  &lt;p&gt;The &lt;em&gt;FT&lt;/em&gt;'s Robert Armstrong argues that the boom in data center investment need not turn to bust. The biggest risk is that debt financing will come to play too big a role in the buildout.&lt;/p&gt;  &lt;p&gt;Last year, David Rotman wrote for &lt;em&gt;MIT Technology Review&lt;/em&gt; about how we can make sure AI works for us in boosting productivity, and what course corrections will be required.&lt;br /&gt;David also wrote this piece about how we can best measure the impact of basic R&amp;amp;D funding on economic growth, and why it can often be bigger than you might think.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;strong&gt;Welcome back to The State of AI, a new collaboration between the &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;Financial Times&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt; and &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;. Every Monday for the next two weeks, writers from both publications will debate one aspect of the generative AI revolution reshaping global power.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This week, Richard Waters, &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;FT&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt; columnist and former West Coast editor, talks with &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;‚Äôs editor at large David Rotman about the true impact of AI on the job market.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Bonus: If you're an MIT Technology Review subscriber, you can join David and Richard, alongside MIT Technology Review‚Äôs editor in chief, Mat Honan, for an exclusive conversation live on Tuesday, December 9 at 1pm ET about this topic. Sign up to be a part here. &lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1127510" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/4c1e48f3-a8d9-fed3-bfeb-c686add0bb5d.png?w=1200" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Richard Waters writes&lt;/strong&gt;: &lt;/p&gt; 
 &lt;p&gt;Any far-reaching new technology is always uneven in its adoption, but few have been more uneven than generative AI. That makes it hard to assess its likely impact on individual businesses, let alone on productivity across the economy as a whole.&lt;/p&gt;  &lt;p&gt;At one extreme, AI coding assistants have revolutionized the work of software developers. Mark Zuckerberg recently predicted that half of Meta‚Äôs code would be written by AI within a year. At the other extreme, most companies are seeing little if any benefit from their initial investments. A widely cited study from MIT found that so far, 95% of gen AI projects produce zero return.&lt;/p&gt; 
 &lt;p&gt;That has provided fuel for the skeptics who maintain that‚Äîby its very nature as a probabilistic technology prone to hallucinating‚Äîgenerative AI will never have a deep impact on business.&lt;/p&gt;  &lt;p&gt;To many students of tech history, though, the lack of immediate impact is just the normal lag associated with transformative new technologies. Erik Brynjolfsson, then an assistant professor at MIT, first described what he called the ‚Äúproductivity paradox of IT‚Äù in the early 1990s. Despite plenty of anecdotal evidence that technology was changing the way people worked, it wasn‚Äôt showing up in the aggregate data in the form of higher productivity growth. Brynjolfsson‚Äôs conclusion was that it just took time for businesses to adapt.&lt;/p&gt;  &lt;p&gt;Big investments in IT finally showed through with a notable rebound in US productivity growth starting in the mid-1990s. But that tailed off a decade later and was followed by a second lull.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Richard Waters and David Rotman" class="wp-image-1126831" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/FT_TR_Newsletter-Episode-05.jpg?w=2731" /&gt;&lt;div class="image-credit"&gt;FT/MIT TECHNOLOGY REVIEW | ADOBE STOCK&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;In the case of AI, companies need to build new infrastructure (particularly data platforms), redesign core business processes, and retrain workers before they can expect to see results. If a lag effect explains the slow results, there may at least be reasons for optimism: Much of the cloud computing infrastructure needed to bring generative AI to a wider business audience is already in place.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The opportunities and the challenges are both enormous. An executive at one Fortune 500 company says his organization has carried out a comprehensive review of its use of analytics and concluded that its workers, overall, add little or no value. Rooting out the old software and replacing that inefficient human labor with AI might yield significant results. But, as this person says, such an overhaul would require big changes to existing processes and take years to carry out.&lt;/p&gt;  &lt;p&gt;There are some early encouraging signs. US productivity growth, stuck at 1% to 1.5% for more than a decade and a half, rebounded to more than 2% last year. It probably hit the same level in the first nine months of this year, though the lack of official data due to the recent US government shutdown makes this impossible to confirm.&lt;/p&gt;  &lt;p&gt;It is impossible to tell, though, how durable this rebound will be or how much can be attributed to AI. The effects of new technologies are seldom felt in isolation. Instead, the benefits compound. AI is riding earlier investments in cloud and mobile computing. In the same way, the latest AI boom may only be the precursor to breakthroughs in fields that have a wider impact on the economy, such as robotics. ChatGPT might have caught the popular imagination, but OpenAI‚Äôs chatbot is unlikely to have the final word.&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1128631" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/FT-State-of-AI-Ep5.png?w=794" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;David Rotman replies:&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;This is my favorite discussion these days when it comes to artificial intelligence. How will AI affect overall economic productivity? Forget about the mesmerizing videos, the promise of companionship, and the prospect of agents to do tedious everyday tasks‚Äîthe bottom line will be whether AI can grow the economy, and that means increasing productivity.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But, as you say, it‚Äôs hard to pin down just how AI is affecting such growth or how it will do so in the future. Erik Brynjolfsson predicts that, like other so-called general purpose technologies, AI will follow a J curve in which initially there is a slow, even negative, effect on productivity as companies invest heavily in the technology before finally reaping the rewards. And then the boom.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But there is a counterexample undermining the just-be-patient argument. Productivity growth from IT picked up in the mid-1990s but since the mid-2000s has been relatively dismal. Despite smartphones and social media and apps like Slack and Uber, digital technologies have done little to produce robust economic growth. A strong productivity boost never came.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here‚Äôs why this story might matter to you, according to AI. This is a beta feature and AI hallucinates‚Äîit might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Daron Acemoglu, an economist at MIT and a 2024 Nobel Prize winner, argues that the productivity gains from generative AI will be far smaller and take far longer than AI optimists think. The reason is that though the technology is impressive in many ways, the field is too narrowly focused on products that have little relevance to the largest business sectors.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;The statistic you cite that 95% of AI projects lack business benefits is telling.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Take manufacturing. No question, some version of AI could help; imagine a worker on the factory floor snapping a picture of a problem and asking an AI agent for advice. The problem is that the big tech companies creating AI aren‚Äôt really interested in solving such mundane tasks, and their large foundation models, mostly trained on the internet, aren‚Äôt all that helpful.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It‚Äôs easy to blame the lack of productivity impact from AI so far on business practices and poorly trained workers. Your example of the executive of the Fortune 500 company sounds all too familiar. But it‚Äôs more useful to ask how AI can be trained and fine-tuned to give workers, like nurses and teachers and those on the factory floor, more capabilities and make them more productive at their jobs.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The distinction matters. Some companies announcing large layoffs recently cited AI as the reason. The worry, however, is that it‚Äôs just a short-term cost-saving scheme. As economists like Brynjolfsson and Acemoglu agree, the productivity boost from AI will come when it‚Äôs used to create new types of jobs and augment the abilities of workers, not when it is used just to slash jobs to reduce costs.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Richard Waters responds :&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I see we‚Äôre both feeling pretty cautious, David, so I‚Äôll try to end on a positive note.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Some analyses assume that a much greater share of existing work is within the reach of today‚Äôs AI. McKinsey reckons 60% (versus 20% for Acemoglu) and puts annual productivity gains across the economy at as much as 3.4%. Also, calculations like these are based on automation of existing tasks; any new uses of AI that enhance existing jobs would, as you suggest, be a bonus (and not just in economic terms).&lt;/p&gt;  &lt;p&gt;Cost-cutting always seems to be the first order of business with any new technology. But we‚Äôre still in the early stages and AI is moving fast, so we can always hope&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;FT&lt;/em&gt; chief economics commentator Martin Wolf has been skeptical about whether tech investment boosts productivity but says AI might prove him wrong. The downside: Job losses and wealth concentration might lead to ‚Äútechno-feudalism.‚Äù&lt;/p&gt;  &lt;p&gt;The &lt;em&gt;FT&lt;/em&gt;'s Robert Armstrong argues that the boom in data center investment need not turn to bust. The biggest risk is that debt financing will come to play too big a role in the buildout.&lt;/p&gt;  &lt;p&gt;Last year, David Rotman wrote for &lt;em&gt;MIT Technology Review&lt;/em&gt; about how we can make sure AI works for us in boosting productivity, and what course corrections will be required.&lt;br /&gt;David also wrote this piece about how we can best measure the impact of basic R&amp;amp;D funding on economic growth, and why it can often be bigger than you might think.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/01/1127872/the-state-of-ai-welcome-to-the-economic-singularity/</guid><pubDate>Mon, 01 Dec 2025 16:30:00 +0000</pubDate></item><item><title>[NEW] Nvidia‚Äôs $2B Synopsys bet tightens its grip on the chip-design stack (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/nvidias-2b-synopsys-bet-tightens-its-grip-on-the-chip-design-stack/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2192215566.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia is investing $2 billion into Synopsys, which makes software and components for designing semiconductor chips. The deal deepens their existing partnership at a time when analysts have started to scrutinize increasingly common circular AI-industry deals and warn of a potential bubble.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia said it bought Synopsys shares at $414.79 each as part of a multi-year partnership to integrate Nvidia‚Äôs AI hardware and computing capabilities into Synopsys‚Äôs electronic design automation (EDA) and simulation software. The deal will help Synopsys transition its platform from CPU-based computing to GPUs, a shift it hopes will speed up chip-design workflows, per a release.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal gave Synopsys‚Äôs stock a lift by signaling long-term growth ‚Äì a boon after the company recently reported weakness in its IP segment due to U.S. export restrictions and issues at a major customer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Nvidia, the investment strengthens its influence over Synopsys‚Äôs widely used EDA tools at a time when chip-design competition is starting to heat up. It also comes after major investors such as SoftBank and Peter Thiel have sold off their Nvidia positions.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2192215566.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia is investing $2 billion into Synopsys, which makes software and components for designing semiconductor chips. The deal deepens their existing partnership at a time when analysts have started to scrutinize increasingly common circular AI-industry deals and warn of a potential bubble.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia said it bought Synopsys shares at $414.79 each as part of a multi-year partnership to integrate Nvidia‚Äôs AI hardware and computing capabilities into Synopsys‚Äôs electronic design automation (EDA) and simulation software. The deal will help Synopsys transition its platform from CPU-based computing to GPUs, a shift it hopes will speed up chip-design workflows, per a release.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal gave Synopsys‚Äôs stock a lift by signaling long-term growth ‚Äì a boon after the company recently reported weakness in its IP segment due to U.S. export restrictions and issues at a major customer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Nvidia, the investment strengthens its influence over Synopsys‚Äôs widely used EDA tools at a time when chip-design competition is starting to heat up. It also comes after major investors such as SoftBank and Peter Thiel have sold off their Nvidia positions.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/nvidias-2b-synopsys-bet-tightens-its-grip-on-the-chip-design-stack/</guid><pubDate>Mon, 01 Dec 2025 16:32:50 +0000</pubDate></item><item><title>[NEW] OpenAI‚Äôs investment into Thrive Holdings is its latest circular deal (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/openais-investment-into-thrive-holdings-is-its-latest-circular-deal/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2197181602.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is taking an ownership stake in Thrive Holdings, whose parent company is one of the AI giant‚Äôs major investors, Thrive Capital.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Thrive Holdings operates like a private equity firm for AI, rolling up companies that it believes could benefit from the tech in sectors like accounting and IT services.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Neither company disclosed the terms of the deal, but it will involve OpenAI embedding engineering, research, and product teams within Thrive‚Äôs companies to accelerate AI adoption and boost efficiency, the company says. If those companies succeed, OpenAI‚Äôs stake will grow, and it will get compensated for its services, according to reporting from CNBC.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership follows a pattern of circular dealmaking for the $500 billion AI giant, which also recently took stakes in infrastructure partners like Advanced Micro Devices and CoreWeave. Analysts will be watching to see if Thrive-owned firms actually succeed in building long-term profitable businesses using OpenAI‚Äôs tech, or if the result is really just pumped-up valuations based on speculative market potential.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2197181602.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is taking an ownership stake in Thrive Holdings, whose parent company is one of the AI giant‚Äôs major investors, Thrive Capital.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Thrive Holdings operates like a private equity firm for AI, rolling up companies that it believes could benefit from the tech in sectors like accounting and IT services.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Neither company disclosed the terms of the deal, but it will involve OpenAI embedding engineering, research, and product teams within Thrive‚Äôs companies to accelerate AI adoption and boost efficiency, the company says. If those companies succeed, OpenAI‚Äôs stake will grow, and it will get compensated for its services, according to reporting from CNBC.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership follows a pattern of circular dealmaking for the $500 billion AI giant, which also recently took stakes in infrastructure partners like Advanced Micro Devices and CoreWeave. Analysts will be watching to see if Thrive-owned firms actually succeed in building long-term profitable businesses using OpenAI‚Äôs tech, or if the result is really just pumped-up valuations based on speculative market potential.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/openais-investment-into-thrive-holdings-is-its-latest-circular-deal/</guid><pubDate>Mon, 01 Dec 2025 16:58:17 +0000</pubDate></item><item><title>[NEW] At NeurIPS, NVIDIA Advances Open Model Development for Digital and Physical AI (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Researchers worldwide rely on open-source technologies as the foundation of their work. To equip the community with the latest advancements in digital and physical AI, NVIDIA is further expanding its collection of open AI models, datasets and tools ‚Äî with potential applications in virtually every research field.&lt;/p&gt;
&lt;p&gt;At NeurIPS, one of the world‚Äôs top AI conferences, NVIDIA is unveiling open physical AI models and tools to support research, including Alpamayo-R1, the world‚Äôs first industry-scale open reasoning vision language action (VLA) model for autonomous driving. In digital AI, NVIDIA is releasing new models and datasets for speech and AI safety.&lt;/p&gt;
&lt;p&gt;NVIDIA researchers are presenting over 70 papers, talks and workshops at the conference, sharing innovative projects that span AI reasoning, medical research, autonomous vehicle (AV) development and more.&lt;/p&gt;
&lt;p&gt;These initiatives deepen NVIDIA‚Äôs commitment to open source ‚Äî an effort recognized by a new Openness Index from Artificial Analysis, an independent organization that benchmarks AI. The Artificial Analysis Open Index rates the NVIDIA Nemotron family of open technologies for frontier AI development among the most open in the AI ecosystem based on the permissibility of the model licenses, data transparency and availability of technical details.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-large wp-image-87920" height="730" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Artificial-Analysis-Openness-Index-1680x730.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA DRIVE Alpamayo-R1 Opens New Research Frontier for Autonomous Driving&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA DRIVE Alpamayo-R1 (AR1), the world‚Äôs first open reasoning VLA model for AV research, integrates chain-of-thought AI reasoning with path planning ‚Äî a component critical for advancing AV safety in complex road scenarios and enabling level 4 autonomy.&lt;/p&gt;
&lt;p&gt;While previous iterations of self-driving models struggled with nuanced situations ‚Äî a pedestrian-heavy intersection, an upcoming lane closure or a double-parked vehicle in a bike lane ‚Äî reasoning gives autonomous vehicles the common sense to drive more like humans do.&lt;/p&gt;
&lt;p&gt;AR1 accomplishes this by breaking down a scenario and reasoning through each step. It considers all possible trajectories, then uses contextual data to choose the best route.&lt;/p&gt;
&lt;p&gt;For example, by tapping into the chain-of-thought reasoning enabled by AR1, an AV driving in a pedestrian-heavy area next to a bike lane could take in data from its path, incorporate reasoning traces ‚Äî explanations on why it took certain actions ‚Äî and use that information to plan its future trajectory, such as moving away from the bike lane or stopping for potential jaywalkers.&lt;/p&gt;

&lt;p&gt;AR1‚Äôs open foundation, based on NVIDIA Cosmos Reason, lets researchers customize the model for their own non-commercial use cases, whether for benchmarking or building experimental AV applications.&lt;/p&gt;
&lt;p&gt;For post-training AR1, reinforcement learning has proven especially effective ‚Äî researchers observed a significant improvement in reasoning capabilities with AR1 compared with the pretrained model.&lt;/p&gt;
&lt;p&gt;NVIDIA DRIVE Alpamayo-R1 will be available on GitHub and Hugging Face, and a subset of the data used to train and evaluate the model is available in the NVIDIA Physical AI Open Datasets. NVIDIA has also released the open-source AlpaSim framework to evaluate AR1.&lt;/p&gt;
&lt;p&gt;Learn more about reasoning VLA models for autonomous driving.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Customizing NVIDIA Cosmos for Any Physical AI Use Case&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Developers can learn how to use and post-train Cosmos-based models using step-by-step recipes, quick-start inference examples and advanced post-training workflows now available in the Cosmos Cookbook. It‚Äôs a comprehensive guide for physical AI developers that covers every step in AI development, including data curation, synthetic data generation and model evaluation.&lt;/p&gt;
&lt;p&gt;There are virtually limitless possibilities for Cosmos-based applications. The latest examples from NVIDIA include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;LidarGen&lt;/b&gt;, the first world model that can generate lidar data for AV simulation.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Omniverse NuRec Fixer&lt;/b&gt;, a model for AV and robotics simulation that taps into NVIDIA Cosmos Predict to near-instantly address artifacts in neurally reconstructed data, such as blurs and holes from novel views or noisy data.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Cosmos Policy&lt;/b&gt;, a framework for turning large pretrained video models into robust robot policies ‚Äî a set of rules that dictate a robot‚Äôs behavior.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;ProtoMotions3&lt;/b&gt;, an open-source, GPU-accelerated framework built on NVIDIA Newton and Isaac Lab for training physically simulated digital humans and humanoid robots with realistic scenes generated by Cosmos world foundation models (WFMs).&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87924"&gt;&lt;img alt="alt" class="size-large wp-image-87924" height="1020" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/LidarGen-1680x1020.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87924"&gt;Sample outputs from the LidarGen model, built on Cosmos. The top row shows the input data with generated lidar data overlaid. The middle row shows generated and real lidar range maps. Bottom left shows the real lidar point cloud, while bottom right shows the point cloud generated by LidarGen.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Policy models can be trained in NVIDIA Isaac Lab and Isaac Sim , and data generated from the policy models can then be used to post-train NVIDIA GR00T N models for robotics.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87927"&gt;&lt;img alt="alt" class="size-full wp-image-87927" height="523" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/ProtoMotions.gif" width="800" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87927"&gt;Humanoid policy trained with ProtoMotions3 in Isaac Sim, with 3D background scene generated by Lyra with Cosmos WFM.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;NVIDIA ecosystem partners are developing their latest technologies with Cosmos WFMs.&lt;/p&gt;
&lt;p&gt;AV developer&amp;nbsp;Voxel51 is contributing model recipes to the Cosmos Cookbook. Physical AI developers 1X, Figure AI, Foretellix, Gatik, Oxa, PlusAI and X-Humanoid are using WFMs for their latest physical AI applications. And researchers at ETH Zurich are presenting a NeurIPS paper that highlights using Cosmos models for realistic and cohesive 3D scene creation.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;NVIDIA Nemotron Additions Bolster the Digital AI Developer Toolkit&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA is also releasing new multi-speaker speech AI models, a new model with reasoning capabilities and datasets for AI safety, as well as open tools to generate high-quality synthetic datasets for reinforcement learning and domain-specific model customization. These tools include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;MultiTalker Parakeet&lt;/b&gt;: An automatic speech recognition model for streaming audio that can understand multiple speakers, even in overlapped or fast-paced conversations.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Sortformer&lt;/b&gt;: A state-of-the-art model that can accurately distinguish multiple speakers within an audio stream ‚Äî a process called diarization ‚Äî in real time.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Nemotron Content Safety Reasoning&lt;/b&gt;: A reasoning-based AI safety model that dynamically enforces custom policies across domains.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Nemotron Safety Audio&lt;/b&gt; &lt;b&gt;Dataset&lt;/b&gt;: A synthetic dataset that helps train models to detect unsafe audio content, enabling the development of guardrails that work across text and audio modalities.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NeMo Gym&lt;/b&gt;: an open-source library that accelerates and simplifies the development of reinforcement learning environments for LLM training. NeMo Gym also contains a growing collection of ready-to-use training environments to enable Reinforcement Learning from Verifiable Reward (RLVR).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NeMo Data Designer Library&lt;/strong&gt;: Now open-sourced under Apache 2.0, this library provides an end-to-end toolkit to generate, validate and refine high-quality synthetic datasets for generative AI development, including domain-specific model customization and evaluation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;NVIDIA ecosystem partners using NVIDIA Nemotron and NeMo tools to build secure, specialized agentic AI include CrowdStrike, Palantir and ServiceNow.&lt;/p&gt;
&lt;p&gt;NeurIPS attendees can explore these innovations at the Nemotron Summit, taking place today, from 4-8 p.m. PT, with an opening address by Bryan Catanzaro, vice president of applied deep learning research at NVIDIA.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA Research Furthers Language AI Innovation&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Of the dozens of NVIDIA-authored research papers at NeurIPS, here are a few highlights advancing language models:&lt;/p&gt;

&lt;p&gt;&lt;i&gt;View the full list of &lt;/i&gt;&lt;i&gt;events at NeurIPS&lt;/i&gt;&lt;i&gt;, running through Sunday, Dec. 7, in San Diego.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Researchers worldwide rely on open-source technologies as the foundation of their work. To equip the community with the latest advancements in digital and physical AI, NVIDIA is further expanding its collection of open AI models, datasets and tools ‚Äî with potential applications in virtually every research field.&lt;/p&gt;
&lt;p&gt;At NeurIPS, one of the world‚Äôs top AI conferences, NVIDIA is unveiling open physical AI models and tools to support research, including Alpamayo-R1, the world‚Äôs first industry-scale open reasoning vision language action (VLA) model for autonomous driving. In digital AI, NVIDIA is releasing new models and datasets for speech and AI safety.&lt;/p&gt;
&lt;p&gt;NVIDIA researchers are presenting over 70 papers, talks and workshops at the conference, sharing innovative projects that span AI reasoning, medical research, autonomous vehicle (AV) development and more.&lt;/p&gt;
&lt;p&gt;These initiatives deepen NVIDIA‚Äôs commitment to open source ‚Äî an effort recognized by a new Openness Index from Artificial Analysis, an independent organization that benchmarks AI. The Artificial Analysis Open Index rates the NVIDIA Nemotron family of open technologies for frontier AI development among the most open in the AI ecosystem based on the permissibility of the model licenses, data transparency and availability of technical details.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-large wp-image-87920" height="730" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Artificial-Analysis-Openness-Index-1680x730.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA DRIVE Alpamayo-R1 Opens New Research Frontier for Autonomous Driving&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA DRIVE Alpamayo-R1 (AR1), the world‚Äôs first open reasoning VLA model for AV research, integrates chain-of-thought AI reasoning with path planning ‚Äî a component critical for advancing AV safety in complex road scenarios and enabling level 4 autonomy.&lt;/p&gt;
&lt;p&gt;While previous iterations of self-driving models struggled with nuanced situations ‚Äî a pedestrian-heavy intersection, an upcoming lane closure or a double-parked vehicle in a bike lane ‚Äî reasoning gives autonomous vehicles the common sense to drive more like humans do.&lt;/p&gt;
&lt;p&gt;AR1 accomplishes this by breaking down a scenario and reasoning through each step. It considers all possible trajectories, then uses contextual data to choose the best route.&lt;/p&gt;
&lt;p&gt;For example, by tapping into the chain-of-thought reasoning enabled by AR1, an AV driving in a pedestrian-heavy area next to a bike lane could take in data from its path, incorporate reasoning traces ‚Äî explanations on why it took certain actions ‚Äî and use that information to plan its future trajectory, such as moving away from the bike lane or stopping for potential jaywalkers.&lt;/p&gt;

&lt;p&gt;AR1‚Äôs open foundation, based on NVIDIA Cosmos Reason, lets researchers customize the model for their own non-commercial use cases, whether for benchmarking or building experimental AV applications.&lt;/p&gt;
&lt;p&gt;For post-training AR1, reinforcement learning has proven especially effective ‚Äî researchers observed a significant improvement in reasoning capabilities with AR1 compared with the pretrained model.&lt;/p&gt;
&lt;p&gt;NVIDIA DRIVE Alpamayo-R1 will be available on GitHub and Hugging Face, and a subset of the data used to train and evaluate the model is available in the NVIDIA Physical AI Open Datasets. NVIDIA has also released the open-source AlpaSim framework to evaluate AR1.&lt;/p&gt;
&lt;p&gt;Learn more about reasoning VLA models for autonomous driving.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Customizing NVIDIA Cosmos for Any Physical AI Use Case&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Developers can learn how to use and post-train Cosmos-based models using step-by-step recipes, quick-start inference examples and advanced post-training workflows now available in the Cosmos Cookbook. It‚Äôs a comprehensive guide for physical AI developers that covers every step in AI development, including data curation, synthetic data generation and model evaluation.&lt;/p&gt;
&lt;p&gt;There are virtually limitless possibilities for Cosmos-based applications. The latest examples from NVIDIA include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;LidarGen&lt;/b&gt;, the first world model that can generate lidar data for AV simulation.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Omniverse NuRec Fixer&lt;/b&gt;, a model for AV and robotics simulation that taps into NVIDIA Cosmos Predict to near-instantly address artifacts in neurally reconstructed data, such as blurs and holes from novel views or noisy data.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Cosmos Policy&lt;/b&gt;, a framework for turning large pretrained video models into robust robot policies ‚Äî a set of rules that dictate a robot‚Äôs behavior.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;ProtoMotions3&lt;/b&gt;, an open-source, GPU-accelerated framework built on NVIDIA Newton and Isaac Lab for training physically simulated digital humans and humanoid robots with realistic scenes generated by Cosmos world foundation models (WFMs).&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87924"&gt;&lt;img alt="alt" class="size-large wp-image-87924" height="1020" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/LidarGen-1680x1020.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87924"&gt;Sample outputs from the LidarGen model, built on Cosmos. The top row shows the input data with generated lidar data overlaid. The middle row shows generated and real lidar range maps. Bottom left shows the real lidar point cloud, while bottom right shows the point cloud generated by LidarGen.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Policy models can be trained in NVIDIA Isaac Lab and Isaac Sim , and data generated from the policy models can then be used to post-train NVIDIA GR00T N models for robotics.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87927"&gt;&lt;img alt="alt" class="size-full wp-image-87927" height="523" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/ProtoMotions.gif" width="800" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87927"&gt;Humanoid policy trained with ProtoMotions3 in Isaac Sim, with 3D background scene generated by Lyra with Cosmos WFM.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;NVIDIA ecosystem partners are developing their latest technologies with Cosmos WFMs.&lt;/p&gt;
&lt;p&gt;AV developer&amp;nbsp;Voxel51 is contributing model recipes to the Cosmos Cookbook. Physical AI developers 1X, Figure AI, Foretellix, Gatik, Oxa, PlusAI and X-Humanoid are using WFMs for their latest physical AI applications. And researchers at ETH Zurich are presenting a NeurIPS paper that highlights using Cosmos models for realistic and cohesive 3D scene creation.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;NVIDIA Nemotron Additions Bolster the Digital AI Developer Toolkit&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA is also releasing new multi-speaker speech AI models, a new model with reasoning capabilities and datasets for AI safety, as well as open tools to generate high-quality synthetic datasets for reinforcement learning and domain-specific model customization. These tools include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;MultiTalker Parakeet&lt;/b&gt;: An automatic speech recognition model for streaming audio that can understand multiple speakers, even in overlapped or fast-paced conversations.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Sortformer&lt;/b&gt;: A state-of-the-art model that can accurately distinguish multiple speakers within an audio stream ‚Äî a process called diarization ‚Äî in real time.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Nemotron Content Safety Reasoning&lt;/b&gt;: A reasoning-based AI safety model that dynamically enforces custom policies across domains.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Nemotron Safety Audio&lt;/b&gt; &lt;b&gt;Dataset&lt;/b&gt;: A synthetic dataset that helps train models to detect unsafe audio content, enabling the development of guardrails that work across text and audio modalities.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NeMo Gym&lt;/b&gt;: an open-source library that accelerates and simplifies the development of reinforcement learning environments for LLM training. NeMo Gym also contains a growing collection of ready-to-use training environments to enable Reinforcement Learning from Verifiable Reward (RLVR).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NeMo Data Designer Library&lt;/strong&gt;: Now open-sourced under Apache 2.0, this library provides an end-to-end toolkit to generate, validate and refine high-quality synthetic datasets for generative AI development, including domain-specific model customization and evaluation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;NVIDIA ecosystem partners using NVIDIA Nemotron and NeMo tools to build secure, specialized agentic AI include CrowdStrike, Palantir and ServiceNow.&lt;/p&gt;
&lt;p&gt;NeurIPS attendees can explore these innovations at the Nemotron Summit, taking place today, from 4-8 p.m. PT, with an opening address by Bryan Catanzaro, vice president of applied deep learning research at NVIDIA.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA Research Furthers Language AI Innovation&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Of the dozens of NVIDIA-authored research papers at NeurIPS, here are a few highlights advancing language models:&lt;/p&gt;

&lt;p&gt;&lt;i&gt;View the full list of &lt;/i&gt;&lt;i&gt;events at NeurIPS&lt;/i&gt;&lt;i&gt;, running through Sunday, Dec. 7, in San Diego.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/</guid><pubDate>Mon, 01 Dec 2025 17:00:48 +0000</pubDate></item><item><title>[NEW] MIT offshoot Liquid AI releases blueprint for enterprise-grade small-model training (AI | VentureBeat)</title><link>https://venturebeat.com/ai/mit-offshoot-liquid-ai-releases-blueprint-for-enterprise-grade-small-model</link><description>[unable to retrieve full-text content]&lt;p&gt;When Liquid AI, a startup f&lt;a href="https://aimmediahouse.com/market-industry/from-worm-brains-to-a-2-billion-ai-unicorn-liquid-ai-defies-conventional-ai-limits"&gt;ounded by MIT computer scientists back in 2023&lt;/a&gt;, introduced&lt;a href="https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models"&gt; its Liquid Foundation Models series 2 (LFM2) in July 2025&lt;/a&gt;, the pitch was straightforward: deliver the fastest on-device foundation models on the market using the new &amp;quot;liquid&amp;quot; architecture, with training and inference efficiency that made small models a serious alternative to cloud-only large language models (LLMs) such as OpenAI&amp;#x27;s GPT series and Google&amp;#x27;s Gemini. &lt;/p&gt;&lt;p&gt;The initial release shipped dense checkpoints at 350M, 700M, and 1.2B parameters, a hybrid architecture heavily weighted toward gated short convolutions, and benchmark numbers that placed LFM2 ahead of similarly sized competitors like Qwen3, Llama 3.2, and Gemma 3 on both quality and CPU throughput. The message to enterprises was clear: real-time, privacy-preserving AI on phones, laptops, and vehicles no longer required sacrificing capability for latency.&lt;/p&gt;&lt;p&gt;In the months since that launch, Liquid has expanded LFM2 into a broader product line ‚Äî adding&lt;a href="https://venturebeat.com/ai/what-if-weve-been-doing-agentic-ai-all-wrong-mit-offshoot-liquid-ai-offers"&gt; task-and-domain-specialized variants&lt;/a&gt;, a &lt;a href="https://venturebeat.com/ai/liquid-ai-wants-to-give-smartphones-small-fast-ai-that-can-see-with-new-lfm2-vl-model"&gt;small video ingestion and analysis model&lt;/a&gt;, and an &lt;a href="https://venturebeat.com/ai/finally-a-dev-kit-for-designing-on-device-mobile-ai-apps-is-here-liquid-ais-leap"&gt;edge-focused deployment stack called LEAP&lt;/a&gt;  ‚Äî and positioned the models as the control layer for on-device and on-prem agentic systems. &lt;/p&gt;&lt;p&gt;Now, with &lt;a href="https://arxiv.org/abs/2511.23404"&gt;the publication of the detailed, 51-page LFM2 technical report on arXiv&lt;/a&gt;, the company is going a step further: making public the architecture search process, training data mixture, distillation objective, curriculum strategy, and post-training pipeline behind those models. &lt;/p&gt;&lt;p&gt;And unlike earlier open models, LFM2 is built around a repeatable recipe: a hardware-in-the-loop search process, a training curriculum that compensates for smaller parameter budgets, and a post-training pipeline tuned for instruction following and tool use. &lt;/p&gt;&lt;p&gt;Rather than just offering weights and an API, Liquid is effectively publishing a detailed blueprint that other organizations can use as a reference for training their own small, efficient models from scratch, tuned to their own hardware and deployment constraints.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;A model family designed around real constraints, not GPU labs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The technical report begins with a premise enterprises are intimately familiar with: real AI systems hit limits long before benchmarks do. Latency budgets, peak memory ceilings, and thermal throttling define what can actually run in production‚Äîespecially on laptops, tablets, commodity servers, and mobile devices.&lt;/p&gt;&lt;p&gt;To address this, Liquid AI performed architecture search directly on target hardware, including Snapdragon mobile SoCs and Ryzen laptop CPUs. The result is a consistent outcome across sizes: a minimal hybrid architecture dominated by &lt;b&gt;gated short convolution blocks&lt;/b&gt; and a small number of &lt;b&gt;grouped-query attention (GQA)&lt;/b&gt; layers. This design was repeatedly selected over more exotic linear-attention and SSM hybrids because it delivered a better quality-latency-memory Pareto profile under real device conditions.&lt;/p&gt;&lt;p&gt;This matters for enterprise teams in three ways:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Predictability.&lt;/b&gt; The architecture is simple, parameter-efficient, and stable across model sizes from 350M to 2.6B.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Operational portability.&lt;/b&gt; Dense and MoE variants share the same structural backbone, simplifying deployment across mixed hardware fleets.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;On-device feasibility.&lt;/b&gt; Prefill and decode throughput on CPUs surpass comparable open models by roughly 2√ó in many cases, reducing the need to offload routine tasks to cloud inference endpoints.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Instead of optimizing for academic novelty, the report reads as a systematic attempt to design models enterprises can &lt;i&gt;actually ship.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;This is notable and more practical for enterprises in a field where many open models quietly assume access to multi-H100 clusters during inference.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;A training pipeline tuned for enterprise-relevant behavior&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;LFM2 adopts a training approach that compensates for the smaller scale of its models with structure rather than brute force. Key elements include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;10‚Äì12T token pre-training&lt;/b&gt; and an additional &lt;b&gt;32K-context mid-training phase&lt;/b&gt;, which extends the model‚Äôs useful context window without exploding compute costs.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;decoupled Top-K knowledge distillation objective&lt;/b&gt; that sidesteps the instability of standard KL distillation when teachers provide only partial logits.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;three-stage post-training sequence&lt;/b&gt;‚ÄîSFT, length-normalized preference alignment, and model merging‚Äîdesigned to produce more reliable instruction following and tool-use behavior.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For enterprise AI developers, the significance is that LFM2 models behave less like ‚Äútiny LLMs‚Äù and more like practical agents able to follow structured formats, adhere to JSON schemas, and manage multi-turn chat flows. Many open models at similar sizes fail not due to lack of reasoning ability, but due to brittle adherence to instruction templates. The LFM2 post-training recipe directly targets these rough edges.&lt;/p&gt;&lt;p&gt;In other words: Liquid AI optimized small models for &lt;i&gt;operational reliability&lt;/i&gt;, not just scoreboards.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Multimodality designed for device constraints, not lab demos&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The LFM2-VL and LFM2-Audio variants reflect another shift: multimodality built around &lt;b&gt;token efficiency&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;Rather than embedding a massive vision transformer directly into an LLM, LFM2-VL attaches a SigLIP2 encoder through a connector that aggressively reduces visual token count via PixelUnshuffle. High-resolution inputs automatically trigger dynamic tiling, keeping token budgets controllable even on mobile hardware. LFM2-Audio uses a bifurcated audio path‚Äîone for embeddings, one for generation‚Äîsupporting real-time transcription or speech-to-speech on modest CPUs.&lt;/p&gt;&lt;p&gt;For enterprise platform architects, this design points toward a practical future where:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;document understanding happens directly on endpoints such as field devices;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;audio transcription and speech agents run locally for privacy compliance;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;multimodal agents operate within fixed latency envelopes without streaming data off-device.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The through-line is the same: multimodal capability without requiring a GPU farm.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Retrieval models built for agent systems, not legacy search&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;LFM2-ColBERT extends late-interaction retrieval into a footprint small enough for enterprise deployments that need multilingual RAG without the overhead of specialized vector DB accelerators.&lt;/p&gt;&lt;p&gt;This is particularly meaningful as organizations begin to orchestrate fleets of agents. Fast local retrieval‚Äîrunning on the same hardware as the reasoning model‚Äîreduces latency and provides a governance win: documents never leave the device boundary.&lt;/p&gt;&lt;p&gt;Taken together, the VL, Audio, and ColBERT variants show LFM2 as a modular system, not a single model drop.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The emerging blueprint for hybrid enterprise AI architectures&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Across all variants, the LFM2 report implicitly sketches what tomorrow‚Äôs enterprise AI stack will look like: &lt;b&gt;hybrid local-cloud orchestration&lt;/b&gt;, where small, fast models operating on devices handle time-critical perception, formatting, tool invocation, and judgment tasks, while larger models in the cloud offer heavyweight reasoning when needed.&lt;/p&gt;&lt;p&gt;Several trends converge here:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Cost control.&lt;/b&gt; Running routine inference locally avoids unpredictable cloud billing.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Latency determinism.&lt;/b&gt; TTFT and decode stability matter in agent workflows; on-device eliminates network jitter.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Governance and compliance.&lt;/b&gt; Local execution simplifies PII handling, data residency, and auditability.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Resilience.&lt;/b&gt; Agentic systems degrade gracefully if the cloud path becomes unavailable.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Enterprises adopting these architectures will likely treat small on-device models as the ‚Äúcontrol plane‚Äù of agentic workflows, with large cloud models serving as on-demand accelerators.&lt;/p&gt;&lt;p&gt;LFM2 is one of the clearest open-source foundations for that control layer to date.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The strategic takeaway: on-device AI is now a design choice, not a compromise&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For years, organizations building AI features have accepted that ‚Äúreal AI‚Äù requires cloud inference. LFM2 challenges that assumption. The models perform competitively across reasoning, instruction following, multilingual tasks, and RAG‚Äîwhile simultaneously achieving substantial latency gains over other open small-model families.&lt;/p&gt;&lt;p&gt;For CIOs and CTOs finalizing 2026 roadmaps, the implication is direct: &lt;b&gt;small, open, on-device models are now strong enough to carry meaningful slices of production workloads.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;LFM2 will not replace frontier cloud models for frontier-scale reasoning. But it offers something enterprises arguably need more: a reproducible, open, and operationally feasible foundation for &lt;b&gt;agentic systems that must run anywhere&lt;/b&gt;, from phones to industrial endpoints to air-gapped secure facilities.&lt;/p&gt;&lt;p&gt;In the broadening landscape of enterprise AI, LFM2 is less a research milestone and more a sign of architectural convergence. The future is not cloud or edge‚Äîit‚Äôs both, operating in concert. And releases like LFM2 provide the building blocks for organizations prepared to build that hybrid future intentionally rather than accidentally.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;When Liquid AI, a startup f&lt;a href="https://aimmediahouse.com/market-industry/from-worm-brains-to-a-2-billion-ai-unicorn-liquid-ai-defies-conventional-ai-limits"&gt;ounded by MIT computer scientists back in 2023&lt;/a&gt;, introduced&lt;a href="https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models"&gt; its Liquid Foundation Models series 2 (LFM2) in July 2025&lt;/a&gt;, the pitch was straightforward: deliver the fastest on-device foundation models on the market using the new &amp;quot;liquid&amp;quot; architecture, with training and inference efficiency that made small models a serious alternative to cloud-only large language models (LLMs) such as OpenAI&amp;#x27;s GPT series and Google&amp;#x27;s Gemini. &lt;/p&gt;&lt;p&gt;The initial release shipped dense checkpoints at 350M, 700M, and 1.2B parameters, a hybrid architecture heavily weighted toward gated short convolutions, and benchmark numbers that placed LFM2 ahead of similarly sized competitors like Qwen3, Llama 3.2, and Gemma 3 on both quality and CPU throughput. The message to enterprises was clear: real-time, privacy-preserving AI on phones, laptops, and vehicles no longer required sacrificing capability for latency.&lt;/p&gt;&lt;p&gt;In the months since that launch, Liquid has expanded LFM2 into a broader product line ‚Äî adding&lt;a href="https://venturebeat.com/ai/what-if-weve-been-doing-agentic-ai-all-wrong-mit-offshoot-liquid-ai-offers"&gt; task-and-domain-specialized variants&lt;/a&gt;, a &lt;a href="https://venturebeat.com/ai/liquid-ai-wants-to-give-smartphones-small-fast-ai-that-can-see-with-new-lfm2-vl-model"&gt;small video ingestion and analysis model&lt;/a&gt;, and an &lt;a href="https://venturebeat.com/ai/finally-a-dev-kit-for-designing-on-device-mobile-ai-apps-is-here-liquid-ais-leap"&gt;edge-focused deployment stack called LEAP&lt;/a&gt;  ‚Äî and positioned the models as the control layer for on-device and on-prem agentic systems. &lt;/p&gt;&lt;p&gt;Now, with &lt;a href="https://arxiv.org/abs/2511.23404"&gt;the publication of the detailed, 51-page LFM2 technical report on arXiv&lt;/a&gt;, the company is going a step further: making public the architecture search process, training data mixture, distillation objective, curriculum strategy, and post-training pipeline behind those models. &lt;/p&gt;&lt;p&gt;And unlike earlier open models, LFM2 is built around a repeatable recipe: a hardware-in-the-loop search process, a training curriculum that compensates for smaller parameter budgets, and a post-training pipeline tuned for instruction following and tool use. &lt;/p&gt;&lt;p&gt;Rather than just offering weights and an API, Liquid is effectively publishing a detailed blueprint that other organizations can use as a reference for training their own small, efficient models from scratch, tuned to their own hardware and deployment constraints.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;A model family designed around real constraints, not GPU labs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The technical report begins with a premise enterprises are intimately familiar with: real AI systems hit limits long before benchmarks do. Latency budgets, peak memory ceilings, and thermal throttling define what can actually run in production‚Äîespecially on laptops, tablets, commodity servers, and mobile devices.&lt;/p&gt;&lt;p&gt;To address this, Liquid AI performed architecture search directly on target hardware, including Snapdragon mobile SoCs and Ryzen laptop CPUs. The result is a consistent outcome across sizes: a minimal hybrid architecture dominated by &lt;b&gt;gated short convolution blocks&lt;/b&gt; and a small number of &lt;b&gt;grouped-query attention (GQA)&lt;/b&gt; layers. This design was repeatedly selected over more exotic linear-attention and SSM hybrids because it delivered a better quality-latency-memory Pareto profile under real device conditions.&lt;/p&gt;&lt;p&gt;This matters for enterprise teams in three ways:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Predictability.&lt;/b&gt; The architecture is simple, parameter-efficient, and stable across model sizes from 350M to 2.6B.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Operational portability.&lt;/b&gt; Dense and MoE variants share the same structural backbone, simplifying deployment across mixed hardware fleets.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;On-device feasibility.&lt;/b&gt; Prefill and decode throughput on CPUs surpass comparable open models by roughly 2√ó in many cases, reducing the need to offload routine tasks to cloud inference endpoints.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Instead of optimizing for academic novelty, the report reads as a systematic attempt to design models enterprises can &lt;i&gt;actually ship.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;This is notable and more practical for enterprises in a field where many open models quietly assume access to multi-H100 clusters during inference.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;A training pipeline tuned for enterprise-relevant behavior&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;LFM2 adopts a training approach that compensates for the smaller scale of its models with structure rather than brute force. Key elements include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;10‚Äì12T token pre-training&lt;/b&gt; and an additional &lt;b&gt;32K-context mid-training phase&lt;/b&gt;, which extends the model‚Äôs useful context window without exploding compute costs.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;decoupled Top-K knowledge distillation objective&lt;/b&gt; that sidesteps the instability of standard KL distillation when teachers provide only partial logits.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;three-stage post-training sequence&lt;/b&gt;‚ÄîSFT, length-normalized preference alignment, and model merging‚Äîdesigned to produce more reliable instruction following and tool-use behavior.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For enterprise AI developers, the significance is that LFM2 models behave less like ‚Äútiny LLMs‚Äù and more like practical agents able to follow structured formats, adhere to JSON schemas, and manage multi-turn chat flows. Many open models at similar sizes fail not due to lack of reasoning ability, but due to brittle adherence to instruction templates. The LFM2 post-training recipe directly targets these rough edges.&lt;/p&gt;&lt;p&gt;In other words: Liquid AI optimized small models for &lt;i&gt;operational reliability&lt;/i&gt;, not just scoreboards.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Multimodality designed for device constraints, not lab demos&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The LFM2-VL and LFM2-Audio variants reflect another shift: multimodality built around &lt;b&gt;token efficiency&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;Rather than embedding a massive vision transformer directly into an LLM, LFM2-VL attaches a SigLIP2 encoder through a connector that aggressively reduces visual token count via PixelUnshuffle. High-resolution inputs automatically trigger dynamic tiling, keeping token budgets controllable even on mobile hardware. LFM2-Audio uses a bifurcated audio path‚Äîone for embeddings, one for generation‚Äîsupporting real-time transcription or speech-to-speech on modest CPUs.&lt;/p&gt;&lt;p&gt;For enterprise platform architects, this design points toward a practical future where:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;document understanding happens directly on endpoints such as field devices;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;audio transcription and speech agents run locally for privacy compliance;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;multimodal agents operate within fixed latency envelopes without streaming data off-device.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The through-line is the same: multimodal capability without requiring a GPU farm.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Retrieval models built for agent systems, not legacy search&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;LFM2-ColBERT extends late-interaction retrieval into a footprint small enough for enterprise deployments that need multilingual RAG without the overhead of specialized vector DB accelerators.&lt;/p&gt;&lt;p&gt;This is particularly meaningful as organizations begin to orchestrate fleets of agents. Fast local retrieval‚Äîrunning on the same hardware as the reasoning model‚Äîreduces latency and provides a governance win: documents never leave the device boundary.&lt;/p&gt;&lt;p&gt;Taken together, the VL, Audio, and ColBERT variants show LFM2 as a modular system, not a single model drop.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The emerging blueprint for hybrid enterprise AI architectures&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Across all variants, the LFM2 report implicitly sketches what tomorrow‚Äôs enterprise AI stack will look like: &lt;b&gt;hybrid local-cloud orchestration&lt;/b&gt;, where small, fast models operating on devices handle time-critical perception, formatting, tool invocation, and judgment tasks, while larger models in the cloud offer heavyweight reasoning when needed.&lt;/p&gt;&lt;p&gt;Several trends converge here:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Cost control.&lt;/b&gt; Running routine inference locally avoids unpredictable cloud billing.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Latency determinism.&lt;/b&gt; TTFT and decode stability matter in agent workflows; on-device eliminates network jitter.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Governance and compliance.&lt;/b&gt; Local execution simplifies PII handling, data residency, and auditability.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Resilience.&lt;/b&gt; Agentic systems degrade gracefully if the cloud path becomes unavailable.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Enterprises adopting these architectures will likely treat small on-device models as the ‚Äúcontrol plane‚Äù of agentic workflows, with large cloud models serving as on-demand accelerators.&lt;/p&gt;&lt;p&gt;LFM2 is one of the clearest open-source foundations for that control layer to date.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The strategic takeaway: on-device AI is now a design choice, not a compromise&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For years, organizations building AI features have accepted that ‚Äúreal AI‚Äù requires cloud inference. LFM2 challenges that assumption. The models perform competitively across reasoning, instruction following, multilingual tasks, and RAG‚Äîwhile simultaneously achieving substantial latency gains over other open small-model families.&lt;/p&gt;&lt;p&gt;For CIOs and CTOs finalizing 2026 roadmaps, the implication is direct: &lt;b&gt;small, open, on-device models are now strong enough to carry meaningful slices of production workloads.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;LFM2 will not replace frontier cloud models for frontier-scale reasoning. But it offers something enterprises arguably need more: a reproducible, open, and operationally feasible foundation for &lt;b&gt;agentic systems that must run anywhere&lt;/b&gt;, from phones to industrial endpoints to air-gapped secure facilities.&lt;/p&gt;&lt;p&gt;In the broadening landscape of enterprise AI, LFM2 is less a research milestone and more a sign of architectural convergence. The future is not cloud or edge‚Äîit‚Äôs both, operating in concert. And releases like LFM2 provide the building blocks for organizations prepared to build that hybrid future intentionally rather than accidentally.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/mit-offshoot-liquid-ai-releases-blueprint-for-enterprise-grade-small-model</guid><pubDate>Mon, 01 Dec 2025 17:24:00 +0000</pubDate></item></channel></rss>