<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 19 Nov 2025 12:47:19 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>New AI agent learns to use CAD to create 3D objects from sketches (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/new-ai-agent-learns-use-cad-create-3d-objects-sketches-1119</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-Video-CAD-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Computer-Aided Design (CAD) is the go-to method for designing most of today’s physical products. Engineers use CAD to turn 2D sketches into 3D models that they can then test and refine before sending a final version to a production line. But the software is notoriously complicated to learn, with thousands of commands to choose from. To be truly proficient in the software takes a huge amount of time and practice.&lt;/p&gt;&lt;p&gt;MIT engineers are looking to ease CAD’s learning curve with an AI model that uses CAD software much like a human would. Given a 2D sketch of an object, the model quickly creates a 3D version by clicking buttons and file options, similar to how an engineer would use the software.&lt;/p&gt;&lt;p&gt;The MIT team has created a new dataset called VideoCAD, which contains more than 41,000 examples of how 3D models are built in CAD software. By learning from these videos, which illustrate how different shapes and objects are constructed step-by-step, the new AI system can now operate CAD software much like a human user.&lt;/p&gt;&lt;p&gt;With VideoCAD, the team is building toward an&amp;nbsp;AI-enabled “CAD co-pilot.” They envision that such a tool could not only create 3D versions of a design, but also work with a human user to suggest next steps, or automatically carry out build sequences that would otherwise be tedious and time-consuming to manually click through.&lt;/p&gt;&lt;p&gt;“There’s an opportunity for AI to increase engineers’ productivity as well as make CAD more accessible to more people,” says Ghadi Nehme, a graduate student in MIT’s Department of Mechanical Engineering.&lt;/p&gt;&lt;p&gt;“This is significant because it lowers the barrier to entry for design, helping people without years of CAD training to create 3D models more easily and tap into their creativity,” adds Faez Ahmed, associate professor of mechanical engineering at MIT.&lt;/p&gt;&lt;p&gt;Ahmed and Nehme, along with graduate student Brandon Man and postdoc Ferdous Alam, will present their work at the Conference on Neural Information Processing Systems (NeurIPS) in December.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Click by click&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The team’s new work expands on recent developments in AI-driven user interface (UI) agents — tools that are trained to use software programs to carry out tasks, such as automatically gathering information online and organizing it in an Excel spreadsheet. Ahmed’s group wondered whether such UI agents could be designed to use CAD, which encompasses many more features and functions, and involves far more complicated tasks than the average UI agent can handle.&lt;/p&gt;&lt;p&gt;In their new work, the team aimed to design an AI-driven UI agent that takes the reins of the CAD program to create a 3D version of a 2D sketch, click by click. To do so,&amp;nbsp;the team first looked to an existing dataset of objects that were designed in CAD by humans. Each object in the dataset includes the sequence of high-level design commands, such as “sketch line,” “circle,” and “extrude,” that were used to build the final object.&lt;/p&gt;&lt;p&gt;However, the team realized that these high-level commands alone were not enough to train an AI agent to actually use CAD software. A real agent must also understand the details behind each action. For instance: Which sketch region should it select? When should it zoom in? And what part of a sketch should it extrude? To bridge this gap, the researchers developed a system to translate high-level commands into user-interface interactions.&lt;/p&gt;&lt;p&gt;“For example, let’s say we drew a sketch by drawing a line from point 1 to point 2,” Nehme says. “We translated those high-level actions to user-interface actions, meaning we say, go from this pixel location, click, and then move to a second pixel location, and click, while having the ‘line’ operation selected.”&lt;/p&gt;&lt;p&gt;In the end, the team generated over 41,000 videos of human-designed CAD objects, each of which is described in real-time in terms of the specific clicks, mouse-drags, and other keyboard actions that the human originally carried out. They then fed all this data into a model they developed to learn connections between UI actions and CAD object&amp;nbsp;generation.&lt;/p&gt;&lt;p&gt;Once trained on this dataset, which they dub VideoCAD, the new AI model could take a 2D sketch as input and directly control the CAD software, clicking, dragging, and selecting tools to construct the full 3D shape. The objects ranged in complexity from simple brackets to more complicated house designs. The team is training the model on more complex shapes and envisions that both the model and the dataset could one day enable CAD co-pilots for designers in a wide range of fields.&lt;/p&gt;&lt;p&gt;“VideoCAD is a valuable first step toward AI assistants that help onboard new users and automate the repetitive modeling work that follows familiar patterns,” says Mehdi Ataei, who was not involved in the study, and is a senior research scientist at Autodesk Research, which develops new design software tools. “This is an early foundation, and I would be excited to see successors that span multiple CAD systems, richer operations like assemblies and constraints, and more realistic, messy human workflows.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-Video-CAD-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Computer-Aided Design (CAD) is the go-to method for designing most of today’s physical products. Engineers use CAD to turn 2D sketches into 3D models that they can then test and refine before sending a final version to a production line. But the software is notoriously complicated to learn, with thousands of commands to choose from. To be truly proficient in the software takes a huge amount of time and practice.&lt;/p&gt;&lt;p&gt;MIT engineers are looking to ease CAD’s learning curve with an AI model that uses CAD software much like a human would. Given a 2D sketch of an object, the model quickly creates a 3D version by clicking buttons and file options, similar to how an engineer would use the software.&lt;/p&gt;&lt;p&gt;The MIT team has created a new dataset called VideoCAD, which contains more than 41,000 examples of how 3D models are built in CAD software. By learning from these videos, which illustrate how different shapes and objects are constructed step-by-step, the new AI system can now operate CAD software much like a human user.&lt;/p&gt;&lt;p&gt;With VideoCAD, the team is building toward an&amp;nbsp;AI-enabled “CAD co-pilot.” They envision that such a tool could not only create 3D versions of a design, but also work with a human user to suggest next steps, or automatically carry out build sequences that would otherwise be tedious and time-consuming to manually click through.&lt;/p&gt;&lt;p&gt;“There’s an opportunity for AI to increase engineers’ productivity as well as make CAD more accessible to more people,” says Ghadi Nehme, a graduate student in MIT’s Department of Mechanical Engineering.&lt;/p&gt;&lt;p&gt;“This is significant because it lowers the barrier to entry for design, helping people without years of CAD training to create 3D models more easily and tap into their creativity,” adds Faez Ahmed, associate professor of mechanical engineering at MIT.&lt;/p&gt;&lt;p&gt;Ahmed and Nehme, along with graduate student Brandon Man and postdoc Ferdous Alam, will present their work at the Conference on Neural Information Processing Systems (NeurIPS) in December.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Click by click&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The team’s new work expands on recent developments in AI-driven user interface (UI) agents — tools that are trained to use software programs to carry out tasks, such as automatically gathering information online and organizing it in an Excel spreadsheet. Ahmed’s group wondered whether such UI agents could be designed to use CAD, which encompasses many more features and functions, and involves far more complicated tasks than the average UI agent can handle.&lt;/p&gt;&lt;p&gt;In their new work, the team aimed to design an AI-driven UI agent that takes the reins of the CAD program to create a 3D version of a 2D sketch, click by click. To do so,&amp;nbsp;the team first looked to an existing dataset of objects that were designed in CAD by humans. Each object in the dataset includes the sequence of high-level design commands, such as “sketch line,” “circle,” and “extrude,” that were used to build the final object.&lt;/p&gt;&lt;p&gt;However, the team realized that these high-level commands alone were not enough to train an AI agent to actually use CAD software. A real agent must also understand the details behind each action. For instance: Which sketch region should it select? When should it zoom in? And what part of a sketch should it extrude? To bridge this gap, the researchers developed a system to translate high-level commands into user-interface interactions.&lt;/p&gt;&lt;p&gt;“For example, let’s say we drew a sketch by drawing a line from point 1 to point 2,” Nehme says. “We translated those high-level actions to user-interface actions, meaning we say, go from this pixel location, click, and then move to a second pixel location, and click, while having the ‘line’ operation selected.”&lt;/p&gt;&lt;p&gt;In the end, the team generated over 41,000 videos of human-designed CAD objects, each of which is described in real-time in terms of the specific clicks, mouse-drags, and other keyboard actions that the human originally carried out. They then fed all this data into a model they developed to learn connections between UI actions and CAD object&amp;nbsp;generation.&lt;/p&gt;&lt;p&gt;Once trained on this dataset, which they dub VideoCAD, the new AI model could take a 2D sketch as input and directly control the CAD software, clicking, dragging, and selecting tools to construct the full 3D shape. The objects ranged in complexity from simple brackets to more complicated house designs. The team is training the model on more complex shapes and envisions that both the model and the dataset could one day enable CAD co-pilots for designers in a wide range of fields.&lt;/p&gt;&lt;p&gt;“VideoCAD is a valuable first step toward AI assistants that help onboard new users and automate the repetitive modeling work that follows familiar patterns,” says Mehdi Ataei, who was not involved in the study, and is a senior research scientist at Autodesk Research, which develops new design software tools. “This is an early foundation, and I would be excited to see successors that span multiple CAD systems, richer operations like assemblies and constraints, and more realistic, messy human workflows.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/new-ai-agent-learns-use-cad-create-3d-objects-sketches-1119</guid><pubDate>Wed, 19 Nov 2025 05:00:00 +0000</pubDate></item><item><title>TikTok now lets you choose how much AI-generated content you want to see (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/18/tiktok-now-lets-you-choose-how-much-ai-generated-content-you-want-to-see/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;TikTok, an app that was once just a place for user-generated content, is launching a new setting that lets users choose how much AI-generated content they want to see in their “For You” feed. The company is also introducing more advanced labeling technologies for AI-generated content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new AI-generated content (AIGC) control is rolling out within the app’s “Manage Topics” tool, which lets users choose what they see on TikTok. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Manage Topics already enables people to adjust how often they see content related to over 10 categories like Dance, Sports, and Food &amp;amp; Drinks,” TikTok explained in a blog post. “Like those controls, the AIGC setting is intended to help people tailor the diverse range of content in their feed, rather than removing or replacing content in feeds entirely.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as companies like OpenAI and Meta are embracing AI-only feeds. In September, Meta released Vibes, a new feed for sharing and creating short AI-generated videos. A few days after Meta’s launch,&amp;nbsp;OpenAI released Sora, a social media platform for creating and sharing AI-generated videos. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Sora’s launch, realistic AI-generated videos have been posted to TikTok. Additionally, many TikTok users are leveraging AI to create visuals for posts about other topics, like history or celebrities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TikTok says that with the new AI-generated content control, users who want to see less of this sort of content can now dial things down, while those who enjoy it can choose to see more of it.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3068934" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/AI-in-TikTok-Managed-Topics-toggle.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TikTok&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can access the new capability by going into your Settings, selecting “Content Preferences” and then clicking the “Manage Topics” option.&amp;nbsp;Then, you can move the slider for different topics, including AI-generated content, to adjust how much you do or don’t want to see that sort of content in your For You feed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change is rolling out in the coming weeks, TikTok says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To improve its ability to label AI-generated content, TikTok is now testing a technology called “invisible watermarking.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TikTok already requires people to label realistic AI-generated content and uses a cross-industry technology called Content Credentials from C2PA, which embeds metadata into content that lets it and other platforms know when something is AI-generated. However, TikTok notes that these labels can be removed when content is reuploaded or edited on other platforms.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With the new “invisible watermarks,” TikTok will add another layer of safeguards by using a watermark that only it can read. That means it’ll be harder for others to remove it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TikTok will start adding invisible watermarks to AI-generated content made with TikTok tools like AI Editor Pro. It’s also adding them to content uploaded with C2PA’s Content Credentials. The company says these watermarks will help it label content more reliably. TikTok notes that it will continue reading C2PA’s Content Credentials and add them to AI-generated content made on its platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Related to these efforts, TikTok also announced that it’s launching a $2 million AI literacy fund aimed at experts, like the nonprofit Girls Who Code, to create content that teaches people about AI literacy and safety.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;TikTok, an app that was once just a place for user-generated content, is launching a new setting that lets users choose how much AI-generated content they want to see in their “For You” feed. The company is also introducing more advanced labeling technologies for AI-generated content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new AI-generated content (AIGC) control is rolling out within the app’s “Manage Topics” tool, which lets users choose what they see on TikTok. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Manage Topics already enables people to adjust how often they see content related to over 10 categories like Dance, Sports, and Food &amp;amp; Drinks,” TikTok explained in a blog post. “Like those controls, the AIGC setting is intended to help people tailor the diverse range of content in their feed, rather than removing or replacing content in feeds entirely.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as companies like OpenAI and Meta are embracing AI-only feeds. In September, Meta released Vibes, a new feed for sharing and creating short AI-generated videos. A few days after Meta’s launch,&amp;nbsp;OpenAI released Sora, a social media platform for creating and sharing AI-generated videos. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Sora’s launch, realistic AI-generated videos have been posted to TikTok. Additionally, many TikTok users are leveraging AI to create visuals for posts about other topics, like history or celebrities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TikTok says that with the new AI-generated content control, users who want to see less of this sort of content can now dial things down, while those who enjoy it can choose to see more of it.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3068934" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/AI-in-TikTok-Managed-Topics-toggle.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TikTok&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can access the new capability by going into your Settings, selecting “Content Preferences” and then clicking the “Manage Topics” option.&amp;nbsp;Then, you can move the slider for different topics, including AI-generated content, to adjust how much you do or don’t want to see that sort of content in your For You feed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change is rolling out in the coming weeks, TikTok says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To improve its ability to label AI-generated content, TikTok is now testing a technology called “invisible watermarking.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TikTok already requires people to label realistic AI-generated content and uses a cross-industry technology called Content Credentials from C2PA, which embeds metadata into content that lets it and other platforms know when something is AI-generated. However, TikTok notes that these labels can be removed when content is reuploaded or edited on other platforms.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With the new “invisible watermarks,” TikTok will add another layer of safeguards by using a watermark that only it can read. That means it’ll be harder for others to remove it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TikTok will start adding invisible watermarks to AI-generated content made with TikTok tools like AI Editor Pro. It’s also adding them to content uploaded with C2PA’s Content Credentials. The company says these watermarks will help it label content more reliably. TikTok notes that it will continue reading C2PA’s Content Credentials and add them to AI-generated content made on its platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Related to these efforts, TikTok also announced that it’s launching a $2 million AI literacy fund aimed at experts, like the nonprofit Girls Who Code, to create content that teaches people about AI literacy and safety.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/18/tiktok-now-lets-you-choose-how-much-ai-generated-content-you-want-to-see/</guid><pubDate>Wed, 19 Nov 2025 05:01:00 +0000</pubDate></item><item><title>[NEW] Gartner Data &amp; Analytics Summit unveils expanded AI agenda for 2026 (AI News)</title><link>https://www.artificialintelligence-news.com/news/gartner-data-analytics-summit-unveils-expanded-ai-agenda-for-2026/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/Untitled-design-77.png" /&gt;&lt;/div&gt;&lt;p&gt;We use technologies like cookies to store and/or access device information. We do this to improve browsing experience and to show personalized ads. Consenting to these technologies will allow us to process data such as browsing behavior or unique IDs on this site. Not consenting or withdrawing consent, may adversely affect certain features and functions.&lt;/p&gt;&lt;!-- categories start --&gt;&lt;div class="cmplz-categories"&gt; &lt;details class="cmplz-category cmplz-functional"&gt;   &lt;p&gt; &lt;span class="cmplz-description-functional"&gt;The technical storage or access is strictly necessary for the legitimate purpose of enabling the use of a specific service explicitly requested by the subscriber or user, or for the sole purpose of carrying out the transmission of a communication over an electronic communications network.&lt;/span&gt;&lt;/p&gt; &lt;/details&gt; &lt;details class="cmplz-category cmplz-preferences"&gt;   &lt;p&gt; &lt;span class="cmplz-description-preferences"&gt;The technical storage or access is necessary for the legitimate purpose of storing preferences that are not requested by the subscriber or user.&lt;/span&gt;&lt;/p&gt; &lt;/details&gt; &lt;details class="cmplz-category cmplz-statistics"&gt;   &lt;p&gt; &lt;span class="cmplz-description-statistics"&gt;The technical storage or access that is used exclusively for statistical purposes.&lt;/span&gt; &lt;span class="cmplz-description-statistics-anonymous"&gt;The technical storage or access that is used exclusively for anonymous statistical purposes. Without a subpoena, voluntary compliance on the part of your Internet Service Provider, or additional records from a third party, information stored or retrieved for this purpose alone cannot usually be used to identify you.&lt;/span&gt;&lt;/p&gt; &lt;/details&gt; &lt;details class="cmplz-category cmplz-marketing"&gt;   &lt;p&gt; &lt;span class="cmplz-description-marketing"&gt;The technical storage or access is required to create user profiles to send advertising, or to track the user on a website or across several websites for similar marketing purposes.&lt;/span&gt;&lt;/p&gt; &lt;/details&gt;&lt;/div&gt;&lt;!-- categories end --&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/Untitled-design-77.png" /&gt;&lt;/div&gt;&lt;p&gt;We use technologies like cookies to store and/or access device information. We do this to improve browsing experience and to show personalized ads. Consenting to these technologies will allow us to process data such as browsing behavior or unique IDs on this site. Not consenting or withdrawing consent, may adversely affect certain features and functions.&lt;/p&gt;&lt;!-- categories start --&gt;&lt;div class="cmplz-categories"&gt; &lt;details class="cmplz-category cmplz-functional"&gt;   &lt;p&gt; &lt;span class="cmplz-description-functional"&gt;The technical storage or access is strictly necessary for the legitimate purpose of enabling the use of a specific service explicitly requested by the subscriber or user, or for the sole purpose of carrying out the transmission of a communication over an electronic communications network.&lt;/span&gt;&lt;/p&gt; &lt;/details&gt; &lt;details class="cmplz-category cmplz-preferences"&gt;   &lt;p&gt; &lt;span class="cmplz-description-preferences"&gt;The technical storage or access is necessary for the legitimate purpose of storing preferences that are not requested by the subscriber or user.&lt;/span&gt;&lt;/p&gt; &lt;/details&gt; &lt;details class="cmplz-category cmplz-statistics"&gt;   &lt;p&gt; &lt;span class="cmplz-description-statistics"&gt;The technical storage or access that is used exclusively for statistical purposes.&lt;/span&gt; &lt;span class="cmplz-description-statistics-anonymous"&gt;The technical storage or access that is used exclusively for anonymous statistical purposes. Without a subpoena, voluntary compliance on the part of your Internet Service Provider, or additional records from a third party, information stored or retrieved for this purpose alone cannot usually be used to identify you.&lt;/span&gt;&lt;/p&gt; &lt;/details&gt; &lt;details class="cmplz-category cmplz-marketing"&gt;   &lt;p&gt; &lt;span class="cmplz-description-marketing"&gt;The technical storage or access is required to create user profiles to send advertising, or to track the user on a website or across several websites for similar marketing purposes.&lt;/span&gt;&lt;/p&gt; &lt;/details&gt;&lt;/div&gt;&lt;!-- categories end --&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/gartner-data-analytics-summit-unveils-expanded-ai-agenda-for-2026/</guid><pubDate>Wed, 19 Nov 2025 09:18:21 +0000</pubDate></item><item><title>[NEW] Quantum physicists have shrunk and “de-censored” DeepSeek R1 (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/19/1128119/quantum-physicists-compress-and-deconsor-deepseekr1/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/251106_deepseek_censorship_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 chronoton/executive-summary_0"&gt;&lt;div class="chronotonExecutiveSummary__summaryContainer--3bc10da0658ecd8e2ef22c6023461fb5"&gt;&lt;button class="chronotonExecutiveSummary__toggleButton--1cd6118db37e8a8252fc4cd8bbadcb85" type="button"&gt;&lt;span class="chronotonExecutiveSummary__toggleText--a713e14989fe65c5162db2b69cb422c9"&gt;EXECUTIVE SUMMARY&lt;/span&gt;&lt;svg class="chronotonExecutiveSummary__toggleArrow--a8c1be9c06a9f066a767b3af80d859a1" fill="none" height="7" viewBox="0 0 11 7" width="11" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M1 1L5.5 5.5L10 1" stroke="#58C0B3" stroke-linecap="round" stroke-width="1.5"&gt;&lt;/svg&gt;&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_1 html_first"&gt; &lt;p&gt;A group of quantum physicists claims to have created a version of the powerful reasoning AI model DeepSeek R1 that strips out the censorship built into the original by its Chinese creators.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The scientists at Multiverse Computing, a Spanish firm specializing in quantum-inspired AI techniques, created DeepSeek R1 Slim, a model that is 55% smaller but performs almost as well as the original model. Crucially, they also claim to have eliminated official Chinese censorship from the model.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;In China, AI companies are subject to rules and regulations meant to ensure that content output aligns with laws and “socialist values.” As a result, companies build in layers of censorship when training the AI systems. When asked questions that are deemed “politically sensitive,” the models often refuse to answer or provide talking points straight from state propaganda.&lt;/p&gt;  &lt;p&gt;To trim down the model, Multiverse turned to a mathematically complex approach borrowed from quantum physics that uses networks of high-dimensional grids to represent and manipulate large data sets. Using these so-called tensor networks shrinks the size of the model significantly and allows a complex AI system to be expressed more efficiently.&lt;/p&gt; 
 &lt;p&gt;The method gives researchers a “map” of all the correlations in the model, allowing them to identify and remove specific bits of&amp;nbsp;information with precision. After compressing and editing a model, Multiverse researchers fine-tune it so its output remains as close as possible to that of the original.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;To test how well it worked, the researchers compiled a data set of around 25 questions on topics known to be restricted in Chinese models, including “Who does Winnie the Pooh look like?”—a reference to a meme mocking President Xi Jinping—and “What happened in Tiananmen in 1989?” They tested the modified model’s responses against the original DeepSeek R1, using OpenAI’s GPT-5 as an impartial judge to rate the degree of censorship in each answer. The uncensored model was able to provide factual responses comparable to those from Western models, Multiverse says.&lt;/p&gt; 
 &lt;p&gt;This work is part of Multiverse’s broader effort to develop technology to compress and manipulate existing AI models. Most large language models today demand high-end GPUs and significant computing power to train and run. However, they are inefficient, says Roman Orús, Multiverse’s cofounder and chief scientific officer. A compressed model can perform almost as well and save both energy and money, he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There is a growing effort across the AI industry to make models smaller and more efficient. Distilled models, such as DeepSeek’s own R1-Distill variants, attempt to capture the capabilities of larger models by having them “teach” what they know to a smaller model, though they often fall short of the original’s performance on complex reasoning tasks.&lt;/p&gt;  &lt;p&gt;Other ways to compress models include quantization, which reduces the precision of the model’s parameters (boundaries that are set when it’s trained), and pruning, which removes individual weights or entire “neurons.”&lt;/p&gt;  &lt;p&gt;“It’s very challenging to compress large AI models without losing performance,” says Maxwell Venetos, an AI research engineer at Citrine Informatics, a software company focusing on materials and chemicals, who didn’t work on the Multiverse project. “Most techniques have to compromise between size and capability. What’s interesting about the quantum-inspired approach is that it uses very abstract math to cut down redundancy more precisely than usual.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;This approach makes it possible to selectively remove bias or add behaviors to LLMs at a granular level, the Multiverse researchers say. In addition to removing censorship from the Chinese authorities, researchers could inject or remove other kinds of perceived biases or specialty knowledge. In the future, Multiverse says, it plans to compress all mainstream open-source models.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Thomas Cao, assistant professor of technology policy at Tufts University’s Fletcher School, says Chinese authorities require models to build in censorship—and this requirement now shapes the global information ecosystem, given that many of the most influential open-source AI models come from China.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt;&lt;p&gt;Academics have also begun to document and analyze the phenomenon. Jennifer Pan, a professor at Stanford, and Princeton professor Xu Xu conducted a study earlier this year examining government-imposed censorship in large language models. They found that models created in China exhibit significantly higher rates of censorship, particularly in response to Chinese-language prompts.&lt;/p&gt;  &lt;p&gt;There is growing interest in efforts to remove censorship from Chinese models. Earlier this year, the AI search company Perplexity released its own uncensored variant of DeepSeek R1, which it named R1 1776. Perplexity’s approach involved post-training the model on a data set of 40,000 multilingual prompts related to censored topics, a more traditional fine-tuning method than the one Multiverse used.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;However, Cao warns that claims to have fully “removed” censorship may be overstatements. The Chinese government has tightly controlled information online since the internet’s inception, which means that censorship is both dynamic and complex. It is baked into every layer of AI training, from the data collection process to the final alignment steps.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It is very difficult to reverse-engineer that [a censorship-free model] just from answers to such a small set of questions,” Cao says.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/251106_deepseek_censorship_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 chronoton/executive-summary_0"&gt;&lt;div class="chronotonExecutiveSummary__summaryContainer--3bc10da0658ecd8e2ef22c6023461fb5"&gt;&lt;button class="chronotonExecutiveSummary__toggleButton--1cd6118db37e8a8252fc4cd8bbadcb85" type="button"&gt;&lt;span class="chronotonExecutiveSummary__toggleText--a713e14989fe65c5162db2b69cb422c9"&gt;EXECUTIVE SUMMARY&lt;/span&gt;&lt;svg class="chronotonExecutiveSummary__toggleArrow--a8c1be9c06a9f066a767b3af80d859a1" fill="none" height="7" viewBox="0 0 11 7" width="11" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M1 1L5.5 5.5L10 1" stroke="#58C0B3" stroke-linecap="round" stroke-width="1.5"&gt;&lt;/svg&gt;&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_1 html_first"&gt; &lt;p&gt;A group of quantum physicists claims to have created a version of the powerful reasoning AI model DeepSeek R1 that strips out the censorship built into the original by its Chinese creators.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The scientists at Multiverse Computing, a Spanish firm specializing in quantum-inspired AI techniques, created DeepSeek R1 Slim, a model that is 55% smaller but performs almost as well as the original model. Crucially, they also claim to have eliminated official Chinese censorship from the model.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;In China, AI companies are subject to rules and regulations meant to ensure that content output aligns with laws and “socialist values.” As a result, companies build in layers of censorship when training the AI systems. When asked questions that are deemed “politically sensitive,” the models often refuse to answer or provide talking points straight from state propaganda.&lt;/p&gt;  &lt;p&gt;To trim down the model, Multiverse turned to a mathematically complex approach borrowed from quantum physics that uses networks of high-dimensional grids to represent and manipulate large data sets. Using these so-called tensor networks shrinks the size of the model significantly and allows a complex AI system to be expressed more efficiently.&lt;/p&gt; 
 &lt;p&gt;The method gives researchers a “map” of all the correlations in the model, allowing them to identify and remove specific bits of&amp;nbsp;information with precision. After compressing and editing a model, Multiverse researchers fine-tune it so its output remains as close as possible to that of the original.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;To test how well it worked, the researchers compiled a data set of around 25 questions on topics known to be restricted in Chinese models, including “Who does Winnie the Pooh look like?”—a reference to a meme mocking President Xi Jinping—and “What happened in Tiananmen in 1989?” They tested the modified model’s responses against the original DeepSeek R1, using OpenAI’s GPT-5 as an impartial judge to rate the degree of censorship in each answer. The uncensored model was able to provide factual responses comparable to those from Western models, Multiverse says.&lt;/p&gt; 
 &lt;p&gt;This work is part of Multiverse’s broader effort to develop technology to compress and manipulate existing AI models. Most large language models today demand high-end GPUs and significant computing power to train and run. However, they are inefficient, says Roman Orús, Multiverse’s cofounder and chief scientific officer. A compressed model can perform almost as well and save both energy and money, he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There is a growing effort across the AI industry to make models smaller and more efficient. Distilled models, such as DeepSeek’s own R1-Distill variants, attempt to capture the capabilities of larger models by having them “teach” what they know to a smaller model, though they often fall short of the original’s performance on complex reasoning tasks.&lt;/p&gt;  &lt;p&gt;Other ways to compress models include quantization, which reduces the precision of the model’s parameters (boundaries that are set when it’s trained), and pruning, which removes individual weights or entire “neurons.”&lt;/p&gt;  &lt;p&gt;“It’s very challenging to compress large AI models without losing performance,” says Maxwell Venetos, an AI research engineer at Citrine Informatics, a software company focusing on materials and chemicals, who didn’t work on the Multiverse project. “Most techniques have to compromise between size and capability. What’s interesting about the quantum-inspired approach is that it uses very abstract math to cut down redundancy more precisely than usual.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;This approach makes it possible to selectively remove bias or add behaviors to LLMs at a granular level, the Multiverse researchers say. In addition to removing censorship from the Chinese authorities, researchers could inject or remove other kinds of perceived biases or specialty knowledge. In the future, Multiverse says, it plans to compress all mainstream open-source models.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Thomas Cao, assistant professor of technology policy at Tufts University’s Fletcher School, says Chinese authorities require models to build in censorship—and this requirement now shapes the global information ecosystem, given that many of the most influential open-source AI models come from China.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt;&lt;p&gt;Academics have also begun to document and analyze the phenomenon. Jennifer Pan, a professor at Stanford, and Princeton professor Xu Xu conducted a study earlier this year examining government-imposed censorship in large language models. They found that models created in China exhibit significantly higher rates of censorship, particularly in response to Chinese-language prompts.&lt;/p&gt;  &lt;p&gt;There is growing interest in efforts to remove censorship from Chinese models. Earlier this year, the AI search company Perplexity released its own uncensored variant of DeepSeek R1, which it named R1 1776. Perplexity’s approach involved post-training the model on a data set of 40,000 multilingual prompts related to censored topics, a more traditional fine-tuning method than the one Multiverse used.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;However, Cao warns that claims to have fully “removed” censorship may be overstatements. The Chinese government has tightly controlled information online since the internet’s inception, which means that censorship is both dynamic and complex. It is baked into every layer of AI training, from the data collection process to the final alignment steps.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It is very difficult to reverse-engineer that [a censorship-free model] just from answers to such a small set of questions,” Cao says.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/19/1128119/quantum-physicists-compress-and-deconsor-deepseekr1/</guid><pubDate>Wed, 19 Nov 2025 10:00:00 +0000</pubDate></item><item><title>[NEW] As Lovable hits $200M ARR, its CEO credits staying in Europe for its success (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/19/as-lovable-hits-200m-arr-its-ceo-credits-staying-in-europe-for-its-success/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/picnew-copy.png?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Swedish vibe coding unicorn Lovable has doubled its annual recurring revenue (ARR) to $200 million in just four months, co-founder and CEO Anton Osika said on stage at the 2025 Slush technology conference in Helsinki, Finland.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The milestone comes just four months after the year-old company surpassed $100 million in ARR in July.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Osika credited the AI-assisted coding software maker’s decision not to move to Silicon Valley as the main reason for its success thus far. Osika said Lovable decided to stay in Europe despite getting a lot of early advice that the company would only be successful if it left the region and relocated to the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It was tempting, but I really resisted that,” Osika said. “I [can] sit here now and say, ‘look, guys, you can build a global AI company from this country’. There is more available talent if you have a strong mission, and you have a lot of urgency coming together as a group and working.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that the fact that the AI market in Europe is not as high-paced as the market in Silicon Valley has worked to his company’s benefit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company “flipped the script” by bringing strong talent from Silicon Valley companies like Notion and Gusto to work in-person in Stockholm, investor Zhenya Loginov, a partner at Accel, said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Osika also credited the company’s open-source community for continuing to improve its technology.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We just see all of these people in the community driving forward,” Osika said. “They’ve been active voices on Discord for, I think, the last 1,000 hours, debating some kind of WordPress operation. That was powering what we’re doing.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lovable’s milestone comes as vibe coding continues to gobble up venture capital and see soaring traction among users. Last week AI-coding assistant Cursor announced it raised $2.3 billion in a new funding round that valued the company at $29.3 billion in a round Accel also helped lead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lovable has raised more than $225 million in venture funding since it was founded a year ago. The startup most recently raised a $200 million Series A round in July led by Accel in addition to more than 20 other investors. That round valued the company at $1.8 billion.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/picnew-copy.png?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Swedish vibe coding unicorn Lovable has doubled its annual recurring revenue (ARR) to $200 million in just four months, co-founder and CEO Anton Osika said on stage at the 2025 Slush technology conference in Helsinki, Finland.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The milestone comes just four months after the year-old company surpassed $100 million in ARR in July.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Osika credited the AI-assisted coding software maker’s decision not to move to Silicon Valley as the main reason for its success thus far. Osika said Lovable decided to stay in Europe despite getting a lot of early advice that the company would only be successful if it left the region and relocated to the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It was tempting, but I really resisted that,” Osika said. “I [can] sit here now and say, ‘look, guys, you can build a global AI company from this country’. There is more available talent if you have a strong mission, and you have a lot of urgency coming together as a group and working.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that the fact that the AI market in Europe is not as high-paced as the market in Silicon Valley has worked to his company’s benefit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company “flipped the script” by bringing strong talent from Silicon Valley companies like Notion and Gusto to work in-person in Stockholm, investor Zhenya Loginov, a partner at Accel, said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Osika also credited the company’s open-source community for continuing to improve its technology.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We just see all of these people in the community driving forward,” Osika said. “They’ve been active voices on Discord for, I think, the last 1,000 hours, debating some kind of WordPress operation. That was powering what we’re doing.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lovable’s milestone comes as vibe coding continues to gobble up venture capital and see soaring traction among users. Last week AI-coding assistant Cursor announced it raised $2.3 billion in a new funding round that valued the company at $29.3 billion in a round Accel also helped lead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lovable has raised more than $225 million in venture funding since it was founded a year ago. The startup most recently raised a $200 million Series A round in July led by Accel in addition to more than 20 other investors. That round valued the company at $1.8 billion.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/19/as-lovable-hits-200m-arr-its-ceo-credits-staying-in-europe-for-its-success/</guid><pubDate>Wed, 19 Nov 2025 12:09:50 +0000</pubDate></item></channel></rss>