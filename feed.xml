<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 31 Jul 2025 12:50:19 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>Kleiner Perkins-backed Ambiq pops on IPO debut (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/30/kleiner-perkins-backed-ambiq-pops-on-ipo-debut/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2227082921.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Ambiq Micro, a 15-year-old manufacturer of energy-efficient chips for wearable and medical devices, closed its first day of trading on Wednesday at $38.53 a share, a 61% increase from the $24 IPO price the company set the previous day.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The success of the IPO signals strong investor demand in the public market for new small-cap companies benefiting from AI innovation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ambiq closed its first day as a public company with a valuation of $656 million (excluding employee options). This represents a significant increase from its last private funding valuation of $450 million in 2023, according to PitchBook.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ambiq has pitched itself as well-positioned to capitalize on the growth driven by AI. “Because we’re so low energy, we can put more intelligence and more AI on board” of edge processors, the company’s CTO Scott Hanson told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the three months that ended March 31, Ambiq posted a net loss of $8.3 million against revenues of $15.7 million, the company’s S1 filing shows. The Q1 results mark a slight improvement from the first quarter of 2024, when the company reported a $9.8 million loss on $15.2 million in revenue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kleiner Perkins and EDB Investments, a Singaporean state-backed entity, are the largest outside backers of Ambiq, according to the filing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wen Hsieh, who was a general partner at Kleiner Perkins until 2023, first backed Ambiq when the company raised its Series C in 2014. Hsieh also invested in Ambiq after he launched his own venture firm, Matter Venture Partners, two years ago.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2227082921.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Ambiq Micro, a 15-year-old manufacturer of energy-efficient chips for wearable and medical devices, closed its first day of trading on Wednesday at $38.53 a share, a 61% increase from the $24 IPO price the company set the previous day.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The success of the IPO signals strong investor demand in the public market for new small-cap companies benefiting from AI innovation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ambiq closed its first day as a public company with a valuation of $656 million (excluding employee options). This represents a significant increase from its last private funding valuation of $450 million in 2023, according to PitchBook.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ambiq has pitched itself as well-positioned to capitalize on the growth driven by AI. “Because we’re so low energy, we can put more intelligence and more AI on board” of edge processors, the company’s CTO Scott Hanson told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the three months that ended March 31, Ambiq posted a net loss of $8.3 million against revenues of $15.7 million, the company’s S1 filing shows. The Q1 results mark a slight improvement from the first quarter of 2024, when the company reported a $9.8 million loss on $15.2 million in revenue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kleiner Perkins and EDB Investments, a Singaporean state-backed entity, are the largest outside backers of Ambiq, according to the filing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wen Hsieh, who was a general partner at Kleiner Perkins until 2023, first backed Ambiq when the company raised its Series C in 2014. Hsieh also invested in Ambiq after he launched his own venture firm, Matter Venture Partners, two years ago.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/30/kleiner-perkins-backed-ambiq-pops-on-ipo-debut/</guid><pubDate>Thu, 31 Jul 2025 01:00:24 +0000</pubDate></item><item><title>GitHub Copilot crosses 20 million all-time users (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/30/github-copilot-crosses-20-million-all-time-users/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1785159335.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;GitHub Copilot, an AI coding tool offered by Microsoft-owned GitHub, has now reached more than 20 million users, Microsoft CEO Satya Nadella said on the company’s earnings call Wednesday. A GitHub spokesperson confirmed to TechCrunch that this number represents “all-time users.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That means five million people have tried out GitHub Copilot for the first time in the last three months; the company reported in April the tool had reached 15 million users. Microsoft and GitHub don’t report how many of these 20 million people have continued to use the AI coding tool on a monthly or daily basis — though those metrics are likely far lower.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft also reported GitHub Copilot, which is among the most popular AI coding tools offered today, is used by 90% of the Fortune 100. The product’s growth among enterprise customers has also grown about 75% compared to last quarter, according to the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI coding tools are rising in popularity, and they seem to be one of the few AI products generating notable revenue. In 2024, Nadella said GitHub Copilot was a larger business than all of GitHub was when Microsoft acquired it in 2018. In the year since, it seems GitHub Copilot’s growth rate has continued in a positive direction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The world’s most popular AI coding tools still have tiny user bases compared to AI chatbots like ChatGPT and Gemini, which attract hundreds of millions of users every month. Of course, software engineering is more niche than the general informational queries offered by AI chatbots. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, software engineers and their employers seem to be willing to pay a premium for AI coding tools. And with Microsoft’s long list of enterprise customers and GitHub’s ecosystem of developers, GitHub Copilot is well positioned to dominate the market for enterprise AI coding tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor, another popular AI coding tool, wants to challenge GitHub Copilot in the enterprise, and it’s been scooping up talent from fledgling AI startups to do so. Cursor reportedly had more than a million people using its product every day in March, according to Bloomberg. At that time, the company generated about $200 million in annualized recurring revenue. Today, Cursor’s ARR is more than $500 million, suggesting there are now a lot more people using its products everyday.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;While GitHub Copilot and Cursor initially sought to tackle different parts of the developer experience, they’re steadily converging into similar products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both companies have recently introduced AI agents to review code and catch bugs introduced by humans. Github and Cursor are also both trying to create AI agents that automate programmer workflows, allowing developers to offload tasks altogether. Nadella said during Wednesday’s earnings call that GitHub was seeing great momentum with their AI coding agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond Cursor, GitHub has an array of well-capitalized competitors that would like to sell AI coding tools to the enterprise. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There’s Google — which acquired the leaders of AI coding startup Windsurf — as well as Cognition, the maker of Devin that subsequently acquired the rest of Windsurf’s team. That’s not to mention OpenAI and Anthropic, which are both building out their own AI coding offerings powered by in-house AI models, Codex and Claude Code respectively, in an attempt to win the market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The nascent space is quickly heating up into one of AI’s most competitive markets. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1785159335.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;GitHub Copilot, an AI coding tool offered by Microsoft-owned GitHub, has now reached more than 20 million users, Microsoft CEO Satya Nadella said on the company’s earnings call Wednesday. A GitHub spokesperson confirmed to TechCrunch that this number represents “all-time users.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That means five million people have tried out GitHub Copilot for the first time in the last three months; the company reported in April the tool had reached 15 million users. Microsoft and GitHub don’t report how many of these 20 million people have continued to use the AI coding tool on a monthly or daily basis — though those metrics are likely far lower.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft also reported GitHub Copilot, which is among the most popular AI coding tools offered today, is used by 90% of the Fortune 100. The product’s growth among enterprise customers has also grown about 75% compared to last quarter, according to the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI coding tools are rising in popularity, and they seem to be one of the few AI products generating notable revenue. In 2024, Nadella said GitHub Copilot was a larger business than all of GitHub was when Microsoft acquired it in 2018. In the year since, it seems GitHub Copilot’s growth rate has continued in a positive direction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The world’s most popular AI coding tools still have tiny user bases compared to AI chatbots like ChatGPT and Gemini, which attract hundreds of millions of users every month. Of course, software engineering is more niche than the general informational queries offered by AI chatbots. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, software engineers and their employers seem to be willing to pay a premium for AI coding tools. And with Microsoft’s long list of enterprise customers and GitHub’s ecosystem of developers, GitHub Copilot is well positioned to dominate the market for enterprise AI coding tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor, another popular AI coding tool, wants to challenge GitHub Copilot in the enterprise, and it’s been scooping up talent from fledgling AI startups to do so. Cursor reportedly had more than a million people using its product every day in March, according to Bloomberg. At that time, the company generated about $200 million in annualized recurring revenue. Today, Cursor’s ARR is more than $500 million, suggesting there are now a lot more people using its products everyday.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;While GitHub Copilot and Cursor initially sought to tackle different parts of the developer experience, they’re steadily converging into similar products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both companies have recently introduced AI agents to review code and catch bugs introduced by humans. Github and Cursor are also both trying to create AI agents that automate programmer workflows, allowing developers to offload tasks altogether. Nadella said during Wednesday’s earnings call that GitHub was seeing great momentum with their AI coding agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond Cursor, GitHub has an array of well-capitalized competitors that would like to sell AI coding tools to the enterprise. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There’s Google — which acquired the leaders of AI coding startup Windsurf — as well as Cognition, the maker of Devin that subsequently acquired the rest of Windsurf’s team. That’s not to mention OpenAI and Anthropic, which are both building out their own AI coding offerings powered by in-house AI models, Codex and Claude Code respectively, in an attempt to win the market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The nascent space is quickly heating up into one of AI’s most competitive markets. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/30/github-copilot-crosses-20-million-all-time-users/</guid><pubDate>Thu, 31 Jul 2025 01:16:55 +0000</pubDate></item><item><title>[NEW] The two people shaping the future of OpenAI’s research (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/31/1120885/the-two-people-shaping-the-future-of-openais-research/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/250724-AI-hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;For the past couple of years, OpenAI has felt like a one-man brand. With his showbiz style and fundraising glitz, CEO Sam Altman overshadows all other big names on the firm’s roster. Even his bungled ouster ended with him back on top—and more famous than ever. But look past the charismatic frontman and you get a clearer sense of where this company is going. After all, Altman is not the one building the technology on which its reputation rests.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That responsibility falls to OpenAI’s twin heads of research—chief research officer Mark Chen and chief scientist Jakub Pachocki. Between them, they share the role of making sure OpenAI stays one step ahead of powerhouse rivals like Google.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;I sat down with Chen and Pachocki for an exclusive conversation during a recent trip the pair made to London, where OpenAI set up its first international office in 2023. We talked about how they manage the inherent tension between research and product. We also talked about why they think coding and math are the keys to more capable all-purpose models; what they really mean when they talk about AGI; and what happened to OpenAI’s superalignment team, set up by the firm’s cofounder and former chief scientist Ilya Sutskever to prevent a hypothetical superintelligence from going rogue, which disbanded soon after he quit.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In particular, I wanted to get a sense of where their heads are at in the run-up to OpenAI’s biggest product release in months: GPT-5.&lt;/p&gt; 
 &lt;p&gt;Reports are out that the firm’s next-generation model will be launched in August. OpenAI’s official line—well, Altman’s—is that it will release GPT-5 “soon.” Anticipation is high. The leaps OpenAI made with GPT-3 and then GPT-4 raised the bar of what was thought possible with this technology. And yet delays to the launch of GPT-5 have fueled rumors that OpenAI has struggled to build a model that meets its own—not to mention everyone else’s—expectations.&lt;/p&gt;  &lt;p&gt;Altman has been uncharacteristically modest: “[GPT-5] is an experimental model that incorporates new research techniques we will use in future models,” he posted on X—which makes it sound more like a work in progress than another horizon-shifting release.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But expectation management is part of the job for a company that for the last several years has set the agenda for the industry. And Chen and Pachocki set the agenda inside OpenAI.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Twin peaks&lt;/strong&gt;&amp;nbsp;&lt;/h3&gt;  &lt;p&gt;The firm’s main London office is in St James’s Park, a few hundred meters east of Buckingham Palace. But I met Chen and Pachocki in a conference room in a coworking space near King’s Cross, which OpenAI keeps as a kind of pied-à-terre in the heart of London’s tech neighborhood (Google DeepMind and Meta are just around the corner). OpenAI’s head of research communications, Laurance Fauconnet, sat with an open laptop at the end of the table.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Chen, who was wearing a maroon polo shirt, is clean-cut, almost preppy. He’s media trained and comfortable talking to a reporter. (That’s him flirting with a chatbot in the “Introducing GPT-4o” video.) Pachocki, in a black elephant-logo tee, has more of a TV-movie hacker look. He stares at his hands a lot when he speaks.&lt;/p&gt;  &lt;p&gt;But the pair are a tighter double act than they first appear. Pachocki summed up their roles. Chen shapes and manages the research teams, he said. “I am responsible for setting the research roadmap and establishing our long-term technical vision.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;“But there’s fluidity in the roles,” Chen said. “We’re both researchers, we pull on technical threads. Whatever we see that we can pull on and fix, that’s what we do.”&lt;/p&gt;  &lt;p&gt;Chen joined the company in 2018 after working as a quantitative trader at the Wall Street firm Jane Street Capital, where he developed machine-learning models for futures trading. At OpenAI he spearheaded the creation of DALL-E, the firm’s breakthrough generative image model. He then worked on adding image recognition to GPT‑4 and led the development of Codex, the generative coding model that powers GitHub Copilot.&lt;/p&gt;  &lt;p&gt;Pachocki left an academic career in theoretical computer science to join OpenAI in 2017 and replaced Sutskever as chief scientist in 2024. He is the key architect of OpenAI’s so-called reasoning models—especially o1 and o3—which are designed to tackle complex tasks in science, math, and coding.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When we met they were buzzing, fresh off the high of two new back-to-back wins for their company’s technology.&lt;/p&gt; 

 &lt;p&gt;On July 16, one of OpenAI’s large language models came in second in the AtCoder World Tour Finals, one of the world’s most hardcore programming competitions. On July 19, OpenAI announced that one of its models had achieved gold-medal-level results on the 2025 International Math Olympiad, one of the world’s most prestigious math contests.&lt;/p&gt;  &lt;p&gt;The math result made headlines, not only because of OpenAI’s remarkable achievement, but because rival Google DeepMind revealed two days later that one of its models had achieved the same score in the same competition. Google DeepMind had played by the competition’s rules and waited for its results to be checked by the organizers before making an announcement; OpenAI had in effect marked its own answers.&lt;/p&gt;  &lt;p&gt;For Chen and Pachocki, the result speaks for itself. Anyway, it’s the programming win they’re most excited about. “I think that’s quite underrated,” Chen told me. A gold medal result in the International Math Olympiad puts you somewhere in the top 20 to 50 competitors, he said. But in the AtCoder contest OpenAI’s model placed in the top two: “To break into a really different tier of human performance—that’s unprecedented.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Ship, ship, ship!&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;People at OpenAI still like to say they work at a research lab. But the company is very different from the one it was before the release of ChatGPT three years ago. The firm is now in a race with the biggest and richest technology companies in the world and valued at $300 billion. Envelope-pushing research and eye-catching demos no longer cut it. It needs to ship products and get them into people’s hands—and boy, it does.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;OpenAI has kept up a run of new releases—putting out major updates to its GPT-4 series, launching a string of generative image and video models, and introducing the ability to talk to ChatGPT with your voice. Six months ago it kicked off a new wave of so-called reasoning models with its o1 release, soon followed by o3. And last week it released its browser-using agent Operator to the public. It now claims that more than 400 million people use its products every week and submit 2.5 billion prompts a day.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;OpenAI’s incoming CEO of applications, Fidji Simo, plans to keep up the momentum. In a memo to the company, she told employees she is looking forward to “helping get OpenAI’s technologies into the hands of more people around the world,” where they will “unlock more opportunities for more people than any other technology in history.” Expect the products to keep coming.&lt;/p&gt;  &lt;p&gt;I asked how OpenAI juggles open-ended research and product development. “This is something we have been thinking about for a very long time, long before ChatGPT,” Pachocki said. “If we are actually serious about trying to build artificial general intelligence, clearly there will be so much that you can do with this technology along the way, so many tangents you can go down that will be big products.” In other words, keep shaking the tree and harvest what you can.&lt;/p&gt;  &lt;p&gt;A talking point that comes up with OpenAI folks is that putting experimental models out into the world was a necessary part of research. The goal was to make people aware of how good this technology had become. “We want to educate people about what’s coming so that we can participate in what will be a very hard societal conversation,” Altman told me back in 2022. The makers of this strange new technology were also curious what it might be for: OpenAI was keen to get it into people’s hands to see what they would do with it.&lt;/p&gt; 
 &lt;p&gt;Is that still the case? They answered at the same time. “Yeah!” Chen said. “To some extent,” Pachocki said. Chen laughed: “No, go ahead.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“I wouldn’t say research iterates on product,” said Pachocki. “But now that models are at the edge of the capabilities that can be measured by classical benchmarks and a lot of the long-standing challenges that we’ve been thinking about are starting to fall, we’re at the point where it really is about what the models can do in the real world.”&lt;/p&gt; 
 &lt;p&gt;Like taking on humans in coding competitions. The person who beat OpenAI’s model at this year’s AtCoder contest, held in Japan, was a programmer named Przemysław Dębiak, also known as Psyho. The contest was a puzzle-solving marathon in which competitors had 10 hours to find the most efficient way to solve a complex coding problem. After his win, Psyho posted on X: “I’m completely exhausted ... I’m barely alive.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Chen and Pachocki have strong ties to the world of competitive coding. Both have competed in international coding contests in the past and Chen coaches the USA Computing Olympiad team. I asked whether that personal enthusiasm for competitive coding colors their sense of how big a deal it is for a model to perform well at such a challenge.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;They both laughed. “Definitely,” said Pachocki. “So: Psyho is kind of a legend. He’s been the number one competitor for many years. He’s also actually a friend of mine—we used to compete together in these contests.” Dębiak also used to work with Pachocki at OpenAI.&lt;/p&gt;  &lt;p&gt;When Pachocki competed in coding contests he favored those that focused on shorter problems with concrete solutions. But Dębiak liked longer, open-ended problems without an obvious correct answer.&lt;/p&gt;  &lt;p&gt;“He used to poke fun at me, saying that the kind of contest I was into will be automated long before the ones he liked,” Pachocki recalled. “So I was seriously invested in the performance of this model in this latest competition.”&lt;/p&gt;  &lt;p&gt;Pachocki told me he was glued to the late-night livestream from Tokyo, watching his model come in second: “Psyho resists for now.”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“We’ve tracked the performance of LLMs on coding contests for a while,” said Chen. “We’ve watched them become better than me, better than Jakub. It feels something like Lee Sedol playing Go.”&lt;/p&gt;  &lt;p&gt;Lee is the master Go player who lost a series of matches to DeepMind’s game-playing model AlphaGo in 2016. The results stunned the international Go community and led Lee to give up professional play. Last year he told the &lt;em&gt;New York Times&lt;/em&gt;: “Losing to AI, in a sense, meant my entire world was collapsing ... I could no longer enjoy the game.” And yet, unlike Lee, Chen and Pachocki are thrilled to be surpassed.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But why should the rest of us care about these niche wins? It’s clear that this technology—designed to mimic and, ultimately, stand in for human intelligence—is being built by people whose idea of peak intelligence is acing a math contest or holding your own against a legendary coder. Is it a problem that this view of intelligence is skewed toward the mathematical, analytical end of the scale?&lt;/p&gt;  &lt;p&gt;“I mean, I think you are right that—you know, selfishly, we do want to create models which accelerate ourselves,” Chen told me. “We see that as a very fast factor to progress.”&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;The argument researchers like Chen and Pachocki make is that math and coding are the bedrock for a far more general form of intelligence, one that can solve a wide range of problems in ways we might not have thought of ourselves. “We’re talking about programming and math here,” said Pachocki. “But it’s really about creativity, coming up with novel ideas, connecting ideas from different places.”&lt;/p&gt;  &lt;p&gt;Look at the two recent competitions: “In both cases, there were problems which required very hard, out-of-the-box thinking. Psyho spent half the programming competition thinking and then came up with a solution that was really novel and quite different from anything that our model looked at.”&lt;/p&gt;  &lt;p&gt;“This is really what we’re after,” Pachocki continued. “How do we get models to discover this sort of novel insight? To actually advance our knowledge? I think they are already capable of that in some limited ways. But I think this technology has the potential to really accelerate scientific progress.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I returned to the question about whether the focus on math and programming was a problem, conceding that maybe it’s fine if what we’re building are tools to help us do science. We don't necessarily want large language models to replace politicians and have people skills, I suggested.&lt;/p&gt;  &lt;p&gt;Chen pulled a face and looked up at the ceiling:&lt;strong&gt; &lt;/strong&gt;“Why not?”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;What’s missing&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;OpenAI was founded with a level of hubris that stood out even by Silicon Valley standards, boasting about its goal of building AGI back when talk of AGI still sounded kooky. OpenAI remains as gung-ho about AGI as ever, and it has done more than most to make AGI a mainstream multibillion-dollar concern. It’s not there yet, though. I asked Chen and Pachocki what they think is missing.&lt;/p&gt;  &lt;p&gt;“I think the way to envision the future is to really, deeply study the technology that we see today,” Pachocki said. “From the beginning, OpenAI has looked at deep learning as this very mysterious and clearly very powerful technology with a lot of potential. We’ve been trying to understand its bottlenecks. What can it do? What can it not do?”&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;At the current cutting edge, Chen said, are reasoning models, which break down problems into smaller, more manageable steps, but even they have limits: “You know, you have these models which know a lot of things but can’t chain that knowledge together. Why is that? Why can’t it do that in a way that humans can?”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;OpenAI is throwing everything at answering that question.&lt;/p&gt;  &lt;p&gt;“We are probably still, like, at the very beginning of this reasoning paradigm,” Pachocki told me. “Really, we are thinking about how to get these models to learn and explore over the long term and actually deliver very new ideas.”&lt;/p&gt;  &lt;p&gt;Chen pushed the point home: “I really don’t consider reasoning done. We’ve definitely not solved it. You have to read so much text to get a kind of approximation of what humans know.”&lt;/p&gt;  &lt;p&gt;OpenAI won’t say what data it uses to train its models or give details about their size and shape—only that it is working hard to make all stages of the development process more efficient.&lt;/p&gt;  &lt;p&gt;Those efforts make them confident that so-called scaling laws—which suggest that models will continue to get better the more compute you throw at them—show no sign of breaking down.&lt;/p&gt;  &lt;p&gt;“I don’t think there’s evidence that scaling laws are dead in any sense,” Chen insisted. “There have always been bottlenecks, right? Sometimes they’re to do with the way models are built. Sometimes they’re to do with data. But fundamentally it’s just about finding the research that breaks you through the current bottleneck.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The faith in progress is unshakeable. I brought up something Pachocki had said about AGI in an interview with &lt;em&gt;Nature&lt;/em&gt; in May: “When I joined OpenAI in 2017, I was still among the biggest skeptics at the company.” He looked doubtful.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“I’m not sure I was skeptical about the concept,” he said. “But I think I was—” He paused, looking at his hands on the table in front of him. “When I joined OpenAI, I expected the timelines to be longer to get to the point that we are now.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_17"&gt; &lt;p&gt;“There’s a lot of consequences of AI,” he said. “But the one I think the most about is automated research. When we look at human history, a lot of it is about technological progress, about humans building new technologies. The point when computers can develop new technologies themselves seems like a very important, um, inflection point.&lt;/p&gt;  &lt;p&gt;“We already see these models assist scientists. But when they are able to work on longer horizons—when they’re able to establish research programs for themselves—the world will feel meaningfully different.”&lt;/p&gt;  &lt;p&gt;For Chen, that ability for models to work by themselves for longer is key. “I mean, I do think everyone has their own definitions of AGI,” he said. “But this concept of autonomous time—just the amount of time that the model can spend making productive progress on a difficult problem without hitting a dead end—that’s one of the big things that we’re after.”&lt;/p&gt;  &lt;p&gt;It’s a bold vision—and far beyond the capabilities of today’s models. But I was nevertheless struck by how Chen and Pachocki made AGI sound almost mundane. Compare this with how Sutskever responded when I spoke to him 18 months ago. “It’s going to be monumental, earth-shattering,” he told me. “There will be a before and an after.” Faced with the immensity of what he was building, Sutskever switched the focus of his career from designing better and better models to figuring out how to control a technology that he believed would soon be smarter than himself.&lt;/p&gt;  &lt;p&gt;Two years ago Sutskever set up what he called a superalignment team that he would co-lead with another OpenAI safety researcher, Jan Leike. The claim was that this team would funnel a full fifth of OpenAI’s resources into figuring out how to control a hypothetical superintelligence. Today, most of the people on the superalignment team, including Sutskever and Leike, have left the company and the team no longer exists.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When Leike quit, he said it was because the team had not been given the support he felt it deserved. He posted this on X: “Building smarter-than-human machines is an inherently dangerous endeavor. OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products.” Other departing researchers shared similar statements.&lt;/p&gt;  &lt;p&gt;I asked Chen and Pachocki what they make of such concerns. “A lot of these things are highly personal decisions,” Chen said. “You know, a researcher can kind of, you know—”&lt;/p&gt;  &lt;p&gt;He started again. “They might have a belief that the field is going to evolve in a certain way and that their research is going to pan out and is going to bear fruit. And, you know, maybe the company doesn’t reshape in the way that you want it to. It’s a very dynamic field.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_19"&gt;&lt;p&gt;“A lot of these things are personal decisions,” he repeated. “Sometimes the field is just evolving in a way that is less consistent with the way that you’re doing research.”&lt;/p&gt;  &lt;p&gt;But alignment, both of them insist, is now part of the core business rather than the concern of one specific team. According to Pachocki, these models don’t work at all unless they work as you expect them to. There’s also little desire to focus on aligning a hypothetical superintelligence with your objectives when doing so with existing models is already enough of a challenge.&lt;/p&gt;  &lt;p&gt;“Two years ago the risks that we were imagining were mostly theoretical risks,” Pachocki said. “The world today looks very different, and I think a lot of alignment problems are now very practically motivated.”&lt;/p&gt;  &lt;p&gt;Still, experimental technology is being spun into mass-market products faster than ever before. Does that really never lead to disagreements between the two of them?&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;“&lt;/strong&gt;I am often afforded the luxury of really kind of thinking about the long term, where the technology is headed,” Pachocki said. “Contending with the reality of the process—both in terms of people and also, like, the broader company needs—falls on Mark. It’s not really a disagreement, but there is a natural tension between these different objectives and the different challenges that the company is facing that materializes between us.”&lt;/p&gt;  &lt;p&gt;Chen jumped in: “I think it’s just a very delicate balance.”&amp;nbsp;&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/250724-AI-hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;For the past couple of years, OpenAI has felt like a one-man brand. With his showbiz style and fundraising glitz, CEO Sam Altman overshadows all other big names on the firm’s roster. Even his bungled ouster ended with him back on top—and more famous than ever. But look past the charismatic frontman and you get a clearer sense of where this company is going. After all, Altman is not the one building the technology on which its reputation rests.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That responsibility falls to OpenAI’s twin heads of research—chief research officer Mark Chen and chief scientist Jakub Pachocki. Between them, they share the role of making sure OpenAI stays one step ahead of powerhouse rivals like Google.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;I sat down with Chen and Pachocki for an exclusive conversation during a recent trip the pair made to London, where OpenAI set up its first international office in 2023. We talked about how they manage the inherent tension between research and product. We also talked about why they think coding and math are the keys to more capable all-purpose models; what they really mean when they talk about AGI; and what happened to OpenAI’s superalignment team, set up by the firm’s cofounder and former chief scientist Ilya Sutskever to prevent a hypothetical superintelligence from going rogue, which disbanded soon after he quit.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In particular, I wanted to get a sense of where their heads are at in the run-up to OpenAI’s biggest product release in months: GPT-5.&lt;/p&gt; 
 &lt;p&gt;Reports are out that the firm’s next-generation model will be launched in August. OpenAI’s official line—well, Altman’s—is that it will release GPT-5 “soon.” Anticipation is high. The leaps OpenAI made with GPT-3 and then GPT-4 raised the bar of what was thought possible with this technology. And yet delays to the launch of GPT-5 have fueled rumors that OpenAI has struggled to build a model that meets its own—not to mention everyone else’s—expectations.&lt;/p&gt;  &lt;p&gt;Altman has been uncharacteristically modest: “[GPT-5] is an experimental model that incorporates new research techniques we will use in future models,” he posted on X—which makes it sound more like a work in progress than another horizon-shifting release.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But expectation management is part of the job for a company that for the last several years has set the agenda for the industry. And Chen and Pachocki set the agenda inside OpenAI.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Twin peaks&lt;/strong&gt;&amp;nbsp;&lt;/h3&gt;  &lt;p&gt;The firm’s main London office is in St James’s Park, a few hundred meters east of Buckingham Palace. But I met Chen and Pachocki in a conference room in a coworking space near King’s Cross, which OpenAI keeps as a kind of pied-à-terre in the heart of London’s tech neighborhood (Google DeepMind and Meta are just around the corner). OpenAI’s head of research communications, Laurance Fauconnet, sat with an open laptop at the end of the table.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Chen, who was wearing a maroon polo shirt, is clean-cut, almost preppy. He’s media trained and comfortable talking to a reporter. (That’s him flirting with a chatbot in the “Introducing GPT-4o” video.) Pachocki, in a black elephant-logo tee, has more of a TV-movie hacker look. He stares at his hands a lot when he speaks.&lt;/p&gt;  &lt;p&gt;But the pair are a tighter double act than they first appear. Pachocki summed up their roles. Chen shapes and manages the research teams, he said. “I am responsible for setting the research roadmap and establishing our long-term technical vision.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;“But there’s fluidity in the roles,” Chen said. “We’re both researchers, we pull on technical threads. Whatever we see that we can pull on and fix, that’s what we do.”&lt;/p&gt;  &lt;p&gt;Chen joined the company in 2018 after working as a quantitative trader at the Wall Street firm Jane Street Capital, where he developed machine-learning models for futures trading. At OpenAI he spearheaded the creation of DALL-E, the firm’s breakthrough generative image model. He then worked on adding image recognition to GPT‑4 and led the development of Codex, the generative coding model that powers GitHub Copilot.&lt;/p&gt;  &lt;p&gt;Pachocki left an academic career in theoretical computer science to join OpenAI in 2017 and replaced Sutskever as chief scientist in 2024. He is the key architect of OpenAI’s so-called reasoning models—especially o1 and o3—which are designed to tackle complex tasks in science, math, and coding.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When we met they were buzzing, fresh off the high of two new back-to-back wins for their company’s technology.&lt;/p&gt; 

 &lt;p&gt;On July 16, one of OpenAI’s large language models came in second in the AtCoder World Tour Finals, one of the world’s most hardcore programming competitions. On July 19, OpenAI announced that one of its models had achieved gold-medal-level results on the 2025 International Math Olympiad, one of the world’s most prestigious math contests.&lt;/p&gt;  &lt;p&gt;The math result made headlines, not only because of OpenAI’s remarkable achievement, but because rival Google DeepMind revealed two days later that one of its models had achieved the same score in the same competition. Google DeepMind had played by the competition’s rules and waited for its results to be checked by the organizers before making an announcement; OpenAI had in effect marked its own answers.&lt;/p&gt;  &lt;p&gt;For Chen and Pachocki, the result speaks for itself. Anyway, it’s the programming win they’re most excited about. “I think that’s quite underrated,” Chen told me. A gold medal result in the International Math Olympiad puts you somewhere in the top 20 to 50 competitors, he said. But in the AtCoder contest OpenAI’s model placed in the top two: “To break into a really different tier of human performance—that’s unprecedented.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Ship, ship, ship!&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;People at OpenAI still like to say they work at a research lab. But the company is very different from the one it was before the release of ChatGPT three years ago. The firm is now in a race with the biggest and richest technology companies in the world and valued at $300 billion. Envelope-pushing research and eye-catching demos no longer cut it. It needs to ship products and get them into people’s hands—and boy, it does.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;OpenAI has kept up a run of new releases—putting out major updates to its GPT-4 series, launching a string of generative image and video models, and introducing the ability to talk to ChatGPT with your voice. Six months ago it kicked off a new wave of so-called reasoning models with its o1 release, soon followed by o3. And last week it released its browser-using agent Operator to the public. It now claims that more than 400 million people use its products every week and submit 2.5 billion prompts a day.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;OpenAI’s incoming CEO of applications, Fidji Simo, plans to keep up the momentum. In a memo to the company, she told employees she is looking forward to “helping get OpenAI’s technologies into the hands of more people around the world,” where they will “unlock more opportunities for more people than any other technology in history.” Expect the products to keep coming.&lt;/p&gt;  &lt;p&gt;I asked how OpenAI juggles open-ended research and product development. “This is something we have been thinking about for a very long time, long before ChatGPT,” Pachocki said. “If we are actually serious about trying to build artificial general intelligence, clearly there will be so much that you can do with this technology along the way, so many tangents you can go down that will be big products.” In other words, keep shaking the tree and harvest what you can.&lt;/p&gt;  &lt;p&gt;A talking point that comes up with OpenAI folks is that putting experimental models out into the world was a necessary part of research. The goal was to make people aware of how good this technology had become. “We want to educate people about what’s coming so that we can participate in what will be a very hard societal conversation,” Altman told me back in 2022. The makers of this strange new technology were also curious what it might be for: OpenAI was keen to get it into people’s hands to see what they would do with it.&lt;/p&gt; 
 &lt;p&gt;Is that still the case? They answered at the same time. “Yeah!” Chen said. “To some extent,” Pachocki said. Chen laughed: “No, go ahead.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“I wouldn’t say research iterates on product,” said Pachocki. “But now that models are at the edge of the capabilities that can be measured by classical benchmarks and a lot of the long-standing challenges that we’ve been thinking about are starting to fall, we’re at the point where it really is about what the models can do in the real world.”&lt;/p&gt; 
 &lt;p&gt;Like taking on humans in coding competitions. The person who beat OpenAI’s model at this year’s AtCoder contest, held in Japan, was a programmer named Przemysław Dębiak, also known as Psyho. The contest was a puzzle-solving marathon in which competitors had 10 hours to find the most efficient way to solve a complex coding problem. After his win, Psyho posted on X: “I’m completely exhausted ... I’m barely alive.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Chen and Pachocki have strong ties to the world of competitive coding. Both have competed in international coding contests in the past and Chen coaches the USA Computing Olympiad team. I asked whether that personal enthusiasm for competitive coding colors their sense of how big a deal it is for a model to perform well at such a challenge.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;They both laughed. “Definitely,” said Pachocki. “So: Psyho is kind of a legend. He’s been the number one competitor for many years. He’s also actually a friend of mine—we used to compete together in these contests.” Dębiak also used to work with Pachocki at OpenAI.&lt;/p&gt;  &lt;p&gt;When Pachocki competed in coding contests he favored those that focused on shorter problems with concrete solutions. But Dębiak liked longer, open-ended problems without an obvious correct answer.&lt;/p&gt;  &lt;p&gt;“He used to poke fun at me, saying that the kind of contest I was into will be automated long before the ones he liked,” Pachocki recalled. “So I was seriously invested in the performance of this model in this latest competition.”&lt;/p&gt;  &lt;p&gt;Pachocki told me he was glued to the late-night livestream from Tokyo, watching his model come in second: “Psyho resists for now.”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“We’ve tracked the performance of LLMs on coding contests for a while,” said Chen. “We’ve watched them become better than me, better than Jakub. It feels something like Lee Sedol playing Go.”&lt;/p&gt;  &lt;p&gt;Lee is the master Go player who lost a series of matches to DeepMind’s game-playing model AlphaGo in 2016. The results stunned the international Go community and led Lee to give up professional play. Last year he told the &lt;em&gt;New York Times&lt;/em&gt;: “Losing to AI, in a sense, meant my entire world was collapsing ... I could no longer enjoy the game.” And yet, unlike Lee, Chen and Pachocki are thrilled to be surpassed.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But why should the rest of us care about these niche wins? It’s clear that this technology—designed to mimic and, ultimately, stand in for human intelligence—is being built by people whose idea of peak intelligence is acing a math contest or holding your own against a legendary coder. Is it a problem that this view of intelligence is skewed toward the mathematical, analytical end of the scale?&lt;/p&gt;  &lt;p&gt;“I mean, I think you are right that—you know, selfishly, we do want to create models which accelerate ourselves,” Chen told me. “We see that as a very fast factor to progress.”&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;The argument researchers like Chen and Pachocki make is that math and coding are the bedrock for a far more general form of intelligence, one that can solve a wide range of problems in ways we might not have thought of ourselves. “We’re talking about programming and math here,” said Pachocki. “But it’s really about creativity, coming up with novel ideas, connecting ideas from different places.”&lt;/p&gt;  &lt;p&gt;Look at the two recent competitions: “In both cases, there were problems which required very hard, out-of-the-box thinking. Psyho spent half the programming competition thinking and then came up with a solution that was really novel and quite different from anything that our model looked at.”&lt;/p&gt;  &lt;p&gt;“This is really what we’re after,” Pachocki continued. “How do we get models to discover this sort of novel insight? To actually advance our knowledge? I think they are already capable of that in some limited ways. But I think this technology has the potential to really accelerate scientific progress.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I returned to the question about whether the focus on math and programming was a problem, conceding that maybe it’s fine if what we’re building are tools to help us do science. We don't necessarily want large language models to replace politicians and have people skills, I suggested.&lt;/p&gt;  &lt;p&gt;Chen pulled a face and looked up at the ceiling:&lt;strong&gt; &lt;/strong&gt;“Why not?”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;What’s missing&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;OpenAI was founded with a level of hubris that stood out even by Silicon Valley standards, boasting about its goal of building AGI back when talk of AGI still sounded kooky. OpenAI remains as gung-ho about AGI as ever, and it has done more than most to make AGI a mainstream multibillion-dollar concern. It’s not there yet, though. I asked Chen and Pachocki what they think is missing.&lt;/p&gt;  &lt;p&gt;“I think the way to envision the future is to really, deeply study the technology that we see today,” Pachocki said. “From the beginning, OpenAI has looked at deep learning as this very mysterious and clearly very powerful technology with a lot of potential. We’ve been trying to understand its bottlenecks. What can it do? What can it not do?”&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;At the current cutting edge, Chen said, are reasoning models, which break down problems into smaller, more manageable steps, but even they have limits: “You know, you have these models which know a lot of things but can’t chain that knowledge together. Why is that? Why can’t it do that in a way that humans can?”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;OpenAI is throwing everything at answering that question.&lt;/p&gt;  &lt;p&gt;“We are probably still, like, at the very beginning of this reasoning paradigm,” Pachocki told me. “Really, we are thinking about how to get these models to learn and explore over the long term and actually deliver very new ideas.”&lt;/p&gt;  &lt;p&gt;Chen pushed the point home: “I really don’t consider reasoning done. We’ve definitely not solved it. You have to read so much text to get a kind of approximation of what humans know.”&lt;/p&gt;  &lt;p&gt;OpenAI won’t say what data it uses to train its models or give details about their size and shape—only that it is working hard to make all stages of the development process more efficient.&lt;/p&gt;  &lt;p&gt;Those efforts make them confident that so-called scaling laws—which suggest that models will continue to get better the more compute you throw at them—show no sign of breaking down.&lt;/p&gt;  &lt;p&gt;“I don’t think there’s evidence that scaling laws are dead in any sense,” Chen insisted. “There have always been bottlenecks, right? Sometimes they’re to do with the way models are built. Sometimes they’re to do with data. But fundamentally it’s just about finding the research that breaks you through the current bottleneck.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The faith in progress is unshakeable. I brought up something Pachocki had said about AGI in an interview with &lt;em&gt;Nature&lt;/em&gt; in May: “When I joined OpenAI in 2017, I was still among the biggest skeptics at the company.” He looked doubtful.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“I’m not sure I was skeptical about the concept,” he said. “But I think I was—” He paused, looking at his hands on the table in front of him. “When I joined OpenAI, I expected the timelines to be longer to get to the point that we are now.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_17"&gt; &lt;p&gt;“There’s a lot of consequences of AI,” he said. “But the one I think the most about is automated research. When we look at human history, a lot of it is about technological progress, about humans building new technologies. The point when computers can develop new technologies themselves seems like a very important, um, inflection point.&lt;/p&gt;  &lt;p&gt;“We already see these models assist scientists. But when they are able to work on longer horizons—when they’re able to establish research programs for themselves—the world will feel meaningfully different.”&lt;/p&gt;  &lt;p&gt;For Chen, that ability for models to work by themselves for longer is key. “I mean, I do think everyone has their own definitions of AGI,” he said. “But this concept of autonomous time—just the amount of time that the model can spend making productive progress on a difficult problem without hitting a dead end—that’s one of the big things that we’re after.”&lt;/p&gt;  &lt;p&gt;It’s a bold vision—and far beyond the capabilities of today’s models. But I was nevertheless struck by how Chen and Pachocki made AGI sound almost mundane. Compare this with how Sutskever responded when I spoke to him 18 months ago. “It’s going to be monumental, earth-shattering,” he told me. “There will be a before and an after.” Faced with the immensity of what he was building, Sutskever switched the focus of his career from designing better and better models to figuring out how to control a technology that he believed would soon be smarter than himself.&lt;/p&gt;  &lt;p&gt;Two years ago Sutskever set up what he called a superalignment team that he would co-lead with another OpenAI safety researcher, Jan Leike. The claim was that this team would funnel a full fifth of OpenAI’s resources into figuring out how to control a hypothetical superintelligence. Today, most of the people on the superalignment team, including Sutskever and Leike, have left the company and the team no longer exists.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When Leike quit, he said it was because the team had not been given the support he felt it deserved. He posted this on X: “Building smarter-than-human machines is an inherently dangerous endeavor. OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products.” Other departing researchers shared similar statements.&lt;/p&gt;  &lt;p&gt;I asked Chen and Pachocki what they make of such concerns. “A lot of these things are highly personal decisions,” Chen said. “You know, a researcher can kind of, you know—”&lt;/p&gt;  &lt;p&gt;He started again. “They might have a belief that the field is going to evolve in a certain way and that their research is going to pan out and is going to bear fruit. And, you know, maybe the company doesn’t reshape in the way that you want it to. It’s a very dynamic field.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_19"&gt;&lt;p&gt;“A lot of these things are personal decisions,” he repeated. “Sometimes the field is just evolving in a way that is less consistent with the way that you’re doing research.”&lt;/p&gt;  &lt;p&gt;But alignment, both of them insist, is now part of the core business rather than the concern of one specific team. According to Pachocki, these models don’t work at all unless they work as you expect them to. There’s also little desire to focus on aligning a hypothetical superintelligence with your objectives when doing so with existing models is already enough of a challenge.&lt;/p&gt;  &lt;p&gt;“Two years ago the risks that we were imagining were mostly theoretical risks,” Pachocki said. “The world today looks very different, and I think a lot of alignment problems are now very practically motivated.”&lt;/p&gt;  &lt;p&gt;Still, experimental technology is being spun into mass-market products faster than ever before. Does that really never lead to disagreements between the two of them?&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;“&lt;/strong&gt;I am often afforded the luxury of really kind of thinking about the long term, where the technology is headed,” Pachocki said. “Contending with the reality of the process—both in terms of people and also, like, the broader company needs—falls on Mark. It’s not really a disagreement, but there is a natural tension between these different objectives and the different challenges that the company is facing that materializes between us.”&lt;/p&gt;  &lt;p&gt;Chen jumped in: “I think it’s just a very delicate balance.”&amp;nbsp;&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/31/1120885/the-two-people-shaping-the-future-of-openais-research/</guid><pubDate>Thu, 31 Jul 2025 09:06:48 +0000</pubDate></item><item><title>[NEW] The Download: OpenAI’s future research, and US climate regulation is under threat (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/31/1120890/the-download-openais-future-research-and-us-climate-regulation-is-under-threat/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The two people shaping the future of OpenAI’s research&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Will Douglas Heaven&lt;/em&gt;&lt;/p&gt;&lt;p&gt;For the past couple of years, OpenAI has felt like a one-man brand. With his showbiz style and fundraising glitz, CEO Sam Altman overshadows all other big names on the firm’s roster.&lt;/p&gt;&lt;p&gt;But Altman is not the one building the technology on which its reputation rests. That responsibility falls to OpenAI’s twin heads of research—chief research officer Mark Chen and chief scientist Jakub Pachocki. Between them, they share the role of making sure OpenAI stays one step ahead of powerhouse rivals like Google.&lt;/p&gt;&lt;p&gt;I recently sat down with Chen and Pachocki for an exclusive conversation which covered everything from how they manage the inherent tension between research and product, to what they really mean when they talk about AGI, to what happened to OpenAI’s superalignment team.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I also wanted to get a sense of where their heads are at in the run-up to OpenAI’s biggest product release in months: GPT-5.&lt;strong&gt; &lt;/strong&gt;Read the full story.&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;An EPA rule change threatens to gut US climate regulations&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;The mechanism that allows the US federal government to regulate climate change is on the chopping block.&lt;/p&gt;  &lt;p&gt;On Tuesday, US Environmental Protection Agency administrator Lee Zeldin announced that the agency is taking aim at the endangerment finding, a 2009 rule that’s essentially the tentpole supporting federal greenhouse-gas regulations.&lt;/p&gt;  &lt;p&gt;This might sound like an obscure legal situation, but it’s a really big deal for climate policy in the US. So let’s look at what this rule says now, what the proposed change looks like, and what it all means. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;This story is part of MIT Technology Review’s “America Undone” series, examining how the foundations of US success in science and innovation are currently under threat. &lt;/strong&gt;&lt;strong&gt;You can read the rest here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;It appeared first in The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The AI Hype Index: The White House’s war on “woke AI”&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;Separating AI reality from hyped-up fiction isn’t always easy. That’s why we’ve created the AI Hype Index—a simple, at-a-glance summary of everything you need to know about the state of the industry. Take a look at this month’s edition of the index here.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;1 Trump has announced a new US health care records system&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Experts warn the initiative could leave patients’ medical records open to abuse. (NYT $)&lt;br /&gt;+ &lt;em&gt;Big Tech has pledged to work with providers and health systems. &lt;/em&gt;(The Hill)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 China says it’s worried Nvidia’s chips have serious security issues&lt;/strong&gt;&lt;br /&gt;Just as the company sought to resume sales in the country. (Reuters)&lt;br /&gt;+&lt;em&gt;Experts reportedly found the chips featured location tracking tech. &lt;/em&gt;(FT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Mark Zuckerberg believes superintelligence “is now in sight”&lt;/strong&gt;&lt;br /&gt;Although he didn’t illuminate what it even means. (The Guardian)&lt;br /&gt;+ &lt;em&gt;Zuckerberg has taken a leaf out of the Altman playbook. &lt;/em&gt;(NY Mag $)&lt;br /&gt;+ &lt;em&gt;Don’t expect Meta to open source any of those superintelligent models. &lt;/em&gt;(TechCrunch&lt;strong&gt;)&lt;br /&gt;&lt;/strong&gt;+ &lt;em&gt;Tech billionaires are making a risky bet with humanity’s future. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4&lt;/strong&gt; &lt;strong&gt;NASA is in turmoil&lt;/strong&gt;&lt;br /&gt;Without a permanent leader, workers are leaving in their thousands. (WP $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;5 Google removed negative articles about a tech CEO from search results&lt;br /&gt;&lt;/strong&gt;After someone made fraudulent requests using its Refresh Outdated Content Tool. (404 Media)&lt;br /&gt;+ &lt;em&gt;They exploited a bug in the tool to get pages removed. &lt;/em&gt;(Ars Technica)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 How AI has transformed data center design&lt;br /&gt;&lt;/strong&gt;They need to accommodate a lot more heat and power than they used to. (FT $)&lt;br /&gt;+ &lt;em&gt;A proposed Wyoming data center would use more electricity than its homes. &lt;/em&gt;(Ars Technica)&lt;br /&gt;+ &lt;em&gt;Apple manufacturer Foxconn wants to get involved in building data centers. &lt;/em&gt;(CNBC)&lt;br /&gt;+ &lt;em&gt;Should we be moving data centers to space? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;7 AI agents can probe websites for security weaknesses&lt;/strong&gt;&lt;br /&gt;Especially shoddily-constructed vibe-coded ones. (Wired $)&lt;br /&gt;+ &lt;em&gt;Cyberattacks by AI agents are coming. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 New forms of life have been filmed at the ocean’s deepest points&lt;br /&gt;&lt;/strong&gt;The abundance of life was amazing, the Chinese-led research team says. (BBC)&lt;br /&gt;+ &lt;em&gt;Meet the divers trying to figure out how deep humans can go. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;9 TikTok is adding Footnotes to its clips&lt;/strong&gt;&lt;br /&gt;As AI-generated videos become even harder to spot. (The Verge)&lt;br /&gt;+ &lt;em&gt;This fake viral clip of rabbits on a trampoline is a great example. &lt;/em&gt;(404 Media)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 What it’s like to attend an Elon Musk fan fest&lt;/strong&gt;&lt;br /&gt;X Takeover promised to unite Tesla and SpaceX-heads alike. (Insider $)&lt;br /&gt;+ &lt;em&gt;Some people who definitely aren’t fans: neighbors of Tesla’s diner. &lt;/em&gt;(404 Media)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; 
 &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Patients across America should be very worried that their medical records are going to be used in ways that harm them and their families.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Lawrence Gostin, a Georgetown University law professor specializing in public health, warns of the potential repercussions of the Trump administration’s new health data tracking system, the Associated Press reports.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2024/06/Palm_Springs_Surf_Club-0025-social.jpg" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The cost of building the perfect wave&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For nearly as long as surfing has existed, surfers have been obsessed with the search for the perfect wave.&lt;/p&gt;&lt;p&gt;While this hunt has taken surfers from tropical coastlines to icebergs, these days that search may take place closer to home. That is, at least, the vision presented by developers and boosters in the growing industry of surf pools, spurred by advances in wave-­generating technology that have finally created artificial waves surfers actually want to ride.&lt;/p&gt;&lt;p&gt;But there’s a problem: some of these pools are in drought-ridden areas, and face fierce local opposition. At the core of these fights is a question that’s also at the heart of the sport: What is the cost of finding, or now creating, the perfect wave—and who will have to bear it? Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Eileen Guo&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Maybe airplane food isn’t so bad after all.&lt;br /&gt;+ An unwitting metal detectorist uncovered some ancient armor in the Czech Republic that may have been worn during the Trojan war.&lt;br /&gt;+ Talking of the siege of Troy, tickets for Christopher Nolan’s retelling of The Odyssey are already selling out a year before it’s released.&lt;br /&gt;+ This fun website refreshes every few seconds with a new picture of someone pointing at your mouse pointer.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The two people shaping the future of OpenAI’s research&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Will Douglas Heaven&lt;/em&gt;&lt;/p&gt;&lt;p&gt;For the past couple of years, OpenAI has felt like a one-man brand. With his showbiz style and fundraising glitz, CEO Sam Altman overshadows all other big names on the firm’s roster.&lt;/p&gt;&lt;p&gt;But Altman is not the one building the technology on which its reputation rests. That responsibility falls to OpenAI’s twin heads of research—chief research officer Mark Chen and chief scientist Jakub Pachocki. Between them, they share the role of making sure OpenAI stays one step ahead of powerhouse rivals like Google.&lt;/p&gt;&lt;p&gt;I recently sat down with Chen and Pachocki for an exclusive conversation which covered everything from how they manage the inherent tension between research and product, to what they really mean when they talk about AGI, to what happened to OpenAI’s superalignment team.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I also wanted to get a sense of where their heads are at in the run-up to OpenAI’s biggest product release in months: GPT-5.&lt;strong&gt; &lt;/strong&gt;Read the full story.&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;An EPA rule change threatens to gut US climate regulations&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;The mechanism that allows the US federal government to regulate climate change is on the chopping block.&lt;/p&gt;  &lt;p&gt;On Tuesday, US Environmental Protection Agency administrator Lee Zeldin announced that the agency is taking aim at the endangerment finding, a 2009 rule that’s essentially the tentpole supporting federal greenhouse-gas regulations.&lt;/p&gt;  &lt;p&gt;This might sound like an obscure legal situation, but it’s a really big deal for climate policy in the US. So let’s look at what this rule says now, what the proposed change looks like, and what it all means. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;This story is part of MIT Technology Review’s “America Undone” series, examining how the foundations of US success in science and innovation are currently under threat. &lt;/strong&gt;&lt;strong&gt;You can read the rest here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;It appeared first in The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The AI Hype Index: The White House’s war on “woke AI”&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;Separating AI reality from hyped-up fiction isn’t always easy. That’s why we’ve created the AI Hype Index—a simple, at-a-glance summary of everything you need to know about the state of the industry. Take a look at this month’s edition of the index here.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;1 Trump has announced a new US health care records system&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Experts warn the initiative could leave patients’ medical records open to abuse. (NYT $)&lt;br /&gt;+ &lt;em&gt;Big Tech has pledged to work with providers and health systems. &lt;/em&gt;(The Hill)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 China says it’s worried Nvidia’s chips have serious security issues&lt;/strong&gt;&lt;br /&gt;Just as the company sought to resume sales in the country. (Reuters)&lt;br /&gt;+&lt;em&gt;Experts reportedly found the chips featured location tracking tech. &lt;/em&gt;(FT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Mark Zuckerberg believes superintelligence “is now in sight”&lt;/strong&gt;&lt;br /&gt;Although he didn’t illuminate what it even means. (The Guardian)&lt;br /&gt;+ &lt;em&gt;Zuckerberg has taken a leaf out of the Altman playbook. &lt;/em&gt;(NY Mag $)&lt;br /&gt;+ &lt;em&gt;Don’t expect Meta to open source any of those superintelligent models. &lt;/em&gt;(TechCrunch&lt;strong&gt;)&lt;br /&gt;&lt;/strong&gt;+ &lt;em&gt;Tech billionaires are making a risky bet with humanity’s future. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4&lt;/strong&gt; &lt;strong&gt;NASA is in turmoil&lt;/strong&gt;&lt;br /&gt;Without a permanent leader, workers are leaving in their thousands. (WP $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;5 Google removed negative articles about a tech CEO from search results&lt;br /&gt;&lt;/strong&gt;After someone made fraudulent requests using its Refresh Outdated Content Tool. (404 Media)&lt;br /&gt;+ &lt;em&gt;They exploited a bug in the tool to get pages removed. &lt;/em&gt;(Ars Technica)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 How AI has transformed data center design&lt;br /&gt;&lt;/strong&gt;They need to accommodate a lot more heat and power than they used to. (FT $)&lt;br /&gt;+ &lt;em&gt;A proposed Wyoming data center would use more electricity than its homes. &lt;/em&gt;(Ars Technica)&lt;br /&gt;+ &lt;em&gt;Apple manufacturer Foxconn wants to get involved in building data centers. &lt;/em&gt;(CNBC)&lt;br /&gt;+ &lt;em&gt;Should we be moving data centers to space? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;7 AI agents can probe websites for security weaknesses&lt;/strong&gt;&lt;br /&gt;Especially shoddily-constructed vibe-coded ones. (Wired $)&lt;br /&gt;+ &lt;em&gt;Cyberattacks by AI agents are coming. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 New forms of life have been filmed at the ocean’s deepest points&lt;br /&gt;&lt;/strong&gt;The abundance of life was amazing, the Chinese-led research team says. (BBC)&lt;br /&gt;+ &lt;em&gt;Meet the divers trying to figure out how deep humans can go. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;9 TikTok is adding Footnotes to its clips&lt;/strong&gt;&lt;br /&gt;As AI-generated videos become even harder to spot. (The Verge)&lt;br /&gt;+ &lt;em&gt;This fake viral clip of rabbits on a trampoline is a great example. &lt;/em&gt;(404 Media)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 What it’s like to attend an Elon Musk fan fest&lt;/strong&gt;&lt;br /&gt;X Takeover promised to unite Tesla and SpaceX-heads alike. (Insider $)&lt;br /&gt;+ &lt;em&gt;Some people who definitely aren’t fans: neighbors of Tesla’s diner. &lt;/em&gt;(404 Media)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; 
 &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Patients across America should be very worried that their medical records are going to be used in ways that harm them and their families.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Lawrence Gostin, a Georgetown University law professor specializing in public health, warns of the potential repercussions of the Trump administration’s new health data tracking system, the Associated Press reports.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2024/06/Palm_Springs_Surf_Club-0025-social.jpg" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The cost of building the perfect wave&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For nearly as long as surfing has existed, surfers have been obsessed with the search for the perfect wave.&lt;/p&gt;&lt;p&gt;While this hunt has taken surfers from tropical coastlines to icebergs, these days that search may take place closer to home. That is, at least, the vision presented by developers and boosters in the growing industry of surf pools, spurred by advances in wave-­generating technology that have finally created artificial waves surfers actually want to ride.&lt;/p&gt;&lt;p&gt;But there’s a problem: some of these pools are in drought-ridden areas, and face fierce local opposition. At the core of these fights is a question that’s also at the heart of the sport: What is the cost of finding, or now creating, the perfect wave—and who will have to bear it? Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Eileen Guo&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Maybe airplane food isn’t so bad after all.&lt;br /&gt;+ An unwitting metal detectorist uncovered some ancient armor in the Czech Republic that may have been worn during the Trojan war.&lt;br /&gt;+ Talking of the siege of Troy, tickets for Christopher Nolan’s retelling of The Odyssey are already selling out a year before it’s released.&lt;br /&gt;+ This fun website refreshes every few seconds with a new picture of someone pointing at your mouse pointer.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/31/1120890/the-download-openais-future-research-and-us-climate-regulation-is-under-threat/</guid><pubDate>Thu, 31 Jul 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] 24/7 compliance monitoring: The AI advantage in data protection (AI News)</title><link>https://www.artificialintelligence-news.com/news/24-7-compliance-monitoring-the-ai-advantage-in-data-protection/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/07/pexels-thisisengineering-3862610-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Data protection compliance has evolved from a periodic checklist exercise to a continuous responsibility. With cyber threats emerging and regulatory requirements becoming increasingly stringent, organisations can’t afford to rely on manual compliance monitoring approaches. The advent of artificial intelligence has transformed the challenge, offering capabilities for continuous oversight and real-time protection of sensitive data.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-evolution-of-compliance-monitoring"&gt;The evolution of compliance monitoring&lt;/h3&gt;&lt;p&gt;Traditional compliance monitoring is characterised by annual assessments and reactive responses to incidents. While this approach is sufficient for simpler regulatory environments, it falls short in addressing the complexities of modern data protection. The General Data Protection Regulation (GDPR), the Data Protection Act 2018, and emerging frameworks like the Digital Services Act demand compliance and demonstrable, ongoing adherence to data handling protocols.&lt;/p&gt;&lt;p&gt;The shift to continuous monitoring represents a change in how organisations approach compliance. Rather than periodic snapshots of compliance status, businesses are better off with real-time visibility in their security posture. The transformation has been driven by several factors: the increasing volume and velocity of data processing, the sophistication of cyber threats, and the evolution of regulatory expectations towards proactive rather than reactive compliance.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-ai-powered-continuous-monitoring-capabilities"&gt;AI-powered continuous monitoring capabilities&lt;/h3&gt;&lt;p&gt;Artificial intelligence brings several advantages to compliance monitoring that human-led processes cannot match. Machine learning algorithms can process vast quantities of data in real-time, identifying patterns and anomalies that would be difficult for human analysts to detect manually. Systems can simultaneously monitor multiple data streams, user activities, and system behaviours in all of an organisation’s digital infrastructure.&lt;/p&gt;&lt;p&gt;AI-powered monitoring systems excel at pattern recognition, learning from historical data to establish baselines of normal behaviour. When deviations occur – whether through unauthorised access attempts, unusual data transfers, or policy violations – they can immediately flag potential compliance breaches. The capability extends beyond simple rule-based detection; AI systems can identify subtle indicators that may suggest emerging compliance risks before they transform into actual violations.&lt;/p&gt;&lt;p&gt;AI systems can contextualise compliance events in broader organisational and regulatory frameworks. Rather than generating isolated alerts, intelligent monitoring platforms can assess the significance of events based on factors like data sensitivity, user roles, regulatory requirements, and potential business impact. Contextual awareness enables more targeted and effective compliance responses.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-real-time-threat-detection-and-response"&gt;Real-time threat detection and response&lt;/h3&gt;&lt;p&gt;The speed of AI-powered monitoring represents perhaps its most significant advantage over traditional approaches. While manual compliance reviews might detect violations up to days or weeks after they occur, AI systems can identify and respond to potential breaches in seconds or minutes. This rapid response capability is important to minimise the impact of data protection incidents and ensure swift remediation.&lt;/p&gt;&lt;p&gt;Real-time monitoring lets organisations implement dynamic compliance controls that adapt to changing circumstances. For instance, if AI systems detect unusual data access patterns that suggest potential unauthorised activity, they can trigger additional authentication requirements or temporarily restrict access to sensitive resources. A proactive approach can prevent compliance violations before they occur, rather than documenting them after the fact.&lt;/p&gt;&lt;p&gt;The integration of AI with automated response mechanisms further enhances protection capabilities. When potential violations are detected, systems can automatically initiate predefined response protocols, like isolating affected systems, notifying relevant personnel, or implementing emergency access controls. Automation helps ensure consistent and timely responses, regardless of when incidents occur or whether human operators are immediately available.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-comprehensive-coverage-across-digital-assets"&gt;Comprehensive coverage across digital assets&lt;/h3&gt;&lt;p&gt;Modern organisations operate complex digital ecosystems that span cloud services, on-premises infrastructure, mobile devices, and third-party applications. AI-powered compliance monitoring can provide unified oversight in diverse environments, helping ensure consistent protection standards regardless of where data resides or how it is processed.&lt;/p&gt;&lt;p&gt;Cloud environments, in particular, benefit from AI-driven monitoring. The dynamic nature of cloud infrastructure – with resources being created, modified, and destroyed continuously – makes manual compliance oversight difficult. AI systems can track configuration changes, monitor data flows, and ensure that security controls remain properly configured as environments evolve. This capability is important in maintaining compliance in cloud-centric business operations.&lt;/p&gt;&lt;p&gt;Additionally, AI can monitor compliance in the full data lifecycle, from collection and processing to storage and deletion. By implementing a compliance automation platform like Thoropass, organisations can help ensure that data handling practices are consistent with regulatory requirements throughout each stage of processing. Comprehensive coverage helps organisations maintain demonstrable compliance even as data volumes and processing complexity continue to grow.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-predictive-analytics-for-compliance-risk-management"&gt;Predictive analytics for compliance risk management&lt;/h3&gt;&lt;p&gt;Beyond reactive monitoring, AI can provide predictive analytics that can identify potential compliance risks before they materialise. Analysing historical patterns, user behaviours, and system configurations lets AI systems predict scenarios that may lead to compliance violations. Predictive capability allows organisations to implement preventive measures and address vulnerabilities proactively.&lt;/p&gt;&lt;p&gt;Predictive analytics can also inform compliance strategy and resource allocation, and identifying areas of highest risk and predicting future compliance challenges helps organisations prioritise their security investments and compliance efforts. The strategic application of AI ensures that limited resources are directed towards the most dangerous areas of risk.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-regulatory-reporting-and-documentation-benefits"&gt;Regulatory reporting and documentation benefits&lt;/h3&gt;&lt;p&gt;AI-powered monitoring systems perform well at generating comprehensive audit trails and compliance documentation. Systems can automatically collect, correlate, and present evidence of compliance activities in formats suitable for regulatory reporting. Such capability reduces the administrative burden associated with compliance documentation and helps ensure accuracy and completeness.&lt;/p&gt;&lt;p&gt;Automated reporting capabilities also enable more frequent and detailed compliance assessments. Rather than waiting for annual audits, organisations can generate real-time compliance reports that provide continuous visibility into their data protection posture. An ongoing assessment capability helps organisations identify and address compliance gaps more quickly, reducing the risk of regulatory violations.&lt;/p&gt;&lt;p&gt;The transition to AI-powered compliance monitoring represents a technological upgrade and signifies a shift towards more effective, efficient, and comprehensive data protection. As regulatory requirements evolve and cyber threats become more sophisticated, the ability to maintain continuous oversight of data protection compliance becomes not just advantageous, but essential. Organisations that adopt AI-driven capabilities position themselves to meet current compliance requirements and adapt successfully to tomorrow’s regulatory landscape.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Guest author: Sally Giles&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Image source: Pexels&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/07/pexels-thisisengineering-3862610-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Data protection compliance has evolved from a periodic checklist exercise to a continuous responsibility. With cyber threats emerging and regulatory requirements becoming increasingly stringent, organisations can’t afford to rely on manual compliance monitoring approaches. The advent of artificial intelligence has transformed the challenge, offering capabilities for continuous oversight and real-time protection of sensitive data.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-evolution-of-compliance-monitoring"&gt;The evolution of compliance monitoring&lt;/h3&gt;&lt;p&gt;Traditional compliance monitoring is characterised by annual assessments and reactive responses to incidents. While this approach is sufficient for simpler regulatory environments, it falls short in addressing the complexities of modern data protection. The General Data Protection Regulation (GDPR), the Data Protection Act 2018, and emerging frameworks like the Digital Services Act demand compliance and demonstrable, ongoing adherence to data handling protocols.&lt;/p&gt;&lt;p&gt;The shift to continuous monitoring represents a change in how organisations approach compliance. Rather than periodic snapshots of compliance status, businesses are better off with real-time visibility in their security posture. The transformation has been driven by several factors: the increasing volume and velocity of data processing, the sophistication of cyber threats, and the evolution of regulatory expectations towards proactive rather than reactive compliance.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-ai-powered-continuous-monitoring-capabilities"&gt;AI-powered continuous monitoring capabilities&lt;/h3&gt;&lt;p&gt;Artificial intelligence brings several advantages to compliance monitoring that human-led processes cannot match. Machine learning algorithms can process vast quantities of data in real-time, identifying patterns and anomalies that would be difficult for human analysts to detect manually. Systems can simultaneously monitor multiple data streams, user activities, and system behaviours in all of an organisation’s digital infrastructure.&lt;/p&gt;&lt;p&gt;AI-powered monitoring systems excel at pattern recognition, learning from historical data to establish baselines of normal behaviour. When deviations occur – whether through unauthorised access attempts, unusual data transfers, or policy violations – they can immediately flag potential compliance breaches. The capability extends beyond simple rule-based detection; AI systems can identify subtle indicators that may suggest emerging compliance risks before they transform into actual violations.&lt;/p&gt;&lt;p&gt;AI systems can contextualise compliance events in broader organisational and regulatory frameworks. Rather than generating isolated alerts, intelligent monitoring platforms can assess the significance of events based on factors like data sensitivity, user roles, regulatory requirements, and potential business impact. Contextual awareness enables more targeted and effective compliance responses.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-real-time-threat-detection-and-response"&gt;Real-time threat detection and response&lt;/h3&gt;&lt;p&gt;The speed of AI-powered monitoring represents perhaps its most significant advantage over traditional approaches. While manual compliance reviews might detect violations up to days or weeks after they occur, AI systems can identify and respond to potential breaches in seconds or minutes. This rapid response capability is important to minimise the impact of data protection incidents and ensure swift remediation.&lt;/p&gt;&lt;p&gt;Real-time monitoring lets organisations implement dynamic compliance controls that adapt to changing circumstances. For instance, if AI systems detect unusual data access patterns that suggest potential unauthorised activity, they can trigger additional authentication requirements or temporarily restrict access to sensitive resources. A proactive approach can prevent compliance violations before they occur, rather than documenting them after the fact.&lt;/p&gt;&lt;p&gt;The integration of AI with automated response mechanisms further enhances protection capabilities. When potential violations are detected, systems can automatically initiate predefined response protocols, like isolating affected systems, notifying relevant personnel, or implementing emergency access controls. Automation helps ensure consistent and timely responses, regardless of when incidents occur or whether human operators are immediately available.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-comprehensive-coverage-across-digital-assets"&gt;Comprehensive coverage across digital assets&lt;/h3&gt;&lt;p&gt;Modern organisations operate complex digital ecosystems that span cloud services, on-premises infrastructure, mobile devices, and third-party applications. AI-powered compliance monitoring can provide unified oversight in diverse environments, helping ensure consistent protection standards regardless of where data resides or how it is processed.&lt;/p&gt;&lt;p&gt;Cloud environments, in particular, benefit from AI-driven monitoring. The dynamic nature of cloud infrastructure – with resources being created, modified, and destroyed continuously – makes manual compliance oversight difficult. AI systems can track configuration changes, monitor data flows, and ensure that security controls remain properly configured as environments evolve. This capability is important in maintaining compliance in cloud-centric business operations.&lt;/p&gt;&lt;p&gt;Additionally, AI can monitor compliance in the full data lifecycle, from collection and processing to storage and deletion. By implementing a compliance automation platform like Thoropass, organisations can help ensure that data handling practices are consistent with regulatory requirements throughout each stage of processing. Comprehensive coverage helps organisations maintain demonstrable compliance even as data volumes and processing complexity continue to grow.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-predictive-analytics-for-compliance-risk-management"&gt;Predictive analytics for compliance risk management&lt;/h3&gt;&lt;p&gt;Beyond reactive monitoring, AI can provide predictive analytics that can identify potential compliance risks before they materialise. Analysing historical patterns, user behaviours, and system configurations lets AI systems predict scenarios that may lead to compliance violations. Predictive capability allows organisations to implement preventive measures and address vulnerabilities proactively.&lt;/p&gt;&lt;p&gt;Predictive analytics can also inform compliance strategy and resource allocation, and identifying areas of highest risk and predicting future compliance challenges helps organisations prioritise their security investments and compliance efforts. The strategic application of AI ensures that limited resources are directed towards the most dangerous areas of risk.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-regulatory-reporting-and-documentation-benefits"&gt;Regulatory reporting and documentation benefits&lt;/h3&gt;&lt;p&gt;AI-powered monitoring systems perform well at generating comprehensive audit trails and compliance documentation. Systems can automatically collect, correlate, and present evidence of compliance activities in formats suitable for regulatory reporting. Such capability reduces the administrative burden associated with compliance documentation and helps ensure accuracy and completeness.&lt;/p&gt;&lt;p&gt;Automated reporting capabilities also enable more frequent and detailed compliance assessments. Rather than waiting for annual audits, organisations can generate real-time compliance reports that provide continuous visibility into their data protection posture. An ongoing assessment capability helps organisations identify and address compliance gaps more quickly, reducing the risk of regulatory violations.&lt;/p&gt;&lt;p&gt;The transition to AI-powered compliance monitoring represents a technological upgrade and signifies a shift towards more effective, efficient, and comprehensive data protection. As regulatory requirements evolve and cyber threats become more sophisticated, the ability to maintain continuous oversight of data protection compliance becomes not just advantageous, but essential. Organisations that adopt AI-driven capabilities position themselves to meet current compliance requirements and adapt successfully to tomorrow’s regulatory landscape.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Guest author: Sally Giles&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Image source: Pexels&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/24-7-compliance-monitoring-the-ai-advantage-in-data-protection/</guid><pubDate>Thu, 31 Jul 2025 12:16:03 +0000</pubDate></item></channel></rss>