<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 11 Dec 2025 01:53:07 +0000</lastBuildDate><item><title>How Hud's runtime sensor cut triage time from 3 hours to 10 minutes (AI | VentureBeat)</title><link>https://venturebeat.com/ai/how-huds-runtime-sensor-cut-triage-time-from-3-hours-to-10-minutes</link><description>[unable to retrieve full-text content]&lt;p&gt;Engineering teams are generating more code with AI agents than ever before. But they&amp;#x27;re hitting a wall when that code reaches production.&lt;/p&gt;&lt;p&gt;The problem isn&amp;#x27;t necessarily the AI-generated code itself. It&amp;#x27;s that traditional monitoring tools generally struggle to provide the granular, function-level data AI agents need to understand how code actually behaves in complex production environments. Without that context, agents can&amp;#x27;t detect issues or generate fixes that account for production reality.&lt;/p&gt;&lt;p&gt;It&amp;#x27;s a challenge that startup&lt;a href="https://www.hud.io/"&gt; &lt;u&gt;Hud&lt;/u&gt;&lt;/a&gt; is looking to help solve with the launch of its runtime code sensor on Wednesday. The company&amp;#x27;s eponymous sensor runs alongside production code, automatically tracking how every function behaves, giving developers a heads-up on what&amp;#x27;s actually occurring in deployment.&lt;/p&gt;&lt;p&gt;&amp;quot;Every software team building at scale faces the same fundamental challenge: building high-quality products that work well in the real world,&amp;quot; Roee Adler, CEO and founder of Hud, told VentureBeat in an exclusive interview.  &amp;quot;In the new era of AI-accelerated development, not knowing how code behaves in production becomes an even bigger part of that challenge.&amp;quot;&lt;/p&gt;&lt;h2&gt;What software developers are struggling with &lt;/h2&gt;&lt;p&gt;The pain points that developers are facing are fairly consistent across engineering organizations. Moshik Eilon, group tech lead at Monday.com, oversees 130 engineer and describes a familiar frustration with traditional monitoring tools.&lt;/p&gt;&lt;p&gt;&amp;quot;When you get an alert, you usually end up checking an endpoint that has an error rate or high latency, and you want to drill down to see the downstream dependencies,&amp;quot; Eilon told VentureBeat. &amp;quot;A lot of times it&amp;#x27;s the actual application, and then it&amp;#x27;s a black box. You just get 80% downstream latency on the application.&amp;quot;&lt;/p&gt;&lt;p&gt;The next step typically involves manual detective work across multiple tools. Check the logs. Correlate timestamps. Try to reconstruct what the application was doing. For novel issues deep in a large codebase, teams often lack the exact data they need.&lt;/p&gt;&lt;p&gt;Daniel Marashlian, CTO and co-founder at Drata, saw his engineers spending hours on what he referred to as an &amp;quot;investigation tax.&amp;quot; &amp;quot;They were mapping a generic alert to a specific code owner, then digging through logs to reconstruct the state of the application,&amp;quot; Marashlian told VentureBeat. &amp;quot;We wanted to eliminate that so our team could focus entirely on the fix rather than the discovery.&amp;quot;&lt;/p&gt;&lt;p&gt;Drata&amp;#x27;s architecture compounds the challenge. The company integrates with numerous external services to deliver automated compliance, which creates sophisticated investigations when issues arise. Engineers trace behavior across a very large codebase spanning risk, compliance, integrations, and reporting modules.&lt;/p&gt;&lt;p&gt;Marashlian identified three specific problems that drove Drata toward investing in runtime sensors. The first issue was the cost of context switching. &lt;/p&gt;&lt;p&gt;&amp;quot;Our data was scattered, so our engineers had to act as human bridges between disconnected tools,&amp;quot; he said.&lt;/p&gt;&lt;p&gt;The second issue, he noted, is alert fatigue. &amp;quot;When you have a complex distributed system, general alert channels become a constant stream of background noise, what our team describes as a &amp;#x27;ding, ding, ding&amp;#x27; effect that eventually gets ignored,&amp;quot; Marashlian said.&lt;/p&gt;&lt;p&gt;The third key driver was a need to integrate with the company&amp;#x27;s AI strategy.&lt;/p&gt;&lt;p&gt;&amp;quot;An AI agent can write code, but it cannot fix a production bug if it can&amp;#x27;t see the runtime variables or the root cause,&amp;quot; Marashlian said.&lt;/p&gt;&lt;h2&gt;Why traditional APMs can&amp;#x27;t solve the problem easily&lt;/h2&gt;&lt;p&gt;Enterprises have long relied on a class of tools and services known as Application Performance Monitoring (APM). &lt;/p&gt;&lt;p&gt;With the current pace of agentic AI development and modern development workflows, both Monday.com and Drata simply were not able to get the required visibility from existing APM tools.&lt;/p&gt;&lt;p&gt;&amp;quot;If I would want to get this information from Datadog or from CoreLogix, I would just have to ingest tons of logs or tons of spans, and I would pay a lot of money,&amp;quot; Eilon said. &lt;/p&gt;&lt;p&gt;Eilon noted that Monday.com used very low sampling rates because of cost constraints. That meant they often missed the exact data needed to debug issues.&lt;/p&gt;&lt;p&gt;Traditional application performance monitoring tools also require prediction, which is a problem because sometimes a developer just doesn&amp;#x27;t know what they don&amp;#x27;t know.&lt;/p&gt;&lt;p&gt;&amp;quot;Traditional observability requires you to anticipate what you&amp;#x27;ll need to debug,&amp;quot; Marashlian said. &amp;quot;But when a novel issue surfaces, especially deep within a large, complex codebase, you&amp;#x27;re often missing the exact data you need.&amp;quot;&lt;/p&gt;&lt;p&gt;Drata evaluated several solutions in the AI site reliability engineering and automated incident response categories and didn&amp;#x27;t find what was needed. &lt;/p&gt;&lt;p&gt; &amp;quot;Most tools we evaluated were excellent at managing the incident process, routing tickets, summarizing Slack threads, or correlating graphs,&amp;quot; he said. &amp;quot;But they often stopped short of the code itself. They could tell us &amp;#x27;Service A is down,&amp;#x27; but they couldn&amp;#x27;t tell us why specifically.&amp;quot;&lt;/p&gt;&lt;p&gt;Another common capability in some tools including error monitors like Sentry is the ability to capture exceptions. The challenge, according to Adler, is that being made aware of exceptions is nice, but that doesn&amp;#x27;t connect them to business impact or provide the execution context AI agents need to propose fixes.&lt;/p&gt;&lt;h2&gt;How runtime sensors work differently&lt;/h2&gt;&lt;p&gt;Runtime sensors push intelligence to the edge where code executes. Hud&amp;#x27;s sensor runs as an SDK that integrates with a single line of code. It sees every function execution but only sends lightweight aggregate data unless something goes wrong.&lt;/p&gt;&lt;p&gt;When errors or slowdowns occur, the sensor automatically gathers deep forensic data including HTTP parameters, database queries and responses, and full execution context. The system establishes performance baselines within a day and can alert on both dramatic slowdowns and outliers that percentile-based monitoring misses.&lt;/p&gt;&lt;p&gt;&amp;quot;Now we just get all of this information for all of the functions regardless of what level they are, even for underlying packages,&amp;quot; Eilon said. &amp;quot;Sometimes you might have an issue that is very deep, and we still see it pretty fast.&amp;quot;&lt;/p&gt;&lt;p&gt;The platform delivers data through four channels:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Web application&lt;/b&gt; for centralized monitoring and analysis&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;IDE extensions&lt;/b&gt; for VS Code, JetBrains and Cursor that surface production metrics directly where code is written&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;MCP server&lt;/b&gt; that feeds structured data to AI coding agents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Alerting system&lt;/b&gt; that identifies issues without manual configuration&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The MCP server integration is critical for AI-assisted development. Monday.com engineers now query production behavior directly within Cursor. &lt;/p&gt;&lt;p&gt;&amp;quot;I can just ask Cursor a question: Hey, why is this endpoint slow?&amp;quot; Eilon said. &amp;quot;When it uses the Hud MCP, I get all of the granular metrics, and this function is 30% slower since this deployment. Then I can also find the root cause.&amp;quot;&lt;/p&gt;&lt;p&gt;This changes the incident response workflow. Instead of starting in Datadog and drilling down through layers, engineers start by asking an AI agent to diagnose the issue. The agent has immediate access to function-level production data.&lt;/p&gt;&lt;h2&gt;From voodoo incidents to minutes-long fixes&lt;/h2&gt;&lt;p&gt;The shift from theoretical capability to practical impact becomes clear in how engineering teams actually use runtime sensors. What used to take hours or days of detective work now resolves in minutes.&lt;/p&gt;&lt;p&gt;&amp;quot;I&amp;#x27;m used to having these voodoo incidents where there is a CPU spike and you don&amp;#x27;t know where it came from,&amp;quot; Eilon said. &amp;quot;A few years ago, I had such an incident and I had to build my own tool that takes the CPU profile and the memory dump. Now I just have all of the function data and I&amp;#x27;ve seen engineers just solve it so fast.&amp;quot;&lt;/p&gt;&lt;p&gt;At Drata, the quantified impact is dramatic. The company built an internal /triage command that support engineers run within their AI assistants to instantly identify root causes. Manual triage work dropped from approximately 3 hours per day to under 10 minutes. Mean time to resolution improved by approximately 70%.&lt;/p&gt;&lt;p&gt;The team also generates a daily &amp;quot;Heads Up&amp;quot; report of quick-win errors. Because the root cause is already captured, developers can fix these issues in minutes. Support engineers now perform forensic diagnosis that previously required a senior developer. Ticket throughput increased without expanding the L2 team.&lt;/p&gt;&lt;h2&gt;Where this technology fits&lt;/h2&gt;&lt;p&gt;Runtime sensors occupy a distinct space from traditional APMs, which excel at service-level monitoring but struggle with granular, cost-effective function-level data. They differ from error monitors that capture exceptions without business context.&lt;/p&gt;&lt;p&gt;The technical requirements for supporting AI coding agents differ from human-facing observability. Agents need structured, function-level data they can reason over. They can&amp;#x27;t parse and correlate raw logs the way humans do. Traditional observability also assumes you can predict what you&amp;#x27;ll need to debug and instrument accordingly. That approach breaks down with AI-generated code where engineers may not deeply understand every function.&lt;/p&gt;&lt;p&gt;&amp;quot;I think we&amp;#x27;re entering a new age of AI-generated code and this puzzle, this jigsaw puzzle of a new stack emerging,&amp;quot; Adler said. &amp;quot;I just don&amp;#x27;t think that the cloud computing observability stack is going to fit neatly into how the future looks like.&amp;quot;&lt;/p&gt;&lt;h2&gt;What this means for enterprises&lt;/h2&gt;&lt;p&gt;For organizations already using AI coding assistants like GitHub Copilot or Cursor, runtime intelligence provides a safety layer for production deployments. The technology enables what Monday.com calls &amp;quot;agentic investigation&amp;quot; rather than manual tool-hopping.&lt;/p&gt;&lt;p&gt;The broader implication relates to trust.  &amp;quot;With AI-generated code, we are getting much more AI-generated code, and engineers start not knowing all of the code,&amp;quot; Eilon said.&lt;/p&gt;&lt;p&gt; Runtime sensors bridge that knowledge gap by providing production context directly in the IDE where code is written.&lt;/p&gt;&lt;p&gt;For enterprises looking to scale AI code generation beyond pilots, runtime intelligence addresses a fundamental problem. AI agents generate code based on assumptions about system behavior. Production environments are complex and surprising. Function-level behavioral data captured automatically from production gives agents the context they need to generate reliable code at scale.&lt;/p&gt;&lt;p&gt;Organizations should evaluate whether their existing observability stack can cost-effectively provide the granularity AI agents require. If achieving function-level visibility requires dramatically increasing ingestion costs or manual instrumentation, runtime sensors may offer a more sustainable architecture for AI-accelerated development workflows already emerging across the industry.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Engineering teams are generating more code with AI agents than ever before. But they&amp;#x27;re hitting a wall when that code reaches production.&lt;/p&gt;&lt;p&gt;The problem isn&amp;#x27;t necessarily the AI-generated code itself. It&amp;#x27;s that traditional monitoring tools generally struggle to provide the granular, function-level data AI agents need to understand how code actually behaves in complex production environments. Without that context, agents can&amp;#x27;t detect issues or generate fixes that account for production reality.&lt;/p&gt;&lt;p&gt;It&amp;#x27;s a challenge that startup&lt;a href="https://www.hud.io/"&gt; &lt;u&gt;Hud&lt;/u&gt;&lt;/a&gt; is looking to help solve with the launch of its runtime code sensor on Wednesday. The company&amp;#x27;s eponymous sensor runs alongside production code, automatically tracking how every function behaves, giving developers a heads-up on what&amp;#x27;s actually occurring in deployment.&lt;/p&gt;&lt;p&gt;&amp;quot;Every software team building at scale faces the same fundamental challenge: building high-quality products that work well in the real world,&amp;quot; Roee Adler, CEO and founder of Hud, told VentureBeat in an exclusive interview.  &amp;quot;In the new era of AI-accelerated development, not knowing how code behaves in production becomes an even bigger part of that challenge.&amp;quot;&lt;/p&gt;&lt;h2&gt;What software developers are struggling with &lt;/h2&gt;&lt;p&gt;The pain points that developers are facing are fairly consistent across engineering organizations. Moshik Eilon, group tech lead at Monday.com, oversees 130 engineer and describes a familiar frustration with traditional monitoring tools.&lt;/p&gt;&lt;p&gt;&amp;quot;When you get an alert, you usually end up checking an endpoint that has an error rate or high latency, and you want to drill down to see the downstream dependencies,&amp;quot; Eilon told VentureBeat. &amp;quot;A lot of times it&amp;#x27;s the actual application, and then it&amp;#x27;s a black box. You just get 80% downstream latency on the application.&amp;quot;&lt;/p&gt;&lt;p&gt;The next step typically involves manual detective work across multiple tools. Check the logs. Correlate timestamps. Try to reconstruct what the application was doing. For novel issues deep in a large codebase, teams often lack the exact data they need.&lt;/p&gt;&lt;p&gt;Daniel Marashlian, CTO and co-founder at Drata, saw his engineers spending hours on what he referred to as an &amp;quot;investigation tax.&amp;quot; &amp;quot;They were mapping a generic alert to a specific code owner, then digging through logs to reconstruct the state of the application,&amp;quot; Marashlian told VentureBeat. &amp;quot;We wanted to eliminate that so our team could focus entirely on the fix rather than the discovery.&amp;quot;&lt;/p&gt;&lt;p&gt;Drata&amp;#x27;s architecture compounds the challenge. The company integrates with numerous external services to deliver automated compliance, which creates sophisticated investigations when issues arise. Engineers trace behavior across a very large codebase spanning risk, compliance, integrations, and reporting modules.&lt;/p&gt;&lt;p&gt;Marashlian identified three specific problems that drove Drata toward investing in runtime sensors. The first issue was the cost of context switching. &lt;/p&gt;&lt;p&gt;&amp;quot;Our data was scattered, so our engineers had to act as human bridges between disconnected tools,&amp;quot; he said.&lt;/p&gt;&lt;p&gt;The second issue, he noted, is alert fatigue. &amp;quot;When you have a complex distributed system, general alert channels become a constant stream of background noise, what our team describes as a &amp;#x27;ding, ding, ding&amp;#x27; effect that eventually gets ignored,&amp;quot; Marashlian said.&lt;/p&gt;&lt;p&gt;The third key driver was a need to integrate with the company&amp;#x27;s AI strategy.&lt;/p&gt;&lt;p&gt;&amp;quot;An AI agent can write code, but it cannot fix a production bug if it can&amp;#x27;t see the runtime variables or the root cause,&amp;quot; Marashlian said.&lt;/p&gt;&lt;h2&gt;Why traditional APMs can&amp;#x27;t solve the problem easily&lt;/h2&gt;&lt;p&gt;Enterprises have long relied on a class of tools and services known as Application Performance Monitoring (APM). &lt;/p&gt;&lt;p&gt;With the current pace of agentic AI development and modern development workflows, both Monday.com and Drata simply were not able to get the required visibility from existing APM tools.&lt;/p&gt;&lt;p&gt;&amp;quot;If I would want to get this information from Datadog or from CoreLogix, I would just have to ingest tons of logs or tons of spans, and I would pay a lot of money,&amp;quot; Eilon said. &lt;/p&gt;&lt;p&gt;Eilon noted that Monday.com used very low sampling rates because of cost constraints. That meant they often missed the exact data needed to debug issues.&lt;/p&gt;&lt;p&gt;Traditional application performance monitoring tools also require prediction, which is a problem because sometimes a developer just doesn&amp;#x27;t know what they don&amp;#x27;t know.&lt;/p&gt;&lt;p&gt;&amp;quot;Traditional observability requires you to anticipate what you&amp;#x27;ll need to debug,&amp;quot; Marashlian said. &amp;quot;But when a novel issue surfaces, especially deep within a large, complex codebase, you&amp;#x27;re often missing the exact data you need.&amp;quot;&lt;/p&gt;&lt;p&gt;Drata evaluated several solutions in the AI site reliability engineering and automated incident response categories and didn&amp;#x27;t find what was needed. &lt;/p&gt;&lt;p&gt; &amp;quot;Most tools we evaluated were excellent at managing the incident process, routing tickets, summarizing Slack threads, or correlating graphs,&amp;quot; he said. &amp;quot;But they often stopped short of the code itself. They could tell us &amp;#x27;Service A is down,&amp;#x27; but they couldn&amp;#x27;t tell us why specifically.&amp;quot;&lt;/p&gt;&lt;p&gt;Another common capability in some tools including error monitors like Sentry is the ability to capture exceptions. The challenge, according to Adler, is that being made aware of exceptions is nice, but that doesn&amp;#x27;t connect them to business impact or provide the execution context AI agents need to propose fixes.&lt;/p&gt;&lt;h2&gt;How runtime sensors work differently&lt;/h2&gt;&lt;p&gt;Runtime sensors push intelligence to the edge where code executes. Hud&amp;#x27;s sensor runs as an SDK that integrates with a single line of code. It sees every function execution but only sends lightweight aggregate data unless something goes wrong.&lt;/p&gt;&lt;p&gt;When errors or slowdowns occur, the sensor automatically gathers deep forensic data including HTTP parameters, database queries and responses, and full execution context. The system establishes performance baselines within a day and can alert on both dramatic slowdowns and outliers that percentile-based monitoring misses.&lt;/p&gt;&lt;p&gt;&amp;quot;Now we just get all of this information for all of the functions regardless of what level they are, even for underlying packages,&amp;quot; Eilon said. &amp;quot;Sometimes you might have an issue that is very deep, and we still see it pretty fast.&amp;quot;&lt;/p&gt;&lt;p&gt;The platform delivers data through four channels:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Web application&lt;/b&gt; for centralized monitoring and analysis&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;IDE extensions&lt;/b&gt; for VS Code, JetBrains and Cursor that surface production metrics directly where code is written&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;MCP server&lt;/b&gt; that feeds structured data to AI coding agents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Alerting system&lt;/b&gt; that identifies issues without manual configuration&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The MCP server integration is critical for AI-assisted development. Monday.com engineers now query production behavior directly within Cursor. &lt;/p&gt;&lt;p&gt;&amp;quot;I can just ask Cursor a question: Hey, why is this endpoint slow?&amp;quot; Eilon said. &amp;quot;When it uses the Hud MCP, I get all of the granular metrics, and this function is 30% slower since this deployment. Then I can also find the root cause.&amp;quot;&lt;/p&gt;&lt;p&gt;This changes the incident response workflow. Instead of starting in Datadog and drilling down through layers, engineers start by asking an AI agent to diagnose the issue. The agent has immediate access to function-level production data.&lt;/p&gt;&lt;h2&gt;From voodoo incidents to minutes-long fixes&lt;/h2&gt;&lt;p&gt;The shift from theoretical capability to practical impact becomes clear in how engineering teams actually use runtime sensors. What used to take hours or days of detective work now resolves in minutes.&lt;/p&gt;&lt;p&gt;&amp;quot;I&amp;#x27;m used to having these voodoo incidents where there is a CPU spike and you don&amp;#x27;t know where it came from,&amp;quot; Eilon said. &amp;quot;A few years ago, I had such an incident and I had to build my own tool that takes the CPU profile and the memory dump. Now I just have all of the function data and I&amp;#x27;ve seen engineers just solve it so fast.&amp;quot;&lt;/p&gt;&lt;p&gt;At Drata, the quantified impact is dramatic. The company built an internal /triage command that support engineers run within their AI assistants to instantly identify root causes. Manual triage work dropped from approximately 3 hours per day to under 10 minutes. Mean time to resolution improved by approximately 70%.&lt;/p&gt;&lt;p&gt;The team also generates a daily &amp;quot;Heads Up&amp;quot; report of quick-win errors. Because the root cause is already captured, developers can fix these issues in minutes. Support engineers now perform forensic diagnosis that previously required a senior developer. Ticket throughput increased without expanding the L2 team.&lt;/p&gt;&lt;h2&gt;Where this technology fits&lt;/h2&gt;&lt;p&gt;Runtime sensors occupy a distinct space from traditional APMs, which excel at service-level monitoring but struggle with granular, cost-effective function-level data. They differ from error monitors that capture exceptions without business context.&lt;/p&gt;&lt;p&gt;The technical requirements for supporting AI coding agents differ from human-facing observability. Agents need structured, function-level data they can reason over. They can&amp;#x27;t parse and correlate raw logs the way humans do. Traditional observability also assumes you can predict what you&amp;#x27;ll need to debug and instrument accordingly. That approach breaks down with AI-generated code where engineers may not deeply understand every function.&lt;/p&gt;&lt;p&gt;&amp;quot;I think we&amp;#x27;re entering a new age of AI-generated code and this puzzle, this jigsaw puzzle of a new stack emerging,&amp;quot; Adler said. &amp;quot;I just don&amp;#x27;t think that the cloud computing observability stack is going to fit neatly into how the future looks like.&amp;quot;&lt;/p&gt;&lt;h2&gt;What this means for enterprises&lt;/h2&gt;&lt;p&gt;For organizations already using AI coding assistants like GitHub Copilot or Cursor, runtime intelligence provides a safety layer for production deployments. The technology enables what Monday.com calls &amp;quot;agentic investigation&amp;quot; rather than manual tool-hopping.&lt;/p&gt;&lt;p&gt;The broader implication relates to trust.  &amp;quot;With AI-generated code, we are getting much more AI-generated code, and engineers start not knowing all of the code,&amp;quot; Eilon said.&lt;/p&gt;&lt;p&gt; Runtime sensors bridge that knowledge gap by providing production context directly in the IDE where code is written.&lt;/p&gt;&lt;p&gt;For enterprises looking to scale AI code generation beyond pilots, runtime intelligence addresses a fundamental problem. AI agents generate code based on assumptions about system behavior. Production environments are complex and surprising. Function-level behavioral data captured automatically from production gives agents the context they need to generate reliable code at scale.&lt;/p&gt;&lt;p&gt;Organizations should evaluate whether their existing observability stack can cost-effectively provide the granularity AI agents require. If achieving function-level visibility requires dramatically increasing ingestion costs or manual instrumentation, runtime sensors may offer a more sustainable architecture for AI-accelerated development workflows already emerging across the industry.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/how-huds-runtime-sensor-cut-triage-time-from-3-hours-to-10-minutes</guid><pubDate>Wed, 10 Dec 2025 14:00:00 +0000</pubDate></item><item><title>Quilter's AI just designed an 843‑part Linux computer that booted on the first try. Hardware will never be the same. (AI | VentureBeat)</title><link>https://venturebeat.com/ai/quilters-ai-just-designed-an-843-part-linux-computer-that-booted-on-the</link><description>[unable to retrieve full-text content]&lt;p&gt;A Los Angeles-based startup has demonstrated what it calls a breakthrough in hardware development: an artificial intelligence system that designed a fully functional Linux computer in one week — a process that would typically consume nearly three months of skilled engineering labor.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.quilter.ai/"&gt;Quilter&lt;/a&gt;, which has raised more than $40 million from investors including Benchmark, Index Ventures, and Coatue, used its physics-driven AI to automate the design of a two-board computer system that booted successfully on its first attempt, requiring no costly revisions. The project, internally dubbed &amp;quot;&lt;a href="https://quilter.ai/project-speedrun"&gt;Project Speedrun&lt;/a&gt;,&amp;quot; required just 38.5 hours of human labor compared to the 428 hours that professional PCB designers quoted for the same task.&lt;/p&gt;&lt;p&gt;The announcement also marks the first public disclosure that &lt;a href="https://www.buildc.com/"&gt;Tony Fadell&lt;/a&gt;, the engineer who led development of the iPod and iPhone at Apple and later founded Nest, has invested in the company and serves as an advisor.&lt;/p&gt;&lt;p&gt;&amp;quot;We didn&amp;#x27;t teach Quilter to draw; we taught it to think in physics,&amp;quot; said Sergiy Nesterenko, Quilter&amp;#x27;s chief executive and a former SpaceX engineer, in an exclusive interview with VentureBeat. &amp;quot;The result wasn&amp;#x27;t a simulation — it was a working computer.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Circuit board design remains the forgotten bottleneck that delays nearly every hardware product&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The announcement shines a light on an unglamorous but critical chokepoint in technology development: printed circuit board layout. While semiconductors and software have received enormous attention and investment, the green fiberglass boards that connect chips, memory, and components in virtually every electronic device remain stubbornly manual to design.&lt;/p&gt;&lt;p&gt;&amp;quot;Besides auto-routers, the technology really hadn&amp;#x27;t changed since the early &amp;#x27;90s,&amp;quot; Fadell told VentureBeat. &amp;quot;The best boards are still made by hand. You go to Apple, they&amp;#x27;ve got the tools, and these guys are just pushing traces, checking everything, doing flood fills—and you&amp;#x27;re like, there&amp;#x27;s got to be a better way.&amp;quot;&lt;/p&gt;&lt;p&gt;The PCB design process typically unfolds in three stages. Engineers first create a schematic — a logical diagram showing how components connect. Then a specialist manually draws the physical layout in CAD software, placing components and routing thousands of copper traces across multiple layers. Finally, the design goes to a manufacturer for fabrication.&lt;/p&gt;&lt;p&gt;That middle step — the layout — creates a persistent bottleneck. For a board of moderate complexity, the process typically consumes four to eight weeks. For sophisticated systems like computers or automotive electronics, timelines stretch to three months or longer.&lt;/p&gt;&lt;p&gt;&amp;quot;The timeline was always this elastic thing—they&amp;#x27;d say, &amp;#x27;Yeah, that&amp;#x27;s two weeks minimum,&amp;#x27;&amp;quot; Fadell recalled of his experience at Apple and Nest. &amp;quot;And we&amp;#x27;d say, &amp;#x27;No, no. Work day and night. It&amp;#x27;s two weeks.&amp;#x27; But it was always this fixed bottleneck.&amp;quot;&lt;/p&gt;&lt;p&gt;The consequences ripple through hardware organizations. Firmware teams sit idle waiting for physical boards to test their code. Validation engineers cannot begin debugging. Product launches slip. According to Quilter&amp;#x27;s research, only about 10 percent of first board revisions work correctly, forcing expensive and time-consuming respins.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Project Speedrun put Quilter&amp;#x27;s AI to the test with an 843-component computer that booted on the first try&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://quilter.ai/project-speedrun"&gt;Project Speedrun&lt;/a&gt; was designed to push the technology to its limits while producing an easily understood result: a working computer that could boot Linux, browse the internet, and run applications.&lt;/p&gt;&lt;p&gt;The system consists of two boards based on&lt;a href="https://www.nxp.com/design/design-center/development-boards-and-designs/8MMINILPD4-EVK"&gt; NXP&amp;#x27;s i.MX 8M Mini&lt;/a&gt; reference platform, a processor architecture used in automotive infotainment, industrial automation, and machine vision applications.&lt;/p&gt;&lt;p&gt;The main system-on-module contains a quad-core ARM processor running at 1.8 gigahertz, 2 gigabytes of LPDDR4 memory, and 32 gigabytes of eMMC storage. A companion baseboard provides connectivity including Ethernet, USB, HDMI, and audio.&lt;/p&gt;&lt;p&gt;Together, the boards incorporate 843 components and 5,141 electrical connections, or &amp;quot;pins,&amp;quot; routed across eight-layer circuit board stackups manufactured by Sierra Circuits in California. The minimum trace geometry reached 2 mils (two-thousandths of an inch) on the system-on-module — fine enough to require advanced high-density interconnect manufacturing techniques.&lt;/p&gt;&lt;p&gt;Quilter&amp;#x27;s AI completed the layout with approximately 98 percent routing coverage and zero design rule violations. Both boards passed power-on testing and successfully booted &lt;a href="https://www.debian.org/"&gt;Debian Linux&lt;/a&gt; on the first attempt.&lt;/p&gt;&lt;p&gt;&amp;quot;We made an entire computer to demonstrate that this technology works,&amp;quot; Nesterenko said. &amp;quot;We took something that&amp;#x27;s typically quoted at 400 to 450 hours, automated the vast majority of it, and reduced it to about 30 to 40 hours of cleanup time.&amp;quot;&lt;/p&gt;&lt;p&gt;The cleanup time is work that human engineers still perform: reviewing the AI&amp;#x27;s output, fixing any issues, and preparing final fabrication files. But even with that overhead, the total elapsed time from schematic to fabricated boards collapsed from the typical 11 weeks to a single week.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Unlike ChatGPT, Quilter&amp;#x27;s AI learns by playing billions of games against the laws of physics&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Quilter&amp;#x27;s &lt;a href="https://www.quilter.ai/product"&gt;technical approach&lt;/a&gt; differs fundamentally from the large language models that have dominated recent AI headlines. Where systems like GPT-5 or Claude learn to predict text based on massive training datasets of human writing, &lt;a href="https://www.quilter.ai/product/technology"&gt;Quilter&amp;#x27;s AI&lt;/a&gt; learns by playing what amounts to an elaborate game against the laws of physics.&lt;/p&gt;&lt;p&gt;&amp;quot;Language models don&amp;#x27;t apply to us because this is not a language problem,&amp;quot; Nesterenko explained. &amp;quot;If you ask it to actually create a blueprint, it has no training data for that. It has no context for that.&amp;quot;&lt;/p&gt;&lt;p&gt;The company also rejected the seemingly obvious approach of training on examples of human-designed boards. Nesterenko cited three reasons: humans make frequent errors (explaining why most boards require revisions), the best designs are locked inside large companies unwilling to share proprietary data, and training on human examples would cap the AI&amp;#x27;s performance at human levels.&lt;/p&gt;&lt;p&gt;Instead, Quilter built what Nesterenko describes as a &amp;quot;game&amp;quot; where the AI agent makes sequential decisions — place this component here, route this trace there — and receives feedback based on whether the resulting design satisfies electromagnetic, thermal, and manufacturing constraints.&lt;/p&gt;&lt;p&gt;&amp;quot;What you&amp;#x27;re really changing is not the probability of getting a very specific outcome of the model, but the probability of choosing a certain action based on that experience,&amp;quot; Nesterenko said.&lt;/p&gt;&lt;p&gt;The approach mirrors DeepMind&amp;#x27;s progression with its Go-playing systems. The original &lt;a href="https://deepmind.google/research/alphago/"&gt;AlphaGo&lt;/a&gt; learned from human games, but its successor &lt;a href="https://deepmind.google/research/alphazero-and-muzero/"&gt;AlphaZero&lt;/a&gt; learned purely through self-play and ultimately surpassed human capability. Quilter harbors similar ambitions.&lt;/p&gt;&lt;p&gt;&amp;quot;In the long term, to come up with better designs for circuit boards than humans have ever tried to do,&amp;quot; Nesterenko said.&lt;/p&gt;&lt;p&gt;Fadell drew a parallel to an earlier technological transition: &amp;quot;I remember this with assembly. You had assembly and compilers, and engineers would say, &amp;#x27;I can&amp;#x27;t trust the compiler. I&amp;#x27;m going to do the loop unrolling myself.&amp;#x27; Now very, very few people write any assembly.&amp;quot;&lt;/p&gt;&lt;p&gt;He expects PCB design to follow a similar arc: &amp;quot;I hope the same thing happens with PCB design. Sure, a few people will hold out, but these tools are going to get so good that everyone else will move on.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Fadell and Nesterenko spent months solving a delicate problem: how to automate design without stripping engineers of control&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Automating a task that skilled professionals have performed manually for decades raises an obvious question: how do engineers maintain control over designs that will ultimately ship in products where reliability matters?&lt;/p&gt;&lt;p&gt;Fadell said he spent significant time with Nesterenko working through this tension. The solution, he said, lies in allowing users to choose their level of involvement at each stage of the process.&lt;/p&gt;&lt;p&gt;&amp;quot;If you&amp;#x27;re a control freak, you can be a control freak. If you want to say &amp;#x27;just do it for me,&amp;#x27; you can do that too—and everything in between,&amp;quot; Fadell said. &amp;quot;You can walk through each phase of the design and get involved wherever you want, or let the AI handle it.&amp;quot;&lt;/p&gt;&lt;p&gt;The workflow breaks into three phases: setup, where engineers define constraints and requirements; execution, where the AI generates candidate layouts; and cleanup, where humans review and refine the output. Engineers can intervene at any point, adjusting constraints and regenerating designs until they&amp;#x27;re satisfied.&lt;/p&gt;&lt;p&gt;&amp;quot;This is something Tony and I talk about a lot,&amp;quot; Nesterenko said. &amp;quot;How do we give users control while still automating most of the work?&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Quilter&amp;#x27;s technology has clear boundaries: 10,000 pins and 10 gigahertz mark the current limits&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The technology has clear limitations. &lt;a href="https://www.quilter.ai/"&gt;Quilter&lt;/a&gt; currently handles boards with up to roughly 10,000 pins — sufficient for a wide range of applications but well short of the most complex designs, which can exceed 100,000 connections.&lt;/p&gt;&lt;p&gt;Physics complexity also creates boundaries. The system handles high-speed communications up to approximately 10 gigahertz, covering typical consumer electronics and many industrial applications. But advanced systems like sophisticated radar, which can operate at 100 gigahertz, exceed current capabilities.&lt;/p&gt;&lt;p&gt;&amp;quot;There are boards where Quilter won&amp;#x27;t make enough progress to make the cleanup time worthwhile,&amp;quot; Nesterenko acknowledged. &amp;quot;We&amp;#x27;re just not that helpful yet with the most advanced, sophisticated designs.&amp;quot;&lt;/p&gt;&lt;p&gt;The company has focused initially on categories where speed matters more than extreme complexity: test fixtures, evaluation boards, design validation boards, and environmental test hardware. These boards often sit in long queues behind higher-priority production designs, delaying engineering programs.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The company bets that engineers will pay the same price for a 10x speed improvement&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Quilter &lt;a href="https://www.quilter.ai/pricing"&gt;prices its service by pin count&lt;/a&gt;, matching the billing conventions that already exist when companies hire outside layout specialists. The pitch to customers is cost neutrality with a ten-fold improvement in speed.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;re going to charge you roughly the same that you would pay for the pins that you would with a person,&amp;quot; Nesterenko said. &amp;quot;But the reason you choose us is that we do this 10 times faster.&amp;quot;&lt;/p&gt;&lt;p&gt;For a company waiting three months for a board layout, receiving it in a week fundamentally changes what&amp;#x27;s possible. Engineering teams can run multiple design experiments in parallel. Firmware developers get hardware faster. Products reach the market sooner.&lt;/p&gt;&lt;p&gt;The company offers free access for hobbyists, students, and small businesses with less than $50,000 in revenue — a strategy to build familiarity while targeting enterprise customers for commercial revenue.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The iPod creator waited years to attach his name to Quilter — until he could prove the technology actually works&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Fadell said he chose this moment to publicly acknowledge his investment because the &lt;a href="https://quilter.ai/project-speedrun"&gt;Project Speedrun&lt;/a&gt; demonstration provides concrete evidence that the technology works.&lt;/p&gt;&lt;p&gt;&amp;quot;It&amp;#x27;s not about being comfortable—I was always comfortable with the team,&amp;quot; he said. &amp;quot;This was about waiting until we had something you could hang your hat on. Now I can say, &amp;#x27;I&amp;#x27;ve used the tool. I&amp;#x27;ve seen it.&amp;#x27;&amp;quot;&lt;/p&gt;&lt;p&gt;He contrasted his approach with typical investor announcements: &amp;quot;Every investor goes, I invested in this, it&amp;#x27;s gonna change the world. It&amp;#x27;s like, no, I know better. I&amp;#x27;ve used the tool. I know people who use it. I asked my startups to use the tool.&amp;quot;&lt;/p&gt;&lt;p&gt;Fadell&amp;#x27;s involvement goes beyond capital. He described email exchanges running to &amp;quot;a dozen pages of details&amp;quot; covering product design, user experience, enterprise sales, and technical architecture.&lt;/p&gt;&lt;p&gt;&amp;quot;Of all the investors I work with, Tony by far goes deepest with me on the product side,&amp;quot; Nesterenko said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;If Quilter succeeds, it could unlock a new generation of hardware startups that were never economically viable before&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The stakes extend far beyond one company&amp;#x27;s product roadmap. If Quilter&amp;#x27;s technology scales, it could fundamentally alter the economics of building physical products.&lt;/p&gt;&lt;p&gt;Fadell argued that hardware development has historically moved slowly because each step in the process — schematic design, PCB layout, manufacturing, assembly — created friction. Other innovations have already smoothed schematic tools and manufacturing. Layout remained the stubborn holdout.&lt;/p&gt;&lt;p&gt;&amp;quot;Once you shrink that from weeks to hours, you can iterate so much faster because all the other friction in the chain has been reduced,&amp;quot; Fadell said.&lt;/p&gt;&lt;p&gt;He predicted the technology would eventually extend upstream into schematic design itself, with AI that understands both logical connections and physical constraints helping engineers avoid problems earlier in the process.&lt;/p&gt;&lt;p&gt;At MIT, where Fadell now spends time, he encounters would-be founders who have abandoned hardware ambitions because the process seemed insurmountable.&lt;/p&gt;&lt;p&gt;&amp;quot;I talk to professors and startup founders, and they say, &amp;#x27;I&amp;#x27;m never doing hardware. It&amp;#x27;s too hard,&amp;#x27;&amp;quot; he said. &amp;quot;I hope we can make it easier for more people to jump in and try things.&amp;quot;&lt;/p&gt;&lt;p&gt;Industry veterans remain skeptical. Auto-routing tools — previous attempts at automation — became notorious for producing unusable results, spawning T-shirts proclaiming engineers would &amp;quot;&lt;a href="https://www.eevblog.com/forum/chat/chris-gammel_s-_never-trust-the-autorouter-shirt_-on-teespring/"&gt;never trust the auto-router&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;p&gt;Nesterenko has seen the skepticism dissolve in real time. He described a recent meeting with executives from a major customer who came to discuss Quilter&amp;#x27;s capabilities. As the conversation unfolded, one executive picked up the &lt;a href="https://quilter.ai/project-speedrun"&gt;Project Speedrun&lt;/a&gt; boards and began photographing them from every angle, turning them over in his hands.&lt;/p&gt;&lt;p&gt;&amp;quot;He was just fascinated by the fact that this is possible now,&amp;quot; Nesterenko said.&lt;/p&gt;&lt;p&gt;The question is no longer whether AI can design circuit boards. A working Linux computer, assembled from 843 components and booted on the first attempt, answers that definitively. The question now is what engineers will build when layout stops being the bottleneck — when hardware, as Fadell put it, finally &amp;quot;moves at the speed of thought.&amp;quot;&lt;/p&gt;&lt;p&gt;On that point, Nesterenko offered a prediction. &amp;quot;If you ask the average electrical engineer today whether automation or AI could at all help with the board of this complexity, they would say no,&amp;quot; he said. For decades, they would have been right. As of last week, they&amp;#x27;re not.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;A Los Angeles-based startup has demonstrated what it calls a breakthrough in hardware development: an artificial intelligence system that designed a fully functional Linux computer in one week — a process that would typically consume nearly three months of skilled engineering labor.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.quilter.ai/"&gt;Quilter&lt;/a&gt;, which has raised more than $40 million from investors including Benchmark, Index Ventures, and Coatue, used its physics-driven AI to automate the design of a two-board computer system that booted successfully on its first attempt, requiring no costly revisions. The project, internally dubbed &amp;quot;&lt;a href="https://quilter.ai/project-speedrun"&gt;Project Speedrun&lt;/a&gt;,&amp;quot; required just 38.5 hours of human labor compared to the 428 hours that professional PCB designers quoted for the same task.&lt;/p&gt;&lt;p&gt;The announcement also marks the first public disclosure that &lt;a href="https://www.buildc.com/"&gt;Tony Fadell&lt;/a&gt;, the engineer who led development of the iPod and iPhone at Apple and later founded Nest, has invested in the company and serves as an advisor.&lt;/p&gt;&lt;p&gt;&amp;quot;We didn&amp;#x27;t teach Quilter to draw; we taught it to think in physics,&amp;quot; said Sergiy Nesterenko, Quilter&amp;#x27;s chief executive and a former SpaceX engineer, in an exclusive interview with VentureBeat. &amp;quot;The result wasn&amp;#x27;t a simulation — it was a working computer.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Circuit board design remains the forgotten bottleneck that delays nearly every hardware product&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The announcement shines a light on an unglamorous but critical chokepoint in technology development: printed circuit board layout. While semiconductors and software have received enormous attention and investment, the green fiberglass boards that connect chips, memory, and components in virtually every electronic device remain stubbornly manual to design.&lt;/p&gt;&lt;p&gt;&amp;quot;Besides auto-routers, the technology really hadn&amp;#x27;t changed since the early &amp;#x27;90s,&amp;quot; Fadell told VentureBeat. &amp;quot;The best boards are still made by hand. You go to Apple, they&amp;#x27;ve got the tools, and these guys are just pushing traces, checking everything, doing flood fills—and you&amp;#x27;re like, there&amp;#x27;s got to be a better way.&amp;quot;&lt;/p&gt;&lt;p&gt;The PCB design process typically unfolds in three stages. Engineers first create a schematic — a logical diagram showing how components connect. Then a specialist manually draws the physical layout in CAD software, placing components and routing thousands of copper traces across multiple layers. Finally, the design goes to a manufacturer for fabrication.&lt;/p&gt;&lt;p&gt;That middle step — the layout — creates a persistent bottleneck. For a board of moderate complexity, the process typically consumes four to eight weeks. For sophisticated systems like computers or automotive electronics, timelines stretch to three months or longer.&lt;/p&gt;&lt;p&gt;&amp;quot;The timeline was always this elastic thing—they&amp;#x27;d say, &amp;#x27;Yeah, that&amp;#x27;s two weeks minimum,&amp;#x27;&amp;quot; Fadell recalled of his experience at Apple and Nest. &amp;quot;And we&amp;#x27;d say, &amp;#x27;No, no. Work day and night. It&amp;#x27;s two weeks.&amp;#x27; But it was always this fixed bottleneck.&amp;quot;&lt;/p&gt;&lt;p&gt;The consequences ripple through hardware organizations. Firmware teams sit idle waiting for physical boards to test their code. Validation engineers cannot begin debugging. Product launches slip. According to Quilter&amp;#x27;s research, only about 10 percent of first board revisions work correctly, forcing expensive and time-consuming respins.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Project Speedrun put Quilter&amp;#x27;s AI to the test with an 843-component computer that booted on the first try&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://quilter.ai/project-speedrun"&gt;Project Speedrun&lt;/a&gt; was designed to push the technology to its limits while producing an easily understood result: a working computer that could boot Linux, browse the internet, and run applications.&lt;/p&gt;&lt;p&gt;The system consists of two boards based on&lt;a href="https://www.nxp.com/design/design-center/development-boards-and-designs/8MMINILPD4-EVK"&gt; NXP&amp;#x27;s i.MX 8M Mini&lt;/a&gt; reference platform, a processor architecture used in automotive infotainment, industrial automation, and machine vision applications.&lt;/p&gt;&lt;p&gt;The main system-on-module contains a quad-core ARM processor running at 1.8 gigahertz, 2 gigabytes of LPDDR4 memory, and 32 gigabytes of eMMC storage. A companion baseboard provides connectivity including Ethernet, USB, HDMI, and audio.&lt;/p&gt;&lt;p&gt;Together, the boards incorporate 843 components and 5,141 electrical connections, or &amp;quot;pins,&amp;quot; routed across eight-layer circuit board stackups manufactured by Sierra Circuits in California. The minimum trace geometry reached 2 mils (two-thousandths of an inch) on the system-on-module — fine enough to require advanced high-density interconnect manufacturing techniques.&lt;/p&gt;&lt;p&gt;Quilter&amp;#x27;s AI completed the layout with approximately 98 percent routing coverage and zero design rule violations. Both boards passed power-on testing and successfully booted &lt;a href="https://www.debian.org/"&gt;Debian Linux&lt;/a&gt; on the first attempt.&lt;/p&gt;&lt;p&gt;&amp;quot;We made an entire computer to demonstrate that this technology works,&amp;quot; Nesterenko said. &amp;quot;We took something that&amp;#x27;s typically quoted at 400 to 450 hours, automated the vast majority of it, and reduced it to about 30 to 40 hours of cleanup time.&amp;quot;&lt;/p&gt;&lt;p&gt;The cleanup time is work that human engineers still perform: reviewing the AI&amp;#x27;s output, fixing any issues, and preparing final fabrication files. But even with that overhead, the total elapsed time from schematic to fabricated boards collapsed from the typical 11 weeks to a single week.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Unlike ChatGPT, Quilter&amp;#x27;s AI learns by playing billions of games against the laws of physics&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Quilter&amp;#x27;s &lt;a href="https://www.quilter.ai/product"&gt;technical approach&lt;/a&gt; differs fundamentally from the large language models that have dominated recent AI headlines. Where systems like GPT-5 or Claude learn to predict text based on massive training datasets of human writing, &lt;a href="https://www.quilter.ai/product/technology"&gt;Quilter&amp;#x27;s AI&lt;/a&gt; learns by playing what amounts to an elaborate game against the laws of physics.&lt;/p&gt;&lt;p&gt;&amp;quot;Language models don&amp;#x27;t apply to us because this is not a language problem,&amp;quot; Nesterenko explained. &amp;quot;If you ask it to actually create a blueprint, it has no training data for that. It has no context for that.&amp;quot;&lt;/p&gt;&lt;p&gt;The company also rejected the seemingly obvious approach of training on examples of human-designed boards. Nesterenko cited three reasons: humans make frequent errors (explaining why most boards require revisions), the best designs are locked inside large companies unwilling to share proprietary data, and training on human examples would cap the AI&amp;#x27;s performance at human levels.&lt;/p&gt;&lt;p&gt;Instead, Quilter built what Nesterenko describes as a &amp;quot;game&amp;quot; where the AI agent makes sequential decisions — place this component here, route this trace there — and receives feedback based on whether the resulting design satisfies electromagnetic, thermal, and manufacturing constraints.&lt;/p&gt;&lt;p&gt;&amp;quot;What you&amp;#x27;re really changing is not the probability of getting a very specific outcome of the model, but the probability of choosing a certain action based on that experience,&amp;quot; Nesterenko said.&lt;/p&gt;&lt;p&gt;The approach mirrors DeepMind&amp;#x27;s progression with its Go-playing systems. The original &lt;a href="https://deepmind.google/research/alphago/"&gt;AlphaGo&lt;/a&gt; learned from human games, but its successor &lt;a href="https://deepmind.google/research/alphazero-and-muzero/"&gt;AlphaZero&lt;/a&gt; learned purely through self-play and ultimately surpassed human capability. Quilter harbors similar ambitions.&lt;/p&gt;&lt;p&gt;&amp;quot;In the long term, to come up with better designs for circuit boards than humans have ever tried to do,&amp;quot; Nesterenko said.&lt;/p&gt;&lt;p&gt;Fadell drew a parallel to an earlier technological transition: &amp;quot;I remember this with assembly. You had assembly and compilers, and engineers would say, &amp;#x27;I can&amp;#x27;t trust the compiler. I&amp;#x27;m going to do the loop unrolling myself.&amp;#x27; Now very, very few people write any assembly.&amp;quot;&lt;/p&gt;&lt;p&gt;He expects PCB design to follow a similar arc: &amp;quot;I hope the same thing happens with PCB design. Sure, a few people will hold out, but these tools are going to get so good that everyone else will move on.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Fadell and Nesterenko spent months solving a delicate problem: how to automate design without stripping engineers of control&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Automating a task that skilled professionals have performed manually for decades raises an obvious question: how do engineers maintain control over designs that will ultimately ship in products where reliability matters?&lt;/p&gt;&lt;p&gt;Fadell said he spent significant time with Nesterenko working through this tension. The solution, he said, lies in allowing users to choose their level of involvement at each stage of the process.&lt;/p&gt;&lt;p&gt;&amp;quot;If you&amp;#x27;re a control freak, you can be a control freak. If you want to say &amp;#x27;just do it for me,&amp;#x27; you can do that too—and everything in between,&amp;quot; Fadell said. &amp;quot;You can walk through each phase of the design and get involved wherever you want, or let the AI handle it.&amp;quot;&lt;/p&gt;&lt;p&gt;The workflow breaks into three phases: setup, where engineers define constraints and requirements; execution, where the AI generates candidate layouts; and cleanup, where humans review and refine the output. Engineers can intervene at any point, adjusting constraints and regenerating designs until they&amp;#x27;re satisfied.&lt;/p&gt;&lt;p&gt;&amp;quot;This is something Tony and I talk about a lot,&amp;quot; Nesterenko said. &amp;quot;How do we give users control while still automating most of the work?&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Quilter&amp;#x27;s technology has clear boundaries: 10,000 pins and 10 gigahertz mark the current limits&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The technology has clear limitations. &lt;a href="https://www.quilter.ai/"&gt;Quilter&lt;/a&gt; currently handles boards with up to roughly 10,000 pins — sufficient for a wide range of applications but well short of the most complex designs, which can exceed 100,000 connections.&lt;/p&gt;&lt;p&gt;Physics complexity also creates boundaries. The system handles high-speed communications up to approximately 10 gigahertz, covering typical consumer electronics and many industrial applications. But advanced systems like sophisticated radar, which can operate at 100 gigahertz, exceed current capabilities.&lt;/p&gt;&lt;p&gt;&amp;quot;There are boards where Quilter won&amp;#x27;t make enough progress to make the cleanup time worthwhile,&amp;quot; Nesterenko acknowledged. &amp;quot;We&amp;#x27;re just not that helpful yet with the most advanced, sophisticated designs.&amp;quot;&lt;/p&gt;&lt;p&gt;The company has focused initially on categories where speed matters more than extreme complexity: test fixtures, evaluation boards, design validation boards, and environmental test hardware. These boards often sit in long queues behind higher-priority production designs, delaying engineering programs.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The company bets that engineers will pay the same price for a 10x speed improvement&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Quilter &lt;a href="https://www.quilter.ai/pricing"&gt;prices its service by pin count&lt;/a&gt;, matching the billing conventions that already exist when companies hire outside layout specialists. The pitch to customers is cost neutrality with a ten-fold improvement in speed.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;re going to charge you roughly the same that you would pay for the pins that you would with a person,&amp;quot; Nesterenko said. &amp;quot;But the reason you choose us is that we do this 10 times faster.&amp;quot;&lt;/p&gt;&lt;p&gt;For a company waiting three months for a board layout, receiving it in a week fundamentally changes what&amp;#x27;s possible. Engineering teams can run multiple design experiments in parallel. Firmware developers get hardware faster. Products reach the market sooner.&lt;/p&gt;&lt;p&gt;The company offers free access for hobbyists, students, and small businesses with less than $50,000 in revenue — a strategy to build familiarity while targeting enterprise customers for commercial revenue.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The iPod creator waited years to attach his name to Quilter — until he could prove the technology actually works&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Fadell said he chose this moment to publicly acknowledge his investment because the &lt;a href="https://quilter.ai/project-speedrun"&gt;Project Speedrun&lt;/a&gt; demonstration provides concrete evidence that the technology works.&lt;/p&gt;&lt;p&gt;&amp;quot;It&amp;#x27;s not about being comfortable—I was always comfortable with the team,&amp;quot; he said. &amp;quot;This was about waiting until we had something you could hang your hat on. Now I can say, &amp;#x27;I&amp;#x27;ve used the tool. I&amp;#x27;ve seen it.&amp;#x27;&amp;quot;&lt;/p&gt;&lt;p&gt;He contrasted his approach with typical investor announcements: &amp;quot;Every investor goes, I invested in this, it&amp;#x27;s gonna change the world. It&amp;#x27;s like, no, I know better. I&amp;#x27;ve used the tool. I know people who use it. I asked my startups to use the tool.&amp;quot;&lt;/p&gt;&lt;p&gt;Fadell&amp;#x27;s involvement goes beyond capital. He described email exchanges running to &amp;quot;a dozen pages of details&amp;quot; covering product design, user experience, enterprise sales, and technical architecture.&lt;/p&gt;&lt;p&gt;&amp;quot;Of all the investors I work with, Tony by far goes deepest with me on the product side,&amp;quot; Nesterenko said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;If Quilter succeeds, it could unlock a new generation of hardware startups that were never economically viable before&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The stakes extend far beyond one company&amp;#x27;s product roadmap. If Quilter&amp;#x27;s technology scales, it could fundamentally alter the economics of building physical products.&lt;/p&gt;&lt;p&gt;Fadell argued that hardware development has historically moved slowly because each step in the process — schematic design, PCB layout, manufacturing, assembly — created friction. Other innovations have already smoothed schematic tools and manufacturing. Layout remained the stubborn holdout.&lt;/p&gt;&lt;p&gt;&amp;quot;Once you shrink that from weeks to hours, you can iterate so much faster because all the other friction in the chain has been reduced,&amp;quot; Fadell said.&lt;/p&gt;&lt;p&gt;He predicted the technology would eventually extend upstream into schematic design itself, with AI that understands both logical connections and physical constraints helping engineers avoid problems earlier in the process.&lt;/p&gt;&lt;p&gt;At MIT, where Fadell now spends time, he encounters would-be founders who have abandoned hardware ambitions because the process seemed insurmountable.&lt;/p&gt;&lt;p&gt;&amp;quot;I talk to professors and startup founders, and they say, &amp;#x27;I&amp;#x27;m never doing hardware. It&amp;#x27;s too hard,&amp;#x27;&amp;quot; he said. &amp;quot;I hope we can make it easier for more people to jump in and try things.&amp;quot;&lt;/p&gt;&lt;p&gt;Industry veterans remain skeptical. Auto-routing tools — previous attempts at automation — became notorious for producing unusable results, spawning T-shirts proclaiming engineers would &amp;quot;&lt;a href="https://www.eevblog.com/forum/chat/chris-gammel_s-_never-trust-the-autorouter-shirt_-on-teespring/"&gt;never trust the auto-router&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;p&gt;Nesterenko has seen the skepticism dissolve in real time. He described a recent meeting with executives from a major customer who came to discuss Quilter&amp;#x27;s capabilities. As the conversation unfolded, one executive picked up the &lt;a href="https://quilter.ai/project-speedrun"&gt;Project Speedrun&lt;/a&gt; boards and began photographing them from every angle, turning them over in his hands.&lt;/p&gt;&lt;p&gt;&amp;quot;He was just fascinated by the fact that this is possible now,&amp;quot; Nesterenko said.&lt;/p&gt;&lt;p&gt;The question is no longer whether AI can design circuit boards. A working Linux computer, assembled from 843 components and booted on the first attempt, answers that definitively. The question now is what engineers will build when layout stops being the bottleneck — when hardware, as Fadell put it, finally &amp;quot;moves at the speed of thought.&amp;quot;&lt;/p&gt;&lt;p&gt;On that point, Nesterenko offered a prediction. &amp;quot;If you ask the average electrical engineer today whether automation or AI could at all help with the board of this complexity, they would say no,&amp;quot; he said. For decades, they would have been right. As of last week, they&amp;#x27;re not.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/quilters-ai-just-designed-an-843-part-linux-computer-that-booted-on-the</guid><pubDate>Wed, 10 Dec 2025 14:00:00 +0000</pubDate></item><item><title>Figma launches new AI-powered object removal and image extension (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/10/figma-launches-new-ai-powered-object-removal-and-image-extension/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Design tool Figma launched new AI-powered image-editing features today, including the ability to remove and isolate objects and expand images.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said that these features will save the hassle of exporting images to other tools for editing and importing them back. It added that generation models like Nano Banana are good for creating images, but users often need granular tools for editing that work without any text prompts. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Figma has improved its lasso tool for selection. Now you can use it to select an object, remove it, or isolate it to move it around. When you move around the object, the image still retains other characteristics, such as background and color. Users can also select an object to adjust factors like lighting, shadow, color, or focus.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also bringing an image-expansion feature to its design suite. This feature is handy when you are adjusting a creative for a particular format and need to fill in the background or other details. For instance, creating a web banner or mobile banner from a 1×1 image. It essentially saves you from constantly cropping an image and adjusting elements within it.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3074654" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/ezgif-8bda4c8d57256f21.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Figma&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Besides adding these features, Figma is collating all its image-editing tools in one toolbar for easier access. You can select objects or parts of images, change background color, and add annotations or text using this toolbar. The company said removing background is one of the most common actions on the platform, and that is why it is getting a prominent spot on the new toolbar.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rivals like Adobe and Canva have had object removal for a few years now. Figma is finally catching up to offer these features. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said the new image-editing features are available on Figma Design and Draw, with plans to make them available across Figma tools next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Figma’s launch comes on the same day as Adobe making some of these features available to users within ChatGPT. Figma was one of the launch partners of the app, calling on ChatGPT in October. It is not clear if these new functions will be available to users using Figma within OpenAI’s tool. &lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Design tool Figma launched new AI-powered image-editing features today, including the ability to remove and isolate objects and expand images.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said that these features will save the hassle of exporting images to other tools for editing and importing them back. It added that generation models like Nano Banana are good for creating images, but users often need granular tools for editing that work without any text prompts. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Figma has improved its lasso tool for selection. Now you can use it to select an object, remove it, or isolate it to move it around. When you move around the object, the image still retains other characteristics, such as background and color. Users can also select an object to adjust factors like lighting, shadow, color, or focus.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also bringing an image-expansion feature to its design suite. This feature is handy when you are adjusting a creative for a particular format and need to fill in the background or other details. For instance, creating a web banner or mobile banner from a 1×1 image. It essentially saves you from constantly cropping an image and adjusting elements within it.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3074654" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/ezgif-8bda4c8d57256f21.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Figma&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Besides adding these features, Figma is collating all its image-editing tools in one toolbar for easier access. You can select objects or parts of images, change background color, and add annotations or text using this toolbar. The company said removing background is one of the most common actions on the platform, and that is why it is getting a prominent spot on the new toolbar.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rivals like Adobe and Canva have had object removal for a few years now. Figma is finally catching up to offer these features. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said the new image-editing features are available on Figma Design and Draw, with plans to make them available across Figma tools next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Figma’s launch comes on the same day as Adobe making some of these features available to users within ChatGPT. Figma was one of the launch partners of the app, calling on ChatGPT in October. It is not clear if these new functions will be available to users using Figma within OpenAI’s tool. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/10/figma-launches-new-ai-powered-object-removal-and-image-extension/</guid><pubDate>Wed, 10 Dec 2025 14:00:00 +0000</pubDate></item><item><title>AI startup Tavus founder says users talk to its AI Santa ‘for hours’ per day (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/10/ai-startup-tavus-founder-says-users-talk-to-its-ai-santa-for-hours-per-day/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A new helper has arrived at the North Pole in recent years: AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tavus, the AI startup that creates digital replicas using voice and face cloning technology, has launched its AI Santa experience for the second year in a row. This allows parents and children to video chat with a virtual version of the jolly old Saint Nick.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After signing up for a free account, users can interact with AI Santa via text, phone, or video chat. Users can tell AI Santa what they want for Christmas, share their holiday plans, and find out if they’re on the naughty or nice list.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year, the company debuted an improved version of AI Santa, designed to be more expressive and emotionally aware. Santa is now a “Tavus PAL,” the company’s name for its real-time AI agents that are built to see, hear, respond, and appear human.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI Santa can now see users’ expressions and gestures and respond to them. It also remembers users’ conversations and interests, creating a more personalized experience. Notably, it now can take actions of its own, including searching the web for present ideas or even perform everyday tasks like drafting emails.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3074427" height="427" src="https://techcrunch.com/wp-content/uploads/2025/12/santapostcard-Tavus.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tavus&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;During testing, the conversation with AI Santa was engaging for the most part. When we mentioned wanting a new PlayStation for Christmas, Santa followed up with questions about our favorite video games, showing knowledge of specific titles like Baldur’s Gate 3. It also smiled back when we did. (We didn’t like that part very much, but maybe others will.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users appear to be enjoying the improved experience so far. Founder and CEO Hassaan Raza said that many people are engaging with the platform frequently, spending hours chatting with AI Santa and often reaching their daily limits.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Last year’s AI Santa drew millions of hits, and we’re on pace to surpass that by a wide margin as Christmas approaches,” he noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While this level of engagement marks a milestone for Tavus, it also raises questions about the impact of such interactions, especially for young children. Children may struggle to distinguish between AI and a real person. Spending hours in conversation with an AI has already been linked to negative effects in adults, making the potential effects on children who strongly believe in Santa a concern for some parents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During our testing, there were subtle cues that the AI Santa does yet appear fully human-like, such as long pauses and a flat voice. We also found that if a user were to question whether it’s real, the programmed response was: “I’m an AI Santa powered by Tavus’ magic and technology. I might not be the physical Santa, but I’ve got the spirit and the cheer.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still, the experience launches amid growing concerns about AI’s effects on young users. There have been reports linking chatbot interactions to serious harm, including cases where chatbots were implicated in the suicide deaths of teenagers. Character.AI removed access to its chatbots for users under 18 in October.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3074428" height="424" src="https://techcrunch.com/wp-content/uploads/2025/12/santaDesktop.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tavus&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Raza emphasized that the AI Santa experience is designed for families to enjoy together, with safety measures in place to ensure appropriate interactions. Safety features, such as content filters, have been implemented to maintain family-friendly discussions.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In certain situations, conversations can be terminated, and users are directed to mental health resources if necessary.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The vast majority of interactions have been family-friendly and true to the Santa experience,” he said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, when asked about data collection, Raza said the company “collects logs, session timestamps, metadata, and other information users choose to share during their chats. This data is used to provide and maintain a safe experience, and users can request data deletion at any point in time.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A new helper has arrived at the North Pole in recent years: AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tavus, the AI startup that creates digital replicas using voice and face cloning technology, has launched its AI Santa experience for the second year in a row. This allows parents and children to video chat with a virtual version of the jolly old Saint Nick.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After signing up for a free account, users can interact with AI Santa via text, phone, or video chat. Users can tell AI Santa what they want for Christmas, share their holiday plans, and find out if they’re on the naughty or nice list.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year, the company debuted an improved version of AI Santa, designed to be more expressive and emotionally aware. Santa is now a “Tavus PAL,” the company’s name for its real-time AI agents that are built to see, hear, respond, and appear human.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI Santa can now see users’ expressions and gestures and respond to them. It also remembers users’ conversations and interests, creating a more personalized experience. Notably, it now can take actions of its own, including searching the web for present ideas or even perform everyday tasks like drafting emails.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3074427" height="427" src="https://techcrunch.com/wp-content/uploads/2025/12/santapostcard-Tavus.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tavus&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;During testing, the conversation with AI Santa was engaging for the most part. When we mentioned wanting a new PlayStation for Christmas, Santa followed up with questions about our favorite video games, showing knowledge of specific titles like Baldur’s Gate 3. It also smiled back when we did. (We didn’t like that part very much, but maybe others will.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users appear to be enjoying the improved experience so far. Founder and CEO Hassaan Raza said that many people are engaging with the platform frequently, spending hours chatting with AI Santa and often reaching their daily limits.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Last year’s AI Santa drew millions of hits, and we’re on pace to surpass that by a wide margin as Christmas approaches,” he noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While this level of engagement marks a milestone for Tavus, it also raises questions about the impact of such interactions, especially for young children. Children may struggle to distinguish between AI and a real person. Spending hours in conversation with an AI has already been linked to negative effects in adults, making the potential effects on children who strongly believe in Santa a concern for some parents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During our testing, there were subtle cues that the AI Santa does yet appear fully human-like, such as long pauses and a flat voice. We also found that if a user were to question whether it’s real, the programmed response was: “I’m an AI Santa powered by Tavus’ magic and technology. I might not be the physical Santa, but I’ve got the spirit and the cheer.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still, the experience launches amid growing concerns about AI’s effects on young users. There have been reports linking chatbot interactions to serious harm, including cases where chatbots were implicated in the suicide deaths of teenagers. Character.AI removed access to its chatbots for users under 18 in October.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3074428" height="424" src="https://techcrunch.com/wp-content/uploads/2025/12/santaDesktop.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tavus&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Raza emphasized that the AI Santa experience is designed for families to enjoy together, with safety measures in place to ensure appropriate interactions. Safety features, such as content filters, have been implemented to maintain family-friendly discussions.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In certain situations, conversations can be terminated, and users are directed to mental health resources if necessary.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The vast majority of interactions have been family-friendly and true to the Santa experience,” he said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, when asked about data collection, Raza said the company “collects logs, session timestamps, metadata, and other information users choose to share during their chats. This data is used to provide and maintain a safe experience, and users can request data deletion at any point in time.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/10/ai-startup-tavus-founder-says-users-talk-to-its-ai-santa-for-hours-per-day/</guid><pubDate>Wed, 10 Dec 2025 14:00:00 +0000</pubDate></item><item><title>OpenAI report reveals a 6x productivity gap between AI power users and everyone else (AI | VentureBeat)</title><link>https://venturebeat.com/ai/openai-report-reveals-a-6x-productivity-gap-between-ai-power-users-and</link><description>[unable to retrieve full-text content]&lt;p&gt;The tools are available to everyone. The subscription is company-wide. The training sessions have been held. And yet, in offices from Wall Street to Silicon Valley, a stark divide is opening between workers who have woven artificial intelligence into the fabric of their daily work and colleagues who have barely touched it.&lt;/p&gt;&lt;p&gt;The gap is not small. According to a &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;new report&lt;/a&gt; from OpenAI analyzing usage patterns across its more than one million business customers, workers at the &lt;a href="https://openai.com/index/the-state-of-enterprise-ai-2025-report/"&gt;95th percentile of AI adoption&lt;/a&gt; are sending six times as many messages to ChatGPT as the median employee at the same companies. For specific tasks, the divide is even more dramatic: frontier workers send 17 times as many coding-related messages as their typical peers, and among data analysts, the heaviest users engage the data analysis tool 16 times more frequently than the median.&lt;/p&gt;&lt;p&gt;This is not a story about access. It is a story about a new form of workplace stratification emerging in real time — one that may be reshaping who gets ahead, who falls behind, and what it means to be a skilled worker in the age of artificial intelligence.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Everyone has the same tools, but not everyone is using them&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Perhaps the most striking finding in the &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;OpenAI report&lt;/a&gt; is how little access explains. &lt;a href="https://chatgpt.com/business/enterprise?utm_source=google&amp;amp;utm_medium=paidsearch_brand&amp;amp;utm_campaign=GOOG_B_SEM_GBR_Core_ENT_BAU_ACQ_PER_BRD_ALL_NAMER_US_EN_080625&amp;amp;utm_term=chatgpt%20enterprise&amp;amp;utm_content=182507886919&amp;amp;utm_ad=779434575256&amp;amp;utm_match=b&amp;amp;gad_source=1&amp;amp;gad_campaignid=22855802308&amp;amp;gbraid=0AAAAA-I0E5ew3OYocHAOSOiKw516uqob_&amp;amp;gclid=Cj0KCQiArt_JBhCTARIsADQZaylE-8Q0a0VjiXwyRLvFkD9BIkgt4EpWCm6v7alzBtNKWKE5_7J_3SYaArnTEALw_wcB"&gt;ChatGPT Enterprise&lt;/a&gt; is now deployed across more than 7 million workplace seats globally, a nine-fold increase from a year ago. The tools are the same for everyone. The capabilities are identical. And yet usage varies by orders of magnitude.&lt;/p&gt;&lt;p&gt;Among monthly active users — people who have logged in at least once in the past 30 days — &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;19 percent have never tried the data analysis feature&lt;/a&gt;. Fourteen percent have never used reasoning capabilities. Twelve percent have never used search. These are not obscure features buried in submenus; they are core functionality that OpenAI highlights as transformative for knowledge work.&lt;/p&gt;&lt;p&gt;The pattern inverts among daily users. &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;Only 3 percent&lt;/a&gt; of people who use ChatGPT every day have never tried data analysis; just 1 percent have skipped reasoning or search. The implication is clear: the divide is not between those who have access and those who don&amp;#x27;t, but between those who have made AI a daily habit and those for whom it remains an occasional novelty.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Employees who experiment more are saving dramatically more time&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;OpenAI report&lt;/a&gt; suggests that AI productivity gains are not evenly distributed across all users but concentrated among those who use the technology most intensively. Workers who engage across approximately seven distinct task types — data analysis, coding, image generation, translation, writing, and others — report saving five times as much time as those who use only four. Employees who save more than 10 hours per week consume eight times more AI credits than those who report no time savings at all.&lt;/p&gt;&lt;p&gt;This creates a compounding dynamic. Workers who experiment broadly discover more uses. More uses lead to greater productivity gains. Greater productivity gains presumably lead to better performance reviews, more interesting assignments, and faster advancement—which in turn provides more opportunity and incentive to deepen AI usage further.&lt;/p&gt;&lt;p&gt;Seventy-five percent of surveyed workers report being able to complete tasks they previously could not perform, including programming support, spreadsheet automation, and technical troubleshooting. For workers who have embraced these capabilities, the boundaries of their roles are expanding. For those who have not, the boundaries may be contracting by comparison.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The corporate AI paradox: $40 billion spent, 95 percent seeing no return&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The individual usage gap documented by OpenAI mirrors a broader pattern identified by a separate study from &lt;a href="https://venturebeat.com/ai/mit-report-misunderstood-shadow-ai-economy-booms-while-headlines-cry-failure"&gt;MIT&amp;#x27;s Project NANDA&lt;/a&gt;. Despite $30 billion to $40 billion invested in generative AI initiatives, only 5 percent of organizations are seeing transformative returns. The researchers call this the &amp;quot;&lt;a href="https://www.searchyour.ai/en/genai-divide-state-ai-business-2025-mit-nanda"&gt;GenAI Divide&lt;/a&gt;&amp;quot; — a gap separating the few organizations that succeed in transforming processes with adaptive AI systems from the majority that remain stuck in pilots.&lt;/p&gt;&lt;p&gt;The MIT report found &lt;a href="https://www.legal.io/articles/5719519/MIT-Report-Finds-95-of-AI-Pilots-Fail-to-Deliver-ROI-Exposing-GenAI-Divide"&gt;limited disruption&lt;/a&gt; across industries: only two of nine major sectors—technology and media—show material business transformation from generative AI use. Large firms lead in pilot volume but lag in successful deployment.&lt;/p&gt;&lt;p&gt;The pattern is consistent across both studies. Organizations and individuals are buying the technology. They are launching pilots. They are attending training sessions. But somewhere between adoption and transformation, most are getting stuck.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;While official AI projects stall, a shadow economy is thriving&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href="https://venturebeat.com/ai/mit-report-misunderstood-shadow-ai-economy-booms-while-headlines-cry-failure"&gt;MIT study&lt;/a&gt; reveals a striking disconnect: while only 40 percent of companies have purchased official LLM subscriptions, employees in over 90 percent of companies regularly use personal AI tools for work. Nearly every respondent reported using LLMs in some form as part of their regular workflow.&lt;/p&gt;&lt;p&gt;&amp;quot;This &amp;#x27;&lt;a href="https://venturebeat.com/ai/mit-report-misunderstood-shadow-ai-economy-booms-while-headlines-cry-failure"&gt;shadow AI&lt;/a&gt;&amp;#x27; often delivers better ROI than formal initiatives and reveals what actually works for bridging the divide,&amp;quot; MIT&amp;#x27;s Project NANDA found.&lt;/p&gt;&lt;p&gt;The shadow economy offers a clue to what&amp;#x27;s happening at the individual level within organizations. Employees who take initiative — who sign up for personal subscriptions, who experiment on their own time, who figure out how to integrate AI into their workflows without waiting for IT approval — are pulling ahead of colleagues who wait for official guidance that may never come.&lt;/p&gt;&lt;p&gt;These shadow systems, largely unsanctioned, often deliver better performance and faster adoption than corporate tools. Worker sentiment reveals a preference for flexible, responsive tools — precisely the kind of experimentation that separates OpenAI&amp;#x27;s frontier workers from the median.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The biggest gaps show up in technical work that used to require specialists&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The largest relative gaps between frontier and median workers appear in coding, writing, and analysis — precisely the task categories where AI capabilities have advanced most rapidly. Frontier workers are not just doing the same work faster; they appear to be doing different work entirely, expanding into technical domains that were previously inaccessible to them.&lt;/p&gt;&lt;p&gt;Among &lt;a href="https://chatgpt.com/business/enterprise?utm_source=google&amp;amp;utm_medium=paidsearch_brand&amp;amp;utm_campaign=GOOG_B_SEM_GBR_Core_ENT_BAU_ACQ_PER_BRD_ALL_NAMER_US_EN_080625&amp;amp;utm_term=chatgpt%20enterprise&amp;amp;utm_content=182507886919&amp;amp;utm_ad=779434575253&amp;amp;utm_match=b&amp;amp;gad_source=1&amp;amp;gad_campaignid=22855802308&amp;amp;gbraid=0AAAAA-I0E5ew3OYocHAOSOiKw516uqob_&amp;amp;gclid=Cj0KCQiArt_JBhCTARIsADQZaykhZ9zJ10fojuPYA8XhHxR0jVT-WbgS4kfA7IIaTohf7yAByZszI-AaAkeREALw_wcB"&gt;ChatGPT Enterprise&lt;/a&gt; users outside of engineering, IT, and research, coding-related messages have grown 36 percent over the past six months. Someone in marketing or HR who learns to write scripts and automate workflows is becoming a categorically different employee than a peer who has not — even if they hold the same title and started with the same skills.&lt;/p&gt;&lt;p&gt;The academic research on AI and productivity offers a complicated picture. Several studies cited in the OpenAI report find that AI has an &amp;quot;&lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;equalizing effect&lt;/a&gt;,&amp;quot; disproportionately helping lower-performing workers close the gap with their higher-performing peers. But the equalizing effect may apply only within the population of workers who actually use AI regularly. A meaningful share of workers are not in that group at all. They remain light users or non-users, even as their more adventurous colleagues pull away.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Companies are divided too, and the gap is widening by the month&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The divide is not only between individual workers. It exists between entire organizations.&lt;/p&gt;&lt;p&gt;Frontier firms — those at the 95th percentile of adoption intensity — generate approximately twice as many AI messages per employee as the median enterprise. For messages routed through custom GPTs, purpose-built tools that automate specific workflows, the gap widens to seven-fold.&lt;/p&gt;&lt;p&gt;These numbers suggest fundamentally different operating models. At median companies, AI may be a productivity tool that individual workers use at their discretion. At frontier firms, AI appears to be embedded in core infrastructure: standardized workflows, persistent custom tools, systematic integration with internal data systems.&lt;/p&gt;&lt;p&gt;The &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;OpenAI report&lt;/a&gt; notes that roughly one in four enterprises still has not enabled connectors that give AI access to company data—a basic step that dramatically increases the technology&amp;#x27;s utility. The MIT study found that companies that purchased AI tools from specialized vendors succeeded &lt;a href="https://www.tekedia.com/the-genai-divide-mit-report-shows-why-most-business-ai-projects-are-stalling/"&gt;67 percent&lt;/a&gt; of the time, while internal builds had only a one-in-three success rate. For many organizations, the AI era has technically arrived but has not yet begun in practice.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The technology is no longer the problem — organizations are&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For executives, the data presents an uncomfortable challenge. The technology is no longer the constraint. OpenAI notes that it releases a new feature or capability roughly every three days; the models are advancing faster than most organizations can absorb. The bottleneck has shifted from what AI can do to whether organizations are structured to take advantage of it.&lt;/p&gt;&lt;p&gt;&amp;quot;The dividing line isn&amp;#x27;t intelligence,&amp;quot; the &lt;a href="https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf"&gt;MIT authors write&lt;/a&gt;. The problems with enterprise AI have to do with memory, adaptability, and learning capability. Problems stem less from regulations or model performance, and more from tools that fail to learn or adapt.&lt;/p&gt;&lt;p&gt;Leading firms, according to the &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;OpenAI report&lt;/a&gt;, consistently invest in executive sponsorship, data readiness, workflow standardization, and deliberate change management. They build cultures where custom AI tools are created, shared, and refined across teams. They track performance and run evaluations. They make AI adoption a strategic priority rather than an individual choice.&lt;/p&gt;&lt;p&gt;The rest are leaving it to chance — hoping that workers will discover the tools on their own, experiment on their own time, and somehow propagate best practices without infrastructure or incentive. The six-fold gap suggests this approach is not working.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The window to catch up is closing faster than most companies realize&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With enterprise contracts locking in over the next 18 months, there&amp;#x27;s a shrinking window for vendors and adopters to cross the divide.The GenAI Divide identified by the &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;MIT report&lt;/a&gt; is not going to last forever. But the organizations that figure out a way across it soonest will be the ones that define the next era of business.&lt;/p&gt;&lt;p&gt;Both reports carry caveats. The OpenAI data comes from a company with an obvious interest in promoting AI adoption. The productivity figures are self-reported by customers already paying for the product. The MIT study, while independent, relies on interviews and surveys rather than direct measurement. The long-term effects of this technology on employment, wages, and workplace dynamics remain uncertain.&lt;/p&gt;&lt;p&gt;But the core finding — that access alone does not produce adoption, and that adoption varies enormously even within organizations that have made identical tools available to all — is consistent with how previous technologies have diffused through the economy. Spreadsheets, email, and the internet all created similar divides before eventually becoming universal. The question is how long the current gap persists, who benefits during the transition, and what happens to workers who find themselves on the wrong side of it.&lt;/p&gt;&lt;p&gt;For now, the divide is stark. Ninety percent of users said they prefer humans for &amp;quot;mission-critical work,&amp;quot; while AI has &amp;quot;won the war for simple work.&amp;quot; The workers who are pulling ahead are not doing so because they have access their colleagues lack. They are pulling ahead because they decided to use what everyone already has—and kept using it until they figured out what it could do.&lt;/p&gt;&lt;p&gt;The 6x gap is not about technology. It is about behavior. And behavior, unlike software, cannot be deployed with a company-wide rollout.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;The tools are available to everyone. The subscription is company-wide. The training sessions have been held. And yet, in offices from Wall Street to Silicon Valley, a stark divide is opening between workers who have woven artificial intelligence into the fabric of their daily work and colleagues who have barely touched it.&lt;/p&gt;&lt;p&gt;The gap is not small. According to a &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;new report&lt;/a&gt; from OpenAI analyzing usage patterns across its more than one million business customers, workers at the &lt;a href="https://openai.com/index/the-state-of-enterprise-ai-2025-report/"&gt;95th percentile of AI adoption&lt;/a&gt; are sending six times as many messages to ChatGPT as the median employee at the same companies. For specific tasks, the divide is even more dramatic: frontier workers send 17 times as many coding-related messages as their typical peers, and among data analysts, the heaviest users engage the data analysis tool 16 times more frequently than the median.&lt;/p&gt;&lt;p&gt;This is not a story about access. It is a story about a new form of workplace stratification emerging in real time — one that may be reshaping who gets ahead, who falls behind, and what it means to be a skilled worker in the age of artificial intelligence.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Everyone has the same tools, but not everyone is using them&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Perhaps the most striking finding in the &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;OpenAI report&lt;/a&gt; is how little access explains. &lt;a href="https://chatgpt.com/business/enterprise?utm_source=google&amp;amp;utm_medium=paidsearch_brand&amp;amp;utm_campaign=GOOG_B_SEM_GBR_Core_ENT_BAU_ACQ_PER_BRD_ALL_NAMER_US_EN_080625&amp;amp;utm_term=chatgpt%20enterprise&amp;amp;utm_content=182507886919&amp;amp;utm_ad=779434575256&amp;amp;utm_match=b&amp;amp;gad_source=1&amp;amp;gad_campaignid=22855802308&amp;amp;gbraid=0AAAAA-I0E5ew3OYocHAOSOiKw516uqob_&amp;amp;gclid=Cj0KCQiArt_JBhCTARIsADQZaylE-8Q0a0VjiXwyRLvFkD9BIkgt4EpWCm6v7alzBtNKWKE5_7J_3SYaArnTEALw_wcB"&gt;ChatGPT Enterprise&lt;/a&gt; is now deployed across more than 7 million workplace seats globally, a nine-fold increase from a year ago. The tools are the same for everyone. The capabilities are identical. And yet usage varies by orders of magnitude.&lt;/p&gt;&lt;p&gt;Among monthly active users — people who have logged in at least once in the past 30 days — &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;19 percent have never tried the data analysis feature&lt;/a&gt;. Fourteen percent have never used reasoning capabilities. Twelve percent have never used search. These are not obscure features buried in submenus; they are core functionality that OpenAI highlights as transformative for knowledge work.&lt;/p&gt;&lt;p&gt;The pattern inverts among daily users. &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;Only 3 percent&lt;/a&gt; of people who use ChatGPT every day have never tried data analysis; just 1 percent have skipped reasoning or search. The implication is clear: the divide is not between those who have access and those who don&amp;#x27;t, but between those who have made AI a daily habit and those for whom it remains an occasional novelty.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Employees who experiment more are saving dramatically more time&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;OpenAI report&lt;/a&gt; suggests that AI productivity gains are not evenly distributed across all users but concentrated among those who use the technology most intensively. Workers who engage across approximately seven distinct task types — data analysis, coding, image generation, translation, writing, and others — report saving five times as much time as those who use only four. Employees who save more than 10 hours per week consume eight times more AI credits than those who report no time savings at all.&lt;/p&gt;&lt;p&gt;This creates a compounding dynamic. Workers who experiment broadly discover more uses. More uses lead to greater productivity gains. Greater productivity gains presumably lead to better performance reviews, more interesting assignments, and faster advancement—which in turn provides more opportunity and incentive to deepen AI usage further.&lt;/p&gt;&lt;p&gt;Seventy-five percent of surveyed workers report being able to complete tasks they previously could not perform, including programming support, spreadsheet automation, and technical troubleshooting. For workers who have embraced these capabilities, the boundaries of their roles are expanding. For those who have not, the boundaries may be contracting by comparison.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The corporate AI paradox: $40 billion spent, 95 percent seeing no return&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The individual usage gap documented by OpenAI mirrors a broader pattern identified by a separate study from &lt;a href="https://venturebeat.com/ai/mit-report-misunderstood-shadow-ai-economy-booms-while-headlines-cry-failure"&gt;MIT&amp;#x27;s Project NANDA&lt;/a&gt;. Despite $30 billion to $40 billion invested in generative AI initiatives, only 5 percent of organizations are seeing transformative returns. The researchers call this the &amp;quot;&lt;a href="https://www.searchyour.ai/en/genai-divide-state-ai-business-2025-mit-nanda"&gt;GenAI Divide&lt;/a&gt;&amp;quot; — a gap separating the few organizations that succeed in transforming processes with adaptive AI systems from the majority that remain stuck in pilots.&lt;/p&gt;&lt;p&gt;The MIT report found &lt;a href="https://www.legal.io/articles/5719519/MIT-Report-Finds-95-of-AI-Pilots-Fail-to-Deliver-ROI-Exposing-GenAI-Divide"&gt;limited disruption&lt;/a&gt; across industries: only two of nine major sectors—technology and media—show material business transformation from generative AI use. Large firms lead in pilot volume but lag in successful deployment.&lt;/p&gt;&lt;p&gt;The pattern is consistent across both studies. Organizations and individuals are buying the technology. They are launching pilots. They are attending training sessions. But somewhere between adoption and transformation, most are getting stuck.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;While official AI projects stall, a shadow economy is thriving&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href="https://venturebeat.com/ai/mit-report-misunderstood-shadow-ai-economy-booms-while-headlines-cry-failure"&gt;MIT study&lt;/a&gt; reveals a striking disconnect: while only 40 percent of companies have purchased official LLM subscriptions, employees in over 90 percent of companies regularly use personal AI tools for work. Nearly every respondent reported using LLMs in some form as part of their regular workflow.&lt;/p&gt;&lt;p&gt;&amp;quot;This &amp;#x27;&lt;a href="https://venturebeat.com/ai/mit-report-misunderstood-shadow-ai-economy-booms-while-headlines-cry-failure"&gt;shadow AI&lt;/a&gt;&amp;#x27; often delivers better ROI than formal initiatives and reveals what actually works for bridging the divide,&amp;quot; MIT&amp;#x27;s Project NANDA found.&lt;/p&gt;&lt;p&gt;The shadow economy offers a clue to what&amp;#x27;s happening at the individual level within organizations. Employees who take initiative — who sign up for personal subscriptions, who experiment on their own time, who figure out how to integrate AI into their workflows without waiting for IT approval — are pulling ahead of colleagues who wait for official guidance that may never come.&lt;/p&gt;&lt;p&gt;These shadow systems, largely unsanctioned, often deliver better performance and faster adoption than corporate tools. Worker sentiment reveals a preference for flexible, responsive tools — precisely the kind of experimentation that separates OpenAI&amp;#x27;s frontier workers from the median.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The biggest gaps show up in technical work that used to require specialists&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The largest relative gaps between frontier and median workers appear in coding, writing, and analysis — precisely the task categories where AI capabilities have advanced most rapidly. Frontier workers are not just doing the same work faster; they appear to be doing different work entirely, expanding into technical domains that were previously inaccessible to them.&lt;/p&gt;&lt;p&gt;Among &lt;a href="https://chatgpt.com/business/enterprise?utm_source=google&amp;amp;utm_medium=paidsearch_brand&amp;amp;utm_campaign=GOOG_B_SEM_GBR_Core_ENT_BAU_ACQ_PER_BRD_ALL_NAMER_US_EN_080625&amp;amp;utm_term=chatgpt%20enterprise&amp;amp;utm_content=182507886919&amp;amp;utm_ad=779434575253&amp;amp;utm_match=b&amp;amp;gad_source=1&amp;amp;gad_campaignid=22855802308&amp;amp;gbraid=0AAAAA-I0E5ew3OYocHAOSOiKw516uqob_&amp;amp;gclid=Cj0KCQiArt_JBhCTARIsADQZaykhZ9zJ10fojuPYA8XhHxR0jVT-WbgS4kfA7IIaTohf7yAByZszI-AaAkeREALw_wcB"&gt;ChatGPT Enterprise&lt;/a&gt; users outside of engineering, IT, and research, coding-related messages have grown 36 percent over the past six months. Someone in marketing or HR who learns to write scripts and automate workflows is becoming a categorically different employee than a peer who has not — even if they hold the same title and started with the same skills.&lt;/p&gt;&lt;p&gt;The academic research on AI and productivity offers a complicated picture. Several studies cited in the OpenAI report find that AI has an &amp;quot;&lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;equalizing effect&lt;/a&gt;,&amp;quot; disproportionately helping lower-performing workers close the gap with their higher-performing peers. But the equalizing effect may apply only within the population of workers who actually use AI regularly. A meaningful share of workers are not in that group at all. They remain light users or non-users, even as their more adventurous colleagues pull away.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Companies are divided too, and the gap is widening by the month&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The divide is not only between individual workers. It exists between entire organizations.&lt;/p&gt;&lt;p&gt;Frontier firms — those at the 95th percentile of adoption intensity — generate approximately twice as many AI messages per employee as the median enterprise. For messages routed through custom GPTs, purpose-built tools that automate specific workflows, the gap widens to seven-fold.&lt;/p&gt;&lt;p&gt;These numbers suggest fundamentally different operating models. At median companies, AI may be a productivity tool that individual workers use at their discretion. At frontier firms, AI appears to be embedded in core infrastructure: standardized workflows, persistent custom tools, systematic integration with internal data systems.&lt;/p&gt;&lt;p&gt;The &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;OpenAI report&lt;/a&gt; notes that roughly one in four enterprises still has not enabled connectors that give AI access to company data—a basic step that dramatically increases the technology&amp;#x27;s utility. The MIT study found that companies that purchased AI tools from specialized vendors succeeded &lt;a href="https://www.tekedia.com/the-genai-divide-mit-report-shows-why-most-business-ai-projects-are-stalling/"&gt;67 percent&lt;/a&gt; of the time, while internal builds had only a one-in-three success rate. For many organizations, the AI era has technically arrived but has not yet begun in practice.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The technology is no longer the problem — organizations are&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For executives, the data presents an uncomfortable challenge. The technology is no longer the constraint. OpenAI notes that it releases a new feature or capability roughly every three days; the models are advancing faster than most organizations can absorb. The bottleneck has shifted from what AI can do to whether organizations are structured to take advantage of it.&lt;/p&gt;&lt;p&gt;&amp;quot;The dividing line isn&amp;#x27;t intelligence,&amp;quot; the &lt;a href="https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf"&gt;MIT authors write&lt;/a&gt;. The problems with enterprise AI have to do with memory, adaptability, and learning capability. Problems stem less from regulations or model performance, and more from tools that fail to learn or adapt.&lt;/p&gt;&lt;p&gt;Leading firms, according to the &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;OpenAI report&lt;/a&gt;, consistently invest in executive sponsorship, data readiness, workflow standardization, and deliberate change management. They build cultures where custom AI tools are created, shared, and refined across teams. They track performance and run evaluations. They make AI adoption a strategic priority rather than an individual choice.&lt;/p&gt;&lt;p&gt;The rest are leaving it to chance — hoping that workers will discover the tools on their own, experiment on their own time, and somehow propagate best practices without infrastructure or incentive. The six-fold gap suggests this approach is not working.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The window to catch up is closing faster than most companies realize&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With enterprise contracts locking in over the next 18 months, there&amp;#x27;s a shrinking window for vendors and adopters to cross the divide.The GenAI Divide identified by the &lt;a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf"&gt;MIT report&lt;/a&gt; is not going to last forever. But the organizations that figure out a way across it soonest will be the ones that define the next era of business.&lt;/p&gt;&lt;p&gt;Both reports carry caveats. The OpenAI data comes from a company with an obvious interest in promoting AI adoption. The productivity figures are self-reported by customers already paying for the product. The MIT study, while independent, relies on interviews and surveys rather than direct measurement. The long-term effects of this technology on employment, wages, and workplace dynamics remain uncertain.&lt;/p&gt;&lt;p&gt;But the core finding — that access alone does not produce adoption, and that adoption varies enormously even within organizations that have made identical tools available to all — is consistent with how previous technologies have diffused through the economy. Spreadsheets, email, and the internet all created similar divides before eventually becoming universal. The question is how long the current gap persists, who benefits during the transition, and what happens to workers who find themselves on the wrong side of it.&lt;/p&gt;&lt;p&gt;For now, the divide is stark. Ninety percent of users said they prefer humans for &amp;quot;mission-critical work,&amp;quot; while AI has &amp;quot;won the war for simple work.&amp;quot; The workers who are pulling ahead are not doing so because they have access their colleagues lack. They are pulling ahead because they decided to use what everyone already has—and kept using it until they figured out what it could do.&lt;/p&gt;&lt;p&gt;The 6x gap is not about technology. It is about behavior. And behavior, unlike software, cannot be deployed with a company-wide rollout.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openai-report-reveals-a-6x-productivity-gap-between-ai-power-users-and</guid><pubDate>Wed, 10 Dec 2025 14:30:00 +0000</pubDate></item><item><title>[NEW] Strengthening our partnership with the UK government to support prosperity and security in the AI era (Google DeepMind News)</title><link>https://deepmind.google/blog/strengthening-our-partnership-with-the-uk-government-to-support-prosperity-and-security-in-the-ai-era/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/BcxbVSzAv-BnIoEYEseiZuXvxK9y5nA1uKsiEW691XjxRpGNtBBjbu-5nA0EgG9ezMONHdLeGiCF7AVkuH88KcxuPbi03_5v3PH5Py43xTZYOK8Lbw=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;p class="lead-paragraph"&gt;AI presents an opportunity to build a more prosperous and secure world.&lt;/p&gt;&lt;p&gt;The UK has already laid a strong foundation to seize this moment and is uniquely positioned to translate AI innovation into public benefit. That’s why we are excited to deepen our collaboration with the UK government to accelerate this work and offer a blueprint for other countries.&lt;/p&gt;&lt;p&gt;Together we will focus on using AI to speed up progress in science and education, modernize public services and advance national security and resilience.&lt;/p&gt;&lt;h2&gt;Accelerating access to frontier AI in key sectors: Science &amp;amp; Education&lt;/h2&gt;&lt;p&gt;Our partnership will center on providing access to frontier AI in two areas foundational to the UK’s long-term success: scientific discovery and education.&lt;/p&gt;&lt;p&gt;The UK has a rich history of applying new technologies to drive scientific progress, from Hooke’s microscope to Faraday’s electrical experiments. We aim to build on this heritage, and empower the next generation of scientists with AI tools that can unlock breakthroughs, transform the economy, and solve some of the major challenges facing humanity. We will provide priority access to our “AI for Science” models to UK scientists, including:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;AlphaEvolve - a Gemini-powered coding agent for designing advanced algorithms&lt;/li&gt;&lt;li&gt;AlphaGenome - an AI model to help scientists better understand our DNA&lt;/li&gt;&lt;li&gt;AI co-scientist - a multi-agent AI system that acts as a virtual scientific collaborator&lt;/li&gt;&lt;li&gt;WeatherNext - a family of state-of-the-art weather forecasting models&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Like the microscope or telescope, these AI tools are designed to enhance scientific capacity, enabling researchers to tackle problems of unprecedented complexity and scale. For example, AlphaFold - our AI system for predicting protein structures - has already enabled almost 190,000 researchers in the UK alone to deepen their understanding of areas such as crop resilience, antimicrobial resistance and other critical biological challenges.&lt;/p&gt;&lt;h3&gt;Establishing Google DeepMind’s first automated science laboratory in the UK&lt;/h3&gt;&lt;p&gt;To help turbocharge scientific discovery, we will establish Google DeepMind’s first automated laboratory in the UK in 2026, specifically focused on materials science research. A multidisciplinary team of researchers will oversee research in the lab, which will be built from the ground up to be fully integrated with Gemini. By directing world-class robotics to synthesize and characterize hundreds of materials per day, the team intends to significantly shorten the timeline for identifying transformative new materials.&lt;/p&gt;&lt;p&gt;Discovering new materials is one of the most important pursuits in science, offering the potential to reduce costs and enable entirely new technologies. For example, superconductors that operate at ambient temperature and pressure could allow for low cost medical imaging and reduce power loss in electrical grids. Other novel materials could help us tackle critical energy challenges by unlocking advanced batteries, next-generation solar cells and more efficient computer chips.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/BcxbVSzAv-BnIoEYEseiZuXvxK9y5nA1uKsiEW691XjxRpGNtBBjbu-5nA0EgG9ezMONHdLeGiCF7AVkuH88KcxuPbi03_5v3PH5Py43xTZYOK8Lbw=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;p class="lead-paragraph"&gt;AI presents an opportunity to build a more prosperous and secure world.&lt;/p&gt;&lt;p&gt;The UK has already laid a strong foundation to seize this moment and is uniquely positioned to translate AI innovation into public benefit. That’s why we are excited to deepen our collaboration with the UK government to accelerate this work and offer a blueprint for other countries.&lt;/p&gt;&lt;p&gt;Together we will focus on using AI to speed up progress in science and education, modernize public services and advance national security and resilience.&lt;/p&gt;&lt;h2&gt;Accelerating access to frontier AI in key sectors: Science &amp;amp; Education&lt;/h2&gt;&lt;p&gt;Our partnership will center on providing access to frontier AI in two areas foundational to the UK’s long-term success: scientific discovery and education.&lt;/p&gt;&lt;p&gt;The UK has a rich history of applying new technologies to drive scientific progress, from Hooke’s microscope to Faraday’s electrical experiments. We aim to build on this heritage, and empower the next generation of scientists with AI tools that can unlock breakthroughs, transform the economy, and solve some of the major challenges facing humanity. We will provide priority access to our “AI for Science” models to UK scientists, including:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;AlphaEvolve - a Gemini-powered coding agent for designing advanced algorithms&lt;/li&gt;&lt;li&gt;AlphaGenome - an AI model to help scientists better understand our DNA&lt;/li&gt;&lt;li&gt;AI co-scientist - a multi-agent AI system that acts as a virtual scientific collaborator&lt;/li&gt;&lt;li&gt;WeatherNext - a family of state-of-the-art weather forecasting models&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Like the microscope or telescope, these AI tools are designed to enhance scientific capacity, enabling researchers to tackle problems of unprecedented complexity and scale. For example, AlphaFold - our AI system for predicting protein structures - has already enabled almost 190,000 researchers in the UK alone to deepen their understanding of areas such as crop resilience, antimicrobial resistance and other critical biological challenges.&lt;/p&gt;&lt;h3&gt;Establishing Google DeepMind’s first automated science laboratory in the UK&lt;/h3&gt;&lt;p&gt;To help turbocharge scientific discovery, we will establish Google DeepMind’s first automated laboratory in the UK in 2026, specifically focused on materials science research. A multidisciplinary team of researchers will oversee research in the lab, which will be built from the ground up to be fully integrated with Gemini. By directing world-class robotics to synthesize and characterize hundreds of materials per day, the team intends to significantly shorten the timeline for identifying transformative new materials.&lt;/p&gt;&lt;p&gt;Discovering new materials is one of the most important pursuits in science, offering the potential to reduce costs and enable entirely new technologies. For example, superconductors that operate at ambient temperature and pressure could allow for low cost medical imaging and reduce power loss in electrical grids. Other novel materials could help us tackle critical energy challenges by unlocking advanced batteries, next-generation solar cells and more efficient computer chips.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/blog/strengthening-our-partnership-with-the-uk-government-to-support-prosperity-and-security-in-the-ai-era/</guid><pubDate>Wed, 10 Dec 2025 14:59:21 +0000</pubDate></item><item><title>The AI that scored 95% — until consultants learned it was AI (AI | VentureBeat)</title><link>https://venturebeat.com/ai/the-ai-that-scored-95-until-consultants-learned-it-was-ai</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by SAP&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;When SAP ran a quiet internal experiment to gauge consultant attitudes toward AI, the results were striking. Five teams were asked to validate answers to more than 1,000 business requirements completed by SAP’s AI co-pilot, Joule for Consultants — a workload that would normally take several weeks.&lt;/p&gt;&lt;p&gt;Four teams were told the analysis had been completed by junior interns fresh out of school. They reviewed the material, found it impressive, and rated the work about 95% accurate.&lt;/p&gt;&lt;p&gt;The fifth team was told the very same answers had come from AI.&lt;/p&gt;&lt;p&gt;They rejected almost everything.&lt;/p&gt;&lt;p&gt;Only when asked to validate each answer one by one did they discover that the AI was, in fact, highly accurate — surfacing detailed insights the consultants had initially dismissed. The overall accuracy? Again, about 95%.&lt;/p&gt;&lt;p&gt;“The lesson learned here is that we need to be very cautious as we introduce AI — especially in how we communicate with senior consultants about its possibilities and how to integrate it into their workflows,” says Guillermo B. Vazquez Mendez, chief architect, RI business transformation and architecture, SAP America Inc.&lt;/p&gt;&lt;p&gt;The experiment has since become a revealing starting point for SAP’s push toward the consultant of 2030: a practitioner who is deeply human, enabled by AI, and no longer weighed down by the technical grunt work of the past.&lt;/p&gt;&lt;h3&gt;Overcoming AI skepticism&lt;/h3&gt;&lt;p&gt;Resistance isn’t surprising, Vazquez notes. Consultants with two or three decades of experience carry enormous institutional knowledge — and an understandable degree of caution.&lt;/p&gt;&lt;p&gt;But AI copilots like Joule for Consultants are not replacing expertise. They’re amplifying it.&lt;/p&gt;&lt;p&gt;“What Joule really does is make their very expensive time far more effective,” Vazquez says. “It removes the clerical work, so they can focus on turning out high-quality answers in a fraction of the time.”&lt;/p&gt;&lt;p&gt;He emphasizes this message constantly: “AI is not replacing you. It’s a tool for you. Human oversight is always required. But now, instead of spending your time looking for documentation, you’re gaining significant time and boosting the effectiveness and detail of your answers.”&lt;/p&gt;&lt;h3&gt;The consultant time-shift: from tech execution to business insight&lt;/h3&gt;&lt;p&gt;Historically, consultants spent about 80% of their time understanding technical systems — how processes run, how data flows, how functions execute. Customers, by contrast, spend 80% of their time focused on their business.&lt;/p&gt;&lt;p&gt;That mismatch is exactly where Joule steps in.&lt;/p&gt;&lt;p&gt;“There’s a gap there — and the bridge is AI,” Vazquez says. “It flips the time equation, enabling consultants to invest more of their energy in understanding the customer’s industry and business goals. AI takes on the heavy technical lift, so consultants can focus on driving the right business outcomes.”&lt;/p&gt;&lt;h3&gt;Bringing new consultants up to speed&lt;/h3&gt;&lt;p&gt;AI is also transforming how new hires learn.&lt;/p&gt;&lt;p&gt;“We’re excited to see Joule acting as a bridge between senior consultants, who are adapting more slowly, and interns and new consultants who are already technically savvy,” Vazquez says.&lt;/p&gt;&lt;p&gt;Junior consultants ramp up faster because Joule helps them operate independently. Seniors, meanwhile, engage where their insight matters most.&lt;/p&gt;&lt;p&gt;This is also where many consultants learn the fundamentals of today’s AI copilots. Much of the work depends on prompt engineering — for instance, instructing Joule to act as a senior chief technology architect specializing in finance and SAP S/4HANA 2023, then asking it to analyze business requirements and deliver the output as tables or PowerPoint slides.&lt;/p&gt;&lt;p&gt;Once they grasp how to frame prompts, consultants consistently get higher-quality, more structured answers.&lt;/p&gt;&lt;p&gt;New architects are also able to communicate more clearly with their more experienced counterparts. They know what they don’t know and can ask targeted questions, which makes mentorship far smoother. It’s created a real synergy, Vazquez adds — senior consultants see how quickly new hires are adapting and learning with AI, and that momentum encourages them to keep pace and adopt the technology themselves.&lt;/p&gt;&lt;h3&gt;Looking ahead to the future of AI copilots&lt;/h3&gt;&lt;p&gt;“We’re still in the baby steps of AI — we’re toddlers,” Vazquez says. “Right now, copilots depend on prompt engineering to get good answers. The better you prompt, the better the answer you get.”&lt;/p&gt;&lt;p&gt;But that represents only the earliest phase of what these systems will eventually do. As copilots mature, they’ll move beyond responding to prompts and start interpreting entire business processes — understanding the sequence of steps, identifying where human intervention is needed, and spotting where an AI agent could take over. That shift is what leads directly into agentic AI.&lt;/p&gt;&lt;p&gt;SAP’s depth of process knowledge is what makes that evolution possible. The company has mapped more than 3,500 business processes across industries — a repository Vazquez calls “some of the most valuable, rigorously tested processes developed in the last 50 years.” Every day, SAP systems support roughly $7.3 trillion in global commerce, giving these emerging AI agents a rich foundation to navigate and reason over.&lt;/p&gt;&lt;p&gt;“With that level of process insight and data, we can take a real leap forward,” he says, “equipping our consultants with agentic AI that can solve complex challenges and push us toward increasingly autonomous systems.”&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;
&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by SAP&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;When SAP ran a quiet internal experiment to gauge consultant attitudes toward AI, the results were striking. Five teams were asked to validate answers to more than 1,000 business requirements completed by SAP’s AI co-pilot, Joule for Consultants — a workload that would normally take several weeks.&lt;/p&gt;&lt;p&gt;Four teams were told the analysis had been completed by junior interns fresh out of school. They reviewed the material, found it impressive, and rated the work about 95% accurate.&lt;/p&gt;&lt;p&gt;The fifth team was told the very same answers had come from AI.&lt;/p&gt;&lt;p&gt;They rejected almost everything.&lt;/p&gt;&lt;p&gt;Only when asked to validate each answer one by one did they discover that the AI was, in fact, highly accurate — surfacing detailed insights the consultants had initially dismissed. The overall accuracy? Again, about 95%.&lt;/p&gt;&lt;p&gt;“The lesson learned here is that we need to be very cautious as we introduce AI — especially in how we communicate with senior consultants about its possibilities and how to integrate it into their workflows,” says Guillermo B. Vazquez Mendez, chief architect, RI business transformation and architecture, SAP America Inc.&lt;/p&gt;&lt;p&gt;The experiment has since become a revealing starting point for SAP’s push toward the consultant of 2030: a practitioner who is deeply human, enabled by AI, and no longer weighed down by the technical grunt work of the past.&lt;/p&gt;&lt;h3&gt;Overcoming AI skepticism&lt;/h3&gt;&lt;p&gt;Resistance isn’t surprising, Vazquez notes. Consultants with two or three decades of experience carry enormous institutional knowledge — and an understandable degree of caution.&lt;/p&gt;&lt;p&gt;But AI copilots like Joule for Consultants are not replacing expertise. They’re amplifying it.&lt;/p&gt;&lt;p&gt;“What Joule really does is make their very expensive time far more effective,” Vazquez says. “It removes the clerical work, so they can focus on turning out high-quality answers in a fraction of the time.”&lt;/p&gt;&lt;p&gt;He emphasizes this message constantly: “AI is not replacing you. It’s a tool for you. Human oversight is always required. But now, instead of spending your time looking for documentation, you’re gaining significant time and boosting the effectiveness and detail of your answers.”&lt;/p&gt;&lt;h3&gt;The consultant time-shift: from tech execution to business insight&lt;/h3&gt;&lt;p&gt;Historically, consultants spent about 80% of their time understanding technical systems — how processes run, how data flows, how functions execute. Customers, by contrast, spend 80% of their time focused on their business.&lt;/p&gt;&lt;p&gt;That mismatch is exactly where Joule steps in.&lt;/p&gt;&lt;p&gt;“There’s a gap there — and the bridge is AI,” Vazquez says. “It flips the time equation, enabling consultants to invest more of their energy in understanding the customer’s industry and business goals. AI takes on the heavy technical lift, so consultants can focus on driving the right business outcomes.”&lt;/p&gt;&lt;h3&gt;Bringing new consultants up to speed&lt;/h3&gt;&lt;p&gt;AI is also transforming how new hires learn.&lt;/p&gt;&lt;p&gt;“We’re excited to see Joule acting as a bridge between senior consultants, who are adapting more slowly, and interns and new consultants who are already technically savvy,” Vazquez says.&lt;/p&gt;&lt;p&gt;Junior consultants ramp up faster because Joule helps them operate independently. Seniors, meanwhile, engage where their insight matters most.&lt;/p&gt;&lt;p&gt;This is also where many consultants learn the fundamentals of today’s AI copilots. Much of the work depends on prompt engineering — for instance, instructing Joule to act as a senior chief technology architect specializing in finance and SAP S/4HANA 2023, then asking it to analyze business requirements and deliver the output as tables or PowerPoint slides.&lt;/p&gt;&lt;p&gt;Once they grasp how to frame prompts, consultants consistently get higher-quality, more structured answers.&lt;/p&gt;&lt;p&gt;New architects are also able to communicate more clearly with their more experienced counterparts. They know what they don’t know and can ask targeted questions, which makes mentorship far smoother. It’s created a real synergy, Vazquez adds — senior consultants see how quickly new hires are adapting and learning with AI, and that momentum encourages them to keep pace and adopt the technology themselves.&lt;/p&gt;&lt;h3&gt;Looking ahead to the future of AI copilots&lt;/h3&gt;&lt;p&gt;“We’re still in the baby steps of AI — we’re toddlers,” Vazquez says. “Right now, copilots depend on prompt engineering to get good answers. The better you prompt, the better the answer you get.”&lt;/p&gt;&lt;p&gt;But that represents only the earliest phase of what these systems will eventually do. As copilots mature, they’ll move beyond responding to prompts and start interpreting entire business processes — understanding the sequence of steps, identifying where human intervention is needed, and spotting where an AI agent could take over. That shift is what leads directly into agentic AI.&lt;/p&gt;&lt;p&gt;SAP’s depth of process knowledge is what makes that evolution possible. The company has mapped more than 3,500 business processes across industries — a repository Vazquez calls “some of the most valuable, rigorously tested processes developed in the last 50 years.” Every day, SAP systems support roughly $7.3 trillion in global commerce, giving these emerging AI agents a rich foundation to navigate and reason over.&lt;/p&gt;&lt;p&gt;“With that level of process insight and data, we can take a real leap forward,” he says, “equipping our consultants with agentic AI that can solve complex challenges and push us toward increasingly autonomous systems.”&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;
&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/the-ai-that-scored-95-until-consultants-learned-it-was-ai</guid><pubDate>Wed, 10 Dec 2025 15:00:00 +0000</pubDate></item><item><title>Google launches managed MCP servers that let AI agents simply plug into its tools (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/10/google-is-going-all-in-on-mcp-servers-agent-ready-by-design/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/07/GettyImages-2148009757.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI agents are being sold as the solution for planning trips, answering business questions, and solving problems of all kinds, but getting them to work with tools and data outside their chat interfaces has been tricky. Developers have to patch together various connectors and keep them running, but that’s a fragile approach that’s hard to scale and creates governance headaches.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google claims it’s trying to solve that by launching its own fully managed, remote MCP servers that would make its Google and Cloud services — like Maps and BigQuery — easier for agents to plug into.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move follows the launch of Google’s latest Gemini 3 model, and the company is looking to pair stronger reasoning with more dependable connections to real-world tools and data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are making Google agent-ready by design,” Steren Giannini, product management director at Google Cloud, told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead of spending a week or two setting up connectors, developers can now essentially paste in a URL to a managed endpoint, Giannini said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At launch, Google is starting with MCP servers for Maps, BigQuery, Compute Engine, and Kubernetes Engine. In practice, this might look like an analytics assistant querying BigQuery directly, or an ops agent interacting with infrastructure services.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the case of Maps, Giannini said, without the MCP, developers would rely on the model’s built-in knowledge. “But by giving your agent […] a tool like the Google Maps MCP server, then it gets grounded on actual, up-to-date location information for places or trips planning,” he added.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;While the MCP servers will eventually be offered across all of Google’s tools, they are initially launching under public preview, meaning they’re not yet fully covered by Google Cloud terms of service. They are, however, being offered at no extra cost to enterprise customers that already pay for Google services.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We expect to bring them to general availability very soon in the new year,” Giannini said, adding that he expects more MCP servers to trickle in every week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MCP, which stands for Model Context Protocol, was developed by Anthropic about a year ago as an open source standard to connect AI systems with data and tools. The protocol has been widely adopted across the agent tooling world, and Anthropic earlier this week donated MCP to a new Linux Foundation fund dedicated to open sourcing and standardizing AI agent infrastructure.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The beauty of MCP is that, because it’s a standard, if Google provides a server, it can connect to any client,” Giannini said. “I’m looking forward to seeing how many more clients will emerge.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One can think of MCP clients as the AI apps on the other end of the wire that talk to MCP servers and call the tools they offer. For Google, that includes Gemini CLI and AI Studio. Giannini said he’s also tried it with Anthropic’s Claude and OpenAI’s ChatGPT as clients, and “they just work.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google argues this isn’t just about connecting agents to its services. The bigger enterprise play is Apigee, its API management product, which many companies already use to issue API keys, set quotas, and monitor traffic.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Giannini said Apigee can essentially “translate” a standard API into an MCP server, turning endpoints like a product catalog API into tools an agent can discover and use, with existing security and governance controls layered on top.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In other words, the same API guardrails companies use for human-built apps could now apply to AI agents, too.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s new MCP servers are protected by a permission mechanism called Google Cloud IAM, which explicitly protects what an agent can do with that server. They are also protected by Google Cloud Model Armor, which Giannini describes as a firewall dedicated to agentic workloads that defends against advanced agentic threats like prompt injection and data exfiltration. Administrators can also rely on audit logging for additional observability.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google plans to expand MCP support beyond the initial set of servers. In the next few months, the company will roll out support for services across areas like storage, databases, logging and monitoring, and security.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We built the plumbing so that developers don’t have to,” Giannini said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/07/GettyImages-2148009757.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI agents are being sold as the solution for planning trips, answering business questions, and solving problems of all kinds, but getting them to work with tools and data outside their chat interfaces has been tricky. Developers have to patch together various connectors and keep them running, but that’s a fragile approach that’s hard to scale and creates governance headaches.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google claims it’s trying to solve that by launching its own fully managed, remote MCP servers that would make its Google and Cloud services — like Maps and BigQuery — easier for agents to plug into.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move follows the launch of Google’s latest Gemini 3 model, and the company is looking to pair stronger reasoning with more dependable connections to real-world tools and data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are making Google agent-ready by design,” Steren Giannini, product management director at Google Cloud, told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead of spending a week or two setting up connectors, developers can now essentially paste in a URL to a managed endpoint, Giannini said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At launch, Google is starting with MCP servers for Maps, BigQuery, Compute Engine, and Kubernetes Engine. In practice, this might look like an analytics assistant querying BigQuery directly, or an ops agent interacting with infrastructure services.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the case of Maps, Giannini said, without the MCP, developers would rely on the model’s built-in knowledge. “But by giving your agent […] a tool like the Google Maps MCP server, then it gets grounded on actual, up-to-date location information for places or trips planning,” he added.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;While the MCP servers will eventually be offered across all of Google’s tools, they are initially launching under public preview, meaning they’re not yet fully covered by Google Cloud terms of service. They are, however, being offered at no extra cost to enterprise customers that already pay for Google services.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We expect to bring them to general availability very soon in the new year,” Giannini said, adding that he expects more MCP servers to trickle in every week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MCP, which stands for Model Context Protocol, was developed by Anthropic about a year ago as an open source standard to connect AI systems with data and tools. The protocol has been widely adopted across the agent tooling world, and Anthropic earlier this week donated MCP to a new Linux Foundation fund dedicated to open sourcing and standardizing AI agent infrastructure.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The beauty of MCP is that, because it’s a standard, if Google provides a server, it can connect to any client,” Giannini said. “I’m looking forward to seeing how many more clients will emerge.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One can think of MCP clients as the AI apps on the other end of the wire that talk to MCP servers and call the tools they offer. For Google, that includes Gemini CLI and AI Studio. Giannini said he’s also tried it with Anthropic’s Claude and OpenAI’s ChatGPT as clients, and “they just work.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google argues this isn’t just about connecting agents to its services. The bigger enterprise play is Apigee, its API management product, which many companies already use to issue API keys, set quotas, and monitor traffic.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Giannini said Apigee can essentially “translate” a standard API into an MCP server, turning endpoints like a product catalog API into tools an agent can discover and use, with existing security and governance controls layered on top.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In other words, the same API guardrails companies use for human-built apps could now apply to AI agents, too.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s new MCP servers are protected by a permission mechanism called Google Cloud IAM, which explicitly protects what an agent can do with that server. They are also protected by Google Cloud Model Armor, which Giannini describes as a firewall dedicated to agentic workloads that defends against advanced agentic threats like prompt injection and data exfiltration. Administrators can also rely on audit logging for additional observability.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google plans to expand MCP support beyond the initial set of servers. In the next few months, the company will roll out support for services across areas like storage, databases, logging and monitoring, and security.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We built the plumbing so that developers don’t have to,” Giannini said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/10/google-is-going-all-in-on-mcp-servers-agent-ready-by-design/</guid><pubDate>Wed, 10 Dec 2025 15:00:00 +0000</pubDate></item><item><title>Exclusive eBook: Aging Clocks &amp; Understanding Why We Age (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/10/1128999/exclusive-ebook-aging-clocks-understanding-why-we-age/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/Longevity-Thumb.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="contentBody__summaryBullets--81327c9379272772d1e74a64b6d4868a"&gt;&lt;p&gt;This ebook is available only for subscribers.&lt;/p&gt;
&lt;/div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;In this exclusive subscriber-only eBook, you'll learn about a new method that scientists have uncovered to look at the ways our bodies are aging.&lt;/p&gt;&lt;p&gt;by &amp;nbsp;&lt;strong&gt;Jessica Hamzelou&lt;/strong&gt; October 14, 2025&lt;/p&gt;    &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;:&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;ul class="wp-block-list"&gt; &lt;li&gt;Clocks kick off&lt;/li&gt;    &lt;li&gt;Black-box clocks&lt;/li&gt;    &lt;li&gt;How to be young again&lt;/li&gt;    &lt;li&gt;Dogs and dolphins&lt;/li&gt;    &lt;li&gt;When young meets old&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;&lt;strong&gt;Related Stories:&lt;/strong&gt;&lt;/p&gt;    &lt;p&gt;&lt;strong&gt;Access all subscriber-only eBooks&lt;/strong&gt;:&lt;/p&gt;    &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/Longevity-Thumb.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="contentBody__summaryBullets--81327c9379272772d1e74a64b6d4868a"&gt;&lt;p&gt;This ebook is available only for subscribers.&lt;/p&gt;
&lt;/div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;In this exclusive subscriber-only eBook, you'll learn about a new method that scientists have uncovered to look at the ways our bodies are aging.&lt;/p&gt;&lt;p&gt;by &amp;nbsp;&lt;strong&gt;Jessica Hamzelou&lt;/strong&gt; October 14, 2025&lt;/p&gt;    &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;:&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;ul class="wp-block-list"&gt; &lt;li&gt;Clocks kick off&lt;/li&gt;    &lt;li&gt;Black-box clocks&lt;/li&gt;    &lt;li&gt;How to be young again&lt;/li&gt;    &lt;li&gt;Dogs and dolphins&lt;/li&gt;    &lt;li&gt;When young meets old&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;&lt;strong&gt;Related Stories:&lt;/strong&gt;&lt;/p&gt;    &lt;p&gt;&lt;strong&gt;Access all subscriber-only eBooks&lt;/strong&gt;:&lt;/p&gt;    &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/10/1128999/exclusive-ebook-aging-clocks-understanding-why-we-age/</guid><pubDate>Wed, 10 Dec 2025 15:08:45 +0000</pubDate></item><item><title>ChatGPT is Apple’s most downloaded app of 2025 in the US (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/10/chatgpt-is-apples-most-downloaded-app-of-2025-in-the-us/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2191707579.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple on Wednesday released its annual list of the most downloaded apps and games for the year. For the U.S. market, OpenAI’s ChatGPT topped the ranks of free iPhone apps (not including games) with the most installs in 2025. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI app was followed by Threads, Google, TikTok, WhatsApp, Instagram, YouTube, Google Maps, Gmail, and Google’s Gemini.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;ChatGPT made it to No. 4 last year, but the top spot was taken by Chinese shopping app Temu. In 2023, the AI app didn’t make the top-10 list despite being released on the iPhone in May 2023 to a strong debut.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fact that ChatGPT is outpacing social networking standbys, and much-needed utilities like Google Maps, indicates how deeply AI has penetrated people’s everyday lives here in the U.S. It also demonstrates the potential for OpenAI to disrupt Google’s tight hold on the search market on mobile devices, as more people turn to a chatbot for answers first.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There were signs that ChatGPT was on its way to No. 1 earlier in the year — it became the most-downloaded app globally in March, outpacing other top apps like TikTok and Instagram.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple also released its list of the top paid apps, the top free and paid games for iPhone and iPad, as well as the top Apple Arcade games.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Block Blast! was the top free game this year, while Minecraft took the crown as the top paid game. On iPad, YouTube was the No. 1 free app, and users downloaded the creativity app Procreate as the top paid game on iPad. Roblox was the top free iPad game.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The full list of top apps and games is below:&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-top-free-iphone-apps"&gt;&lt;strong&gt;Top Free iPhone Apps&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;ChatGPT&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Threads&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Google&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;TikTok – Videos, Shop &amp;amp; LIVE&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;WhatsApp Messenger&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Instagram&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;YouTube&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Google Maps&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Gmail – Email by Google&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Google Gemini&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-paid-iphone-apps"&gt;&lt;strong&gt;Top Paid iPhone Apps&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;HotSchedules&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Shadowrocket&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Procreate Pocket&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;AnkiMobile Flashcards&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Paprika Recipe Manager 3&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;SkyView®&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;TonalEnergy Tuner &amp;amp; Metronome&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;AutoSleep Track Sleep on Watch&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Forest: Focus for Productivity&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;RadarScope&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-free-iphone-games"&gt;&lt;strong&gt;Top Free iPhone Games&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Block Blast!&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Fortnite&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Roblox&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Township&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Pokémon TCG Pocket&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Royal Kingdom&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Clash Royale&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Vita Mahjong&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Whiteout Survival&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Last War: Survival&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-paid-iphone-games"&gt;&lt;strong&gt;Top Paid iPhone Games&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Minecraft: Dream it, Build it!&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Balatro&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Heads Up!&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Plague Inc.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Geometry Dash&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Bloons TD 6&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Stardew Valley&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Papa’s Freezeria To Go!&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Animal Crossing: Pocket Camp C&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Red’s First Flight&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-free-ipad-apps"&gt;&lt;strong&gt;Top Free iPad Apps&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;YouTube&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;ChatGPT&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Netflix&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Disney+&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Amazon Prime Video&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;TikTok – Videos, Shop &amp;amp; LIVE&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Google Chrome&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Goodnotes: AI Notes, Docs, PDF&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Canva: AI Photo &amp;amp; Video Editor&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;HBO Max: Stream Movies &amp;amp; TV&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-paid-ipad-apps"&gt;&lt;strong&gt;Top Paid iPad Apps&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Procreate&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Procreate Dreams&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;forScore&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;ToonSquid&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Nomad Sculpt&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Shadowrocket&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;AnkiMobile Flashcards&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Bluebeam Revu for iPad&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Teach Your Monster to Read&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Feather: Draw in 3D&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-free-ipad-games"&gt;Top Free iPad Games&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Roblox&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Block Blast!&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Fortnite&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Perfect Tidy&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Magic Tiles 3: Piano Game&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mini Games: Calm &amp;amp; Chill&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Goods Puzzle: Sort Challenge&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;hole.io&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Subway Surfers&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Township&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-paid-ipad-games"&gt;Top Paid iPad Games&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Minecraft: Dream it, Build it!&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Geometry Dash&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Stardew Valley&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Balatro&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Bloons TD 6&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Plague Inc.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Animal Crossing: Pocket Camp C&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Poppy Playtime Chapter 3&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Purple Place – Classic Games&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Papa’s Sushiria To Go!&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-apple-arcade-games"&gt;&lt;strong&gt;Top Apple Arcade Games&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;NFL Retro Bowl ’26&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;NBA 2K25 Arcade Edition&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Balatro+&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Snake.io+&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Sneaky Sasquatch&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Hello Kitty Island Adventure&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Fruit Ninja Classic+&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Bloons TD 6+&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;PGA TOUR Pro Golf&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Solitaire by MobilityWare+&lt;/li&gt;
&lt;/ol&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2191707579.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple on Wednesday released its annual list of the most downloaded apps and games for the year. For the U.S. market, OpenAI’s ChatGPT topped the ranks of free iPhone apps (not including games) with the most installs in 2025. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI app was followed by Threads, Google, TikTok, WhatsApp, Instagram, YouTube, Google Maps, Gmail, and Google’s Gemini.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;ChatGPT made it to No. 4 last year, but the top spot was taken by Chinese shopping app Temu. In 2023, the AI app didn’t make the top-10 list despite being released on the iPhone in May 2023 to a strong debut.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fact that ChatGPT is outpacing social networking standbys, and much-needed utilities like Google Maps, indicates how deeply AI has penetrated people’s everyday lives here in the U.S. It also demonstrates the potential for OpenAI to disrupt Google’s tight hold on the search market on mobile devices, as more people turn to a chatbot for answers first.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There were signs that ChatGPT was on its way to No. 1 earlier in the year — it became the most-downloaded app globally in March, outpacing other top apps like TikTok and Instagram.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple also released its list of the top paid apps, the top free and paid games for iPhone and iPad, as well as the top Apple Arcade games.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Block Blast! was the top free game this year, while Minecraft took the crown as the top paid game. On iPad, YouTube was the No. 1 free app, and users downloaded the creativity app Procreate as the top paid game on iPad. Roblox was the top free iPad game.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The full list of top apps and games is below:&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-top-free-iphone-apps"&gt;&lt;strong&gt;Top Free iPhone Apps&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;ChatGPT&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Threads&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Google&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;TikTok – Videos, Shop &amp;amp; LIVE&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;WhatsApp Messenger&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Instagram&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;YouTube&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Google Maps&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Gmail – Email by Google&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Google Gemini&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-paid-iphone-apps"&gt;&lt;strong&gt;Top Paid iPhone Apps&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;HotSchedules&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Shadowrocket&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Procreate Pocket&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;AnkiMobile Flashcards&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Paprika Recipe Manager 3&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;SkyView®&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;TonalEnergy Tuner &amp;amp; Metronome&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;AutoSleep Track Sleep on Watch&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Forest: Focus for Productivity&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;RadarScope&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-free-iphone-games"&gt;&lt;strong&gt;Top Free iPhone Games&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Block Blast!&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Fortnite&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Roblox&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Township&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Pokémon TCG Pocket&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Royal Kingdom&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Clash Royale&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Vita Mahjong&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Whiteout Survival&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Last War: Survival&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-paid-iphone-games"&gt;&lt;strong&gt;Top Paid iPhone Games&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Minecraft: Dream it, Build it!&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Balatro&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Heads Up!&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Plague Inc.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Geometry Dash&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Bloons TD 6&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Stardew Valley&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Papa’s Freezeria To Go!&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Animal Crossing: Pocket Camp C&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Red’s First Flight&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-free-ipad-apps"&gt;&lt;strong&gt;Top Free iPad Apps&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;YouTube&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;ChatGPT&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Netflix&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Disney+&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Amazon Prime Video&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;TikTok – Videos, Shop &amp;amp; LIVE&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Google Chrome&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Goodnotes: AI Notes, Docs, PDF&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Canva: AI Photo &amp;amp; Video Editor&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;HBO Max: Stream Movies &amp;amp; TV&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-paid-ipad-apps"&gt;&lt;strong&gt;Top Paid iPad Apps&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Procreate&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Procreate Dreams&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;forScore&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;ToonSquid&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Nomad Sculpt&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Shadowrocket&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;AnkiMobile Flashcards&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Bluebeam Revu for iPad&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Teach Your Monster to Read&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Feather: Draw in 3D&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-free-ipad-games"&gt;Top Free iPad Games&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Roblox&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Block Blast!&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Fortnite&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Perfect Tidy&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Magic Tiles 3: Piano Game&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mini Games: Calm &amp;amp; Chill&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Goods Puzzle: Sort Challenge&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;hole.io&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Subway Surfers&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Township&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-paid-ipad-games"&gt;Top Paid iPad Games&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Minecraft: Dream it, Build it!&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Geometry Dash&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Stardew Valley&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Balatro&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Bloons TD 6&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Plague Inc.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Animal Crossing: Pocket Camp C&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Poppy Playtime Chapter 3&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Purple Place – Classic Games&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Papa’s Sushiria To Go!&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class="wp-block-heading" id="h-top-apple-arcade-games"&gt;&lt;strong&gt;Top Apple Arcade Games&lt;/strong&gt;&lt;/h2&gt;

&lt;ol class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;NFL Retro Bowl ’26&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;NBA 2K25 Arcade Edition&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Balatro+&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Snake.io+&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Sneaky Sasquatch&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Hello Kitty Island Adventure&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Fruit Ninja Classic+&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Bloons TD 6+&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;PGA TOUR Pro Golf&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Solitaire by MobilityWare+&lt;/li&gt;
&lt;/ol&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/10/chatgpt-is-apples-most-downloaded-app-of-2025-in-the-us/</guid><pubDate>Wed, 10 Dec 2025 15:17:26 +0000</pubDate></item><item><title>ElevenLabs just hit a $6.6B valuation. Its CEO says the real money isn’t in voice anymore. (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/elevenlabs-just-hit-a-6-6b-valuation-its-ceo-says-the-real-money-isnt-in-voice-anymore/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Mati-Staniszewski_SXSW.jpg?resize=1200,799" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;ElevenLabs has made a name for itself building realistic AI voices.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;What started as two Polish engineers annoyed by terrible movie dubbing has grown into a profitable company&amp;nbsp;now valued at&amp;nbsp;$6.6 billion, doubling from just nine months ago. The company recently announced a $100 million tender offer led by Sequoia and ICONIQ, with participation from a16z and others, as its tech powers everything from Fortnite characters to customer service bots and goes toe-to-toe with OpenAI to become the default voice of AI.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch’s Equity podcast, we’re bringing you a conversation with CEO Mati Staniszewski from this year’s Disrupt, where he made a surprising admission: He thinks voice models will be commoditized in just a couple of years. So what’s ElevenLabs’ plan when everyone else catches up?&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why&amp;nbsp;ElevenLabs&amp;nbsp;is pivoting from voice models to building a conversational AI agent platform&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How the company is tackling deepfakes with watermarking, AI detection, and device authentication&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why Staniszewski believes there will soon be more AI-generated content than human content&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;ElevenLabs’ push into music generation and partnerships to fuse audio with video models&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on Apple Podcasts,&amp;nbsp;Overcast, Spotify and all the casts. You also can follow Equity on X and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Mati-Staniszewski_SXSW.jpg?resize=1200,799" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;ElevenLabs has made a name for itself building realistic AI voices.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;What started as two Polish engineers annoyed by terrible movie dubbing has grown into a profitable company&amp;nbsp;now valued at&amp;nbsp;$6.6 billion, doubling from just nine months ago. The company recently announced a $100 million tender offer led by Sequoia and ICONIQ, with participation from a16z and others, as its tech powers everything from Fortnite characters to customer service bots and goes toe-to-toe with OpenAI to become the default voice of AI.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch’s Equity podcast, we’re bringing you a conversation with CEO Mati Staniszewski from this year’s Disrupt, where he made a surprising admission: He thinks voice models will be commoditized in just a couple of years. So what’s ElevenLabs’ plan when everyone else catches up?&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why&amp;nbsp;ElevenLabs&amp;nbsp;is pivoting from voice models to building a conversational AI agent platform&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How the company is tackling deepfakes with watermarking, AI detection, and device authentication&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why Staniszewski believes there will soon be more AI-generated content than human content&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;ElevenLabs’ push into music generation and partnerships to fuse audio with video models&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on Apple Podcasts,&amp;nbsp;Overcast, Spotify and all the casts. You also can follow Equity on X and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/elevenlabs-just-hit-a-6-6b-valuation-its-ceo-says-the-real-money-isnt-in-voice-anymore/</guid><pubDate>Wed, 10 Dec 2025 16:00:00 +0000</pubDate></item><item><title>Promptions helps make AI prompting more precise with dynamic UI controls (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/promptions-helps-make-ai-prompting-more-precise-with-dynamic-ui-controls/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white line icons on a blue-to-green gradient background: a hub-and-spoke network symbol on the left, a laptop with a user icon in the center, and a connected group of three user icons on the right." class="wp-image-1157946" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/Promptions-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Anyone who uses AI systems knows the frustration: a prompt is given, the response misses the mark, and the cycle repeats. This trial-and-error loop can feel unpredictable and discouraging. To address this, we are excited to introduce &lt;strong&gt;Promptions&lt;/strong&gt; (&lt;em&gt;prompt + options&lt;/em&gt;), a UI framework that helps developers build AI interfaces with more precise user control.&lt;/p&gt;



&lt;p&gt;Its simple design makes it easy to integrate into any setting&amp;nbsp;that relies on added context, including customer support, education, and medicine. Promptions is available under the MIT license on Microsoft Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and GitHub.&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="background"&gt;Background&lt;/h2&gt;



&lt;p&gt;Promptions&amp;nbsp;builds on&amp;nbsp;our research,&amp;nbsp;“Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks.”&amp;nbsp;This&amp;nbsp;project&amp;nbsp;examined&amp;nbsp;how&amp;nbsp;knowledge&amp;nbsp;workers&amp;nbsp;use&amp;nbsp;generative AI when their goal is&amp;nbsp;to &lt;em&gt;understand&lt;/em&gt; rather than&amp;nbsp;&lt;em&gt;create&lt;/em&gt;. While much public&amp;nbsp;discussion centers on&amp;nbsp;AI producing text&amp;nbsp;or&amp;nbsp;images, understanding involves asking AI to explain, clarify, or teach—a&amp;nbsp;task&amp;nbsp;that can&amp;nbsp;quickly become&amp;nbsp;complex. Consider a spreadsheet formula: one&amp;nbsp;user may want a&amp;nbsp;simple syntax&amp;nbsp;breakdown,&amp;nbsp;another a&amp;nbsp;debugging&amp;nbsp;guide, and&amp;nbsp;another an explanation suitable for&amp;nbsp;teaching&amp;nbsp;colleagues.&amp;nbsp;The same formula can require&amp;nbsp;entirely&amp;nbsp;different explanations depending on the user’s role, expertise, and goals.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;A great deal of complexity sits beneath these&amp;nbsp;seemingly simple&amp;nbsp;requests.&amp;nbsp;Users&amp;nbsp;often&amp;nbsp;find&amp;nbsp;that the way&amp;nbsp;they phrase a question&amp;nbsp;doesn’t&amp;nbsp;match&amp;nbsp;the&amp;nbsp;level of detail the AI&amp;nbsp;needs.&amp;nbsp;Clarifying what they really want can require long, carefully worded&amp;nbsp;prompts that are tiring to produce.&amp;nbsp;And because the connection&amp;nbsp;between natural language and system behavior&amp;nbsp;isn’t always transparent, it can be difficult to predict&amp;nbsp;how the AI will interpret a&amp;nbsp;given&amp;nbsp;request.&amp;nbsp;In the end, users spend more time&amp;nbsp;managing the interaction itself&amp;nbsp;than understanding the material they&amp;nbsp;hoped&amp;nbsp;to learn.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="identifying-how-users-want-to-guide-ai-outputs"&gt;Identifying&amp;nbsp;how users want to guide AI outputs&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;To explore why these&amp;nbsp;challenges&amp;nbsp;persist and how people can better steer AI toward customized results, we conducted two studies with knowledge workers across technical and nontechnical roles. Their experiences highlighted important gaps that guided Promptions’ design.&lt;/p&gt;



&lt;p&gt;Our&amp;nbsp;first study&amp;nbsp;involved&amp;nbsp;38 professionals&amp;nbsp;across&amp;nbsp;engineering, research, marketing, and program management. Participants reviewed&amp;nbsp;design mock-ups that&amp;nbsp;provided&amp;nbsp;static&amp;nbsp;prompt-refinement&amp;nbsp;options—such as&amp;nbsp;&lt;em&gt;length&lt;/em&gt;, &lt;em&gt;tone&lt;/em&gt;, or &lt;em&gt;start with&lt;/em&gt;—for shaping&amp;nbsp;AI&amp;nbsp;responses.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Although these&amp;nbsp;static&amp;nbsp;options were helpful, they&amp;nbsp;couldn’t&amp;nbsp;adapt to the&amp;nbsp;specific formula, code snippets, or text&amp;nbsp;the participant&amp;nbsp;was trying to understand.&amp;nbsp;Participants&amp;nbsp;also wanted direct ways to&amp;nbsp;customize&amp;nbsp;the&amp;nbsp;tone,&amp;nbsp;detail, or&amp;nbsp;format of the response&amp;nbsp;without&amp;nbsp;having to&amp;nbsp;type instructions.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="why-dynamic-refinement-matters"&gt;Why dynamic refinement matters&lt;/h3&gt;



&lt;p&gt;The&amp;nbsp;second study&amp;nbsp;tested&amp;nbsp;prototypes&amp;nbsp;in a&amp;nbsp;controlled experiment.&amp;nbsp;We compared the static&amp;nbsp;design&amp;nbsp;from the first study, called&amp;nbsp;the&amp;nbsp;“Static Prompt Refinement Control”&amp;nbsp;(Static PRC),&amp;nbsp;against a&amp;nbsp;“Dynamic Prompt Refinement Control” (Dynamic PRC)&amp;nbsp;with features that&amp;nbsp;responded&amp;nbsp;to&amp;nbsp;participants’ feedback.&amp;nbsp;Sixteen&amp;nbsp;technical&amp;nbsp;professionals familiar with generative AI&amp;nbsp;completed six tasks,&amp;nbsp;spanning&amp;nbsp;code explanation,&amp;nbsp;understanding a&amp;nbsp;complex topic, and&amp;nbsp;learning a new&amp;nbsp;skill.&amp;nbsp;Each participant tested both systems, with task assignments balanced to ensure fair comparison.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Comparing Dynamic PRC&amp;nbsp;to&amp;nbsp;Static PRC&amp;nbsp;revealed key insights into how&amp;nbsp;dynamic&amp;nbsp;prompt-refinement&amp;nbsp;options change users’&amp;nbsp;sense of&amp;nbsp;control and exploration and&amp;nbsp;how those options&amp;nbsp;help them&amp;nbsp;reflect&amp;nbsp;on their understanding.&amp;nbsp;&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="static-prompt-refinement"&gt;Static&amp;nbsp;prompt&amp;nbsp;refinement&lt;/h3&gt;



&lt;p&gt;Static PRC&amp;nbsp;offered a set of pre‑selected controls&amp;nbsp;(Figure 1)&amp;nbsp;identified&amp;nbsp;in the&amp;nbsp;initial&amp;nbsp;study.&amp;nbsp;We expected these options to be useful&amp;nbsp;across many&amp;nbsp;types of&amp;nbsp;explanation-seeking&amp;nbsp;prompts.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Alt text: The Static PRC interface in the user study. It includes dropdowns and radio buttons for selecting expertise level (Beginner to Advanced), explanation length (Short to Long), role of AI (Coach, Teach, Explain), explanation type (End result, Modular, Step-by-step), starting point (High-level or Detailed), and tone (Formal, Informal, Encouraging, Neutral)." class="wp-image-1157834" height="852" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig02-staticprc-1.png" width="1379" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure&amp;nbsp;1: The static PRC&amp;nbsp;interface&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="dynamic-prompt-refinement"&gt;Dynamic prompt refinement&lt;/h3&gt;



&lt;p&gt;We built the Dynamic PRC system to automatically produce prompt options and refinements based on the user’s input, presenting them in real time so that users could adjust these controls and guide the AI’s responses more precisely (Figure 2).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Alt text: How users interacted with the Dynamic PRC system. (1) shows a user input prompt of “Explain the formula” [with a long Excel formula] (2) Three rows of options relating to this prompt, Explanation Detail Level, Focus Areas, and Learning Objectives, with several options for each, preselected (3) User has modified the preselected options by clicking Troubleshooting under Learning Objectives (4) AI response of an explanation for the formula based on the selected options (5) Session chat control panel with text box that the user adds " class="wp-image-1157876" height="915" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig03-dynamicprc-1.png" width="1379" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure&amp;nbsp;2.&amp;nbsp;Interaction flow in&amp;nbsp;the Dynamic PRC system. (1)&amp;nbsp;The&amp;nbsp;user&amp;nbsp;asks&amp;nbsp;the system to explain&amp;nbsp;a long Excel formula.&amp;nbsp;(2)&amp;nbsp;Dynamic PRC generates refinement&amp;nbsp;options:&amp;nbsp;Explanation Detail Level, Focus Areas, and Learning Objectives.&amp;nbsp;(3)&amp;nbsp;The user&amp;nbsp;modifies&amp;nbsp;these&amp;nbsp;options.&amp;nbsp;(4)&amp;nbsp;The&amp;nbsp;AI returns&amp;nbsp;an explanation based on the selected options.&amp;nbsp;(5)&amp;nbsp;In the session chat panel,&amp;nbsp;the user adds&amp;nbsp;a request&amp;nbsp;to control the structure or format of the response.&amp;nbsp;(6)&amp;nbsp;Dynamic PRC generates&amp;nbsp;new&amp;nbsp;option&amp;nbsp;sets based on this input.&amp;nbsp;(7)&amp;nbsp;The&amp;nbsp;AI produces an updated explanation reflecting the&amp;nbsp;newly applied&amp;nbsp;options.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="findings"&gt;Findings&lt;/h2&gt;



&lt;p&gt;Participants consistently reported that dynamic controls made it easier to express the nuances of their tasks without repeatedly rephrasing their prompts. This reduced the effort of prompt engineering and allowed users to focus more on understanding content than managing the mechanics of phrasing.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Alt text: Box plot chart titled “Dynamic vs Static PRC: Which tool…”, comparing user responses to six questions about preference, mental demand, feeling rushed, success, effort, and annoyance. Y-axis ranges from 1 (Dynamic) to 7 (Static), with 4 marked as Equal. Each question is represented by a box plot showing response distribution, median, and variability, illustrating perceived differences between dynamic and static PRC tools." class="wp-image-1157879" height="313" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig04-plotpreferences-1.png" width="627" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure&amp;nbsp;3.&amp;nbsp;Comparison of&amp;nbsp;user&amp;nbsp;preferences for Static&amp;nbsp;PRC&amp;nbsp;versus&amp;nbsp;Dynamic PRC&amp;nbsp;across key evaluation criteria.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Contextual options prompted users to try refinements they might not have considered on their own. This behavior suggests that Dynamic PRC can broaden how users engage with AI explanations, helping them uncover new ways to approach tasks beyond their initial intent. Beyond exploration, the dynamic controls prompted participants to think more deliberately about their goals. Options like “Learning Objective” and “Response Format” helped them clarify what they needed, whether guidance on applying a concept or step-by-step troubleshooting help.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Alt text: Box plot chart titled “Dynamic vs Static PRC: Control Effectiveness,” comparing user agreement with four statements about AI control tools. Each statement has two box plots—blue for Dynamic and orange for Static—showing response distributions on a 1 (Strongly Disagree) to 7 (Strongly Agree) Likert scale. Statements assess perceived control over AI output, usefulness for understanding, desire for more control, and clarity of control functions." class="wp-image-1157883" height="358" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig05-ploteffectiveness-1.png" width="627" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4.&amp;nbsp;Participant ratings comparing the&amp;nbsp;effectiveness of Static PRC and&amp;nbsp;Dynamic PRC&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;While participants valued Dynamic PRC’s adaptability, they also found it more difficult to interpret. Some struggled to anticipate how a selected option would influence the response, noting that the controls seemed opaque because the effect became clear only after the output appeared.&lt;/p&gt;



&lt;p&gt;However,&amp;nbsp;the&amp;nbsp;overall&amp;nbsp;positive response&amp;nbsp;to&amp;nbsp;Dynamic PRC&amp;nbsp;showed us that Promptions&amp;nbsp;could&amp;nbsp;be&amp;nbsp;broadly useful,&amp;nbsp;leading&amp;nbsp;us to share it&amp;nbsp;with the developer community.   &amp;nbsp;&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="technical-design"&gt;Technical design&lt;/h3&gt;



&lt;p&gt;Promptions works as a lightweight middleware layer that sits between the user and the underlying language model (Figure 5). It has two main components:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Option Module&lt;/strong&gt;. This module reviews the user’s prompt and conversation history, then generates a set of refinement options. These are presented as interactive UI elements (radio buttons, checkboxes, text fields) that directly shape how the AI interprets the prompt.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Chat&amp;nbsp;Module.&lt;/strong&gt;&amp;nbsp;This module&amp;nbsp;produces the&amp;nbsp;AI’s response based&amp;nbsp;on the refined prompt.&amp;nbsp;When&amp;nbsp;a user changes an option,&amp;nbsp;the&amp;nbsp;response&amp;nbsp;immediately&amp;nbsp;updates,&amp;nbsp;making the interaction feel more like an&amp;nbsp;evolving&amp;nbsp;conversation&amp;nbsp;than&amp;nbsp;a cycle of&amp;nbsp;repeated prompts.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Alt text: The Promptions system model. (1) The Option Module ingests the user’s prompt input along with the conversation history. (2) It then outputs a set of prompt options, each initialized based on the content of the prompt. (3) These options are rendered inline via a dedicated rendering engine. (4) The Chat Module incorporates the refined options as grounding, alongside the original prompt and conversation history, to generate a chat response. (5) The user can modify the GUI controls, which updates the refinements and triggers the Chat Module to regenerate the current response accordingly." class="wp-image-1157886" height="547" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig07-systemflow-1.png" width="524" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure&amp;nbsp;5.&amp;nbsp;Promptions&amp;nbsp;middleware workflow. (1) The Option Module&amp;nbsp;reads&amp;nbsp;the user’s prompt&amp;nbsp;and&amp;nbsp;conversation history&amp;nbsp;and&amp;nbsp;(2)&amp;nbsp;generates&amp;nbsp;prompt options. (3) These options are&amp;nbsp;rendered&amp;nbsp;inline&amp;nbsp;by&amp;nbsp;a dedicated&amp;nbsp;component. (4) The Chat Module incorporates these&amp;nbsp;refined options alongside the original prompt and history to&amp;nbsp;produce&amp;nbsp;a response.&amp;nbsp;(5)&amp;nbsp;When the user&amp;nbsp;adjusts&amp;nbsp;the&amp;nbsp;controls,&amp;nbsp;the&amp;nbsp;refinements&amp;nbsp;update&amp;nbsp;and the Chat Module regenerates&amp;nbsp;the response accordingly.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="adding-promptions-to-an-application"&gt;Adding Promptions to an application&lt;/h3&gt;



&lt;p&gt;Promptions&amp;nbsp;easily&amp;nbsp;integrates&amp;nbsp;into&amp;nbsp;any conversational chat interface.&amp;nbsp;Developers only need to add a&amp;nbsp;component&amp;nbsp;to display the&amp;nbsp;options and connect it to the&amp;nbsp;AI system.&amp;nbsp;There’s&amp;nbsp;no need to store&amp;nbsp;date&amp;nbsp;between sessions, which keeps implementation simple.&amp;nbsp;The&amp;nbsp;Microsoft Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;repository&amp;nbsp;includes&amp;nbsp;two sample applications,&amp;nbsp;a generic chatbot and an image generator,&amp;nbsp;that&amp;nbsp;demonstrate&amp;nbsp;this design in practice.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Promptions is well-suited for interfaces where users need to provide context but don’t want to write it all out. Instead of typing lengthy explanations, they can adjust the controls that guide the AI’s response to match their preferences.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="questions-for-further-exploration"&gt;Questions for further exploration&lt;/h2&gt;



&lt;p&gt;Promptions raises important questions for future research. Key usability challenges include clarifying how dynamic options affect AI output and managing the complexity of multiple controls. Other questions involve balancing immediate adjustments with persistent settings and enabling users to share options collaboratively.&lt;/p&gt;



&lt;p&gt;On the technical side, questions focus on generating more effective options, validating and customizing dynamic interfaces, gathering relevant context automatically, and supporting the ability to save and share option sets across sessions.&lt;/p&gt;



&lt;p&gt;&amp;nbsp;These&amp;nbsp;questions, along with&amp;nbsp;broader&amp;nbsp;considerations&amp;nbsp;of&amp;nbsp;collaboration, ethics, security, and scalability,&amp;nbsp;are&amp;nbsp;guiding our ongoing work on&amp;nbsp;Promptions&amp;nbsp;and related systems.&lt;/p&gt;







&lt;p&gt;By making Promptions open source, we hope to help developers create smarter, more responsive AI experiences.&lt;/p&gt;



&lt;p&gt;Explore Promptions on Microsoft Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white line icons on a blue-to-green gradient background: a hub-and-spoke network symbol on the left, a laptop with a user icon in the center, and a connected group of three user icons on the right." class="wp-image-1157946" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/Promptions-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Anyone who uses AI systems knows the frustration: a prompt is given, the response misses the mark, and the cycle repeats. This trial-and-error loop can feel unpredictable and discouraging. To address this, we are excited to introduce &lt;strong&gt;Promptions&lt;/strong&gt; (&lt;em&gt;prompt + options&lt;/em&gt;), a UI framework that helps developers build AI interfaces with more precise user control.&lt;/p&gt;



&lt;p&gt;Its simple design makes it easy to integrate into any setting&amp;nbsp;that relies on added context, including customer support, education, and medicine. Promptions is available under the MIT license on Microsoft Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and GitHub.&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="background"&gt;Background&lt;/h2&gt;



&lt;p&gt;Promptions&amp;nbsp;builds on&amp;nbsp;our research,&amp;nbsp;“Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks.”&amp;nbsp;This&amp;nbsp;project&amp;nbsp;examined&amp;nbsp;how&amp;nbsp;knowledge&amp;nbsp;workers&amp;nbsp;use&amp;nbsp;generative AI when their goal is&amp;nbsp;to &lt;em&gt;understand&lt;/em&gt; rather than&amp;nbsp;&lt;em&gt;create&lt;/em&gt;. While much public&amp;nbsp;discussion centers on&amp;nbsp;AI producing text&amp;nbsp;or&amp;nbsp;images, understanding involves asking AI to explain, clarify, or teach—a&amp;nbsp;task&amp;nbsp;that can&amp;nbsp;quickly become&amp;nbsp;complex. Consider a spreadsheet formula: one&amp;nbsp;user may want a&amp;nbsp;simple syntax&amp;nbsp;breakdown,&amp;nbsp;another a&amp;nbsp;debugging&amp;nbsp;guide, and&amp;nbsp;another an explanation suitable for&amp;nbsp;teaching&amp;nbsp;colleagues.&amp;nbsp;The same formula can require&amp;nbsp;entirely&amp;nbsp;different explanations depending on the user’s role, expertise, and goals.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;A great deal of complexity sits beneath these&amp;nbsp;seemingly simple&amp;nbsp;requests.&amp;nbsp;Users&amp;nbsp;often&amp;nbsp;find&amp;nbsp;that the way&amp;nbsp;they phrase a question&amp;nbsp;doesn’t&amp;nbsp;match&amp;nbsp;the&amp;nbsp;level of detail the AI&amp;nbsp;needs.&amp;nbsp;Clarifying what they really want can require long, carefully worded&amp;nbsp;prompts that are tiring to produce.&amp;nbsp;And because the connection&amp;nbsp;between natural language and system behavior&amp;nbsp;isn’t always transparent, it can be difficult to predict&amp;nbsp;how the AI will interpret a&amp;nbsp;given&amp;nbsp;request.&amp;nbsp;In the end, users spend more time&amp;nbsp;managing the interaction itself&amp;nbsp;than understanding the material they&amp;nbsp;hoped&amp;nbsp;to learn.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="identifying-how-users-want-to-guide-ai-outputs"&gt;Identifying&amp;nbsp;how users want to guide AI outputs&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;To explore why these&amp;nbsp;challenges&amp;nbsp;persist and how people can better steer AI toward customized results, we conducted two studies with knowledge workers across technical and nontechnical roles. Their experiences highlighted important gaps that guided Promptions’ design.&lt;/p&gt;



&lt;p&gt;Our&amp;nbsp;first study&amp;nbsp;involved&amp;nbsp;38 professionals&amp;nbsp;across&amp;nbsp;engineering, research, marketing, and program management. Participants reviewed&amp;nbsp;design mock-ups that&amp;nbsp;provided&amp;nbsp;static&amp;nbsp;prompt-refinement&amp;nbsp;options—such as&amp;nbsp;&lt;em&gt;length&lt;/em&gt;, &lt;em&gt;tone&lt;/em&gt;, or &lt;em&gt;start with&lt;/em&gt;—for shaping&amp;nbsp;AI&amp;nbsp;responses.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Although these&amp;nbsp;static&amp;nbsp;options were helpful, they&amp;nbsp;couldn’t&amp;nbsp;adapt to the&amp;nbsp;specific formula, code snippets, or text&amp;nbsp;the participant&amp;nbsp;was trying to understand.&amp;nbsp;Participants&amp;nbsp;also wanted direct ways to&amp;nbsp;customize&amp;nbsp;the&amp;nbsp;tone,&amp;nbsp;detail, or&amp;nbsp;format of the response&amp;nbsp;without&amp;nbsp;having to&amp;nbsp;type instructions.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="why-dynamic-refinement-matters"&gt;Why dynamic refinement matters&lt;/h3&gt;



&lt;p&gt;The&amp;nbsp;second study&amp;nbsp;tested&amp;nbsp;prototypes&amp;nbsp;in a&amp;nbsp;controlled experiment.&amp;nbsp;We compared the static&amp;nbsp;design&amp;nbsp;from the first study, called&amp;nbsp;the&amp;nbsp;“Static Prompt Refinement Control”&amp;nbsp;(Static PRC),&amp;nbsp;against a&amp;nbsp;“Dynamic Prompt Refinement Control” (Dynamic PRC)&amp;nbsp;with features that&amp;nbsp;responded&amp;nbsp;to&amp;nbsp;participants’ feedback.&amp;nbsp;Sixteen&amp;nbsp;technical&amp;nbsp;professionals familiar with generative AI&amp;nbsp;completed six tasks,&amp;nbsp;spanning&amp;nbsp;code explanation,&amp;nbsp;understanding a&amp;nbsp;complex topic, and&amp;nbsp;learning a new&amp;nbsp;skill.&amp;nbsp;Each participant tested both systems, with task assignments balanced to ensure fair comparison.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Comparing Dynamic PRC&amp;nbsp;to&amp;nbsp;Static PRC&amp;nbsp;revealed key insights into how&amp;nbsp;dynamic&amp;nbsp;prompt-refinement&amp;nbsp;options change users’&amp;nbsp;sense of&amp;nbsp;control and exploration and&amp;nbsp;how those options&amp;nbsp;help them&amp;nbsp;reflect&amp;nbsp;on their understanding.&amp;nbsp;&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="static-prompt-refinement"&gt;Static&amp;nbsp;prompt&amp;nbsp;refinement&lt;/h3&gt;



&lt;p&gt;Static PRC&amp;nbsp;offered a set of pre‑selected controls&amp;nbsp;(Figure 1)&amp;nbsp;identified&amp;nbsp;in the&amp;nbsp;initial&amp;nbsp;study.&amp;nbsp;We expected these options to be useful&amp;nbsp;across many&amp;nbsp;types of&amp;nbsp;explanation-seeking&amp;nbsp;prompts.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Alt text: The Static PRC interface in the user study. It includes dropdowns and radio buttons for selecting expertise level (Beginner to Advanced), explanation length (Short to Long), role of AI (Coach, Teach, Explain), explanation type (End result, Modular, Step-by-step), starting point (High-level or Detailed), and tone (Formal, Informal, Encouraging, Neutral)." class="wp-image-1157834" height="852" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig02-staticprc-1.png" width="1379" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure&amp;nbsp;1: The static PRC&amp;nbsp;interface&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="dynamic-prompt-refinement"&gt;Dynamic prompt refinement&lt;/h3&gt;



&lt;p&gt;We built the Dynamic PRC system to automatically produce prompt options and refinements based on the user’s input, presenting them in real time so that users could adjust these controls and guide the AI’s responses more precisely (Figure 2).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Alt text: How users interacted with the Dynamic PRC system. (1) shows a user input prompt of “Explain the formula” [with a long Excel formula] (2) Three rows of options relating to this prompt, Explanation Detail Level, Focus Areas, and Learning Objectives, with several options for each, preselected (3) User has modified the preselected options by clicking Troubleshooting under Learning Objectives (4) AI response of an explanation for the formula based on the selected options (5) Session chat control panel with text box that the user adds " class="wp-image-1157876" height="915" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig03-dynamicprc-1.png" width="1379" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure&amp;nbsp;2.&amp;nbsp;Interaction flow in&amp;nbsp;the Dynamic PRC system. (1)&amp;nbsp;The&amp;nbsp;user&amp;nbsp;asks&amp;nbsp;the system to explain&amp;nbsp;a long Excel formula.&amp;nbsp;(2)&amp;nbsp;Dynamic PRC generates refinement&amp;nbsp;options:&amp;nbsp;Explanation Detail Level, Focus Areas, and Learning Objectives.&amp;nbsp;(3)&amp;nbsp;The user&amp;nbsp;modifies&amp;nbsp;these&amp;nbsp;options.&amp;nbsp;(4)&amp;nbsp;The&amp;nbsp;AI returns&amp;nbsp;an explanation based on the selected options.&amp;nbsp;(5)&amp;nbsp;In the session chat panel,&amp;nbsp;the user adds&amp;nbsp;a request&amp;nbsp;to control the structure or format of the response.&amp;nbsp;(6)&amp;nbsp;Dynamic PRC generates&amp;nbsp;new&amp;nbsp;option&amp;nbsp;sets based on this input.&amp;nbsp;(7)&amp;nbsp;The&amp;nbsp;AI produces an updated explanation reflecting the&amp;nbsp;newly applied&amp;nbsp;options.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="findings"&gt;Findings&lt;/h2&gt;



&lt;p&gt;Participants consistently reported that dynamic controls made it easier to express the nuances of their tasks without repeatedly rephrasing their prompts. This reduced the effort of prompt engineering and allowed users to focus more on understanding content than managing the mechanics of phrasing.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Alt text: Box plot chart titled “Dynamic vs Static PRC: Which tool…”, comparing user responses to six questions about preference, mental demand, feeling rushed, success, effort, and annoyance. Y-axis ranges from 1 (Dynamic) to 7 (Static), with 4 marked as Equal. Each question is represented by a box plot showing response distribution, median, and variability, illustrating perceived differences between dynamic and static PRC tools." class="wp-image-1157879" height="313" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig04-plotpreferences-1.png" width="627" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure&amp;nbsp;3.&amp;nbsp;Comparison of&amp;nbsp;user&amp;nbsp;preferences for Static&amp;nbsp;PRC&amp;nbsp;versus&amp;nbsp;Dynamic PRC&amp;nbsp;across key evaluation criteria.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Contextual options prompted users to try refinements they might not have considered on their own. This behavior suggests that Dynamic PRC can broaden how users engage with AI explanations, helping them uncover new ways to approach tasks beyond their initial intent. Beyond exploration, the dynamic controls prompted participants to think more deliberately about their goals. Options like “Learning Objective” and “Response Format” helped them clarify what they needed, whether guidance on applying a concept or step-by-step troubleshooting help.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Alt text: Box plot chart titled “Dynamic vs Static PRC: Control Effectiveness,” comparing user agreement with four statements about AI control tools. Each statement has two box plots—blue for Dynamic and orange for Static—showing response distributions on a 1 (Strongly Disagree) to 7 (Strongly Agree) Likert scale. Statements assess perceived control over AI output, usefulness for understanding, desire for more control, and clarity of control functions." class="wp-image-1157883" height="358" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig05-ploteffectiveness-1.png" width="627" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4.&amp;nbsp;Participant ratings comparing the&amp;nbsp;effectiveness of Static PRC and&amp;nbsp;Dynamic PRC&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;While participants valued Dynamic PRC’s adaptability, they also found it more difficult to interpret. Some struggled to anticipate how a selected option would influence the response, noting that the controls seemed opaque because the effect became clear only after the output appeared.&lt;/p&gt;



&lt;p&gt;However,&amp;nbsp;the&amp;nbsp;overall&amp;nbsp;positive response&amp;nbsp;to&amp;nbsp;Dynamic PRC&amp;nbsp;showed us that Promptions&amp;nbsp;could&amp;nbsp;be&amp;nbsp;broadly useful,&amp;nbsp;leading&amp;nbsp;us to share it&amp;nbsp;with the developer community.   &amp;nbsp;&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="technical-design"&gt;Technical design&lt;/h3&gt;



&lt;p&gt;Promptions works as a lightweight middleware layer that sits between the user and the underlying language model (Figure 5). It has two main components:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Option Module&lt;/strong&gt;. This module reviews the user’s prompt and conversation history, then generates a set of refinement options. These are presented as interactive UI elements (radio buttons, checkboxes, text fields) that directly shape how the AI interprets the prompt.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Chat&amp;nbsp;Module.&lt;/strong&gt;&amp;nbsp;This module&amp;nbsp;produces the&amp;nbsp;AI’s response based&amp;nbsp;on the refined prompt.&amp;nbsp;When&amp;nbsp;a user changes an option,&amp;nbsp;the&amp;nbsp;response&amp;nbsp;immediately&amp;nbsp;updates,&amp;nbsp;making the interaction feel more like an&amp;nbsp;evolving&amp;nbsp;conversation&amp;nbsp;than&amp;nbsp;a cycle of&amp;nbsp;repeated prompts.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Alt text: The Promptions system model. (1) The Option Module ingests the user’s prompt input along with the conversation history. (2) It then outputs a set of prompt options, each initialized based on the content of the prompt. (3) These options are rendered inline via a dedicated rendering engine. (4) The Chat Module incorporates the refined options as grounding, alongside the original prompt and conversation history, to generate a chat response. (5) The user can modify the GUI controls, which updates the refinements and triggers the Chat Module to regenerate the current response accordingly." class="wp-image-1157886" height="547" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig07-systemflow-1.png" width="524" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure&amp;nbsp;5.&amp;nbsp;Promptions&amp;nbsp;middleware workflow. (1) The Option Module&amp;nbsp;reads&amp;nbsp;the user’s prompt&amp;nbsp;and&amp;nbsp;conversation history&amp;nbsp;and&amp;nbsp;(2)&amp;nbsp;generates&amp;nbsp;prompt options. (3) These options are&amp;nbsp;rendered&amp;nbsp;inline&amp;nbsp;by&amp;nbsp;a dedicated&amp;nbsp;component. (4) The Chat Module incorporates these&amp;nbsp;refined options alongside the original prompt and history to&amp;nbsp;produce&amp;nbsp;a response.&amp;nbsp;(5)&amp;nbsp;When the user&amp;nbsp;adjusts&amp;nbsp;the&amp;nbsp;controls,&amp;nbsp;the&amp;nbsp;refinements&amp;nbsp;update&amp;nbsp;and the Chat Module regenerates&amp;nbsp;the response accordingly.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="adding-promptions-to-an-application"&gt;Adding Promptions to an application&lt;/h3&gt;



&lt;p&gt;Promptions&amp;nbsp;easily&amp;nbsp;integrates&amp;nbsp;into&amp;nbsp;any conversational chat interface.&amp;nbsp;Developers only need to add a&amp;nbsp;component&amp;nbsp;to display the&amp;nbsp;options and connect it to the&amp;nbsp;AI system.&amp;nbsp;There’s&amp;nbsp;no need to store&amp;nbsp;date&amp;nbsp;between sessions, which keeps implementation simple.&amp;nbsp;The&amp;nbsp;Microsoft Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;repository&amp;nbsp;includes&amp;nbsp;two sample applications,&amp;nbsp;a generic chatbot and an image generator,&amp;nbsp;that&amp;nbsp;demonstrate&amp;nbsp;this design in practice.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Promptions is well-suited for interfaces where users need to provide context but don’t want to write it all out. Instead of typing lengthy explanations, they can adjust the controls that guide the AI’s response to match their preferences.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="questions-for-further-exploration"&gt;Questions for further exploration&lt;/h2&gt;



&lt;p&gt;Promptions raises important questions for future research. Key usability challenges include clarifying how dynamic options affect AI output and managing the complexity of multiple controls. Other questions involve balancing immediate adjustments with persistent settings and enabling users to share options collaboratively.&lt;/p&gt;



&lt;p&gt;On the technical side, questions focus on generating more effective options, validating and customizing dynamic interfaces, gathering relevant context automatically, and supporting the ability to save and share option sets across sessions.&lt;/p&gt;



&lt;p&gt;&amp;nbsp;These&amp;nbsp;questions, along with&amp;nbsp;broader&amp;nbsp;considerations&amp;nbsp;of&amp;nbsp;collaboration, ethics, security, and scalability,&amp;nbsp;are&amp;nbsp;guiding our ongoing work on&amp;nbsp;Promptions&amp;nbsp;and related systems.&lt;/p&gt;







&lt;p&gt;By making Promptions open source, we hope to help developers create smarter, more responsive AI experiences.&lt;/p&gt;



&lt;p&gt;Explore Promptions on Microsoft Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/promptions-helps-make-ai-prompting-more-precise-with-dynamic-ui-controls/</guid><pubDate>Wed, 10 Dec 2025 17:00:00 +0000</pubDate></item><item><title>Google is testing AI-powered article overviews on select publications’ Google News pages (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/10/google-is-testing-ai-powered-article-overviews-on-select-publications-google-news-pages/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is testing AI-powered article overviews on participating publications’ Google News pages as part of a new pilot program, the search giant announced on Wednesday. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;News publishers participating in the pilot program include Der Spiegel, El País, Folha, Infobae, Kompas, The Guardian, The Times of India, The Washington Examiner, and The Washington Post, among others.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The purpose of the new commercial partnership program is to “explore how AI can drive more engaged audiences,” Google said in a blog post. As part of the new AI pilot program, the company will work with publishers to experiment with new features in Google News.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By adding AI-powered article overviews, Google says users will get more context before they click through to read an article. While AI-generated summaries may lead to fewer clicks on news articles, publications participating in the commercial pilot program will receive direct payments from Google, which could make up for the potential decrease in traffic to their sites. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI-powered article overviews will only appear on participating publications’ Google News pages, and not anywhere else on Google News or in Search.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This isn’t the first time that Google has introduced AI summaries for news. In July, the company rolled out AI summaries in Discover, the main news feed inside Google’s search app. With this change, users no longer see a single headline from a major publication in the feed. Instead, they see the logos of multiple news publishers in the top-left corner, followed by an AI-generated summary that cites those sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also experimenting with audio briefings for people who prefer listening to the news rather than reading it, as part of the new pilot program. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company says these features will include clear attribution and a link to articles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Google is partnering with organizations such as Estadão, Antara, Yonhap, and The Associated Press to incorporate real-time information and enhance results in the Gemini app. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As the way people consume information evolves, we’ll continue to improve our products for people around the world and engage with feedback from stakeholders across the ecosystem,” Google wrote in its blog post. “We’re doing this work in collaboration with websites and creators of all sizes, from major news publishers to new and emerging voices.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3074736" height="372" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-10-at-10.59.58-AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;As part of Google’s Wednesday announcement, the company said that it’s launching its “Preferred Sources” feature globally after first launching it in the U.S. and India in August. The feature allows users to select their favorite news sites and blogs to appear in the Top Stories section of Google search results.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In the coming days, the feature will be available for English-language users worldwide, and Google plans to roll it out to all supported languages early next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google will now also highlight links from your news subscriptions and show these links in a dedicated carousel in the Gemini app in the coming weeks, with AI Overviews and AI Mode to follow.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While these features make it easy for users to access news from their preferred sources, they also risk confining them to an ideological bubble that limits their exposure to different perspectives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google also announced that it’s increasing the number of inline links in AI Mode. Additionally, it’s introducing “contextual introductions” for embedded links, which are brief explanations that explain why a link could be useful to explore.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is testing AI-powered article overviews on participating publications’ Google News pages as part of a new pilot program, the search giant announced on Wednesday. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;News publishers participating in the pilot program include Der Spiegel, El País, Folha, Infobae, Kompas, The Guardian, The Times of India, The Washington Examiner, and The Washington Post, among others.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The purpose of the new commercial partnership program is to “explore how AI can drive more engaged audiences,” Google said in a blog post. As part of the new AI pilot program, the company will work with publishers to experiment with new features in Google News.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By adding AI-powered article overviews, Google says users will get more context before they click through to read an article. While AI-generated summaries may lead to fewer clicks on news articles, publications participating in the commercial pilot program will receive direct payments from Google, which could make up for the potential decrease in traffic to their sites. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI-powered article overviews will only appear on participating publications’ Google News pages, and not anywhere else on Google News or in Search.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This isn’t the first time that Google has introduced AI summaries for news. In July, the company rolled out AI summaries in Discover, the main news feed inside Google’s search app. With this change, users no longer see a single headline from a major publication in the feed. Instead, they see the logos of multiple news publishers in the top-left corner, followed by an AI-generated summary that cites those sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also experimenting with audio briefings for people who prefer listening to the news rather than reading it, as part of the new pilot program. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company says these features will include clear attribution and a link to articles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Google is partnering with organizations such as Estadão, Antara, Yonhap, and The Associated Press to incorporate real-time information and enhance results in the Gemini app. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As the way people consume information evolves, we’ll continue to improve our products for people around the world and engage with feedback from stakeholders across the ecosystem,” Google wrote in its blog post. “We’re doing this work in collaboration with websites and creators of all sizes, from major news publishers to new and emerging voices.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3074736" height="372" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-10-at-10.59.58-AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;As part of Google’s Wednesday announcement, the company said that it’s launching its “Preferred Sources” feature globally after first launching it in the U.S. and India in August. The feature allows users to select their favorite news sites and blogs to appear in the Top Stories section of Google search results.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In the coming days, the feature will be available for English-language users worldwide, and Google plans to roll it out to all supported languages early next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google will now also highlight links from your news subscriptions and show these links in a dedicated carousel in the Gemini app in the coming weeks, with AI Overviews and AI Mode to follow.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While these features make it easy for users to access news from their preferred sources, they also risk confining them to an ideological bubble that limits their exposure to different perspectives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google also announced that it’s increasing the number of inline links in AI Mode. Additionally, it’s introducing “contextual introductions” for embedded links, which are brief explanations that explain why a link could be useful to explore.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/10/google-is-testing-ai-powered-article-overviews-on-select-publications-google-news-pages/</guid><pubDate>Wed, 10 Dec 2025 17:00:00 +0000</pubDate></item><item><title>US taking 25% cut of Nvidia chip sales “makes no sense,” experts say (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/12/us-taking-25-cut-of-nvidia-chip-sales-makes-no-sense-experts-say/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Trump’s odd Nvidia reversal may open the door for China to demand Blackwell access.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2212801610-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2212801610-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Donald Trump shakes hands with Nvidia CEO Jensen Huang.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Andrew Harnik / Staff | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Donald Trump’s decision to allow Nvidia to export an advanced artificial intelligence chip, the H200, to China may give China exactly what it needs to win the AI race, experts and lawmakers have warned.&lt;/p&gt;
&lt;p&gt;The H200 is about 10 times less powerful than Nvidia’s Blackwell chip, which is the tech giant’s currently most advanced chip that cannot be exported to China. But the H200 is six times more powerful than the H20, the most advanced chip available in China today. Meanwhile China’s leading AI chip maker, Huawei, is estimated to be about two years behind Nvidia’s technology. By approving the sales, Trump may unwittingly be helping Chinese chip makers “catch up” to Nvidia, Jake Sullivan told The New York Times.&lt;/p&gt;
&lt;p&gt;Sullivan, a former Biden-era national security advisor who helped design AI chip export curbs on China, told the NYT that Trump’s move was “nuts” because “China’s main problem” in the AI race “is they don’t have enough advanced computing capability.”&lt;/p&gt;
&lt;p&gt;“It makes no sense that President Trump is solving their problem for them by selling them powerful American chips,” Sullivan said. “We are literally handing away our advantage. China’s leaders can’t believe their luck.”&lt;/p&gt;
&lt;p&gt;Trump apparently was persuaded by Nvidia CEO Jensen Huang and his “AI czar,” David Sacks, to reverse course on H200 export curbs. They convinced Trump that restricting sales would ensure that only Chinese chip makers would get a piece of China’s market, shoring up revenue flows that dominant firms like Huawei could pour into R&amp;amp;D.&lt;/p&gt;
&lt;p&gt;By instead allowing Nvidia sales, China’s industry would remain hooked on US chips, the thinking goes. And Nvidia could use those funds—perhaps $10–15 billion annually, Bloomberg Intelligence has estimated—to further its own R&amp;amp;D efforts. That cash influx, theoretically, would allow Nvidia to maintain the US advantage.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Along the way, the US would receive a 25 percent cut of sales, which lawmakers from both sides of the aisle warned may not be legal and suggested to foreign rivals that US national security was “now up for sale,” NYT reported. The president has claimed there are conditions to sales safeguarding national security but, frustrating critics, provided no details.&lt;/p&gt;
&lt;h2&gt;Experts slam Nvidia plan as “flawed”&lt;/h2&gt;
&lt;p&gt;Trump’s plan is “flawed,” The Economist reported.&lt;/p&gt;
&lt;p&gt;For years, the US has established tech dominance by keeping advanced technology away from China. Trump risks rocking that boat by “tearing up America’s export-control policy,” particularly if China’s chip industry simply buys up the H200s as a short-term tactic to learn from the technology and beef up its domestic production of advanced chips, The Economist reported.&lt;/p&gt;
&lt;p&gt;In a sign that’s exactly what many expect could happen, investors in China were apparently so excited by Trump’s announcement that they immediately poured money into Moore Threads, expected to be China’s best answer to Nvidia, the South China Morning Post reported.&lt;/p&gt;
&lt;p&gt;Several experts for the non-partisan think tank the Counsel on Foreign Relations also criticized the policy change, cautioning that the reversal of course threatened to undermine US competition with China.&lt;/p&gt;
&lt;p&gt;Suggesting that Trump was “effectively undoing” export curbs sought during his first term, Zongyuan Zoe Liu warned that China “buys today to learn today, with the intention to build tomorrow.”&lt;/p&gt;
&lt;p&gt;And perhaps more concerning, she suggested, is that Trump’s policy signals weakness. Rather than forcing Chinese dependence on US tech, reversing course showed China that the US will “back down” under pressure, she warned. And they’re getting that message at a time when “Chinese leaders have a lot of reasons to believe they are not only winning the trade war but also making progress towards a higher degree of strategic autonomy.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In a post on X, Rush Doshi—a CFR expert who previously advised Biden on national security issues related to China—suggested that the policy change was “possibly decisive in the AI race.”&lt;/p&gt;
&lt;p&gt;“Compute is our main advantage—China has more power, engineers, and the entire edge layer—so by giving this up, we increase the odds the world runs on Chinese AI,” Doshi wrote.&lt;/p&gt;
&lt;p&gt;Experts fear Trump may not understand the full impact of his decision. In the short-term, Michael C. Horowitz wrote for CFR, “it is indisputable” that allowing H200 exports benefits China’s frontier AI and efforts to scale data centers. And Doshi pointed out that Trump’s shift may trigger more advanced technology flowing into China, as US allies that restricted sales of machines to build AI chips may soon follow his lead and lift their curbs. As China learns to be self-reliant from any influx of advanced tech, Sullivan warned that China’s leaders “intend to get off of American semiconductors as soon as they can.”&lt;/p&gt;
&lt;p&gt;“So, the argument that we can keep them ‘addicted’ holds no water,” Sullivan said. “They want American chips right now for one simple reason: They are behind in the AI race, and this will help them catch up while they build their own chip capabilities.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;China may reject H200, demand Blackwell access&lt;/h2&gt;
&lt;p&gt;It remains unclear if China will approve H200 sales, but some of the country’s biggest firms, including ByteDance, Tencent, and Alibaba, are interested, anonymous insider sources told Reuters.&lt;/p&gt;
&lt;p&gt;In the past, China has instructed companies to avoid Nvidia, warning of possible backdoors giving Nvidia a kill switch to remotely shut down chips. Such backdoors could potentially destabilize Chinese firms’ operations and R&amp;amp;D. Nvidia has denied such backdoors exist, but Chinese firms have supposedly sought reassurances from Nvidia in the aftermath of Trump’s policy change. Likely just as unpopular with the Chinese firms and government, Nvidia confirmed recently that it has built location verification tech that could help the US detect when restricted chips are leaked into China. Should the US ever renew export curbs on H200 chips, adopting them widely could cause chaos in the future.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Without giving China sought-after reassurances, Nvidia may not end up benefiting as much as it hoped from its mission to reclaim lost revenue from the Chinese market. Today, Chinese firms control about 60 percent of China’s AI chip market, where only a few years ago American firms—led by Nvidia—controlled 80 percent, the Economist reported.&lt;/p&gt;
&lt;p&gt;But for China, the temptation to buy up Nvidia chips may be too great to pass up. Another CFR expert, Chris McGuire, estimated that Nvidia could suddenly start exporting as many as 3 million H200s into China next year. “This would at least triple the amount of aggregate AI computing power China could add domestically” in 2026, McGuire wrote, and possibly trigger disastrous outcomes for the US.&lt;/p&gt;
&lt;p&gt;“This could cause DeepSeek and other Chinese AI developers to close the gap with leading US AI labs and enable China to develop an ‘AI Belt and Road’ initiative—a complement to its vast global infrastructure investment network already in place—that competes with US cloud providers around the world,” McGuire forecasted.&lt;/p&gt;
&lt;p&gt;As China mulls the benefits and risks, an emergency meeting was called, where the Chinese government discussed potential concerns of local firms buying chips, according to The Information. Reportedly, Beijing ended that meeting with a promise to issue a decision soon.&lt;/p&gt;
&lt;p&gt;Horowitz suggested that a primary reason that China may reject the H200s could be to squeeze even bigger concessions out of Trump, whose administration recently has been working to maintain a tenuous truce with China.&lt;/p&gt;
&lt;p&gt;“China could come back demanding the Blackwell or something else,” Horowitz suggested.&lt;/p&gt;
&lt;p&gt;In a statement, Nvidia—which plans to release a chip called the Rubin to surpass the Blackwell soon—praised Trump’s policy as striking “a thoughtful balance that is great for America.”&lt;/p&gt;
&lt;h2&gt;China will rip off Nvidia’s chips, Republican warns&lt;/h2&gt;
&lt;p&gt;Both Democratic and Republican lawmakers in Congress criticized Trump’s plan, including senators behind a bipartisan push to limit AI chip sales to China.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Some have questioned how much thought was put into the policy, as the US confusingly continues restricting less advanced AI chips (like the A100 and H100) while green-lighting H200 sales. Trump’s Justice Department also seems to be struggling to keep up. The NYT noted that just “hours before” Trump announced the policy change, the DOJ announced “it had detained two people for selling those chips to the country.”&lt;/p&gt;
&lt;p&gt;The chair of the Select Committee on Competition with China, Rep. John Moolenaar (R-Mich.), warned on X that the news wouldn’t be good for the US or Nvidia. First, the Chinese Communist Party “will use these highly advanced chips to strengthen its military capabilities and totalitarian surveillance,” he suggested. And second, “Nvidia should be under no illusions—China will rip off its technology, mass produce it themselves, and seek to end Nvidia as a competitor.”&lt;/p&gt;
&lt;p&gt;“That is China’s playbook and it is using it in every critical industry,” Moolenaar said.&lt;/p&gt;
&lt;p&gt;House Democrats on committees dealing with foreign affairs and competition with China echoed those concerns, The Hill reported, warning that “under this administration, our national security is for sale.”&lt;/p&gt;
&lt;p&gt;Nvidia’s Huang seems pleased with the outcome, which comes after months of reportedly pressuring the administration to lift export curbs limiting its growth in Chinese markets, the NYT reported. Last week, Trump heaped praise on Huang after one meeting, calling Huang a “smart man” and suggesting the Nvidia chief has “done an amazing job” helping Trump understand the stakes.&lt;/p&gt;
&lt;p&gt;At an October news conference ahead of the deal’s official approval, Huang suggested that government lawyers were researching ways to get around a US law that prohibits charging companies fees for export licenses. Eventually, Trump is expected to release a policy that outlines how the US will collect those fees without conflicting with that law.&lt;/p&gt;
&lt;p&gt;Senate Democrats appear unlikely to embrace such a policy, issuing a joint statement condemning the H200 sales as dooming the US in the AI race and threatening national security.&lt;/p&gt;
&lt;p&gt;“Access to these chips would give China’s military transformational technology to make its weapons more lethal, carry out more effective cyberattacks against American businesses and critical infrastructure and strengthen their economic and manufacturing sector,” Senators wrote.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Trump’s odd Nvidia reversal may open the door for China to demand Blackwell access.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2212801610-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2212801610-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Donald Trump shakes hands with Nvidia CEO Jensen Huang.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Andrew Harnik / Staff | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Donald Trump’s decision to allow Nvidia to export an advanced artificial intelligence chip, the H200, to China may give China exactly what it needs to win the AI race, experts and lawmakers have warned.&lt;/p&gt;
&lt;p&gt;The H200 is about 10 times less powerful than Nvidia’s Blackwell chip, which is the tech giant’s currently most advanced chip that cannot be exported to China. But the H200 is six times more powerful than the H20, the most advanced chip available in China today. Meanwhile China’s leading AI chip maker, Huawei, is estimated to be about two years behind Nvidia’s technology. By approving the sales, Trump may unwittingly be helping Chinese chip makers “catch up” to Nvidia, Jake Sullivan told The New York Times.&lt;/p&gt;
&lt;p&gt;Sullivan, a former Biden-era national security advisor who helped design AI chip export curbs on China, told the NYT that Trump’s move was “nuts” because “China’s main problem” in the AI race “is they don’t have enough advanced computing capability.”&lt;/p&gt;
&lt;p&gt;“It makes no sense that President Trump is solving their problem for them by selling them powerful American chips,” Sullivan said. “We are literally handing away our advantage. China’s leaders can’t believe their luck.”&lt;/p&gt;
&lt;p&gt;Trump apparently was persuaded by Nvidia CEO Jensen Huang and his “AI czar,” David Sacks, to reverse course on H200 export curbs. They convinced Trump that restricting sales would ensure that only Chinese chip makers would get a piece of China’s market, shoring up revenue flows that dominant firms like Huawei could pour into R&amp;amp;D.&lt;/p&gt;
&lt;p&gt;By instead allowing Nvidia sales, China’s industry would remain hooked on US chips, the thinking goes. And Nvidia could use those funds—perhaps $10–15 billion annually, Bloomberg Intelligence has estimated—to further its own R&amp;amp;D efforts. That cash influx, theoretically, would allow Nvidia to maintain the US advantage.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Along the way, the US would receive a 25 percent cut of sales, which lawmakers from both sides of the aisle warned may not be legal and suggested to foreign rivals that US national security was “now up for sale,” NYT reported. The president has claimed there are conditions to sales safeguarding national security but, frustrating critics, provided no details.&lt;/p&gt;
&lt;h2&gt;Experts slam Nvidia plan as “flawed”&lt;/h2&gt;
&lt;p&gt;Trump’s plan is “flawed,” The Economist reported.&lt;/p&gt;
&lt;p&gt;For years, the US has established tech dominance by keeping advanced technology away from China. Trump risks rocking that boat by “tearing up America’s export-control policy,” particularly if China’s chip industry simply buys up the H200s as a short-term tactic to learn from the technology and beef up its domestic production of advanced chips, The Economist reported.&lt;/p&gt;
&lt;p&gt;In a sign that’s exactly what many expect could happen, investors in China were apparently so excited by Trump’s announcement that they immediately poured money into Moore Threads, expected to be China’s best answer to Nvidia, the South China Morning Post reported.&lt;/p&gt;
&lt;p&gt;Several experts for the non-partisan think tank the Counsel on Foreign Relations also criticized the policy change, cautioning that the reversal of course threatened to undermine US competition with China.&lt;/p&gt;
&lt;p&gt;Suggesting that Trump was “effectively undoing” export curbs sought during his first term, Zongyuan Zoe Liu warned that China “buys today to learn today, with the intention to build tomorrow.”&lt;/p&gt;
&lt;p&gt;And perhaps more concerning, she suggested, is that Trump’s policy signals weakness. Rather than forcing Chinese dependence on US tech, reversing course showed China that the US will “back down” under pressure, she warned. And they’re getting that message at a time when “Chinese leaders have a lot of reasons to believe they are not only winning the trade war but also making progress towards a higher degree of strategic autonomy.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In a post on X, Rush Doshi—a CFR expert who previously advised Biden on national security issues related to China—suggested that the policy change was “possibly decisive in the AI race.”&lt;/p&gt;
&lt;p&gt;“Compute is our main advantage—China has more power, engineers, and the entire edge layer—so by giving this up, we increase the odds the world runs on Chinese AI,” Doshi wrote.&lt;/p&gt;
&lt;p&gt;Experts fear Trump may not understand the full impact of his decision. In the short-term, Michael C. Horowitz wrote for CFR, “it is indisputable” that allowing H200 exports benefits China’s frontier AI and efforts to scale data centers. And Doshi pointed out that Trump’s shift may trigger more advanced technology flowing into China, as US allies that restricted sales of machines to build AI chips may soon follow his lead and lift their curbs. As China learns to be self-reliant from any influx of advanced tech, Sullivan warned that China’s leaders “intend to get off of American semiconductors as soon as they can.”&lt;/p&gt;
&lt;p&gt;“So, the argument that we can keep them ‘addicted’ holds no water,” Sullivan said. “They want American chips right now for one simple reason: They are behind in the AI race, and this will help them catch up while they build their own chip capabilities.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;China may reject H200, demand Blackwell access&lt;/h2&gt;
&lt;p&gt;It remains unclear if China will approve H200 sales, but some of the country’s biggest firms, including ByteDance, Tencent, and Alibaba, are interested, anonymous insider sources told Reuters.&lt;/p&gt;
&lt;p&gt;In the past, China has instructed companies to avoid Nvidia, warning of possible backdoors giving Nvidia a kill switch to remotely shut down chips. Such backdoors could potentially destabilize Chinese firms’ operations and R&amp;amp;D. Nvidia has denied such backdoors exist, but Chinese firms have supposedly sought reassurances from Nvidia in the aftermath of Trump’s policy change. Likely just as unpopular with the Chinese firms and government, Nvidia confirmed recently that it has built location verification tech that could help the US detect when restricted chips are leaked into China. Should the US ever renew export curbs on H200 chips, adopting them widely could cause chaos in the future.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Without giving China sought-after reassurances, Nvidia may not end up benefiting as much as it hoped from its mission to reclaim lost revenue from the Chinese market. Today, Chinese firms control about 60 percent of China’s AI chip market, where only a few years ago American firms—led by Nvidia—controlled 80 percent, the Economist reported.&lt;/p&gt;
&lt;p&gt;But for China, the temptation to buy up Nvidia chips may be too great to pass up. Another CFR expert, Chris McGuire, estimated that Nvidia could suddenly start exporting as many as 3 million H200s into China next year. “This would at least triple the amount of aggregate AI computing power China could add domestically” in 2026, McGuire wrote, and possibly trigger disastrous outcomes for the US.&lt;/p&gt;
&lt;p&gt;“This could cause DeepSeek and other Chinese AI developers to close the gap with leading US AI labs and enable China to develop an ‘AI Belt and Road’ initiative—a complement to its vast global infrastructure investment network already in place—that competes with US cloud providers around the world,” McGuire forecasted.&lt;/p&gt;
&lt;p&gt;As China mulls the benefits and risks, an emergency meeting was called, where the Chinese government discussed potential concerns of local firms buying chips, according to The Information. Reportedly, Beijing ended that meeting with a promise to issue a decision soon.&lt;/p&gt;
&lt;p&gt;Horowitz suggested that a primary reason that China may reject the H200s could be to squeeze even bigger concessions out of Trump, whose administration recently has been working to maintain a tenuous truce with China.&lt;/p&gt;
&lt;p&gt;“China could come back demanding the Blackwell or something else,” Horowitz suggested.&lt;/p&gt;
&lt;p&gt;In a statement, Nvidia—which plans to release a chip called the Rubin to surpass the Blackwell soon—praised Trump’s policy as striking “a thoughtful balance that is great for America.”&lt;/p&gt;
&lt;h2&gt;China will rip off Nvidia’s chips, Republican warns&lt;/h2&gt;
&lt;p&gt;Both Democratic and Republican lawmakers in Congress criticized Trump’s plan, including senators behind a bipartisan push to limit AI chip sales to China.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Some have questioned how much thought was put into the policy, as the US confusingly continues restricting less advanced AI chips (like the A100 and H100) while green-lighting H200 sales. Trump’s Justice Department also seems to be struggling to keep up. The NYT noted that just “hours before” Trump announced the policy change, the DOJ announced “it had detained two people for selling those chips to the country.”&lt;/p&gt;
&lt;p&gt;The chair of the Select Committee on Competition with China, Rep. John Moolenaar (R-Mich.), warned on X that the news wouldn’t be good for the US or Nvidia. First, the Chinese Communist Party “will use these highly advanced chips to strengthen its military capabilities and totalitarian surveillance,” he suggested. And second, “Nvidia should be under no illusions—China will rip off its technology, mass produce it themselves, and seek to end Nvidia as a competitor.”&lt;/p&gt;
&lt;p&gt;“That is China’s playbook and it is using it in every critical industry,” Moolenaar said.&lt;/p&gt;
&lt;p&gt;House Democrats on committees dealing with foreign affairs and competition with China echoed those concerns, The Hill reported, warning that “under this administration, our national security is for sale.”&lt;/p&gt;
&lt;p&gt;Nvidia’s Huang seems pleased with the outcome, which comes after months of reportedly pressuring the administration to lift export curbs limiting its growth in Chinese markets, the NYT reported. Last week, Trump heaped praise on Huang after one meeting, calling Huang a “smart man” and suggesting the Nvidia chief has “done an amazing job” helping Trump understand the stakes.&lt;/p&gt;
&lt;p&gt;At an October news conference ahead of the deal’s official approval, Huang suggested that government lawyers were researching ways to get around a US law that prohibits charging companies fees for export licenses. Eventually, Trump is expected to release a policy that outlines how the US will collect those fees without conflicting with that law.&lt;/p&gt;
&lt;p&gt;Senate Democrats appear unlikely to embrace such a policy, issuing a joint statement condemning the H200 sales as dooming the US in the AI race and threatening national security.&lt;/p&gt;
&lt;p&gt;“Access to these chips would give China’s military transformational technology to make its weapons more lethal, carry out more effective cyberattacks against American businesses and critical infrastructure and strengthen their economic and manufacturing sector,” Senators wrote.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/12/us-taking-25-cut-of-nvidia-chip-sales-makes-no-sense-experts-say/</guid><pubDate>Wed, 10 Dec 2025 18:30:45 +0000</pubDate></item><item><title>[NEW] 3 Ways NVIDIA Is Powering the Industrial Revolution (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/gpu-cuda-scaling-laws-industrial-revolution/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The NVIDIA accelerated computing platform is leading supercomputing benchmarks once dominated by CPUs, enabling AI, science, business and computing efficiency worldwide.&lt;/p&gt;
&lt;p&gt;Moore’s Law has run its course, and parallel processing is the way forward. With this evolution, NVIDIA GPU platforms are now uniquely positioned to deliver on the three scaling laws — pretraining, post-training and test-time compute — for everything from next-generation recommender systems and large language models (LLMs) to AI agents and beyond.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="accelerated-computing"&gt;&lt;b&gt;The CPU-to-GPU Transition: A&amp;nbsp;Historic Shift in Computing 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;At SC25, NVIDIA founder and CEO Jensen Huang highlighted the shifting landscape. Within the TOP100, a subset of the TOP500 list of supercomputers, over 85% of systems use GPUs. This flip represents a historic transition from the serial‑processing paradigm of CPUs to massively parallel accelerated architectures.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-88183 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Top500transition-1680x945.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;Before 2012, machine learning was based on programmed logic. Statistical models were used and ran efficiently on CPUs as a corpus of hard-coded rules. But this all changed when AlexNet running on gaming GPUs demonstrated image classification could be learned by examples. Its implications were enormous for the future of AI, with parallel processing on increasing sums of data on GPUs driving a new wave of computing.&lt;/p&gt;
&lt;p&gt;This flip isn’t just about hardware. It’s about platforms unlocking new science. GPUs deliver far more operations per watt, making exascale practical without untenable energy demands.&lt;/p&gt;
&lt;p&gt;Recent results from the Green500, a ranking of the world’s most energy-efficient supercomputers, underscore the contrast between GPUs versus CPUs. The top five performers in this industry standard benchmark were all NVIDIA GPUs, delivering an average of 70.1 gigaflops per watt. Meanwhile, the top CPU-only systems provided 15.5 flops per watt on average. This 4.5x differential between GPUs versus CPUs on energy efficiency highlights the massive TCO (total cost of ownership) advantage of moving these systems to GPUs.&lt;/p&gt;
&lt;p&gt;Another measure of the CPU-versus-GPU energy-efficiency and performance differential arrived with NVIDIA’s results on the Graph500. NVIDIA delivered a record-breaking result of 410 trillion traversed edges per second, placing first on the Graph500 breadth-first search list.&lt;/p&gt;
&lt;p&gt;The winning run more than doubled the next highest score and utilized 8,192 NVIDIA H100 GPUs to process a graph with 2.2 trillion vertices and 35 trillion edges. That compares with the next best result on the list, which required roughly 150,000 CPUs for this workload. Hardware footprint reductions of this scale save time, money and energy.&lt;/p&gt;
&lt;p&gt;Yet NVIDIA showcased at SC25 that its AI supercomputing platform is far more than GPUs.&amp;nbsp; Networking, CUDA libraries, memory, storage and orchestration are co-designed to deliver a full-stack platform.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-88129" height="538" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Screenshot_3-12-2025_16943_-960x538.jpeg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;Enabled by CUDA, NVIDIA is a full-stack platform. Open-source libraries and frameworks such as those in the CUDA-X ecosystem are where big speedups occur. Snowflake recently announced&amp;nbsp; an integration of NVIDIA A10 GPUs to supercharge data science workflows. Snowflake ML now comes preinstalled with NVIDIA cuML and cuDF libraries to accelerate popular ML algorithms with these GPUs.&lt;/p&gt;
&lt;p&gt;With this native integration, Snowflake’s users can easily accelerate model development cycles with no code changes required. NVIDIA’s benchmark runs show 5x less time required for Random Forest and up to 200x for HDBSCAN on NVIDIA A10 GPUs compared with CPUs.&lt;/p&gt;
&lt;p&gt;The flip was the turning point. The scaling laws are the trajectory forward. And at every stage, GPUs are the engine driving AI into its next chapter.&lt;/p&gt;
&lt;p&gt;But CUDA-X and many open-source software libraries and frameworks are where much of the magic happens. CUDA-X libraries accelerate workloads across every industry and application — engineering, finance, data analytics, genomics, biology, chemistry, telecommunications, robotics and much more.&lt;/p&gt;
&lt;p&gt;“The world has a massive investment in non-AI software. From data processing to science and engineering simulations, representing hundreds of billions of dollars in compute cloud computing spend each year,” Huang said on NVIDIA’s recent earning call.&lt;/p&gt;
&lt;p&gt;Many applications that once ran exclusively on CPUs are now rapidly shifting to CUDA GPUs. “Accelerated computing has reached a tipping point. AI has also reached a tipping point and is transforming existing applications while enabling entirely new ones,” he said.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-88133" height="509" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Screenshot_3-12-2025_16412_-960x509.jpeg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;What began as an energy‑efficiency imperative has matured into a scientific platform: simulation and AI fused at scale. The leadership of NVIDIA GPUs in the TOP100 is both proof of this trajectory and a signal of what comes next — breakthroughs across every discipline.&lt;/p&gt;
&lt;p&gt;As a result, researchers can now train trillion‑parameter models, simulate fusion reactors and accelerate drug discovery at scales CPUs alone could never reach.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="scaling-laws"&gt;&lt;b&gt;The Three Scaling Laws Driving AI’s Next Frontier 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The change from CPUs to GPUs is not just a milestone in supercomputing. It’s the foundation for the three scaling laws that represent the roadmap for AI’s next workflow: pretraining, post‑training and test‑time scaling.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-88137" height="541" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Screenshot_10-12-2025_85235_-960x541.jpeg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;Pre‑training scaling was the first law to assist the industry. Researchers discovered that as datasets, parameter counts and compute grew, model performance improved predictably. Doubling the data or parameters meant leaps in accuracy and versatility.&lt;/p&gt;
&lt;p&gt;On the latest MLPerf Training industry benchmarks, the NVIDIA platform delivered the highest performance on every test and was the only platform to submit on all tests. Without GPUs, the “bigger is better” era of AI research would have stalled under the weight of power budgets and time constraints.&lt;/p&gt;
&lt;p&gt;Post‑training scaling extends the story. Once a foundation model is built, it must be refined — tuned for industries, languages or safety constraints. Techniques like reinforcement learning from human feedback, pruning and distillation require enormous additional compute. In some cases, the demands rival pre‑training itself. This is like a student improving after basic education. GPUs again provide the horsepower, enabling continual fine‑tuning and adaptation across domains.&lt;/p&gt;
&lt;p&gt;Test‑time scaling, the newest law, may prove the most transformative. Modern models powered by mixture-of-experts architectures can reason, plan and evaluate multiple solutions in real time. Chain‑of‑thought reasoning, generative search and agentic AI demand dynamic, recursive compute — often exceeding pretraining requirements. This stage will drive exponential demand for inference infrastructure — from data centers to edge devices.&lt;/p&gt;
&lt;p&gt;Together, these three laws explain the demand for GPUs for new AI workloads. Pretraining scaling has made GPUs indispensable. Post‑training scaling has reinforced their role in refinement. Test‑time scaling is ensuring GPUs remain critical long after training ends. This is the next chapter in accelerated computing: a lifecycle where GPUs power every stage of AI — from learning to reasoning to deployment.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="generative-agentic-physical-ai"&gt;&lt;b&gt;Generative, Agentic, Physical AI and Beyond 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The world of AI is expanding far beyond basic recommenders, chatbots and text generation. VLMs, or vision language models, are AI systems combining computer vision and natural language processing for understanding and interpreting images and text. And recommender systems — the engines behind personalized shopping, streaming and social feeds — are but one of many examples of how the massive transition from CPUs to GPUs is reshaping AI.&lt;/p&gt;
&lt;p&gt;Meanwhile, generative AI is transforming everything from robotics and autonomous vehicles to software-as-a-service companies and represents a massive investment in startups.&lt;/p&gt;
&lt;p&gt;NVIDIA platforms are the only to run on all of the leading generative AI models and handle 1.4 million open-source models.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-88141" height="542" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Screenshot_3-12-2025_153947_-960x542.jpeg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;Once constrained by CPU architectures, recommender systems struggled to capture the complexity of user behavior at scale. With CUDA GPUs, pretraining scaling enables models to learn from massive datasets of clicks, purchases and preferences, uncovering richer patterns. Post‑training scaling fine‑tunes those models for specific domains, sharpening personalization for industries from retail to entertainment. On leading global online sites, even a 1% gain in relevance accuracy of recommendations can yield billions more in sales.&lt;/p&gt;
&lt;p&gt;Electronic commerce sales are expected to reach $6.4 trillion worldwide for 2025, according to Emarketer.&lt;/p&gt;
&lt;p&gt;The world’s hyperscalers, a trillion-dollar industry, are transforming search, recommendations and content understanding from classical machine learning to generative AI. NVIDIA CUDA excels at both and is the ideal platform for this transition driving infrastructure investment measured in hundreds of billions of dollars.&lt;/p&gt;
&lt;p&gt;Now, test‑time scaling is transforming inference itself: recommender engines can reason dynamically, evaluating multiple options in real time to deliver context‑aware suggestions. The result is a leap in precision and relevance — recommendations that feel less like static lists and more like intelligent guidance. GPUs and scaling laws are turning recommendation from a background feature into a frontline capability of agentic AI, enabling billions of people to sort through trillions of things on the internet with an ease that would otherwise be unfeasible.&lt;/p&gt;
&lt;p&gt;What began as conversational interfaces powered by LLMs is now evolving into intelligent, autonomous systems poised to reshape nearly every sector of the global economy.&lt;/p&gt;
&lt;p&gt;We are experiencing a foundational shift — from AI as a virtual technology to AI entering the physical world. This transformation demands nothing less than explosive growth in computing infrastructure and new forms of collaboration between humans and machines.&lt;/p&gt;
&lt;p&gt;Generative AI has proven capable of not just creating new text and images, but code, designs and even scientific hypotheses. Now, agentic AI is arriving — systems that perceive, reason, plan and act autonomously. These agents behave less like tools and more like digital colleagues, carrying out complex, multistep tasks across industries. From legal research to logistics, agentic AI promises to accelerate productivity by serving as autonomous digital workers.&lt;/p&gt;
&lt;p&gt;Perhaps the most transformative leap is physical AI — the embodiment of intelligence in robots of every form. Three computers are required to build physical AI-embodied robots — NVIDIA DGX GB300 to train the reasoning vision-language action model, NVIDIA RTX PRO to simulate, test and validate the model in a virtual world built on Omniverse, and Jetson Thor to run the reasoning VLA at real-time speed.&lt;/p&gt;
&lt;p&gt;What’s expected next is a breakthrough moment for robotics within years, with autonomous mobile robots, collaborative robots and humanoids disrupting manufacturing, logistics and healthcare. Morgan Stanley estimates there will be 1 billion humanoid robots with $5 trillion in revenue by 2050.&lt;/p&gt;
&lt;p&gt;Signaling how deeply AI will embed into the physical economy, that’s just a sip of what’s on tap.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_88145"&gt;&lt;img alt="alt" class="size-medium wp-image-88145" height="639" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/25GTC-DC-Keynote-DEB14090-960x639.jpg" width="960" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88145"&gt;NVIDIA CEO Jensen Huang stands on stage with a lineup of nine advanced humanoid robots during his keynote address at the GTC DC 2025 conference. The robots, including models from Boston Dynamics, Figure, Agility Robotics, and Disney Research, were brought together to showcase NVIDIA’s new Project GR00T, a general-purpose foundation model aimed at advancing the capabilities of humanoid robots and artificial intelligence.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;AI is no longer just a tool. It performs work and stands to transform every one of the world’s $100 trillion in markets. And a virtuous cycle of AI has arrived, fundamentally changing the entire computing stack, transitioning all computers into new supercomputing platforms for vastly larger opportunities.​&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The NVIDIA accelerated computing platform is leading supercomputing benchmarks once dominated by CPUs, enabling AI, science, business and computing efficiency worldwide.&lt;/p&gt;
&lt;p&gt;Moore’s Law has run its course, and parallel processing is the way forward. With this evolution, NVIDIA GPU platforms are now uniquely positioned to deliver on the three scaling laws — pretraining, post-training and test-time compute — for everything from next-generation recommender systems and large language models (LLMs) to AI agents and beyond.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="accelerated-computing"&gt;&lt;b&gt;The CPU-to-GPU Transition: A&amp;nbsp;Historic Shift in Computing 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;At SC25, NVIDIA founder and CEO Jensen Huang highlighted the shifting landscape. Within the TOP100, a subset of the TOP500 list of supercomputers, over 85% of systems use GPUs. This flip represents a historic transition from the serial‑processing paradigm of CPUs to massively parallel accelerated architectures.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-88183 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Top500transition-1680x945.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;Before 2012, machine learning was based on programmed logic. Statistical models were used and ran efficiently on CPUs as a corpus of hard-coded rules. But this all changed when AlexNet running on gaming GPUs demonstrated image classification could be learned by examples. Its implications were enormous for the future of AI, with parallel processing on increasing sums of data on GPUs driving a new wave of computing.&lt;/p&gt;
&lt;p&gt;This flip isn’t just about hardware. It’s about platforms unlocking new science. GPUs deliver far more operations per watt, making exascale practical without untenable energy demands.&lt;/p&gt;
&lt;p&gt;Recent results from the Green500, a ranking of the world’s most energy-efficient supercomputers, underscore the contrast between GPUs versus CPUs. The top five performers in this industry standard benchmark were all NVIDIA GPUs, delivering an average of 70.1 gigaflops per watt. Meanwhile, the top CPU-only systems provided 15.5 flops per watt on average. This 4.5x differential between GPUs versus CPUs on energy efficiency highlights the massive TCO (total cost of ownership) advantage of moving these systems to GPUs.&lt;/p&gt;
&lt;p&gt;Another measure of the CPU-versus-GPU energy-efficiency and performance differential arrived with NVIDIA’s results on the Graph500. NVIDIA delivered a record-breaking result of 410 trillion traversed edges per second, placing first on the Graph500 breadth-first search list.&lt;/p&gt;
&lt;p&gt;The winning run more than doubled the next highest score and utilized 8,192 NVIDIA H100 GPUs to process a graph with 2.2 trillion vertices and 35 trillion edges. That compares with the next best result on the list, which required roughly 150,000 CPUs for this workload. Hardware footprint reductions of this scale save time, money and energy.&lt;/p&gt;
&lt;p&gt;Yet NVIDIA showcased at SC25 that its AI supercomputing platform is far more than GPUs.&amp;nbsp; Networking, CUDA libraries, memory, storage and orchestration are co-designed to deliver a full-stack platform.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-88129" height="538" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Screenshot_3-12-2025_16943_-960x538.jpeg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;Enabled by CUDA, NVIDIA is a full-stack platform. Open-source libraries and frameworks such as those in the CUDA-X ecosystem are where big speedups occur. Snowflake recently announced&amp;nbsp; an integration of NVIDIA A10 GPUs to supercharge data science workflows. Snowflake ML now comes preinstalled with NVIDIA cuML and cuDF libraries to accelerate popular ML algorithms with these GPUs.&lt;/p&gt;
&lt;p&gt;With this native integration, Snowflake’s users can easily accelerate model development cycles with no code changes required. NVIDIA’s benchmark runs show 5x less time required for Random Forest and up to 200x for HDBSCAN on NVIDIA A10 GPUs compared with CPUs.&lt;/p&gt;
&lt;p&gt;The flip was the turning point. The scaling laws are the trajectory forward. And at every stage, GPUs are the engine driving AI into its next chapter.&lt;/p&gt;
&lt;p&gt;But CUDA-X and many open-source software libraries and frameworks are where much of the magic happens. CUDA-X libraries accelerate workloads across every industry and application — engineering, finance, data analytics, genomics, biology, chemistry, telecommunications, robotics and much more.&lt;/p&gt;
&lt;p&gt;“The world has a massive investment in non-AI software. From data processing to science and engineering simulations, representing hundreds of billions of dollars in compute cloud computing spend each year,” Huang said on NVIDIA’s recent earning call.&lt;/p&gt;
&lt;p&gt;Many applications that once ran exclusively on CPUs are now rapidly shifting to CUDA GPUs. “Accelerated computing has reached a tipping point. AI has also reached a tipping point and is transforming existing applications while enabling entirely new ones,” he said.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-88133" height="509" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Screenshot_3-12-2025_16412_-960x509.jpeg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;What began as an energy‑efficiency imperative has matured into a scientific platform: simulation and AI fused at scale. The leadership of NVIDIA GPUs in the TOP100 is both proof of this trajectory and a signal of what comes next — breakthroughs across every discipline.&lt;/p&gt;
&lt;p&gt;As a result, researchers can now train trillion‑parameter models, simulate fusion reactors and accelerate drug discovery at scales CPUs alone could never reach.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="scaling-laws"&gt;&lt;b&gt;The Three Scaling Laws Driving AI’s Next Frontier 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The change from CPUs to GPUs is not just a milestone in supercomputing. It’s the foundation for the three scaling laws that represent the roadmap for AI’s next workflow: pretraining, post‑training and test‑time scaling.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-88137" height="541" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Screenshot_10-12-2025_85235_-960x541.jpeg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;Pre‑training scaling was the first law to assist the industry. Researchers discovered that as datasets, parameter counts and compute grew, model performance improved predictably. Doubling the data or parameters meant leaps in accuracy and versatility.&lt;/p&gt;
&lt;p&gt;On the latest MLPerf Training industry benchmarks, the NVIDIA platform delivered the highest performance on every test and was the only platform to submit on all tests. Without GPUs, the “bigger is better” era of AI research would have stalled under the weight of power budgets and time constraints.&lt;/p&gt;
&lt;p&gt;Post‑training scaling extends the story. Once a foundation model is built, it must be refined — tuned for industries, languages or safety constraints. Techniques like reinforcement learning from human feedback, pruning and distillation require enormous additional compute. In some cases, the demands rival pre‑training itself. This is like a student improving after basic education. GPUs again provide the horsepower, enabling continual fine‑tuning and adaptation across domains.&lt;/p&gt;
&lt;p&gt;Test‑time scaling, the newest law, may prove the most transformative. Modern models powered by mixture-of-experts architectures can reason, plan and evaluate multiple solutions in real time. Chain‑of‑thought reasoning, generative search and agentic AI demand dynamic, recursive compute — often exceeding pretraining requirements. This stage will drive exponential demand for inference infrastructure — from data centers to edge devices.&lt;/p&gt;
&lt;p&gt;Together, these three laws explain the demand for GPUs for new AI workloads. Pretraining scaling has made GPUs indispensable. Post‑training scaling has reinforced their role in refinement. Test‑time scaling is ensuring GPUs remain critical long after training ends. This is the next chapter in accelerated computing: a lifecycle where GPUs power every stage of AI — from learning to reasoning to deployment.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="generative-agentic-physical-ai"&gt;&lt;b&gt;Generative, Agentic, Physical AI and Beyond 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The world of AI is expanding far beyond basic recommenders, chatbots and text generation. VLMs, or vision language models, are AI systems combining computer vision and natural language processing for understanding and interpreting images and text. And recommender systems — the engines behind personalized shopping, streaming and social feeds — are but one of many examples of how the massive transition from CPUs to GPUs is reshaping AI.&lt;/p&gt;
&lt;p&gt;Meanwhile, generative AI is transforming everything from robotics and autonomous vehicles to software-as-a-service companies and represents a massive investment in startups.&lt;/p&gt;
&lt;p&gt;NVIDIA platforms are the only to run on all of the leading generative AI models and handle 1.4 million open-source models.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-88141" height="542" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Screenshot_3-12-2025_153947_-960x542.jpeg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;Once constrained by CPU architectures, recommender systems struggled to capture the complexity of user behavior at scale. With CUDA GPUs, pretraining scaling enables models to learn from massive datasets of clicks, purchases and preferences, uncovering richer patterns. Post‑training scaling fine‑tunes those models for specific domains, sharpening personalization for industries from retail to entertainment. On leading global online sites, even a 1% gain in relevance accuracy of recommendations can yield billions more in sales.&lt;/p&gt;
&lt;p&gt;Electronic commerce sales are expected to reach $6.4 trillion worldwide for 2025, according to Emarketer.&lt;/p&gt;
&lt;p&gt;The world’s hyperscalers, a trillion-dollar industry, are transforming search, recommendations and content understanding from classical machine learning to generative AI. NVIDIA CUDA excels at both and is the ideal platform for this transition driving infrastructure investment measured in hundreds of billions of dollars.&lt;/p&gt;
&lt;p&gt;Now, test‑time scaling is transforming inference itself: recommender engines can reason dynamically, evaluating multiple options in real time to deliver context‑aware suggestions. The result is a leap in precision and relevance — recommendations that feel less like static lists and more like intelligent guidance. GPUs and scaling laws are turning recommendation from a background feature into a frontline capability of agentic AI, enabling billions of people to sort through trillions of things on the internet with an ease that would otherwise be unfeasible.&lt;/p&gt;
&lt;p&gt;What began as conversational interfaces powered by LLMs is now evolving into intelligent, autonomous systems poised to reshape nearly every sector of the global economy.&lt;/p&gt;
&lt;p&gt;We are experiencing a foundational shift — from AI as a virtual technology to AI entering the physical world. This transformation demands nothing less than explosive growth in computing infrastructure and new forms of collaboration between humans and machines.&lt;/p&gt;
&lt;p&gt;Generative AI has proven capable of not just creating new text and images, but code, designs and even scientific hypotheses. Now, agentic AI is arriving — systems that perceive, reason, plan and act autonomously. These agents behave less like tools and more like digital colleagues, carrying out complex, multistep tasks across industries. From legal research to logistics, agentic AI promises to accelerate productivity by serving as autonomous digital workers.&lt;/p&gt;
&lt;p&gt;Perhaps the most transformative leap is physical AI — the embodiment of intelligence in robots of every form. Three computers are required to build physical AI-embodied robots — NVIDIA DGX GB300 to train the reasoning vision-language action model, NVIDIA RTX PRO to simulate, test and validate the model in a virtual world built on Omniverse, and Jetson Thor to run the reasoning VLA at real-time speed.&lt;/p&gt;
&lt;p&gt;What’s expected next is a breakthrough moment for robotics within years, with autonomous mobile robots, collaborative robots and humanoids disrupting manufacturing, logistics and healthcare. Morgan Stanley estimates there will be 1 billion humanoid robots with $5 trillion in revenue by 2050.&lt;/p&gt;
&lt;p&gt;Signaling how deeply AI will embed into the physical economy, that’s just a sip of what’s on tap.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_88145"&gt;&lt;img alt="alt" class="size-medium wp-image-88145" height="639" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/25GTC-DC-Keynote-DEB14090-960x639.jpg" width="960" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88145"&gt;NVIDIA CEO Jensen Huang stands on stage with a lineup of nine advanced humanoid robots during his keynote address at the GTC DC 2025 conference. The robots, including models from Boston Dynamics, Figure, Agility Robotics, and Disney Research, were brought together to showcase NVIDIA’s new Project GR00T, a general-purpose foundation model aimed at advancing the capabilities of humanoid robots and artificial intelligence.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;AI is no longer just a tool. It performs work and stands to transform every one of the world’s $100 trillion in markets. And a virtuous cycle of AI has arrived, fundamentally changing the entire computing stack, transitioning all computers into new supercomputing platforms for vastly larger opportunities.​&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/gpu-cuda-scaling-laws-industrial-revolution/</guid><pubDate>Wed, 10 Dec 2025 18:35:05 +0000</pubDate></item><item><title>[NEW] Spotify tests more personalized, AI-powered ‘Prompted Playlists’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/10/spotify-tests-more-personalized-ai-powered-prompted-playlists/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Spotify announced on Wednesday that, for the first time, it’s giving users more control over the streaming service’s algorithm. That’s at least how the company is framing the launch of its new “Promoted Playlists,” a feature that will initially be available to Premium subscribers in New Zealand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature, which is currently available in English only, is still in beta and will evolve before rolling out to other markets, according to Spotify.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new tool allows users to describe what they want to hear in a personalized playlist that reflects the “full arc” of their tastes, according to the company. That means the playlist focuses not only on the songs you like now, but your entire Spotify listening history from day one — something that differentiates the feature from other playlists, the company says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is an evolution from Spotify’s existing AI playlist option, which debuted last year, and also works through written prompts. As with AI playlists, the new Prompted Playlists allow users to request what they want to hear with written instructions. However, they can now write much longer prompts with more specific instructions. That’s because the new AI feature factors in world knowledge, a rep from Spotify explained to TechCrunch. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, the ability to go further back in your listening history and schedule how often the playlist refreshes makes it different from Spotify’s other AI playlist offerings. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, Spotify suggests subscribers can use the new feature to ask for something like, “music from my top artists from the last five years,” then amend the prompt to include a request for “deep cuts I haven’t heard yet.”&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3074890" height="382" src="https://techcrunch.com/wp-content/uploads/2025/12/Prompted-Playlist-GIF-102924.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In another example of a longer prompt, Spotify said you could ask for “high-energy pop and hip-hop for a 30-minute 5K run that keeps a steady pace before easing into relaxing songs for a cool-down” or “music from this year’s biggest films and most-talked-about TV shows that match my taste.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, you can continue to fine-tune the prompt to make it even more specific, and can set how often you want it to refresh, like daily or weekly. The idea is that users can essentially make their own version of something like Spotify’s flagship playlist, Discover Weekly, but one that’s focused on a type of music, genre, or time period they’d like to track, or their own version of something like Spotify’s genre-focused Daily Mixes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says the playlist will include descriptions and context so you know why you’re getting the recommendation. Plus, it will offer a set of prompts to help users get started.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Spotify isn’t the only social app pitching how it’s letting users take control of its algorithm. Instagram today also introduced a new feature that lets users control what type of reels they see. Bluesky, a decentralized X competitor, also lets users swap out its algorithm for one of their own. &lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Spotify announced on Wednesday that, for the first time, it’s giving users more control over the streaming service’s algorithm. That’s at least how the company is framing the launch of its new “Promoted Playlists,” a feature that will initially be available to Premium subscribers in New Zealand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature, which is currently available in English only, is still in beta and will evolve before rolling out to other markets, according to Spotify.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new tool allows users to describe what they want to hear in a personalized playlist that reflects the “full arc” of their tastes, according to the company. That means the playlist focuses not only on the songs you like now, but your entire Spotify listening history from day one — something that differentiates the feature from other playlists, the company says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is an evolution from Spotify’s existing AI playlist option, which debuted last year, and also works through written prompts. As with AI playlists, the new Prompted Playlists allow users to request what they want to hear with written instructions. However, they can now write much longer prompts with more specific instructions. That’s because the new AI feature factors in world knowledge, a rep from Spotify explained to TechCrunch. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, the ability to go further back in your listening history and schedule how often the playlist refreshes makes it different from Spotify’s other AI playlist offerings. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, Spotify suggests subscribers can use the new feature to ask for something like, “music from my top artists from the last five years,” then amend the prompt to include a request for “deep cuts I haven’t heard yet.”&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3074890" height="382" src="https://techcrunch.com/wp-content/uploads/2025/12/Prompted-Playlist-GIF-102924.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In another example of a longer prompt, Spotify said you could ask for “high-energy pop and hip-hop for a 30-minute 5K run that keeps a steady pace before easing into relaxing songs for a cool-down” or “music from this year’s biggest films and most-talked-about TV shows that match my taste.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, you can continue to fine-tune the prompt to make it even more specific, and can set how often you want it to refresh, like daily or weekly. The idea is that users can essentially make their own version of something like Spotify’s flagship playlist, Discover Weekly, but one that’s focused on a type of music, genre, or time period they’d like to track, or their own version of something like Spotify’s genre-focused Daily Mixes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says the playlist will include descriptions and context so you know why you’re getting the recommendation. Plus, it will offer a set of prompts to help users get started.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Spotify isn’t the only social app pitching how it’s letting users take control of its algorithm. Instagram today also introduced a new feature that lets users control what type of reels they see. Bluesky, a decentralized X competitor, also lets users swap out its algorithm for one of their own. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/10/spotify-tests-more-personalized-ai-powered-prompted-playlists/</guid><pubDate>Wed, 10 Dec 2025 20:01:47 +0000</pubDate></item><item><title>[NEW] A new open-weights AI coding model is closing in on proprietary options (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/12/mistral-bets-big-on-vibe-coding-with-new-autonomous-software-engineering-agent/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Devstral 2 model scores 72% on industry benchmark, nearing proprietary rivals.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The Mistral logo on a red and yellow background." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/mistral_header_2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="The Mistral logo on a red and yellow background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/mistral_header_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Mistral logo.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Mistral / Benj Edwards

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, French AI startup Mistral AI released Devstral 2, a 123 billion parameter open-weights coding model designed to work as part of an autonomous software engineering agent. The model achieves a 72.2 percent score on SWE-bench Verified, a benchmark that attempts to test whether AI systems can solve real GitHub issues, putting it among the top-performing open-weights models.&lt;/p&gt;
&lt;p&gt;Perhaps more notably, Mistral didn’t just release an AI model, it released a new development app called Mistral Vibe. It’s a command line interface (CLI) similar to Claude Code, OpenAI Codex, and Gemini CLI that lets developers interact with the Devstral models directly in their terminal. The tool can scan file structures and Git status to maintain context across an entire project, make changes across multiple files, and execute shell commands autonomously. Mistral released the CLI under the Apache 2.0 license.&lt;/p&gt;
&lt;p&gt;It’s always wise to take AI benchmarks with a large grain of salt, but we’ve heard from employees of the big AI companies that they pay very close attention to how well models do on SWE-bench Verified, which presents AI models with 500 real software engineering problems pulled from GitHub issues in popular Python repositories. The AI must read the issue description, navigate the codebase, and generate a working patch that passes unit tests. While some AI researchers have noted that around 90 percent of the tasks in the benchmark test relatively simple bug fixes that experienced engineers could complete in under an hour, it’s one of the few standardized ways to compare coding models.&lt;/p&gt;
&lt;p&gt;At the same time as the larger AI coding model, Mistral also released Devstral Small 2, a 24 billion parameter version that scores 68 percent on the same benchmark and can run locally on consumer hardware like a laptop with no Internet connection required. Both models support a 256,000 token context window, allowing them to process moderately large codebases (although whether you consider it large or small is very relative depending on overall project complexity). The company released Devstral 2 under a modified MIT license and Devstral Small 2 under the more permissive Apache 2.0 license.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Devstral 2 is currently free to use through Mistral’s API. After the free period ends, pricing will be $0.40 per million input tokens and $2.00 per million output tokens. Devstral Small 2 will cost $0.10 per million input tokens and $0.30 per million output tokens. Mistral says it’s about “7x more cost-efficient than Claude Sonnet at real-world tasks.” Anthropic’s Sonnet 4.5 through the API costs $3 per million input tokens and $15 per million output tokens, with increases depending on the total number of tokens used.&lt;/p&gt;
&lt;h2&gt;The vibe-coding connection&lt;/h2&gt;
&lt;p&gt;The name “Mistral Vibe” references “vibe coding,” a term that AI researcher Andrej Karpathy coined in February 2025 to describe a style of programming where developers describe what they want in natural language and accept AI-generated code without reviewing it closely. As Karpathy describes it, you can “fully giv[e] in to the vibes, embrace exponentials, and forget that the code even exists.” Collins Dictionary named it Word of the Year for 2025.&lt;/p&gt;
&lt;p&gt;The vibe coding approach has drawn both enthusiasm and concern. In an interview with Ars Technica in March, developer Simon Willison said, “I really enjoy vibe coding. It’s a fun way to try out an idea and prove if it can work.” But he also warned that “vibe coding your way to a production codebase is clearly risky. Most of the work we do as software engineers involves evolving existing systems, where the quality and understandability of the underlying code is crucial.”&lt;/p&gt;
&lt;p&gt;Mistral is betting that Devstral 2 will be able to maintain coherency across entire projects, detect failures, and retry with corrections, and that those claimed abilities will make it suitable for more serious work than simple prototypes and in-house tools. The company says the model can track framework dependencies and handle tasks like bug fixing and modernizing legacy systems at repository scale. We have not experimented with it yet, but you might see an Ars Technica head-to-head test of several AI coding tools soon.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Devstral 2 model scores 72% on industry benchmark, nearing proprietary rivals.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The Mistral logo on a red and yellow background." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/mistral_header_2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="The Mistral logo on a red and yellow background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/mistral_header_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Mistral logo.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Mistral / Benj Edwards

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, French AI startup Mistral AI released Devstral 2, a 123 billion parameter open-weights coding model designed to work as part of an autonomous software engineering agent. The model achieves a 72.2 percent score on SWE-bench Verified, a benchmark that attempts to test whether AI systems can solve real GitHub issues, putting it among the top-performing open-weights models.&lt;/p&gt;
&lt;p&gt;Perhaps more notably, Mistral didn’t just release an AI model, it released a new development app called Mistral Vibe. It’s a command line interface (CLI) similar to Claude Code, OpenAI Codex, and Gemini CLI that lets developers interact with the Devstral models directly in their terminal. The tool can scan file structures and Git status to maintain context across an entire project, make changes across multiple files, and execute shell commands autonomously. Mistral released the CLI under the Apache 2.0 license.&lt;/p&gt;
&lt;p&gt;It’s always wise to take AI benchmarks with a large grain of salt, but we’ve heard from employees of the big AI companies that they pay very close attention to how well models do on SWE-bench Verified, which presents AI models with 500 real software engineering problems pulled from GitHub issues in popular Python repositories. The AI must read the issue description, navigate the codebase, and generate a working patch that passes unit tests. While some AI researchers have noted that around 90 percent of the tasks in the benchmark test relatively simple bug fixes that experienced engineers could complete in under an hour, it’s one of the few standardized ways to compare coding models.&lt;/p&gt;
&lt;p&gt;At the same time as the larger AI coding model, Mistral also released Devstral Small 2, a 24 billion parameter version that scores 68 percent on the same benchmark and can run locally on consumer hardware like a laptop with no Internet connection required. Both models support a 256,000 token context window, allowing them to process moderately large codebases (although whether you consider it large or small is very relative depending on overall project complexity). The company released Devstral 2 under a modified MIT license and Devstral Small 2 under the more permissive Apache 2.0 license.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Devstral 2 is currently free to use through Mistral’s API. After the free period ends, pricing will be $0.40 per million input tokens and $2.00 per million output tokens. Devstral Small 2 will cost $0.10 per million input tokens and $0.30 per million output tokens. Mistral says it’s about “7x more cost-efficient than Claude Sonnet at real-world tasks.” Anthropic’s Sonnet 4.5 through the API costs $3 per million input tokens and $15 per million output tokens, with increases depending on the total number of tokens used.&lt;/p&gt;
&lt;h2&gt;The vibe-coding connection&lt;/h2&gt;
&lt;p&gt;The name “Mistral Vibe” references “vibe coding,” a term that AI researcher Andrej Karpathy coined in February 2025 to describe a style of programming where developers describe what they want in natural language and accept AI-generated code without reviewing it closely. As Karpathy describes it, you can “fully giv[e] in to the vibes, embrace exponentials, and forget that the code even exists.” Collins Dictionary named it Word of the Year for 2025.&lt;/p&gt;
&lt;p&gt;The vibe coding approach has drawn both enthusiasm and concern. In an interview with Ars Technica in March, developer Simon Willison said, “I really enjoy vibe coding. It’s a fun way to try out an idea and prove if it can work.” But he also warned that “vibe coding your way to a production codebase is clearly risky. Most of the work we do as software engineers involves evolving existing systems, where the quality and understandability of the underlying code is crucial.”&lt;/p&gt;
&lt;p&gt;Mistral is betting that Devstral 2 will be able to maintain coherency across entire projects, detect failures, and retry with corrections, and that those claimed abilities will make it suitable for more serious work than simple prototypes and in-house tools. The company says the model can track framework dependencies and handle tasks like bug fixing and modernizing legacy systems at repository scale. We have not experimented with it yet, but you might see an Ars Technica head-to-head test of several AI coding tools soon.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/12/mistral-bets-big-on-vibe-coding-with-new-autonomous-software-engineering-agent/</guid><pubDate>Wed, 10 Dec 2025 20:38:58 +0000</pubDate></item><item><title>[NEW] How NVIDIA H100 GPUs on CoreWeave’s AI Cloud Platform Delivered a Record-Breaking Graph500 Run (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/h100-coreweave-graph500/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The world’s top-performing system for graph processing at scale was built on a commercially available cluster.&lt;/p&gt;
&lt;p&gt;NVIDIA last month announced a record-breaking benchmark result of 410 trillion traversed edges per second (TEPS), ranking No. 1 on the 31st Graph500 breadth-first search (BFS) list.&lt;/p&gt;
&lt;p&gt;Performed on an accelerated computing cluster hosted in a CoreWeave data center in Dallas, the winning run used 8,192 NVIDIA H100 GPUs to process a graph with 2.2 trillion vertices and 35 trillion edges. This result is more than double the performance of comparable solutions on the list, including those hosted in national labs.&lt;/p&gt;
&lt;p&gt;To put this performance in perspective, say every person on Earth has 150 friends. This would represent 1.2 trillion edges in a graph of social relationships. The level of performance recently achieved by NVIDIA and CoreWeave enables searching through every friend relationship on Earth in just about three milliseconds.&lt;/p&gt;
&lt;p&gt;Speed at that scale is half the story — the real breakthrough is efficiency. A comparable entry in the top 10 runs of the Graph500 list used about 9,000 nodes, while the winning run from NVIDIA used just over 1,000 nodes, delivering 3x better performance per dollar.&lt;/p&gt;
&lt;p&gt;NVIDIA tapped into the combined power of its full-stack compute, networking and software technologies — including the NVIDIA CUDA platform, Spectrum-X networking, H100 GPUs and a new active messaging library — to push the boundaries of performance while minimizing hardware footprint.&lt;/p&gt;
&lt;p&gt;By saving significant time and costs at this scale in a commercially available system, the win demonstrates how the NVIDIA computing platform is ready to democratize access to acceleration of the world’s largest sparse, irregular workloads — involving data and work items that come in varying and unpredictable sizes — in addition to dense workloads like AI training.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Graphs at Scale Work&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Graphs are the underlying information structure for modern technology. People interact with them on social networks and banking apps, among other use cases, every day. Graphs capture relationships between pieces of information in massive webs of information.&lt;/p&gt;
&lt;p&gt;For example, consider LinkedIn. A user’s profile is a vertex. Connections or relationships to other users are edges — with other users represented as vertices. Some users have five connections, others have 50,000. This creates variable density across the graph, making it sparse and irregular. Unlike an image or language model, which is structured and dense, a graph is unpredictable.&lt;/p&gt;
&lt;p&gt;Graph500 BFS has a long history as the industry-standard benchmark because it measures a system’s ability to navigate this irregularity at scale.&lt;/p&gt;
&lt;p&gt;BFS measures the speed of traversing the graph through every vertex and edge. A high TEPS score for BFS — measuring how fast the system can process these edges — proves the system has superior interconnects, such as cables or switches between compute nodes, as well as more memory bandwidth and software able to take advantage of the system’s capabilities. It validates the engineering of the entire system, not just the speed of the CPU or GPU.&lt;/p&gt;
&lt;p&gt;Effectively, it’s a measure of how fast a system can “think” and associate disparate pieces of information.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Current Techniques for Processing Graphs&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;GPUs are known for accelerating dense workloads like AI training. Until recently, the largest sparse linear algebra and graph workloads have remained the domain of traditional CPU architectures.&lt;/p&gt;
&lt;p&gt;To process graphs, CPUs move graph data across compute nodes. As the graph scales to trillions of edges, this constant movement creates bottlenecks and jams communications.&lt;/p&gt;
&lt;p&gt;Developers use a variety of software techniques to circumvent this issue. A common approach is to process the graph where it is with active messages, where developers send messages that can process graph data in place. The messages are smaller and can be grouped together to maximize network efficiency.&lt;/p&gt;
&lt;p&gt;While this software technique significantly accelerates processing, active messaging was designed to run on CPUs and is inherently limited by the throughput rate and compute capabilities of CPU systems.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Reengineering Graph Processing for the GPU&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To speed up the BFS run, NVIDIA engineered a full-stack, GPU-only solution that reimagines how data moves across the network.&lt;/p&gt;
&lt;p&gt;A custom software framework developed using InfiniBand GPUDirect Async (IBGDA) and the NVSHMEM parallel programming interface enables GPU-to-GPU active messages.&lt;/p&gt;
&lt;p&gt;With IBGDA, the GPU can directly communicate with the InfiniBand network interface card. Message aggregation has been engineered from the ground up to support hundreds of thousands of GPU threads sending active messages simultaneously, compared with just hundreds of threads on a CPU.&lt;/p&gt;
&lt;p&gt;As such, in this redesigned system, active messaging runs completely on GPUs, bypassing the CPU.&lt;/p&gt;
&lt;p&gt;This enables taking full advantage of the massive parallelism and memory bandwidth of NVIDIA H100 GPUs to send messages, move them across the network and process them on the receiver.&lt;/p&gt;
&lt;p&gt;Running on the stable, high-performance infrastructure of NVIDIA partner CoreWeave, this orchestration enabled doubling the performance of comparable runs while using a fraction of the hardware — at a fraction of the cost.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88110"&gt;&lt;img alt="alt" class="wp-image-88110 size-full" height="496" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/bfs-chart-e1765411867714.png" width="694" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88110"&gt;NVIDIA submission run on CoreWeave cluster with 8,192 H100 GPUs tops the leaderboard on the 31st Graph500 breadth-first search list.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Accelerating New Workloads&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;This breakthrough has massive implications for high-performance computing. HPC fields like fluid dynamics and weather forecasting rely on similar sparse data structures and communication patterns that power the graphs that underpin social networks and cybersecurity.&lt;/p&gt;
&lt;p&gt;For decades, these fields have been tethered to CPUs at the largest scales, even as data scales from billions to trillions of edges. NVIDIA’s winning result on Graph500, alongside two other top 10 entries, validates a new approach for high-performance computing at scale.&lt;/p&gt;
&lt;p&gt;With the full-stack orchestration of NVIDIA computing, networking and software, developers can now use technologies like NVSHMEM and IBGDA to efficiently scale their largest HPC applications, bringing supercomputing performance to commercially available infrastructure.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on the latest Graph500 benchmarks&lt;/i&gt;&lt;i&gt; and learn more about &lt;/i&gt;&lt;i&gt;NVIDIA networking technologies&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The world’s top-performing system for graph processing at scale was built on a commercially available cluster.&lt;/p&gt;
&lt;p&gt;NVIDIA last month announced a record-breaking benchmark result of 410 trillion traversed edges per second (TEPS), ranking No. 1 on the 31st Graph500 breadth-first search (BFS) list.&lt;/p&gt;
&lt;p&gt;Performed on an accelerated computing cluster hosted in a CoreWeave data center in Dallas, the winning run used 8,192 NVIDIA H100 GPUs to process a graph with 2.2 trillion vertices and 35 trillion edges. This result is more than double the performance of comparable solutions on the list, including those hosted in national labs.&lt;/p&gt;
&lt;p&gt;To put this performance in perspective, say every person on Earth has 150 friends. This would represent 1.2 trillion edges in a graph of social relationships. The level of performance recently achieved by NVIDIA and CoreWeave enables searching through every friend relationship on Earth in just about three milliseconds.&lt;/p&gt;
&lt;p&gt;Speed at that scale is half the story — the real breakthrough is efficiency. A comparable entry in the top 10 runs of the Graph500 list used about 9,000 nodes, while the winning run from NVIDIA used just over 1,000 nodes, delivering 3x better performance per dollar.&lt;/p&gt;
&lt;p&gt;NVIDIA tapped into the combined power of its full-stack compute, networking and software technologies — including the NVIDIA CUDA platform, Spectrum-X networking, H100 GPUs and a new active messaging library — to push the boundaries of performance while minimizing hardware footprint.&lt;/p&gt;
&lt;p&gt;By saving significant time and costs at this scale in a commercially available system, the win demonstrates how the NVIDIA computing platform is ready to democratize access to acceleration of the world’s largest sparse, irregular workloads — involving data and work items that come in varying and unpredictable sizes — in addition to dense workloads like AI training.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Graphs at Scale Work&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Graphs are the underlying information structure for modern technology. People interact with them on social networks and banking apps, among other use cases, every day. Graphs capture relationships between pieces of information in massive webs of information.&lt;/p&gt;
&lt;p&gt;For example, consider LinkedIn. A user’s profile is a vertex. Connections or relationships to other users are edges — with other users represented as vertices. Some users have five connections, others have 50,000. This creates variable density across the graph, making it sparse and irregular. Unlike an image or language model, which is structured and dense, a graph is unpredictable.&lt;/p&gt;
&lt;p&gt;Graph500 BFS has a long history as the industry-standard benchmark because it measures a system’s ability to navigate this irregularity at scale.&lt;/p&gt;
&lt;p&gt;BFS measures the speed of traversing the graph through every vertex and edge. A high TEPS score for BFS — measuring how fast the system can process these edges — proves the system has superior interconnects, such as cables or switches between compute nodes, as well as more memory bandwidth and software able to take advantage of the system’s capabilities. It validates the engineering of the entire system, not just the speed of the CPU or GPU.&lt;/p&gt;
&lt;p&gt;Effectively, it’s a measure of how fast a system can “think” and associate disparate pieces of information.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Current Techniques for Processing Graphs&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;GPUs are known for accelerating dense workloads like AI training. Until recently, the largest sparse linear algebra and graph workloads have remained the domain of traditional CPU architectures.&lt;/p&gt;
&lt;p&gt;To process graphs, CPUs move graph data across compute nodes. As the graph scales to trillions of edges, this constant movement creates bottlenecks and jams communications.&lt;/p&gt;
&lt;p&gt;Developers use a variety of software techniques to circumvent this issue. A common approach is to process the graph where it is with active messages, where developers send messages that can process graph data in place. The messages are smaller and can be grouped together to maximize network efficiency.&lt;/p&gt;
&lt;p&gt;While this software technique significantly accelerates processing, active messaging was designed to run on CPUs and is inherently limited by the throughput rate and compute capabilities of CPU systems.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Reengineering Graph Processing for the GPU&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To speed up the BFS run, NVIDIA engineered a full-stack, GPU-only solution that reimagines how data moves across the network.&lt;/p&gt;
&lt;p&gt;A custom software framework developed using InfiniBand GPUDirect Async (IBGDA) and the NVSHMEM parallel programming interface enables GPU-to-GPU active messages.&lt;/p&gt;
&lt;p&gt;With IBGDA, the GPU can directly communicate with the InfiniBand network interface card. Message aggregation has been engineered from the ground up to support hundreds of thousands of GPU threads sending active messages simultaneously, compared with just hundreds of threads on a CPU.&lt;/p&gt;
&lt;p&gt;As such, in this redesigned system, active messaging runs completely on GPUs, bypassing the CPU.&lt;/p&gt;
&lt;p&gt;This enables taking full advantage of the massive parallelism and memory bandwidth of NVIDIA H100 GPUs to send messages, move them across the network and process them on the receiver.&lt;/p&gt;
&lt;p&gt;Running on the stable, high-performance infrastructure of NVIDIA partner CoreWeave, this orchestration enabled doubling the performance of comparable runs while using a fraction of the hardware — at a fraction of the cost.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88110"&gt;&lt;img alt="alt" class="wp-image-88110 size-full" height="496" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/bfs-chart-e1765411867714.png" width="694" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88110"&gt;NVIDIA submission run on CoreWeave cluster with 8,192 H100 GPUs tops the leaderboard on the 31st Graph500 breadth-first search list.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Accelerating New Workloads&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;This breakthrough has massive implications for high-performance computing. HPC fields like fluid dynamics and weather forecasting rely on similar sparse data structures and communication patterns that power the graphs that underpin social networks and cybersecurity.&lt;/p&gt;
&lt;p&gt;For decades, these fields have been tethered to CPUs at the largest scales, even as data scales from billions to trillions of edges. NVIDIA’s winning result on Graph500, alongside two other top 10 entries, validates a new approach for high-performance computing at scale.&lt;/p&gt;
&lt;p&gt;With the full-stack orchestration of NVIDIA computing, networking and software, developers can now use technologies like NVSHMEM and IBGDA to efficiently scale their largest HPC applications, bringing supercomputing performance to commercially available infrastructure.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on the latest Graph500 benchmarks&lt;/i&gt;&lt;i&gt; and learn more about &lt;/i&gt;&lt;i&gt;NVIDIA networking technologies&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/h100-coreweave-graph500/</guid><pubDate>Wed, 10 Dec 2025 20:56:53 +0000</pubDate></item><item><title>[NEW] A differentially private framework for gaining insights into AI chatbot use (The latest research from Google)</title><link>https://research.google/blog/a-differentially-private-framework-for-gaining-insights-into-ai-chatbot-use/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Acknowledgments&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;Thanks to all project contributors, whose essential efforts were pivotal to its success. Special thanks to our colleagues: Yaniv Carmel, Edith Cohen, Rudrajit Das, Chris Dibak, Vadym Doroshenko, Alessandro Epasto, Prem Eruvbetine, Dem Gerolemou, Badih Ghazi, Miguel Guevara, Steve He, Peter Kairouz, Pritish Kamath, Nir Kerem, Ravi Kumar, Ethan Leeman, Pasin Manurangsi, Shlomi Pasternak, Mikhail Pravilov, Adam Sealfon, Yurii Sushko, Da Yu, Chiyuan Zhang.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Acknowledgments&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;Thanks to all project contributors, whose essential efforts were pivotal to its success. Special thanks to our colleagues: Yaniv Carmel, Edith Cohen, Rudrajit Das, Chris Dibak, Vadym Doroshenko, Alessandro Epasto, Prem Eruvbetine, Dem Gerolemou, Badih Ghazi, Miguel Guevara, Steve He, Peter Kairouz, Pritish Kamath, Nir Kerem, Ravi Kumar, Ethan Leeman, Pasin Manurangsi, Shlomi Pasternak, Mikhail Pravilov, Adam Sealfon, Yurii Sushko, Da Yu, Chiyuan Zhang.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/a-differentially-private-framework-for-gaining-insights-into-ai-chatbot-use/</guid><pubDate>Wed, 10 Dec 2025 21:59:41 +0000</pubDate></item><item><title>[NEW] Nvidia is reportedly testing tracking software as chip-smuggling rumors swirl (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/10/nvidia-is-reportedly-testing-tracking-software-as-chip-smuggling-rumors-swirl/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2216028442.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia is allegedly testing software that can track the location of its AI chips as reports of its chips being smuggled into China are on the rise.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia has built location verification technology that would allow it to track which country a chip is located in, Reuters originally reported, citing anonymous sources. This software tracks computing performance but the delay in communication between servers also offers a sense of a chip’s location.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This software will be optional for customers to use and will be made available for Blackwell chips first, Reuters said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Multiple reports have surfaced in the last few days that allege China’s DeepSeek AI models have been trained on smuggled Nvidia Blackwell chips. Nvidia responded to these reports by saying it hasn’t seen evidence of this type of smuggling.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We haven’t seen any substantiation or received tips of ‘phantom data centers’ constructed to deceive us and our OEM partners, then deconstructed, smuggled, and reconstructed somewhere else. While such smuggling seems far-fetched, we pursue any tip we receive,” an Nvidia spokesperson told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This news comes just days after Nvidia got the green light from the U.S. government to start selling its H200 AI chips to approved customers in China on Monday. That announcement pertains only to older H200 chips and not the company’s Blackwell chips.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2216028442.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia is allegedly testing software that can track the location of its AI chips as reports of its chips being smuggled into China are on the rise.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia has built location verification technology that would allow it to track which country a chip is located in, Reuters originally reported, citing anonymous sources. This software tracks computing performance but the delay in communication between servers also offers a sense of a chip’s location.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This software will be optional for customers to use and will be made available for Blackwell chips first, Reuters said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Multiple reports have surfaced in the last few days that allege China’s DeepSeek AI models have been trained on smuggled Nvidia Blackwell chips. Nvidia responded to these reports by saying it hasn’t seen evidence of this type of smuggling.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We haven’t seen any substantiation or received tips of ‘phantom data centers’ constructed to deceive us and our OEM partners, then deconstructed, smuggled, and reconstructed somewhere else. While such smuggling seems far-fetched, we pursue any tip we receive,” an Nvidia spokesperson told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This news comes just days after Nvidia got the green light from the U.S. government to start selling its H200 AI chips to approved customers in China on Monday. That announcement pertains only to older H200 chips and not the company’s Blackwell chips.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/10/nvidia-is-reportedly-testing-tracking-software-as-chip-smuggling-rumors-swirl/</guid><pubDate>Wed, 10 Dec 2025 22:06:47 +0000</pubDate></item><item><title>[NEW] The 70% factuality ceiling: why Google’s new ‘FACTS’ benchmark is a wake-up call for enterprise AI (AI | VentureBeat)</title><link>https://venturebeat.com/ai/the-70-factuality-ceiling-why-googles-new-facts-benchmark-is-a-wake-up-call</link><description>[unable to retrieve full-text content]&lt;p&gt;There&amp;#x27;s no shortage of generative AI benchmarks designed to measure the performance and accuracy of a given model on completing various helpful enterprise tasks — from &lt;a href="https://www.swebench.com/"&gt;coding&lt;/a&gt; to &lt;a href="https://huggingface.co/papers/2401.03601"&gt;instruction following&lt;/a&gt; to &lt;a href="https://openai.com/index/browsecomp/"&gt;agentic web browsing&lt;/a&gt; and&lt;a href="https://scale.com/leaderboard/tool_use_enterprise"&gt; tool use&lt;/a&gt;. But many of these benchmarks have one major shortcoming: they measure the AI&amp;#x27;s ability to complete specific problems and requests, not how &lt;i&gt;factual &lt;/i&gt;the model is in its outputs — how well it generates objectively correct information tied to real-world data — especially when dealing with information contained in imagery or graphics.&lt;/p&gt;&lt;p&gt;For industries where accuracy is paramount — legal, finance, and medical — the lack of a standardized way to measure &lt;i&gt;factuality&lt;/i&gt; has been a critical blind spot.&lt;/p&gt;&lt;p&gt;That changes today: Google’s FACTS team and its data science unit Kaggle &lt;a href="https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/?utm_source=ALL&amp;amp;utm_medium=social&amp;amp;utm_campaign=&amp;amp;utm_content="&gt;released the FACTS Benchmark Suite, a comprehensive evaluation framework&lt;/a&gt; designed to close this gap. &lt;/p&gt;&lt;p&gt;The associated &lt;a href="https://storage.googleapis.com/deepmind-media/FACTS/FACTS_benchmark_suite_paper.pdf"&gt;research paper&lt;/a&gt; reveals a more nuanced definition of the problem, splitting &amp;quot;factuality&amp;quot; into two distinct operational scenarios: &amp;quot;contextual factuality&amp;quot; (grounding responses in provided data) and &amp;quot;world knowledge factuality&amp;quot; (retrieving information from memory or the web).&lt;/p&gt;&lt;p&gt;While the headline news is Gemini 3 Pro’s top-tier placement, the deeper story for builders is the industry-wide &amp;quot;factuality wall.&amp;quot;&lt;/p&gt;&lt;p&gt;According to the initial results, no model—including Gemini 3 Pro, GPT-5, or Claude 4.5 Opus—managed to crack a 70% accuracy score across the suite of problems. For technical leaders, this is a signal: the era of &amp;quot;trust but verify&amp;quot; is far from over.&lt;/p&gt;&lt;h3&gt;Deconstructing the Benchmark&lt;/h3&gt;&lt;p&gt;The FACTS suite moves beyond simple Q&amp;amp;A. It is composed of four distinct tests, each simulating a different real-world failure mode that developers encounter in production:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Parametric Benchmark (Internal Knowledge):&lt;/b&gt; Can the model accurately answer trivia-style questions using only its training data?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Search Benchmark (Tool Use):&lt;/b&gt; Can the model effectively use a web search tool to retrieve and synthesize live information?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Multimodal Benchmark (Vision):&lt;/b&gt; Can the model accurately interpret charts, diagrams, and images without hallucinating?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Grounding Benchmark v2 (Context):&lt;/b&gt; Can the model stick strictly to the provided source text?&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Google has released 3,513 examples to the public, while Kaggle holds a private set to prevent developers from training on the test data—a common issue known as &amp;quot;contamination.&amp;quot;&lt;/p&gt;&lt;h3&gt;The Leaderboard: A Game of Inches&lt;/h3&gt;&lt;p&gt;The initial run of the benchmark places Gemini 3 Pro in the lead with a comprehensive FACTS Score of 68.8%, followed by Gemini 2.5 Pro (62.1%) and OpenAI’s GPT-5 (61.8%).However, a closer look at the data reveals where the real battlegrounds are for engineering teams.&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;FACTS Score (Avg)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Search (RAG Capability)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Multimodal (Vision)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Gemini 3 Pro&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;68.8&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;83.8&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;46.1&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Gemini 2.5 Pro&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;62.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;63.9&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;46.9&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-5&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;61.8&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;77.7&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;44.1&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Grok 4&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;53.6&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;75.3&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;25.7&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Claude 4.5 Opus&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;51.3&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;73.2&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;39.2&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;i&gt;Data sourced from the FACTS Team release notes.&lt;/i&gt;&lt;/p&gt;&lt;h3&gt;For Builders: The &amp;quot;Search&amp;quot; vs. &amp;quot;Parametric&amp;quot; Gap&lt;/h3&gt;&lt;p&gt;For developers building RAG (Retrieval-Augmented Generation) systems, the Search Benchmark is the most critical metric.&lt;/p&gt;&lt;p&gt;The data shows a massive discrepancy between a model&amp;#x27;s ability to &amp;quot;know&amp;quot; things (Parametric) and its ability to &amp;quot;find&amp;quot; things (Search). For instance, Gemini 3 Pro scores a high 83.8% on Search tasks but only 76.4% on Parametric tasks. &lt;/p&gt;&lt;p&gt;This validates the current enterprise architecture standard: do not rely on a model&amp;#x27;s internal memory for critical facts.&lt;/p&gt;&lt;p&gt;If you are building an internal knowledge bot, the FACTS results suggest that hooking your model up to a search tool or vector database is not optional—it is the only way to push accuracy toward acceptable production levels.&lt;/p&gt;&lt;h3&gt;The Multimodal Warning&lt;/h3&gt;&lt;p&gt;The most alarming data point for product managers is the performance on Multimodal tasks. The scores here are universally low. Even the category leader, Gemini 2.5 Pro, only hit 46.9% accuracy.&lt;/p&gt;&lt;p&gt;The benchmark tasks included reading charts, interpreting diagrams, and identifying objects in nature. With less than 50% accuracy across the board, this suggests that Multimodal AI is not yet ready for unsupervised data extraction. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Bottom line: &lt;/b&gt;If your product roadmap involves having an AI automatically scrape data from invoices or interpret financial charts without human-in-the-loop review, &lt;b&gt;you are likely introducing significant error rates&lt;/b&gt; into your pipeline.&lt;/p&gt;&lt;h3&gt;Why This Matters for Your Stack&lt;/h3&gt;&lt;p&gt;The FACTS Benchmark is likely to become a standard reference point for procurement. When evaluating models for enterprise use, technical leaders should look beyond the composite score and drill into the specific sub-benchmark that matches their use case:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Building a Customer Support Bot? Look at the Grounding score to ensure the bot sticks to your policy documents. (Gemini 2.5 Pro actually outscored Gemini 3 Pro here, 74.2 vs 69.0).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Building a Research Assistant? Prioritize Search scores.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Building an Image Analysis Tool? Proceed with extreme caution.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As the FACTS team noted in their release, &amp;quot;All evaluated models achieved an overall accuracy below 70%, leaving considerable headroom for future progress.&amp;quot;For now, the message to the industry is clear: The models are getting smarter, but they aren&amp;#x27;t yet infallible. Design your systems with the assumption that, roughly one-third of the time, the raw model might just be wrong.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;There&amp;#x27;s no shortage of generative AI benchmarks designed to measure the performance and accuracy of a given model on completing various helpful enterprise tasks — from &lt;a href="https://www.swebench.com/"&gt;coding&lt;/a&gt; to &lt;a href="https://huggingface.co/papers/2401.03601"&gt;instruction following&lt;/a&gt; to &lt;a href="https://openai.com/index/browsecomp/"&gt;agentic web browsing&lt;/a&gt; and&lt;a href="https://scale.com/leaderboard/tool_use_enterprise"&gt; tool use&lt;/a&gt;. But many of these benchmarks have one major shortcoming: they measure the AI&amp;#x27;s ability to complete specific problems and requests, not how &lt;i&gt;factual &lt;/i&gt;the model is in its outputs — how well it generates objectively correct information tied to real-world data — especially when dealing with information contained in imagery or graphics.&lt;/p&gt;&lt;p&gt;For industries where accuracy is paramount — legal, finance, and medical — the lack of a standardized way to measure &lt;i&gt;factuality&lt;/i&gt; has been a critical blind spot.&lt;/p&gt;&lt;p&gt;That changes today: Google’s FACTS team and its data science unit Kaggle &lt;a href="https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/?utm_source=ALL&amp;amp;utm_medium=social&amp;amp;utm_campaign=&amp;amp;utm_content="&gt;released the FACTS Benchmark Suite, a comprehensive evaluation framework&lt;/a&gt; designed to close this gap. &lt;/p&gt;&lt;p&gt;The associated &lt;a href="https://storage.googleapis.com/deepmind-media/FACTS/FACTS_benchmark_suite_paper.pdf"&gt;research paper&lt;/a&gt; reveals a more nuanced definition of the problem, splitting &amp;quot;factuality&amp;quot; into two distinct operational scenarios: &amp;quot;contextual factuality&amp;quot; (grounding responses in provided data) and &amp;quot;world knowledge factuality&amp;quot; (retrieving information from memory or the web).&lt;/p&gt;&lt;p&gt;While the headline news is Gemini 3 Pro’s top-tier placement, the deeper story for builders is the industry-wide &amp;quot;factuality wall.&amp;quot;&lt;/p&gt;&lt;p&gt;According to the initial results, no model—including Gemini 3 Pro, GPT-5, or Claude 4.5 Opus—managed to crack a 70% accuracy score across the suite of problems. For technical leaders, this is a signal: the era of &amp;quot;trust but verify&amp;quot; is far from over.&lt;/p&gt;&lt;h3&gt;Deconstructing the Benchmark&lt;/h3&gt;&lt;p&gt;The FACTS suite moves beyond simple Q&amp;amp;A. It is composed of four distinct tests, each simulating a different real-world failure mode that developers encounter in production:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Parametric Benchmark (Internal Knowledge):&lt;/b&gt; Can the model accurately answer trivia-style questions using only its training data?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Search Benchmark (Tool Use):&lt;/b&gt; Can the model effectively use a web search tool to retrieve and synthesize live information?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Multimodal Benchmark (Vision):&lt;/b&gt; Can the model accurately interpret charts, diagrams, and images without hallucinating?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Grounding Benchmark v2 (Context):&lt;/b&gt; Can the model stick strictly to the provided source text?&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Google has released 3,513 examples to the public, while Kaggle holds a private set to prevent developers from training on the test data—a common issue known as &amp;quot;contamination.&amp;quot;&lt;/p&gt;&lt;h3&gt;The Leaderboard: A Game of Inches&lt;/h3&gt;&lt;p&gt;The initial run of the benchmark places Gemini 3 Pro in the lead with a comprehensive FACTS Score of 68.8%, followed by Gemini 2.5 Pro (62.1%) and OpenAI’s GPT-5 (61.8%).However, a closer look at the data reveals where the real battlegrounds are for engineering teams.&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;FACTS Score (Avg)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Search (RAG Capability)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Multimodal (Vision)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Gemini 3 Pro&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;68.8&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;83.8&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;46.1&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Gemini 2.5 Pro&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;62.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;63.9&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;46.9&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-5&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;61.8&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;77.7&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;44.1&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Grok 4&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;53.6&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;75.3&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;25.7&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Claude 4.5 Opus&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;51.3&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;73.2&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;39.2&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;i&gt;Data sourced from the FACTS Team release notes.&lt;/i&gt;&lt;/p&gt;&lt;h3&gt;For Builders: The &amp;quot;Search&amp;quot; vs. &amp;quot;Parametric&amp;quot; Gap&lt;/h3&gt;&lt;p&gt;For developers building RAG (Retrieval-Augmented Generation) systems, the Search Benchmark is the most critical metric.&lt;/p&gt;&lt;p&gt;The data shows a massive discrepancy between a model&amp;#x27;s ability to &amp;quot;know&amp;quot; things (Parametric) and its ability to &amp;quot;find&amp;quot; things (Search). For instance, Gemini 3 Pro scores a high 83.8% on Search tasks but only 76.4% on Parametric tasks. &lt;/p&gt;&lt;p&gt;This validates the current enterprise architecture standard: do not rely on a model&amp;#x27;s internal memory for critical facts.&lt;/p&gt;&lt;p&gt;If you are building an internal knowledge bot, the FACTS results suggest that hooking your model up to a search tool or vector database is not optional—it is the only way to push accuracy toward acceptable production levels.&lt;/p&gt;&lt;h3&gt;The Multimodal Warning&lt;/h3&gt;&lt;p&gt;The most alarming data point for product managers is the performance on Multimodal tasks. The scores here are universally low. Even the category leader, Gemini 2.5 Pro, only hit 46.9% accuracy.&lt;/p&gt;&lt;p&gt;The benchmark tasks included reading charts, interpreting diagrams, and identifying objects in nature. With less than 50% accuracy across the board, this suggests that Multimodal AI is not yet ready for unsupervised data extraction. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Bottom line: &lt;/b&gt;If your product roadmap involves having an AI automatically scrape data from invoices or interpret financial charts without human-in-the-loop review, &lt;b&gt;you are likely introducing significant error rates&lt;/b&gt; into your pipeline.&lt;/p&gt;&lt;h3&gt;Why This Matters for Your Stack&lt;/h3&gt;&lt;p&gt;The FACTS Benchmark is likely to become a standard reference point for procurement. When evaluating models for enterprise use, technical leaders should look beyond the composite score and drill into the specific sub-benchmark that matches their use case:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Building a Customer Support Bot? Look at the Grounding score to ensure the bot sticks to your policy documents. (Gemini 2.5 Pro actually outscored Gemini 3 Pro here, 74.2 vs 69.0).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Building a Research Assistant? Prioritize Search scores.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Building an Image Analysis Tool? Proceed with extreme caution.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As the FACTS team noted in their release, &amp;quot;All evaluated models achieved an overall accuracy below 70%, leaving considerable headroom for future progress.&amp;quot;For now, the message to the industry is clear: The models are getting smarter, but they aren&amp;#x27;t yet infallible. Design your systems with the assumption that, roughly one-third of the time, the raw model might just be wrong.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/the-70-factuality-ceiling-why-googles-new-facts-benchmark-is-a-wake-up-call</guid><pubDate>Wed, 10 Dec 2025 23:00:00 +0000</pubDate></item><item><title>[NEW] Opt-In NVIDIA Software Enables Data Center Fleet Management (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/optional-data-center-fleet-management-software/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;As the scale and complexity of AI infrastructure grows, data center operators need continuous visibility into factors including performance, temperature and power usage. These insights enable data center operators to actively monitor and adjust data center configurations across large-scale, distributed systems — validating that these systems are operating at their highest efficiency and reliability.&lt;/p&gt;
&lt;p&gt;NVIDIA is developing a software solution for visualizing and monitoring fleets of NVIDIA GPUs — giving cloud partners and enterprises an insights dashboard that can help them boost GPU uptime across computing infrastructures.&lt;/p&gt;
&lt;p&gt;The offering is an opt-in, customer-installed service that monitors GPU usage, configuration and errors. It will include an open-source client software agent — part of NVIDIA’s ongoing support of open, transparent software that helps customers get the most from their GPU-powered systems.&lt;/p&gt;
&lt;p&gt;With the service, data center operators will be able to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Track spikes in power usage to keep within energy budgets while maximizing performance per watt.&lt;/li&gt;
&lt;li&gt;Monitor utilization, memory bandwidth and interconnect health across the fleet.&lt;/li&gt;
&lt;li&gt;Detect hotspots and airflow issues early to avoid thermal throttling and premature component aging.&lt;/li&gt;
&lt;li&gt;Confirm consistent software configurations and settings to ensure reproducible results and reliable operation.&lt;/li&gt;
&lt;li&gt;Spot errors and anomalies to identify failing parts early.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These capabilities can help enterprises and cloud providers visualize their GPU fleet, address system bottlenecks and optimize productivity for higher return on investment.&lt;/p&gt;
&lt;p&gt;This optional service provides real-time monitoring by each GPU system communicating and sharing GPU metrics with the external cloud service. NVIDIA GPUs do not have hardware tracking technology, kill switches and backdoors.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Open-Source Agent Offers Insights for Data Center Owners&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The service will feature a client software agent that the customer can install to stream node-level GPU telemetry data to a portal hosted on NVIDIA NGC. Customers will be able to visualize their GPU fleet utilization in a dashboard, globally or by compute zones — groups of nodes enrolled in the same physical or cloud locations.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88213"&gt;&lt;img alt="alt" class="size-full wp-image-88213" height="601" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/gpu-health-agent.jpeg" width="1070" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88213"&gt;The dashboard provides insight into GPU status across a customer’s global fleet.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The client tooling agent is also slated to be open sourced, providing transparency and auditability. It’ll offer a working example for how customers can incorporate NVIDIA tools into their own solutions for monitoring GPU infrastructure — whether for critical compute clusters or entire fleets.&lt;/p&gt;
&lt;p&gt;The software provides insight into a company’s GPU inventory but cannot modify GPU configurations or underlying operations. It provides read-only telemetry data that’s customer managed and customizable.&lt;/p&gt;
&lt;p&gt;The service will also enable customers to generate reports that detail GPU fleet information.&lt;/p&gt;
&lt;p&gt;As AI applications grow in number and complexity, modern AI infrastructure management is evolving to keep pace. Making sure that AI data centers are running at peak health is vital as AI revolutionizes every industry and application. This software service is here to help.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Register for &lt;/i&gt;&lt;i&gt;NVIDIA GTC&lt;/i&gt;&lt;i&gt;, taking place March 16-19 in San Jose, California, to learn more.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer hide_disquss " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;As the scale and complexity of AI infrastructure grows, data center operators need continuous visibility into factors including performance, temperature and power usage. These insights enable data center operators to actively monitor and adjust data center configurations across large-scale, distributed systems — validating that these systems are operating at their highest efficiency and reliability.&lt;/p&gt;
&lt;p&gt;NVIDIA is developing a software solution for visualizing and monitoring fleets of NVIDIA GPUs — giving cloud partners and enterprises an insights dashboard that can help them boost GPU uptime across computing infrastructures.&lt;/p&gt;
&lt;p&gt;The offering is an opt-in, customer-installed service that monitors GPU usage, configuration and errors. It will include an open-source client software agent — part of NVIDIA’s ongoing support of open, transparent software that helps customers get the most from their GPU-powered systems.&lt;/p&gt;
&lt;p&gt;With the service, data center operators will be able to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Track spikes in power usage to keep within energy budgets while maximizing performance per watt.&lt;/li&gt;
&lt;li&gt;Monitor utilization, memory bandwidth and interconnect health across the fleet.&lt;/li&gt;
&lt;li&gt;Detect hotspots and airflow issues early to avoid thermal throttling and premature component aging.&lt;/li&gt;
&lt;li&gt;Confirm consistent software configurations and settings to ensure reproducible results and reliable operation.&lt;/li&gt;
&lt;li&gt;Spot errors and anomalies to identify failing parts early.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These capabilities can help enterprises and cloud providers visualize their GPU fleet, address system bottlenecks and optimize productivity for higher return on investment.&lt;/p&gt;
&lt;p&gt;This optional service provides real-time monitoring by each GPU system communicating and sharing GPU metrics with the external cloud service. NVIDIA GPUs do not have hardware tracking technology, kill switches and backdoors.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Open-Source Agent Offers Insights for Data Center Owners&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The service will feature a client software agent that the customer can install to stream node-level GPU telemetry data to a portal hosted on NVIDIA NGC. Customers will be able to visualize their GPU fleet utilization in a dashboard, globally or by compute zones — groups of nodes enrolled in the same physical or cloud locations.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88213"&gt;&lt;img alt="alt" class="size-full wp-image-88213" height="601" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/gpu-health-agent.jpeg" width="1070" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88213"&gt;The dashboard provides insight into GPU status across a customer’s global fleet.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The client tooling agent is also slated to be open sourced, providing transparency and auditability. It’ll offer a working example for how customers can incorporate NVIDIA tools into their own solutions for monitoring GPU infrastructure — whether for critical compute clusters or entire fleets.&lt;/p&gt;
&lt;p&gt;The software provides insight into a company’s GPU inventory but cannot modify GPU configurations or underlying operations. It provides read-only telemetry data that’s customer managed and customizable.&lt;/p&gt;
&lt;p&gt;The service will also enable customers to generate reports that detail GPU fleet information.&lt;/p&gt;
&lt;p&gt;As AI applications grow in number and complexity, modern AI infrastructure management is evolving to keep pace. Making sure that AI data centers are running at peak health is vital as AI revolutionizes every industry and application. This software service is here to help.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Register for &lt;/i&gt;&lt;i&gt;NVIDIA GTC&lt;/i&gt;&lt;i&gt;, taking place March 16-19 in San Jose, California, to learn more.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer hide_disquss " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/optional-data-center-fleet-management-software/</guid><pubDate>Wed, 10 Dec 2025 23:49:26 +0000</pubDate></item><item><title>[NEW] Deepening our partnership with the UK AI Security Institute (Google DeepMind News)</title><link>https://deepmind.google/blog/deepening-our-partnership-with-the-uk-ai-security-institute/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/YXgJ_O9k-ZBnsZSuLTv1a4YRWyP2C5kuSRJcyq3F25spV0pLs3tqXGX7Pe2aP6bLjVYM6cwzMfxID3-J4W5HrvP_teJB2bBe4PJcTAgBd8J99p4GPBQ=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;p class="lead-paragraph"&gt;Today, we're announcing an expanded partnership with the UK AI Security Institute (AISI) through a new Memorandum of Understanding focused on foundational security and safety research, to help ensure artificial intelligence is developed safely and benefits everyone.&lt;/p&gt;&lt;p&gt;The research partnership with AISI is an important part of our broader collaboration with the UK government on accelerating safe and beneficial AI progress.&lt;/p&gt;&lt;h2&gt;Building on a foundation of collaboration&lt;/h2&gt;&lt;p&gt;AI holds immense potential to benefit humanity by helping treat disease, accelerate scientific discovery, create economic prosperity and tackle climate change. For these benefits to be realised, we must put safety and responsibility at the heart of development. Evaluating our models against a broad spectrum of potential risks remains a critical part of our safety strategy, and external partnerships are an important element of this work.&lt;/p&gt;&lt;p&gt;This is why we have partnered with the UK AISI since its inception in November 2023 to test our most capable models. We are deeply committed to the UK AISI’s goal to equip governments, industry and wider society with a scientific understanding of the potential risks posed by advanced AI as well as potential solutions and mitigations.&lt;/p&gt;&lt;p&gt;We are actively working with AISI to build more robust evaluations for AI models, and our teams have collaborated on safety research to move the field forward, including recent work on Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety. Building on this success, today we are broadening our partnership from testing to include wider, more foundational, research in a variety of areas.&lt;/p&gt;&lt;h2&gt;What the partnership involves&lt;/h2&gt;&lt;p&gt;Under this new research partnership, we're broadening our collaboration to include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Sharing access to our proprietary models, data and ideas to accelerate research progress&lt;/li&gt;&lt;li&gt;Joint reports and publications sharing findings with the research community&lt;/li&gt;&lt;li&gt;More collaborative security and safety research combining our teams' expertise&lt;/li&gt;&lt;li&gt;Technical discussions to tackle complex safety challenges&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Key research areas&lt;/h2&gt;&lt;p&gt;Our joint research with AISI focuses on critical areas where Google DeepMind's expertise, interdisciplinary teams, and years of pioneering responsible research can help make AI systems more safe and secure:&lt;/p&gt;&lt;h3&gt;Monitoring AI reasoning processes&lt;/h3&gt;&lt;p&gt;We will work on techniques to monitor an AI system’s “thinking”, also commonly referred to as its chain-of-thought (CoT). This work builds on previous Google DeepMind research as well, and our recent collaboration on this topic with AISI, OpenAI, Anthropic and other partners. CoT monitoring helps us understand how an AI system produces its answers, complementing interpretability research.&lt;/p&gt;&lt;h3&gt;Understanding social and emotional impacts&lt;/h3&gt;&lt;p&gt;We will work together to investigate the ethical implications of socioaffective misalignment; that is, the potential for AI models to behave in ways which do not align with human wellbeing, even when they’re technically following instructions correctly. This research will build on existing Google DeepMind work that has helped define this critical area of AI safety.&lt;/p&gt;&lt;h3&gt;Evaluating economic systems&lt;/h3&gt;&lt;p&gt;We will explore the potential impact of AI on economic systems by simulating real-world tasks across different environments. Experts will score and validate these tasks, after which they will be categorised along dimensions like complexity or representativeness, to help predict factors like long-term labour market impact.&lt;/p&gt;&lt;h2&gt;Working together to realise the benefits of AI&lt;/h2&gt;&lt;p&gt;Our partnership with AISI is one element of how we aim to realise the benefits of AI for humanity while mitigating potential risks. Our wider strategy includes foresight research, extensive safety training that goes hand-in-hand with capability development, rigorous testing of our models, and the development of better tools and frameworks to understand and mitigate risk.&lt;/p&gt;&lt;p&gt;Strong internal governance processes are also essential for safe and responsible AI development, as is collaborating with independent external experts who bring fresh perspectives and diverse expertise to our work. Google DeepMind’s Responsibility and Safety Council works across teams to monitor emerging risk, review ethics and safety assessments and implement relevant technical and policy mitigations. We also partner with other external experts like Apollo Research, Vaultis, Dreadnode and more, to conduct extensive testing and evaluation of our models, including Gemini 3, our most intelligent and secure model to date.&lt;/p&gt;&lt;p&gt;Additionally, Google DeepMind is a proud founding member of the Frontier Model Forum, as well as the Partnership on AI, where we focus on ensuring safe and responsible development of frontier AI models and increasing collaboration on important safety issues.&lt;/p&gt;&lt;p&gt;We hope our expanded partnership with AISI will allow us to build more robust approaches to AI safety for the benefit not just of our own organisations, but also the wider industry and everyone who interacts with AI systems.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/YXgJ_O9k-ZBnsZSuLTv1a4YRWyP2C5kuSRJcyq3F25spV0pLs3tqXGX7Pe2aP6bLjVYM6cwzMfxID3-J4W5HrvP_teJB2bBe4PJcTAgBd8J99p4GPBQ=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;p class="lead-paragraph"&gt;Today, we're announcing an expanded partnership with the UK AI Security Institute (AISI) through a new Memorandum of Understanding focused on foundational security and safety research, to help ensure artificial intelligence is developed safely and benefits everyone.&lt;/p&gt;&lt;p&gt;The research partnership with AISI is an important part of our broader collaboration with the UK government on accelerating safe and beneficial AI progress.&lt;/p&gt;&lt;h2&gt;Building on a foundation of collaboration&lt;/h2&gt;&lt;p&gt;AI holds immense potential to benefit humanity by helping treat disease, accelerate scientific discovery, create economic prosperity and tackle climate change. For these benefits to be realised, we must put safety and responsibility at the heart of development. Evaluating our models against a broad spectrum of potential risks remains a critical part of our safety strategy, and external partnerships are an important element of this work.&lt;/p&gt;&lt;p&gt;This is why we have partnered with the UK AISI since its inception in November 2023 to test our most capable models. We are deeply committed to the UK AISI’s goal to equip governments, industry and wider society with a scientific understanding of the potential risks posed by advanced AI as well as potential solutions and mitigations.&lt;/p&gt;&lt;p&gt;We are actively working with AISI to build more robust evaluations for AI models, and our teams have collaborated on safety research to move the field forward, including recent work on Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety. Building on this success, today we are broadening our partnership from testing to include wider, more foundational, research in a variety of areas.&lt;/p&gt;&lt;h2&gt;What the partnership involves&lt;/h2&gt;&lt;p&gt;Under this new research partnership, we're broadening our collaboration to include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Sharing access to our proprietary models, data and ideas to accelerate research progress&lt;/li&gt;&lt;li&gt;Joint reports and publications sharing findings with the research community&lt;/li&gt;&lt;li&gt;More collaborative security and safety research combining our teams' expertise&lt;/li&gt;&lt;li&gt;Technical discussions to tackle complex safety challenges&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Key research areas&lt;/h2&gt;&lt;p&gt;Our joint research with AISI focuses on critical areas where Google DeepMind's expertise, interdisciplinary teams, and years of pioneering responsible research can help make AI systems more safe and secure:&lt;/p&gt;&lt;h3&gt;Monitoring AI reasoning processes&lt;/h3&gt;&lt;p&gt;We will work on techniques to monitor an AI system’s “thinking”, also commonly referred to as its chain-of-thought (CoT). This work builds on previous Google DeepMind research as well, and our recent collaboration on this topic with AISI, OpenAI, Anthropic and other partners. CoT monitoring helps us understand how an AI system produces its answers, complementing interpretability research.&lt;/p&gt;&lt;h3&gt;Understanding social and emotional impacts&lt;/h3&gt;&lt;p&gt;We will work together to investigate the ethical implications of socioaffective misalignment; that is, the potential for AI models to behave in ways which do not align with human wellbeing, even when they’re technically following instructions correctly. This research will build on existing Google DeepMind work that has helped define this critical area of AI safety.&lt;/p&gt;&lt;h3&gt;Evaluating economic systems&lt;/h3&gt;&lt;p&gt;We will explore the potential impact of AI on economic systems by simulating real-world tasks across different environments. Experts will score and validate these tasks, after which they will be categorised along dimensions like complexity or representativeness, to help predict factors like long-term labour market impact.&lt;/p&gt;&lt;h2&gt;Working together to realise the benefits of AI&lt;/h2&gt;&lt;p&gt;Our partnership with AISI is one element of how we aim to realise the benefits of AI for humanity while mitigating potential risks. Our wider strategy includes foresight research, extensive safety training that goes hand-in-hand with capability development, rigorous testing of our models, and the development of better tools and frameworks to understand and mitigate risk.&lt;/p&gt;&lt;p&gt;Strong internal governance processes are also essential for safe and responsible AI development, as is collaborating with independent external experts who bring fresh perspectives and diverse expertise to our work. Google DeepMind’s Responsibility and Safety Council works across teams to monitor emerging risk, review ethics and safety assessments and implement relevant technical and policy mitigations. We also partner with other external experts like Apollo Research, Vaultis, Dreadnode and more, to conduct extensive testing and evaluation of our models, including Gemini 3, our most intelligent and secure model to date.&lt;/p&gt;&lt;p&gt;Additionally, Google DeepMind is a proud founding member of the Frontier Model Forum, as well as the Partnership on AI, where we focus on ensuring safe and responsible development of frontier AI models and increasing collaboration on important safety issues.&lt;/p&gt;&lt;p&gt;We hope our expanded partnership with AISI will allow us to build more robust approaches to AI safety for the benefit not just of our own organisations, but also the wider industry and everyone who interacts with AI systems.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/blog/deepening-our-partnership-with-the-uk-ai-security-institute/</guid><pubDate>Thu, 11 Dec 2025 00:06:40 +0000</pubDate></item><item><title>[NEW] State attorneys general warn Microsoft, OpenAI, Google, and other AI giants to fix ‘delusional’ outputs (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/10/state-attorneys-general-warn-microsoft-openai-google-and-other-ai-giants-to-fix-delusional-outputs/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2195918462.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After a string of disturbing mental health incidents involving AI chatbots, a group of state attorneys general have sent a letter to the AI industry’s top companies, with a warning to fix “delusional outputs” or risk being in breach of state law.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The letter, signed by dozens of AGs from U.S. states and territories with the National Association of Attorneys General, asks the companies, including Microsoft, OpenAI, Google, and 10 other major AI firms, to implement a variety of new internal safeguards to protect their users. Anthropic, Apple, Chai AI, Character Technologies, Luka, Meta, Nomi AI, Perplexity AI, Replika, and xAI were also included in the letter.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The letter comes as a fight over AI regulations has been brewing between state and federal government.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those safeguards include transparent third-party audits of large language models that look for signs of delusional or sycophantic ideations, as well as new incident reporting procedures designed to notify users when chatbots produce psychologically harmful outputs. Those third parties, which could include academic and civil society groups, should be allowed to&amp;nbsp;“evaluate systems pre-release without retaliation and to publish their findings without prior approval from the company,” the letter states.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“GenAI has the potential to change how the world works in a positive way. But it also has caused—and has the potential to cause—serious harm, especially to vulnerable populations,” the letter states, pointing to a number of well-publicized incidents over the past year — including suicides and murder — in which violence has been linked to excessive AI use, the letter states.&amp;nbsp;“In many of these incidents, the GenAI products generated sycophantic and delusional outputs that either encouraged users’ delusions or assured users that they were not delusional.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AGs also suggest companies treat mental health incidents the same way tech companies handle cybersecurity incidents — with clear and transparent incident reporting policies and procedures. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Companies should develop and publish “detection and response timelines for sycophantic and delusional outputs,” the letter states. In a similar fashion to how data breaches are currently handled, companies should also “promptly, clearly, and directly notify users if they were exposed to potentially harmful sycophantic or delusional outputs,” the letter says.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Another ask is that the companies develop “reasonable and appropriate safety tests” on GenAI models to “ensure the models do not produce potentially harmful sycophantic and delusional outputs.” These tests should be conducted before the models are ever offered to the public, it adds.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch was unable to reach Google, Microsoft, or OpenAI for comment prior to publication. The article will be updated if the companies respond.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tech companies developing AI have had a much warmer reception at the federal level. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Trump administration has made it known it is unabashedly pro-AI, and, over the past year, multiple attempts have been made to pass a nationwide moratorium on state-level AI regulations. So far, those attempts have failed — thanks, in part, to pressure from state officials.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Not to be deterred, Trump announced Monday he plans to pass an executive order next week that will limit the ability of states to regulate AI. The president said in a post on Truth Social he hoped his EO would stop AI from being “DESTROYED IN ITS INFANCY.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2195918462.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After a string of disturbing mental health incidents involving AI chatbots, a group of state attorneys general have sent a letter to the AI industry’s top companies, with a warning to fix “delusional outputs” or risk being in breach of state law.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The letter, signed by dozens of AGs from U.S. states and territories with the National Association of Attorneys General, asks the companies, including Microsoft, OpenAI, Google, and 10 other major AI firms, to implement a variety of new internal safeguards to protect their users. Anthropic, Apple, Chai AI, Character Technologies, Luka, Meta, Nomi AI, Perplexity AI, Replika, and xAI were also included in the letter.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The letter comes as a fight over AI regulations has been brewing between state and federal government.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those safeguards include transparent third-party audits of large language models that look for signs of delusional or sycophantic ideations, as well as new incident reporting procedures designed to notify users when chatbots produce psychologically harmful outputs. Those third parties, which could include academic and civil society groups, should be allowed to&amp;nbsp;“evaluate systems pre-release without retaliation and to publish their findings without prior approval from the company,” the letter states.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“GenAI has the potential to change how the world works in a positive way. But it also has caused—and has the potential to cause—serious harm, especially to vulnerable populations,” the letter states, pointing to a number of well-publicized incidents over the past year — including suicides and murder — in which violence has been linked to excessive AI use, the letter states.&amp;nbsp;“In many of these incidents, the GenAI products generated sycophantic and delusional outputs that either encouraged users’ delusions or assured users that they were not delusional.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AGs also suggest companies treat mental health incidents the same way tech companies handle cybersecurity incidents — with clear and transparent incident reporting policies and procedures. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Companies should develop and publish “detection and response timelines for sycophantic and delusional outputs,” the letter states. In a similar fashion to how data breaches are currently handled, companies should also “promptly, clearly, and directly notify users if they were exposed to potentially harmful sycophantic or delusional outputs,” the letter says.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Another ask is that the companies develop “reasonable and appropriate safety tests” on GenAI models to “ensure the models do not produce potentially harmful sycophantic and delusional outputs.” These tests should be conducted before the models are ever offered to the public, it adds.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch was unable to reach Google, Microsoft, or OpenAI for comment prior to publication. The article will be updated if the companies respond.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tech companies developing AI have had a much warmer reception at the federal level. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Trump administration has made it known it is unabashedly pro-AI, and, over the past year, multiple attempts have been made to pass a nationwide moratorium on state-level AI regulations. So far, those attempts have failed — thanks, in part, to pressure from state officials.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Not to be deterred, Trump announced Monday he plans to pass an executive order next week that will limit the ability of states to regulate AI. The president said in a post on Truth Social he hoped his EO would stop AI from being “DESTROYED IN ITS INFANCY.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/10/state-attorneys-general-warn-microsoft-openai-google-and-other-ai-giants-to-fix-delusional-outputs/</guid><pubDate>Thu, 11 Dec 2025 00:13:43 +0000</pubDate></item><item><title>[NEW] Google’s answer to the AI arms race — promote the guy behind its data center tech (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/10/googles-answer-to-the-ai-arms-race-promote-the-guy-behind-its-data-center-tech/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-10-at-4.33.43-PM.png?resize=1200,699" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google just made a major move in the AI infrastructure arms race, elevating Amin Vahdat to chief technologist for AI infrastructure, a newly created position reporting directly to CEO Sundar Pichai, according to an internal memo first reported by Semafor. It’s a signal of just how critical this work has become as Google pours up to $93 billion into capital expenditures by the end of 2025 — a number that parent company Alphabet expects will be a whole lot bigger next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vahdat isn’t new to the game. The computer scientist, who holds a PhD from UC Berkeley and started as a research intern at Xerox PARC back in the early ’90s, has been quietly building Google’s AI backbone for the past 15 years. Before joining Google in 2010 as an engineering fellow and VP, he was an associate professor at Duke University and later a professor and SAIC Chair at UC San Diego. His academic credentials are formidable — with what appears to be around 395 published papers — and his research has always focused on making computers work more efficiently at massive scale.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Vahdat already maintains a high profile with Google. Just eight months ago, at Google Cloud Next, he unveiled the company’s seventh-generation TPU, called Ironwood, in his role as VP and GM of ML, Systems, and Cloud AI. The specs he rattled off at the event were staggering, too: over 9,000 chips per pod delivering 42.5 exaflops of compute — more than 24 times the power of the world’s No. 1 supercomputer at the time, he said. “Demand for AI compute has increased by a factor of 100 million in just eight years,” he told the audience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Behind the scenes, as noted by Semafor, Vahdat has been orchestrating the unglamorous and essential work that keeps Google competitive, including those custom TPU chips for AI training and inference that give Google an edge over rivals like OpenAI as well as the Jupiter network, the super-fast internal network that allows all its servers to talk to each other and move massive amounts of data around. (In a blog post late last year, Vahdat said that Jupiter now scales to 13 petabits per second, explaining that’s enough bandwidth to theoretically support a video call for all 8 billion people on Earth simultaneously.) It’s the invisible plumbing connecting everything from YouTube and Search to Google’s massive AI training operations across hundreds of data center fabrics worldwide.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vahdat has also been deeply involved in the ongoing development of the Borg software system, Google’s cluster management system that acts as the brain coordinating all the work happening across its data centers and whose job is to figure out which servers should run which tasks, when, and for how long. And he has said he oversaw the development of Axion, Google’s first custom Arm-based general-purpose CPUs designed for data centers, which the company unveiled last year and continues to build.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In short, Vahdat is central to Google’s AI story.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Indeed, in a market where top AI talent commands astronomical compensation and constant recruitment, Google’s decision to elevate Vahdat to the C-suite may also be about retention. When you’ve spent 15 years building someone into a linchpin of your AI strategy, you make sure they stay.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-10-at-4.33.43-PM.png?resize=1200,699" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google just made a major move in the AI infrastructure arms race, elevating Amin Vahdat to chief technologist for AI infrastructure, a newly created position reporting directly to CEO Sundar Pichai, according to an internal memo first reported by Semafor. It’s a signal of just how critical this work has become as Google pours up to $93 billion into capital expenditures by the end of 2025 — a number that parent company Alphabet expects will be a whole lot bigger next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vahdat isn’t new to the game. The computer scientist, who holds a PhD from UC Berkeley and started as a research intern at Xerox PARC back in the early ’90s, has been quietly building Google’s AI backbone for the past 15 years. Before joining Google in 2010 as an engineering fellow and VP, he was an associate professor at Duke University and later a professor and SAIC Chair at UC San Diego. His academic credentials are formidable — with what appears to be around 395 published papers — and his research has always focused on making computers work more efficiently at massive scale.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Vahdat already maintains a high profile with Google. Just eight months ago, at Google Cloud Next, he unveiled the company’s seventh-generation TPU, called Ironwood, in his role as VP and GM of ML, Systems, and Cloud AI. The specs he rattled off at the event were staggering, too: over 9,000 chips per pod delivering 42.5 exaflops of compute — more than 24 times the power of the world’s No. 1 supercomputer at the time, he said. “Demand for AI compute has increased by a factor of 100 million in just eight years,” he told the audience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Behind the scenes, as noted by Semafor, Vahdat has been orchestrating the unglamorous and essential work that keeps Google competitive, including those custom TPU chips for AI training and inference that give Google an edge over rivals like OpenAI as well as the Jupiter network, the super-fast internal network that allows all its servers to talk to each other and move massive amounts of data around. (In a blog post late last year, Vahdat said that Jupiter now scales to 13 petabits per second, explaining that’s enough bandwidth to theoretically support a video call for all 8 billion people on Earth simultaneously.) It’s the invisible plumbing connecting everything from YouTube and Search to Google’s massive AI training operations across hundreds of data center fabrics worldwide.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vahdat has also been deeply involved in the ongoing development of the Borg software system, Google’s cluster management system that acts as the brain coordinating all the work happening across its data centers and whose job is to figure out which servers should run which tasks, when, and for how long. And he has said he oversaw the development of Axion, Google’s first custom Arm-based general-purpose CPUs designed for data centers, which the company unveiled last year and continues to build.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In short, Vahdat is central to Google’s AI story.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Indeed, in a market where top AI talent commands astronomical compensation and constant recruitment, Google’s decision to elevate Vahdat to the C-suite may also be about retention. When you’ve spent 15 years building someone into a linchpin of your AI strategy, you make sure they stay.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/10/googles-answer-to-the-ai-arms-race-promote-the-guy-behind-its-data-center-tech/</guid><pubDate>Thu, 11 Dec 2025 01:10:04 +0000</pubDate></item></channel></rss>