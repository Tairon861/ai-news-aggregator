<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 05 Feb 2026 02:23:52 +0000</lastBuildDate><item><title>Nemotron ColEmbed V2: Raising the Bar for Multimodal Retrieval with ViDoRe V3‚Äôs Top Model (Hugging Face - Blog)</title><link>https://huggingface.co/blog/nvidia/nemotron-colembed-v2</link><description>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Ronay Ak's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/d9fa8404f258c96df1c500cffd10752f.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
Modern search systems are increasingly designed to process heterogeneous document images that may contain text, tables, charts, figures, and other visual components. In this context, accurately retrieving relevant information across these diverse modalities is a central challenge. Multimodal embedding models built on top of foundational vision‚Äìlanguage models (VLMs) map diverse content types into a shared representation space, enabling unified retrieval over text, images, and structured visual elements. Although encoding an entire query and candidate document into a single vector is a common practice‚Äîexemplified by our recently released commercial-ready Llama-Nemotron-Embed-VL-1B which prioritizes efficiency and low storage‚Äîthere is an increasing research direction on multi-vector, late-interaction style embedding architectures which provide fine-grained multi-vector interaction between queries and documents. By enabling richer token representations, these models better capture more detailed semantic relationships, and they have shown higher accuracy performance on various (multimodal) benchmarks.
&lt;p&gt;NVIDIA introduces the Nemotron ColEmbed V2 family, a set of late-interaction embedding models available in three sizes‚Äî3B, 4B, and 8B‚Äîdesigned for highly accurate multimodal retrieval. These models adopt a unified approach to text‚Äìimage retrieval and achieve state-of-the-art performance on the ViDoRe V1, V2, and V3 benchmarks.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Nemotron ColEmbed V2 Highlights (TL;DR)
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The nemotron-colembed-vl-8b-v2, nemotron-colembed-vl-4b-v2 and llama-nemotron-colembed-vl-3b-v2 are state-of-the-art late interaction embedding models that rank 1st, 3rd and 6th‚Äîthe highest ranked models in each weight class, as of Feb 3, 2026, on the ViDoRe V3 benchmark: a comprehensive evaluation of visual document retrieval for enterprise use-case benchmark. &lt;/p&gt;
&lt;p&gt;&lt;img alt="late_interaction" src="https://cdn-uploads.huggingface.co/production/uploads/6814af6d59c3d8d6f9b09f2b/A4KY3NSZrrxGEcO6mihU8.png" /&gt;&lt;/p&gt;
&lt;p&gt;The late interaction mechanism introduced by ColBERT for multi-vector embedding matching has been extended in our work to a multimodal setting, enabling fine-grained interactions between query and document tokens, whether textual or visual. As illustrated in the figure, each query token embedding interacts with all document token embeddings via the &lt;code&gt;MaxSim&lt;/code&gt; operator, which selects the maximum similarity for each query token and then sums these maxima to produce the final relevance score. This approach requires storing the token embeddings for the entire document corpus, whether textual or visual, thereby increasing storage requirements. During inference, query token embeddings are computed and matched against the stored document embeddings using the same MaxSim operation. &lt;/p&gt;
&lt;p&gt;Nemotron ColEmbed V2 family of models is intended for researchers exploring visual document retrieval applications where accuracy is paramount. This distinguishes it from our 1B single-vector model released last month, which was designed for commercial environments requiring minimal storage and high throughput. It is instrumental in multimodal RAG systems, where textual queries can be used to retrieve document images, such as pages, text, charts, tables, or infographics. The models output multi-vector embeddings for input queries and documents. Potential applications include multimedia search engines, cross-modal retrieval systems, and conversational AI with rich input understanding.&lt;/p&gt;
&lt;p&gt;As a new benchmark, ViDoRe V3 is designed to set an industry standard for multi-modal enterprise document retrieval. It tackles a key challenge in production RAG systems: accurately extracting information from complex, visually-rich documents.  With its strong multi-modal document retrieval capability, the nemotron-colembed-vl-8b-v2 model ranks &lt;strong&gt;#1&lt;/strong&gt; on the ViDoRe V3 leaderboard, setting a new standard for accuracy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Document Retrieval benchmark (page retrieval) ‚Äì Avg NDCG@10 on ViDoRe V3 public and private tasks.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Models‚Äô Architecture
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The llama-nemotron-colembed-vl-3b-v2 is a transformer-based multimodal embedding model built on top of a VLM based on google/siglip2-giant-opt-patch16-384 and meta-llama/Llama-3.2-3B.  The nemotron-colembed-vl-8b-v2 and nemotron-colembed-vl-4b-v2 multimodal encoder models were built from Qwen3-VL-8B-Instruct and Qwen3-VL-4B-Instruct, respectively. &lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Architecture modifications:
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Our models use bi-directional self-attention instead of the original uni-directional causal self-attention from the LLM decoder models. This allows the model to learn rich representations from the whole input sequence.&lt;/li&gt;
&lt;li&gt;ColBERT-style late interaction mechanism- for each input token, each model outputs an n-dimensional embedding vector of floating-point values, where n is determined by the model‚Äôs hidden size.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Training Methodology
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The nemotron-colembed-vl-8b-v2, nemotron-colembed-vl-4b-v2 and llama-nemotron-colembed-vl-3b-v2 models were trained using a bi-encoder architecture, independently. This involves encoding a pair of sentences (for example, a query and a document) independently using the embedding model. Using contrastive learning, it is used to maximize the late interaction similarity between the query and the document that contains the answer, while minimizing the similarity between the query and sampled negative documents not useful to answer the question.&lt;/p&gt;
&lt;p&gt;The llama-nemotron-colembed-vl-3b-v2 model was trained in a two-stage pipeline:  it was first fine-tuned with 12.5M textQA pairs, and subsequently fine-tuned with text‚Äìimage pairs. The nemotron-colembed-vl-8b-v2,  nemotron-colembed-vl-4b-v2 models were fine-tuned using only text-image pairs (2nd stage).&lt;/p&gt;
&lt;p&gt;Our training datasets contain both text-only and text-image pairs, and we apply hard negative mining following the positive-aware hard negative mining methods presented in the NV-Retriever paper to improve retrieval performance.&lt;/p&gt;
&lt;p&gt;‚ú® &lt;strong&gt;Key Improvements over V1:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;‚öóÔ∏è Advanced Model Merging: Utilizes post-training model merging to combine the strengths of multiple fine-tuned checkpoints. This delivers the accuracy stability of an ensemble without any additional inference latency.&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;üåç Enhanced Synthetic Data: We significantly enriched our training mixture with diverse multilingual synthetic data, improving semantic alignment across languages and complex document types.&lt;/p&gt;
&lt;p&gt;&lt;img alt="modelperfs_vidorev3" src="https://cdn-uploads.huggingface.co/production/uploads/6814af6d59c3d8d6f9b09f2b/J7JIKCDriMjztO1ULw0k3.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Start Building with Nemotron ColEmbed V2
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Nemotron ColEmbed V2 models mark a major step forward in high-accuracy text‚Äìimage retrieval, delivering state-of-the-art results on the ViDoRe V1, V2, and  V3 benchmarks. The availability of 3B, 4B and 8B model variants further establishes a solid foundation for future research and advanced experimentation in multimodal retrieval applications.&lt;/p&gt;
&lt;p&gt;Get started with Nemotron ColEmbed V2 models by downloading the models: nemotron-colembed-vl-8b-v2, nemotron-colembed-vl-4b-v2 and llama-nemotron-colembed-vl-3b-v2, available on Hugging Face. Learn more about the NVIDIA NeMo Retriever family of Nemotron RAG models on the product page, or access the microservice container from NVIDIA NGC. This is an excellent opportunity to explore state-of-the-art retrieval in your own applications and workflows.&lt;/p&gt;
&lt;p&gt;Try NVIDIA Enterprise RAG Blueprint, using the Nemotron RAG models that are powered by the same tech behind our ViDoRe V3 winning. &lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Ronay Ak's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/d9fa8404f258c96df1c500cffd10752f.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
Modern search systems are increasingly designed to process heterogeneous document images that may contain text, tables, charts, figures, and other visual components. In this context, accurately retrieving relevant information across these diverse modalities is a central challenge. Multimodal embedding models built on top of foundational vision‚Äìlanguage models (VLMs) map diverse content types into a shared representation space, enabling unified retrieval over text, images, and structured visual elements. Although encoding an entire query and candidate document into a single vector is a common practice‚Äîexemplified by our recently released commercial-ready Llama-Nemotron-Embed-VL-1B which prioritizes efficiency and low storage‚Äîthere is an increasing research direction on multi-vector, late-interaction style embedding architectures which provide fine-grained multi-vector interaction between queries and documents. By enabling richer token representations, these models better capture more detailed semantic relationships, and they have shown higher accuracy performance on various (multimodal) benchmarks.
&lt;p&gt;NVIDIA introduces the Nemotron ColEmbed V2 family, a set of late-interaction embedding models available in three sizes‚Äî3B, 4B, and 8B‚Äîdesigned for highly accurate multimodal retrieval. These models adopt a unified approach to text‚Äìimage retrieval and achieve state-of-the-art performance on the ViDoRe V1, V2, and V3 benchmarks.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Nemotron ColEmbed V2 Highlights (TL;DR)
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The nemotron-colembed-vl-8b-v2, nemotron-colembed-vl-4b-v2 and llama-nemotron-colembed-vl-3b-v2 are state-of-the-art late interaction embedding models that rank 1st, 3rd and 6th‚Äîthe highest ranked models in each weight class, as of Feb 3, 2026, on the ViDoRe V3 benchmark: a comprehensive evaluation of visual document retrieval for enterprise use-case benchmark. &lt;/p&gt;
&lt;p&gt;&lt;img alt="late_interaction" src="https://cdn-uploads.huggingface.co/production/uploads/6814af6d59c3d8d6f9b09f2b/A4KY3NSZrrxGEcO6mihU8.png" /&gt;&lt;/p&gt;
&lt;p&gt;The late interaction mechanism introduced by ColBERT for multi-vector embedding matching has been extended in our work to a multimodal setting, enabling fine-grained interactions between query and document tokens, whether textual or visual. As illustrated in the figure, each query token embedding interacts with all document token embeddings via the &lt;code&gt;MaxSim&lt;/code&gt; operator, which selects the maximum similarity for each query token and then sums these maxima to produce the final relevance score. This approach requires storing the token embeddings for the entire document corpus, whether textual or visual, thereby increasing storage requirements. During inference, query token embeddings are computed and matched against the stored document embeddings using the same MaxSim operation. &lt;/p&gt;
&lt;p&gt;Nemotron ColEmbed V2 family of models is intended for researchers exploring visual document retrieval applications where accuracy is paramount. This distinguishes it from our 1B single-vector model released last month, which was designed for commercial environments requiring minimal storage and high throughput. It is instrumental in multimodal RAG systems, where textual queries can be used to retrieve document images, such as pages, text, charts, tables, or infographics. The models output multi-vector embeddings for input queries and documents. Potential applications include multimedia search engines, cross-modal retrieval systems, and conversational AI with rich input understanding.&lt;/p&gt;
&lt;p&gt;As a new benchmark, ViDoRe V3 is designed to set an industry standard for multi-modal enterprise document retrieval. It tackles a key challenge in production RAG systems: accurately extracting information from complex, visually-rich documents.  With its strong multi-modal document retrieval capability, the nemotron-colembed-vl-8b-v2 model ranks &lt;strong&gt;#1&lt;/strong&gt; on the ViDoRe V3 leaderboard, setting a new standard for accuracy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Document Retrieval benchmark (page retrieval) ‚Äì Avg NDCG@10 on ViDoRe V3 public and private tasks.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Models‚Äô Architecture
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The llama-nemotron-colembed-vl-3b-v2 is a transformer-based multimodal embedding model built on top of a VLM based on google/siglip2-giant-opt-patch16-384 and meta-llama/Llama-3.2-3B.  The nemotron-colembed-vl-8b-v2 and nemotron-colembed-vl-4b-v2 multimodal encoder models were built from Qwen3-VL-8B-Instruct and Qwen3-VL-4B-Instruct, respectively. &lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Architecture modifications:
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Our models use bi-directional self-attention instead of the original uni-directional causal self-attention from the LLM decoder models. This allows the model to learn rich representations from the whole input sequence.&lt;/li&gt;
&lt;li&gt;ColBERT-style late interaction mechanism- for each input token, each model outputs an n-dimensional embedding vector of floating-point values, where n is determined by the model‚Äôs hidden size.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Training Methodology
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The nemotron-colembed-vl-8b-v2, nemotron-colembed-vl-4b-v2 and llama-nemotron-colembed-vl-3b-v2 models were trained using a bi-encoder architecture, independently. This involves encoding a pair of sentences (for example, a query and a document) independently using the embedding model. Using contrastive learning, it is used to maximize the late interaction similarity between the query and the document that contains the answer, while minimizing the similarity between the query and sampled negative documents not useful to answer the question.&lt;/p&gt;
&lt;p&gt;The llama-nemotron-colembed-vl-3b-v2 model was trained in a two-stage pipeline:  it was first fine-tuned with 12.5M textQA pairs, and subsequently fine-tuned with text‚Äìimage pairs. The nemotron-colembed-vl-8b-v2,  nemotron-colembed-vl-4b-v2 models were fine-tuned using only text-image pairs (2nd stage).&lt;/p&gt;
&lt;p&gt;Our training datasets contain both text-only and text-image pairs, and we apply hard negative mining following the positive-aware hard negative mining methods presented in the NV-Retriever paper to improve retrieval performance.&lt;/p&gt;
&lt;p&gt;‚ú® &lt;strong&gt;Key Improvements over V1:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;‚öóÔ∏è Advanced Model Merging: Utilizes post-training model merging to combine the strengths of multiple fine-tuned checkpoints. This delivers the accuracy stability of an ensemble without any additional inference latency.&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;üåç Enhanced Synthetic Data: We significantly enriched our training mixture with diverse multilingual synthetic data, improving semantic alignment across languages and complex document types.&lt;/p&gt;
&lt;p&gt;&lt;img alt="modelperfs_vidorev3" src="https://cdn-uploads.huggingface.co/production/uploads/6814af6d59c3d8d6f9b09f2b/J7JIKCDriMjztO1ULw0k3.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Start Building with Nemotron ColEmbed V2
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Nemotron ColEmbed V2 models mark a major step forward in high-accuracy text‚Äìimage retrieval, delivering state-of-the-art results on the ViDoRe V1, V2, and  V3 benchmarks. The availability of 3B, 4B and 8B model variants further establishes a solid foundation for future research and advanced experimentation in multimodal retrieval applications.&lt;/p&gt;
&lt;p&gt;Get started with Nemotron ColEmbed V2 models by downloading the models: nemotron-colembed-vl-8b-v2, nemotron-colembed-vl-4b-v2 and llama-nemotron-colembed-vl-3b-v2, available on Hugging Face. Learn more about the NVIDIA NeMo Retriever family of Nemotron RAG models on the product page, or access the microservice container from NVIDIA NGC. This is an excellent opportunity to explore state-of-the-art retrieval in your own applications and workflows.&lt;/p&gt;
&lt;p&gt;Try NVIDIA Enterprise RAG Blueprint, using the Nemotron RAG models that are powered by the same tech behind our ViDoRe V3 winning. &lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/nvidia/nemotron-colembed-v2</guid><pubDate>Wed, 04 Feb 2026 15:00:40 +0000</pubDate></item><item><title>[NEW] ‚ÄãSequential Attention: Making AI models leaner and faster without sacrificing accuracy (The latest research from Google)</title><link>https://research.google/blog/sequential-attention-making-ai-models-leaner-and-faster-without-sacrificing-accuracy/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;h2&gt;The future of sequential attention&lt;/h2&gt;&lt;p&gt;As the increasing integration of AI models in science, engineering and business makes model efficiency more relevant than ever, model structure optimization is crucial for building highly effective yet efficient models. We have identified subset selection as a fundamental challenge related to model efficiency across various deep learning optimization tasks, and Sequential Attention has emerged as a pivotal technique for addressing these problems. Moving forward, we aim to extend the applications of subset selection to increasingly complex domains.&lt;/p&gt;&lt;h3&gt;Feature engineering with real constraints&lt;/h3&gt;&lt;p&gt;Sequential Attention has demonstrated significant quality gains and efficiency savings in optimizing the feature embedding layer in large embedding models (LEMs) used in recommender systems. These models typically have a large number of heterogeneous features with large embedding tables, and so the tasks of feature selection/pruning, feature cross search and embedding dimension optimization are highly impactful. In the future, we would like to allow these feature engineering tasks to take real inference constraints into account, enabling fully automated, continual feature engineering.&lt;/p&gt;&lt;h3&gt;Large language model (LLM) pruning&lt;/h3&gt;&lt;p&gt;The SequentialAttention++ paradigm is a promising direction for LLM pruning. By applying this framework we can enforce structured sparsity (e.g., block sparsity), prune redundant attention heads, embedding dimensions or entire transformer blocks, and significantly reduce model footprint and inference latency while preserving predictive performance.&lt;/p&gt;&lt;h3&gt;Drug discovery and genomics&lt;/h3&gt;&lt;p&gt;Feature selection is vital in the biological sciences. Sequential Attention can be adapted to efficiently extract influential genetic or chemical features from high-dimensional datasets, enhancing both the interpretability and accuracy of models in drug discovery and personalized medicine.&lt;/p&gt;&lt;p&gt;Current research focuses on scaling Sequential Attention to handle massive datasets and highly complex architectures more efficiently. Furthermore, ongoing efforts seek to identify superior pruned model structures and extend rigorous mathematical guarantees to real-world deep learning applications, solidifying the framework‚Äôs reliability across industries.&lt;/p&gt;&lt;p&gt;Subset selection is a core problem central to multiple optimization tasks in deep learning, while Sequential Attention is a key technique to solve these problems. In the future, we will explore more applications of subset selection to solve more challenging problems in broader domains&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;h2&gt;The future of sequential attention&lt;/h2&gt;&lt;p&gt;As the increasing integration of AI models in science, engineering and business makes model efficiency more relevant than ever, model structure optimization is crucial for building highly effective yet efficient models. We have identified subset selection as a fundamental challenge related to model efficiency across various deep learning optimization tasks, and Sequential Attention has emerged as a pivotal technique for addressing these problems. Moving forward, we aim to extend the applications of subset selection to increasingly complex domains.&lt;/p&gt;&lt;h3&gt;Feature engineering with real constraints&lt;/h3&gt;&lt;p&gt;Sequential Attention has demonstrated significant quality gains and efficiency savings in optimizing the feature embedding layer in large embedding models (LEMs) used in recommender systems. These models typically have a large number of heterogeneous features with large embedding tables, and so the tasks of feature selection/pruning, feature cross search and embedding dimension optimization are highly impactful. In the future, we would like to allow these feature engineering tasks to take real inference constraints into account, enabling fully automated, continual feature engineering.&lt;/p&gt;&lt;h3&gt;Large language model (LLM) pruning&lt;/h3&gt;&lt;p&gt;The SequentialAttention++ paradigm is a promising direction for LLM pruning. By applying this framework we can enforce structured sparsity (e.g., block sparsity), prune redundant attention heads, embedding dimensions or entire transformer blocks, and significantly reduce model footprint and inference latency while preserving predictive performance.&lt;/p&gt;&lt;h3&gt;Drug discovery and genomics&lt;/h3&gt;&lt;p&gt;Feature selection is vital in the biological sciences. Sequential Attention can be adapted to efficiently extract influential genetic or chemical features from high-dimensional datasets, enhancing both the interpretability and accuracy of models in drug discovery and personalized medicine.&lt;/p&gt;&lt;p&gt;Current research focuses on scaling Sequential Attention to handle massive datasets and highly complex architectures more efficiently. Furthermore, ongoing efforts seek to identify superior pruned model structures and extend rigorous mathematical guarantees to real-world deep learning applications, solidifying the framework‚Äôs reliability across industries.&lt;/p&gt;&lt;p&gt;Subset selection is a core problem central to multiple optimization tasks in deep learning, while Sequential Attention is a key technique to solve these problems. In the future, we will explore more applications of subset selection to solve more challenging problems in broader domains&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/sequential-attention-making-ai-models-leaner-and-faster-without-sacrificing-accuracy/</guid><pubDate>Wed, 04 Feb 2026 15:14:00 +0000</pubDate></item><item><title>Nemotron Labs: How AI Agents Are Turning Documents Into Real-Time Business Intelligence (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-agents-intelligent-document-processing/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor‚Äôs note: This post is part of the &lt;/i&gt;&lt;i&gt;Nemotron Labs&lt;/i&gt;&lt;i&gt; blog series, which explores how the latest open models, datasets and training techniques help businesses build specialized AI systems and applications on NVIDIA platforms. Each post highlights practical ways to use an open stack to deliver value in production ‚Äî from transparent research copilots to scalable AI agents.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Businesses today face the challenge of uncovering valuable insights buried within a wide variety of documents ‚Äî including reports, presentations, PDFs, web pages and spreadsheets.&lt;/p&gt;
&lt;p&gt;Often, teams piece together insights by manually reviewing files, copying data into spreadsheets, building dashboards and using basic search or template-based optical character recognition (OCR) tools that often miss important details in complex media.&lt;/p&gt;
&lt;p&gt;Intelligent document processing is an AI-powered workflow that automatically reads, understands and extracts insights from documents. It interprets rich formats inside those documents ‚Äî including tables, charts, images and text ‚Äî using AI agents and techniques like retrieval-augmented generation (RAG) to turn the multimodal content into insights that other multi-agent systems and people can easily use.&lt;/p&gt;
&lt;p&gt;With NVIDIA Nemotron open models and GPU-accelerated libraries, organizations can build AI-powered document intelligence systems for research, financial services, legal workflows and more.&lt;/p&gt;
&lt;p&gt;These open models, datasets and training recipes have powered strong results on leaderboards such as MTEB, MMTEB and ViDoRe V3, benchmarks for evaluating multilingual and multimodal retrieval models. Teams can choose from among the best models for tasks like search and question answering.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;How Document Processing Streamlines Business Intelligence&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Document intelligence systems that can pull meaning from complex layouts, scale to huge file libraries and show exactly where an answer came from are incredibly useful in high-stakes environments. These systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Understand rich document content&lt;/b&gt;, moving beyond simple text scraping to capture information from charts, tables, figures and mixed-language pages and treating documents as a human would by recognizing structure, relationships and context‚Äã‚Äã.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Handle large quantities of shifting data&lt;/b&gt;, ingesting and processing massive collections of documents in parallel, and keeping knowledge bases continuously up to date.‚Äã‚Äã&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Find exactly what users need&lt;/b&gt;, helping AI agents pinpoint the most relevant passages, tables or paragraphs to a query so they can respond with precision and accuracy.‚Äã‚Äã&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Show the evidence behind answers&lt;/b&gt; by providing citations to specific pages or charts so teams can gain transparency and auditability, which is critical in regulated industries.‚Äã‚Äã&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-medium wp-image-89600" height="384" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/nemotron-labs-infographic-960x384.jpg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;The result is a shift from static document archives to living knowledge systems that directly power business intelligence, customer experiences and operational workflows.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Document Intelligence at Work&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Intelligent document processing systems built on NVIDIA Nemotron RAG models, Nemotron Parse and accelerated computing are already reshaping how organizations across industries gain insights from their documents.‚Äã‚Äã&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Justt: AI-Native Chargeback Management and Dispute Optimization&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;In financial services, payment disputes create significant revenue loss and operational complexity for merchants, largely because the evidence needed to handle them lives in unstructured formats. Transaction logs, customer communications and policy documents are often fragmented across systems and difficult to process at scale, making dispute handling slow, manual and costly.&lt;/p&gt;
&lt;p&gt;Justt.ai provides an AI-driven platform that automates the full chargeback lifecycle at scale. The platform connects directly to payment service providers and merchant data sources to ingest transaction data, customer interactions and policies, then automatically assembles dispute-specific evidence that aligns with card network and issuer requirements.&lt;/p&gt;
&lt;p&gt;The platform‚Äôs AI-powered dispute optimization, powered by Nemotron Parse, applies predictive analytics to determine which chargebacks to fight or accept, and how to optimize each response for maximum net recovery. Leading hospitality operators like HEI Hotels &amp;amp; Resorts use the platform to automate dispute handling across their properties, recapturing revenue while maintaining guest relationships.&lt;/p&gt;
&lt;p&gt;By pairing document-centric intelligence with decision automation, merchants can recapture a significant portion of revenue lost to illegitimate chargebacks while reducing manual review effort.‚Äã&lt;/p&gt;
&lt;p&gt;Read about how Justt‚Äôs chargeback management tool autonomously processes financial data to handle disputes for merchants.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Docusign: Scaling Agreement Intelligence&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Docusign is the global leader in Intelligent Agreement Management, handling millions of transactions every day for more than 1.8 million customers and over 1 billion users.&lt;/p&gt;
&lt;p&gt;Agreements are the foundation of every business, but the critical information they contain are often buried inside pages of documents. To surface the information, Docusign needed high-fidelity extraction of tables, text and metadata from complex documents like PDFs so organizations could understand and act on obligations, risks and opportunities faster.&lt;/p&gt;
&lt;p&gt;Docusign is evaluating Nemotron Parse for deeper contract understanding at scale. Running on NVIDIA GPUs, the model combines advanced AI with layout detection and OCR. The system can reliably interpret complex tables and reconstruct tables with required information. This reduces the need for manual corrections and helps ensure that even the most complex contracts are processed with the speed and accuracy their customers expect.&lt;/p&gt;
&lt;p&gt;With this foundation, Docusign will transform agreement repositories into structured data that powers contract search, analysis and AI-driven workflows ‚Äî turning agreements into business assets that help organizations and their teams improve visibility, reduce risk and make faster decisions.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Edison Scientific: Research Across Massive Literature Scale&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Edison Scientific‚Äôs Kosmos AI Scientist helps researchers navigate complex scientific landscapes to synthesize literature, identify connections and surface evidence.‚Äã&lt;/p&gt;
&lt;p&gt;Edison needed a way to rapidly and accurately extract structured information from large volumes of PDFs, including equations, tables and figures that traditional information parsing methods often mishandle.‚Äã&lt;/p&gt;
&lt;p&gt;By integrating the NVIDIA Nemotron Parse model into its PaperQA pipeline, Edison can decompose research papers, index key concepts and ground responses in specific passages, improving both throughput and answer quality for scientists.‚Äã‚Äã This approach turns a sprawling research corpus into an interactive, queryable knowledge engine that accelerates hypothesis generation and literature review.‚Äã&lt;/p&gt;
&lt;p&gt;The high efficiency of Nemotron Parse enables cost-efficient serving at scale, allowing Edison‚Äôs team to unlock the whole multimodal pipeline.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Designing an Intelligent Document Processing Application With NVIDIA Technologies&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;A robust, domain-specific document intelligence pipeline requires technologies that can handle data extraction, embedding and reranking, while keeping the data secure and compliant with regulations.‚Äã‚Äã&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Extraction:&lt;/b&gt; Nemotron extraction and OCR models rapidly ingest multimodal PDFs, text, tables, graphs and images to convert them into structured, machine-readable content while preserving layout and semantics.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Embedding:&lt;/b&gt; Nemotron embedding models convert passages, entities and visual elements into vector representations tuned for document retrieval, enabling semantically accurate search.‚Äã‚Äã&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Reranking:&lt;/b&gt; Nemotron reranking models evaluate candidate passages to ensure the most relevant content is surfaced as context for large language models (LLMs), improving answer fidelity and reducing hallucinations.‚Äã‚Äã&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Parsing:&lt;/b&gt; Nemotron Parse models decipher document semantics to extract text and tables with precise spatial grounding and correct reading flow. Overcoming layout variability, they turn unstructured documents into actionable data that enhances the accuracy of LLMs and agentic workflows.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These capabilities are packaged as NVIDIA NIM microservices and foundation models that run efficiently on NVIDIA GPUs, allowing teams to scale from proof of concept to production while keeping sensitive data within their chosen cloud or data center environment.&lt;/p&gt;
&lt;p&gt;The most effective AI systems use a mix of frontier models and open source models like NVIDIA Nemotron, with an LLM router analyzing each task and automatically selecting the model best suited for it. This approach keeps performance strong while managing computing costs and improving efficiency.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Started With NVIDIA Nemotron&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Access a step-by-step tutorial on how to build a document processing pipeline with RAG capabilities. Explore how Nemotron RAG can power specialized agents tailored for different industries.‚Äã&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Plus, experiment with Nemotron RAG models and the NVIDIA NeMo Retriever open library, available on GitHub and Hugging Face, as well as Nemotron Parse on Hugging Face.&lt;/p&gt;
&lt;p&gt;Join the community of developers building with the NVIDIA Blueprint for Enterprise RAG ‚Äî trusted by a dozen industry-leading AI Data Platform providers and available now on build.nvidia.com, GitHub and the NGC catalog.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on agentic AI, &lt;/i&gt;&lt;i&gt;NVIDIA Nemotron&lt;/i&gt;&lt;i&gt; and more by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA AI news&lt;/i&gt;&lt;i&gt;,&lt;/i&gt;&lt;i&gt; joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore &lt;/i&gt;&lt;i&gt;self-paced video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor‚Äôs note: This post is part of the &lt;/i&gt;&lt;i&gt;Nemotron Labs&lt;/i&gt;&lt;i&gt; blog series, which explores how the latest open models, datasets and training techniques help businesses build specialized AI systems and applications on NVIDIA platforms. Each post highlights practical ways to use an open stack to deliver value in production ‚Äî from transparent research copilots to scalable AI agents.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Businesses today face the challenge of uncovering valuable insights buried within a wide variety of documents ‚Äî including reports, presentations, PDFs, web pages and spreadsheets.&lt;/p&gt;
&lt;p&gt;Often, teams piece together insights by manually reviewing files, copying data into spreadsheets, building dashboards and using basic search or template-based optical character recognition (OCR) tools that often miss important details in complex media.&lt;/p&gt;
&lt;p&gt;Intelligent document processing is an AI-powered workflow that automatically reads, understands and extracts insights from documents. It interprets rich formats inside those documents ‚Äî including tables, charts, images and text ‚Äî using AI agents and techniques like retrieval-augmented generation (RAG) to turn the multimodal content into insights that other multi-agent systems and people can easily use.&lt;/p&gt;
&lt;p&gt;With NVIDIA Nemotron open models and GPU-accelerated libraries, organizations can build AI-powered document intelligence systems for research, financial services, legal workflows and more.&lt;/p&gt;
&lt;p&gt;These open models, datasets and training recipes have powered strong results on leaderboards such as MTEB, MMTEB and ViDoRe V3, benchmarks for evaluating multilingual and multimodal retrieval models. Teams can choose from among the best models for tasks like search and question answering.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;How Document Processing Streamlines Business Intelligence&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Document intelligence systems that can pull meaning from complex layouts, scale to huge file libraries and show exactly where an answer came from are incredibly useful in high-stakes environments. These systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Understand rich document content&lt;/b&gt;, moving beyond simple text scraping to capture information from charts, tables, figures and mixed-language pages and treating documents as a human would by recognizing structure, relationships and context‚Äã‚Äã.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Handle large quantities of shifting data&lt;/b&gt;, ingesting and processing massive collections of documents in parallel, and keeping knowledge bases continuously up to date.‚Äã‚Äã&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Find exactly what users need&lt;/b&gt;, helping AI agents pinpoint the most relevant passages, tables or paragraphs to a query so they can respond with precision and accuracy.‚Äã‚Äã&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Show the evidence behind answers&lt;/b&gt; by providing citations to specific pages or charts so teams can gain transparency and auditability, which is critical in regulated industries.‚Äã‚Äã&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-medium wp-image-89600" height="384" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/nemotron-labs-infographic-960x384.jpg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;The result is a shift from static document archives to living knowledge systems that directly power business intelligence, customer experiences and operational workflows.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Document Intelligence at Work&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Intelligent document processing systems built on NVIDIA Nemotron RAG models, Nemotron Parse and accelerated computing are already reshaping how organizations across industries gain insights from their documents.‚Äã‚Äã&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Justt: AI-Native Chargeback Management and Dispute Optimization&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;In financial services, payment disputes create significant revenue loss and operational complexity for merchants, largely because the evidence needed to handle them lives in unstructured formats. Transaction logs, customer communications and policy documents are often fragmented across systems and difficult to process at scale, making dispute handling slow, manual and costly.&lt;/p&gt;
&lt;p&gt;Justt.ai provides an AI-driven platform that automates the full chargeback lifecycle at scale. The platform connects directly to payment service providers and merchant data sources to ingest transaction data, customer interactions and policies, then automatically assembles dispute-specific evidence that aligns with card network and issuer requirements.&lt;/p&gt;
&lt;p&gt;The platform‚Äôs AI-powered dispute optimization, powered by Nemotron Parse, applies predictive analytics to determine which chargebacks to fight or accept, and how to optimize each response for maximum net recovery. Leading hospitality operators like HEI Hotels &amp;amp; Resorts use the platform to automate dispute handling across their properties, recapturing revenue while maintaining guest relationships.&lt;/p&gt;
&lt;p&gt;By pairing document-centric intelligence with decision automation, merchants can recapture a significant portion of revenue lost to illegitimate chargebacks while reducing manual review effort.‚Äã&lt;/p&gt;
&lt;p&gt;Read about how Justt‚Äôs chargeback management tool autonomously processes financial data to handle disputes for merchants.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Docusign: Scaling Agreement Intelligence&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Docusign is the global leader in Intelligent Agreement Management, handling millions of transactions every day for more than 1.8 million customers and over 1 billion users.&lt;/p&gt;
&lt;p&gt;Agreements are the foundation of every business, but the critical information they contain are often buried inside pages of documents. To surface the information, Docusign needed high-fidelity extraction of tables, text and metadata from complex documents like PDFs so organizations could understand and act on obligations, risks and opportunities faster.&lt;/p&gt;
&lt;p&gt;Docusign is evaluating Nemotron Parse for deeper contract understanding at scale. Running on NVIDIA GPUs, the model combines advanced AI with layout detection and OCR. The system can reliably interpret complex tables and reconstruct tables with required information. This reduces the need for manual corrections and helps ensure that even the most complex contracts are processed with the speed and accuracy their customers expect.&lt;/p&gt;
&lt;p&gt;With this foundation, Docusign will transform agreement repositories into structured data that powers contract search, analysis and AI-driven workflows ‚Äî turning agreements into business assets that help organizations and their teams improve visibility, reduce risk and make faster decisions.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Edison Scientific: Research Across Massive Literature Scale&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Edison Scientific‚Äôs Kosmos AI Scientist helps researchers navigate complex scientific landscapes to synthesize literature, identify connections and surface evidence.‚Äã&lt;/p&gt;
&lt;p&gt;Edison needed a way to rapidly and accurately extract structured information from large volumes of PDFs, including equations, tables and figures that traditional information parsing methods often mishandle.‚Äã&lt;/p&gt;
&lt;p&gt;By integrating the NVIDIA Nemotron Parse model into its PaperQA pipeline, Edison can decompose research papers, index key concepts and ground responses in specific passages, improving both throughput and answer quality for scientists.‚Äã‚Äã This approach turns a sprawling research corpus into an interactive, queryable knowledge engine that accelerates hypothesis generation and literature review.‚Äã&lt;/p&gt;
&lt;p&gt;The high efficiency of Nemotron Parse enables cost-efficient serving at scale, allowing Edison‚Äôs team to unlock the whole multimodal pipeline.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Designing an Intelligent Document Processing Application With NVIDIA Technologies&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;A robust, domain-specific document intelligence pipeline requires technologies that can handle data extraction, embedding and reranking, while keeping the data secure and compliant with regulations.‚Äã‚Äã&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Extraction:&lt;/b&gt; Nemotron extraction and OCR models rapidly ingest multimodal PDFs, text, tables, graphs and images to convert them into structured, machine-readable content while preserving layout and semantics.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Embedding:&lt;/b&gt; Nemotron embedding models convert passages, entities and visual elements into vector representations tuned for document retrieval, enabling semantically accurate search.‚Äã‚Äã&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Reranking:&lt;/b&gt; Nemotron reranking models evaluate candidate passages to ensure the most relevant content is surfaced as context for large language models (LLMs), improving answer fidelity and reducing hallucinations.‚Äã‚Äã&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Parsing:&lt;/b&gt; Nemotron Parse models decipher document semantics to extract text and tables with precise spatial grounding and correct reading flow. Overcoming layout variability, they turn unstructured documents into actionable data that enhances the accuracy of LLMs and agentic workflows.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These capabilities are packaged as NVIDIA NIM microservices and foundation models that run efficiently on NVIDIA GPUs, allowing teams to scale from proof of concept to production while keeping sensitive data within their chosen cloud or data center environment.&lt;/p&gt;
&lt;p&gt;The most effective AI systems use a mix of frontier models and open source models like NVIDIA Nemotron, with an LLM router analyzing each task and automatically selecting the model best suited for it. This approach keeps performance strong while managing computing costs and improving efficiency.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Started With NVIDIA Nemotron&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Access a step-by-step tutorial on how to build a document processing pipeline with RAG capabilities. Explore how Nemotron RAG can power specialized agents tailored for different industries.‚Äã&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Plus, experiment with Nemotron RAG models and the NVIDIA NeMo Retriever open library, available on GitHub and Hugging Face, as well as Nemotron Parse on Hugging Face.&lt;/p&gt;
&lt;p&gt;Join the community of developers building with the NVIDIA Blueprint for Enterprise RAG ‚Äî trusted by a dozen industry-leading AI Data Platform providers and available now on build.nvidia.com, GitHub and the NGC catalog.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on agentic AI, &lt;/i&gt;&lt;i&gt;NVIDIA Nemotron&lt;/i&gt;&lt;i&gt; and more by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA AI news&lt;/i&gt;&lt;i&gt;,&lt;/i&gt;&lt;i&gt; joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore &lt;/i&gt;&lt;i&gt;self-paced video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-agents-intelligent-document-processing/</guid><pubDate>Wed, 04 Feb 2026 16:00:36 +0000</pubDate></item><item><title>AI Expo 2026 Day 1: Governance and data readiness enable the agentic enterprise (AI News)</title><link>https://www.artificialintelligence-news.com/news/ai-expo-2026-day-1-governance-data-readiness-enable-agentic-enterprise/</link><description>&lt;p&gt;While the prospect of AI acting as a digital co-worker dominated the day one agenda at the co-located AI &amp;amp; Big Data Expo and Intelligent Automation Conference, the technical sessions focused on the infrastructure to make it work.&lt;/p&gt;&lt;p&gt;A primary topic on the exhibition floor was the progression from passive automation to ‚Äúagentic‚Äù systems. These tools reason, plan, and execute tasks rather than following rigid scripts. Amal Makwana from Citi detailed how these systems act across enterprise workflows. This capability separates them from earlier robotic process automation (RPA).&lt;/p&gt;&lt;p&gt;Scott Ivell and Ire Adewolu of DeepL described this development as closing the ‚Äúautomation gap‚Äù. They argued that agentic AI functions as a digital co-worker rather than a simple tool. Real value is unlocked by reducing the distance between intent and execution. Brian Halpin from SS&amp;amp;C Blue Prism noted that organisations typically must master standard automation before they can deploy agentic AI.&lt;/p&gt;&lt;p&gt;This change requires governance frameworks capable of handling non-deterministic outcomes. Steve Holyer of Informatica, alongside speakers from MuleSoft and Salesforce, argued that architecting these systems requires strict oversight. A governance layer must control how agents access and utilise data to prevent operational failure.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-data-quality-blocks-deployment"&gt;Data quality blocks deployment&lt;/h3&gt;&lt;p&gt;The output of an autonomous system relies on the quality of its input. Andreas Krause from SAP stated that AI fails without trusted, connected enterprise data. For GenAI to function in a corporate context, it must access data that is both accurate and contextually-relevant.&lt;/p&gt;&lt;p&gt;Meni Meller of Gigaspaces addressed the technical challenge of ‚Äúhallucinations‚Äù in LLMs. He advocated for the use of eRAG (retrieval-augmented generation) combined with semantic layers to fix data access issues. This approach allows models to retrieve factual enterprise data in real-time.&lt;/p&gt;&lt;p&gt;Storage and analysis also present challenges. A panel featuring representatives from Equifax, British Gas, and Centrica discussed the necessity of cloud-native, real-time analytics. For these organisations, competitive advantage comes from the ability to execute analytics strategies that are scalable and immediate.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-physical-safety-and-observability"&gt;Physical safety and observability&lt;/h3&gt;&lt;p&gt;The integration of AI extends into physical environments, introducing safety risks that differ from software failures. A panel including Edith-Clare Hall from ARIA and Matthew Howard from IEEE RAS examined how embodied AI is deployed in factories, offices, and public spaces. Safety protocols must be established &lt;em&gt;before&lt;/em&gt; robots interact with humans.&lt;/p&gt;&lt;p&gt;Perla Maiolino from the Oxford Robotics Institute provided a technical perspective on this challenge. Her research into Time-of-Flight (ToF) sensors and electronic skin aims to give robots both self-awareness and environmental awareness. For industries such as manufacturing and logistics, these integrated perception systems prevent accidents.&lt;/p&gt;&lt;p&gt;In software development, observability remains a parallel concern. Yulia Samoylova from Datadog highlighted how AI changes the way teams build and troubleshoot software. As systems become more autonomous, the ability to observe their internal state and reasoning processes becomes necessary for reliability.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-infrastructure-and-adoption-barriers"&gt;Infrastructure and adoption barriers&lt;/h3&gt;&lt;p&gt;Implementation demands reliable infrastructure and a receptive culture. Julian Skeels from Expereo argued that networks must be designed specifically for AI workloads. This involves building sovereign, secure, and ‚Äúalways-on‚Äù network fabrics capable of handling high throughput.&lt;/p&gt;&lt;p&gt;Of course, the human element remains unpredictable. Paul Fermor from IBM Automation warned that traditional automation thinking often underestimates the complexity of AI adoption. He termed this the ‚Äúillusion of AI readiness‚Äù. Jena Miller reinforced this point, noting that strategies must be human-centred to ensure adoption. If the workforce does not trust the tools, the technology yields no return.&lt;/p&gt;&lt;p&gt;Ravi Jay from Sanofi suggested that leaders need to ask operational and ethical questions early on in the process. Success depends on deciding where to build proprietary solutions versus where to buy established platforms.&lt;/p&gt;&lt;p&gt;The sessions from day one of the co-located events indicate that, while technology is moving toward autonomous agents, deployment requires a solid data foundation.&lt;/p&gt;&lt;p&gt;CIOs should focus on establishing data governance frameworks that support retrieval-augmented generation. Network infrastructure must be evaluated to ensure it supports the latency requirements of agentic workloads. Finally, cultural adoption strategies must run parallel to technical implementation.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111908" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-3.png" width="728" /&gt;&lt;/figure&gt;</description><content:encoded>&lt;p&gt;While the prospect of AI acting as a digital co-worker dominated the day one agenda at the co-located AI &amp;amp; Big Data Expo and Intelligent Automation Conference, the technical sessions focused on the infrastructure to make it work.&lt;/p&gt;&lt;p&gt;A primary topic on the exhibition floor was the progression from passive automation to ‚Äúagentic‚Äù systems. These tools reason, plan, and execute tasks rather than following rigid scripts. Amal Makwana from Citi detailed how these systems act across enterprise workflows. This capability separates them from earlier robotic process automation (RPA).&lt;/p&gt;&lt;p&gt;Scott Ivell and Ire Adewolu of DeepL described this development as closing the ‚Äúautomation gap‚Äù. They argued that agentic AI functions as a digital co-worker rather than a simple tool. Real value is unlocked by reducing the distance between intent and execution. Brian Halpin from SS&amp;amp;C Blue Prism noted that organisations typically must master standard automation before they can deploy agentic AI.&lt;/p&gt;&lt;p&gt;This change requires governance frameworks capable of handling non-deterministic outcomes. Steve Holyer of Informatica, alongside speakers from MuleSoft and Salesforce, argued that architecting these systems requires strict oversight. A governance layer must control how agents access and utilise data to prevent operational failure.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-data-quality-blocks-deployment"&gt;Data quality blocks deployment&lt;/h3&gt;&lt;p&gt;The output of an autonomous system relies on the quality of its input. Andreas Krause from SAP stated that AI fails without trusted, connected enterprise data. For GenAI to function in a corporate context, it must access data that is both accurate and contextually-relevant.&lt;/p&gt;&lt;p&gt;Meni Meller of Gigaspaces addressed the technical challenge of ‚Äúhallucinations‚Äù in LLMs. He advocated for the use of eRAG (retrieval-augmented generation) combined with semantic layers to fix data access issues. This approach allows models to retrieve factual enterprise data in real-time.&lt;/p&gt;&lt;p&gt;Storage and analysis also present challenges. A panel featuring representatives from Equifax, British Gas, and Centrica discussed the necessity of cloud-native, real-time analytics. For these organisations, competitive advantage comes from the ability to execute analytics strategies that are scalable and immediate.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-physical-safety-and-observability"&gt;Physical safety and observability&lt;/h3&gt;&lt;p&gt;The integration of AI extends into physical environments, introducing safety risks that differ from software failures. A panel including Edith-Clare Hall from ARIA and Matthew Howard from IEEE RAS examined how embodied AI is deployed in factories, offices, and public spaces. Safety protocols must be established &lt;em&gt;before&lt;/em&gt; robots interact with humans.&lt;/p&gt;&lt;p&gt;Perla Maiolino from the Oxford Robotics Institute provided a technical perspective on this challenge. Her research into Time-of-Flight (ToF) sensors and electronic skin aims to give robots both self-awareness and environmental awareness. For industries such as manufacturing and logistics, these integrated perception systems prevent accidents.&lt;/p&gt;&lt;p&gt;In software development, observability remains a parallel concern. Yulia Samoylova from Datadog highlighted how AI changes the way teams build and troubleshoot software. As systems become more autonomous, the ability to observe their internal state and reasoning processes becomes necessary for reliability.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-infrastructure-and-adoption-barriers"&gt;Infrastructure and adoption barriers&lt;/h3&gt;&lt;p&gt;Implementation demands reliable infrastructure and a receptive culture. Julian Skeels from Expereo argued that networks must be designed specifically for AI workloads. This involves building sovereign, secure, and ‚Äúalways-on‚Äù network fabrics capable of handling high throughput.&lt;/p&gt;&lt;p&gt;Of course, the human element remains unpredictable. Paul Fermor from IBM Automation warned that traditional automation thinking often underestimates the complexity of AI adoption. He termed this the ‚Äúillusion of AI readiness‚Äù. Jena Miller reinforced this point, noting that strategies must be human-centred to ensure adoption. If the workforce does not trust the tools, the technology yields no return.&lt;/p&gt;&lt;p&gt;Ravi Jay from Sanofi suggested that leaders need to ask operational and ethical questions early on in the process. Success depends on deciding where to build proprietary solutions versus where to buy established platforms.&lt;/p&gt;&lt;p&gt;The sessions from day one of the co-located events indicate that, while technology is moving toward autonomous agents, deployment requires a solid data foundation.&lt;/p&gt;&lt;p&gt;CIOs should focus on establishing data governance frameworks that support retrieval-augmented generation. Network infrastructure must be evaluated to ensure it supports the latency requirements of agentic workloads. Finally, cultural adoption strategies must run parallel to technical implementation.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111908" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-3.png" width="728" /&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ai-expo-2026-day-1-governance-data-readiness-enable-agentic-enterprise/</guid><pubDate>Wed, 04 Feb 2026 16:33:34 +0000</pubDate></item><item><title>Roblox‚Äôs 4D creation feature is now available in open beta (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/robloxs-4d-creation-feature-is-now-available-in-open-beta/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last year, Roblox launched an open source AI model that could generate 3D objects on the platform, helping users quickly design digital items such as furniture, vehicles, and accessories. The company claims the tool, called Cube 3D, has so far helped users generate over 1.8 million 3D objects since it was rolled out last March.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday, the company launched the open beta for its anticipated 4D creation feature that lets creators make not just static 3D models, but fully functional and interactive objects. The feature has been in early access since November.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Roblox says 4D creation adds an important new layer: interactivity. With this technology, users can design items that can move and react to players in the game.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3088899" height="408" src="https://techcrunch.com/wp-content/uploads/2026/02/Roblox-4D-Creations.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Roblox&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄãAt launch, there are two types of object templates (called schemas) that creators can try out.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first is the ‚ÄúCar-5‚Äù schema, which is used to create a car made of five separate parts: the main body and four wheels. Previously, cars were a single, solid 3D object that couldn‚Äôt move. The new system breaks down objects into parts and assigns behaviors to each so that they function individually within the virtual world. The AI therefore can generate cars with spinning wheels, making them more realistic and interactive. ‚Äã&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The second is called ‚ÄúBody-1,‚Äù which can generate any object made from a single piece, like a simple box or sculpture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first experience with 4D generation is a game called Wish Master, where players can generate cars they can drive, planes they can fly, and even dragons.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄãIn the future, Roblox plans to let creators make their own schemas so they‚Äôll have more freedom to define how objects behave. The company says it is also developing new technology that could use a reference image to create a detailed 3D model that matches the image‚Äôs style (example below).&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3088898" height="292" src="https://techcrunch.com/wp-content/uploads/2026/02/Upsample-3D-Models.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Roblox&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it is developing more ways to help people create games and experiences using AI, including a project it has dubbed ‚Äúreal-time dreaming.‚Äù Roblox CEO David Baszucki last month explained that this project would let creators build new worlds using ‚Äúkeyboard navigation and sharing real-time text prompts.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The open beta comes on the heels of Roblox‚Äôs recent implementation of mandatory facial verification for users to access chat features in the game, following lawsuits and investigations related to child safety.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last year, Roblox launched an open source AI model that could generate 3D objects on the platform, helping users quickly design digital items such as furniture, vehicles, and accessories. The company claims the tool, called Cube 3D, has so far helped users generate over 1.8 million 3D objects since it was rolled out last March.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday, the company launched the open beta for its anticipated 4D creation feature that lets creators make not just static 3D models, but fully functional and interactive objects. The feature has been in early access since November.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Roblox says 4D creation adds an important new layer: interactivity. With this technology, users can design items that can move and react to players in the game.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3088899" height="408" src="https://techcrunch.com/wp-content/uploads/2026/02/Roblox-4D-Creations.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Roblox&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄãAt launch, there are two types of object templates (called schemas) that creators can try out.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first is the ‚ÄúCar-5‚Äù schema, which is used to create a car made of five separate parts: the main body and four wheels. Previously, cars were a single, solid 3D object that couldn‚Äôt move. The new system breaks down objects into parts and assigns behaviors to each so that they function individually within the virtual world. The AI therefore can generate cars with spinning wheels, making them more realistic and interactive. ‚Äã&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The second is called ‚ÄúBody-1,‚Äù which can generate any object made from a single piece, like a simple box or sculpture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first experience with 4D generation is a game called Wish Master, where players can generate cars they can drive, planes they can fly, and even dragons.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄãIn the future, Roblox plans to let creators make their own schemas so they‚Äôll have more freedom to define how objects behave. The company says it is also developing new technology that could use a reference image to create a detailed 3D model that matches the image‚Äôs style (example below).&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3088898" height="292" src="https://techcrunch.com/wp-content/uploads/2026/02/Upsample-3D-Models.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Roblox&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it is developing more ways to help people create games and experiences using AI, including a project it has dubbed ‚Äúreal-time dreaming.‚Äù Roblox CEO David Baszucki last month explained that this project would let creators build new worlds using ‚Äúkeyboard navigation and sharing real-time text prompts.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The open beta comes on the heels of Roblox‚Äôs recent implementation of mandatory facial verification for users to access chat features in the game, following lawsuits and investigations related to child safety.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/robloxs-4d-creation-feature-is-now-available-in-open-beta/</guid><pubDate>Wed, 04 Feb 2026 17:00:00 +0000</pubDate></item><item><title>3 Questions: Using AI to accelerate the discovery and design of therapeutic drugs (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/3-questions-using-ai-to-accelerate-discovery-design-therapeutic-drugs-0204</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/jim-collins-mit-00.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;&lt;em&gt;In the pursuit of solutions to complex global challenges including disease, energy demands, and climate change, scientific researchers, including at MIT, have turned to artificial intelligence, and to quantitative analysis and modeling, to design and construct engineered cells with novel properties. The engineered cells can be programmed to become new therapeutics ‚Äî battling, and perhaps eradicating, diseases.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;James J. Collins&lt;/em&gt;&lt;em&gt; is one of the founders of the field of synthetic biology, and is also a leading researcher in systems biology, the interdisciplinary approach that uses mathematical analysis and modeling of complex systems to better understand biological systems. His research has led to the development of new classes of diagnostics and therapeutics, including in the detection and treatment of pathogens like Ebola, Zika, SARS-CoV-2, and antibiotic-resistant bacteria. Collins, the Termeer Professor of Medical Engineering and Science and professor of biological engineering at MIT, is a core faculty member of the Institute for Medical Engineering and Science (IMES), the director of the MIT Abdul Latif Jameel Clinic for Machine Learning in Health, as well as an institute member of the Broad Institute of MIT and Harvard, and core founding faculty at the Wyss Institute for Biologically Inspired Engineering, Harvard.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;In this Q&amp;amp;A, Collins speaks about his latest work and goals for this research.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q.&amp;nbsp;&lt;/strong&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;You‚Äôre known for collaborating with colleagues across MIT, and at other institutions. How have these collaborations and affiliations helped you with your research?&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;Collaboration has been central to the work in my lab. At the MIT Jameel Clinic for Machine Learning in Health, I formed a collaboration with Regina Barzilay [the Delta Electronics Professor in the MIT Department of Electrical Engineering and Computer Science and affiliate faculty member at IMES] and Tommi Jaakkola [the Thomas Siebel Professor of Electrical Engineering and Computer Science and the Institute for Data, Systems, and Society] to use deep learning to discover new antibiotics. This effort combined our expertise in artificial intelligence, network biology, and systems microbiology, leading to the discovery of halicin, a potent new antibiotic effective against a broad range of multidrug-resistant bacterial pathogens. Our results were published in &lt;em&gt;Cell&lt;/em&gt; in 2020 and showcased the power of bringing together complementary skill sets to tackle a global health challenge.&lt;/p&gt;&lt;p&gt;At the Wyss Institute, I‚Äôve worked closely with Donald Ingber [the Judah Folkman Professor of Vascular Biology at Harvard Medical School and the Vascular Biology Program at Boston Children‚Äôs Hospital, and Hansj√∂rg Wyss Professor of Biologically Inspired Engineering at Harvard], leveraging his organs-on-chips technology to test the efficacy of AI-discovered and AI-generated antibiotics. These platforms allow us to study how drugs behave in human tissue-like environments, complementing traditional animal experiments and providing a more nuanced view of their therapeutic potential.&lt;/p&gt;&lt;p&gt;The common thread across our many collaborations is the ability to combine computational predictions with cutting-edge experimental platforms, accelerating the path from ideas to validated new therapies.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q.&amp;nbsp;&lt;/strong&gt;Your research has led to many advances in designing novel antibiotics, using generative AI and deep learning. Can you talk about some of the advances you‚Äôve been a part of in the development of drugs that can battle multi-drug-resistant pathogens, and what you see on the horizon for breakthroughs in this arena?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;In 2025, our lab published a study in&amp;nbsp;&lt;em&gt;Cell&lt;/em&gt; demonstrating how generative AI can be used to design completely new antibiotics from scratch. We used genetic algorithms and variational autoencoders to generate millions of candidate molecules, exploring both fragment-based designs and entirely unconstrained chemical space. After computational filtering, retrosynthetic modeling, and medicinal chemistry review, we synthesized 24 compounds and tested them experimentally. Seven showed selective antibacterial activity. One lead, NG1, was highly narrow-spectrum, eradicating multi-drug-resistant&amp;nbsp;&lt;em&gt;Neisseria gonorrhoeae&lt;/em&gt;, including strains resistant to first-line therapies, while sparing commensal species. Another, DN1, targeted methicillin-resistant &lt;em&gt;Staphylococcus aureus&lt;/em&gt; (MRSA) and cleared infections in mice through broad membrane disruption. Both were non-toxic and showed low rates of resistance.&lt;/p&gt;&lt;p&gt;Looking ahead, we are using deep learning to design antibiotics with drug-like properties that make them stronger candidates for clinical development. By integrating AI with high-throughput biological testing, we aim to accelerate the discovery and design of antibiotics that are novel, safe, and effective, ready for real-world therapeutic use. This approach could transform how we respond to drug-resistant bacterial pathogens, moving from a reactive to a proactive strategy in antibiotic development.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q.&amp;nbsp;&lt;/strong&gt;You‚Äôre a co-founder of Phare Bio, a nonprofit organization that uses AI to discover new antibiotics, and the Collins Lab has helped to launch the Antibiotics-AI Project in collaboration with Phare Bio. Can you tell us more about what you hope to accomplish with these collaborations, and how they tie back to your research goals?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;We founded Phare Bio as a nonprofit to take the most promising antibiotic candidates emerging from the Antibiotics-AI Project at MIT and advance them toward the clinic. The idea is to bridge the gap between discovery and development by collaborating with biotech companies, pharmaceutical partners, AI companies, philanthropies, other nonprofits, and even nation states. Akhila Kosaraju has been doing a brilliant job leading Phare Bio, coordinating these efforts and moving candidates forward efficiently.&lt;/p&gt;&lt;p&gt;Recently, we received a grant from ARPA-H to use generative AI to design 15 new antibiotics and develop them as pre-clinical candidates. This project builds directly on our lab‚Äôs research, combining computational design with experimental testing to create novel antibiotics that are ready for further development. By integrating generative AI, biology, and translational partnerships, we hope to create a pipeline that can respond more rapidly to the global threat of antibiotic resistance, ultimately delivering new therapies to patients who need them most.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/jim-collins-mit-00.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;&lt;em&gt;In the pursuit of solutions to complex global challenges including disease, energy demands, and climate change, scientific researchers, including at MIT, have turned to artificial intelligence, and to quantitative analysis and modeling, to design and construct engineered cells with novel properties. The engineered cells can be programmed to become new therapeutics ‚Äî battling, and perhaps eradicating, diseases.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;James J. Collins&lt;/em&gt;&lt;em&gt; is one of the founders of the field of synthetic biology, and is also a leading researcher in systems biology, the interdisciplinary approach that uses mathematical analysis and modeling of complex systems to better understand biological systems. His research has led to the development of new classes of diagnostics and therapeutics, including in the detection and treatment of pathogens like Ebola, Zika, SARS-CoV-2, and antibiotic-resistant bacteria. Collins, the Termeer Professor of Medical Engineering and Science and professor of biological engineering at MIT, is a core faculty member of the Institute for Medical Engineering and Science (IMES), the director of the MIT Abdul Latif Jameel Clinic for Machine Learning in Health, as well as an institute member of the Broad Institute of MIT and Harvard, and core founding faculty at the Wyss Institute for Biologically Inspired Engineering, Harvard.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;In this Q&amp;amp;A, Collins speaks about his latest work and goals for this research.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q.&amp;nbsp;&lt;/strong&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;You‚Äôre known for collaborating with colleagues across MIT, and at other institutions. How have these collaborations and affiliations helped you with your research?&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;Collaboration has been central to the work in my lab. At the MIT Jameel Clinic for Machine Learning in Health, I formed a collaboration with Regina Barzilay [the Delta Electronics Professor in the MIT Department of Electrical Engineering and Computer Science and affiliate faculty member at IMES] and Tommi Jaakkola [the Thomas Siebel Professor of Electrical Engineering and Computer Science and the Institute for Data, Systems, and Society] to use deep learning to discover new antibiotics. This effort combined our expertise in artificial intelligence, network biology, and systems microbiology, leading to the discovery of halicin, a potent new antibiotic effective against a broad range of multidrug-resistant bacterial pathogens. Our results were published in &lt;em&gt;Cell&lt;/em&gt; in 2020 and showcased the power of bringing together complementary skill sets to tackle a global health challenge.&lt;/p&gt;&lt;p&gt;At the Wyss Institute, I‚Äôve worked closely with Donald Ingber [the Judah Folkman Professor of Vascular Biology at Harvard Medical School and the Vascular Biology Program at Boston Children‚Äôs Hospital, and Hansj√∂rg Wyss Professor of Biologically Inspired Engineering at Harvard], leveraging his organs-on-chips technology to test the efficacy of AI-discovered and AI-generated antibiotics. These platforms allow us to study how drugs behave in human tissue-like environments, complementing traditional animal experiments and providing a more nuanced view of their therapeutic potential.&lt;/p&gt;&lt;p&gt;The common thread across our many collaborations is the ability to combine computational predictions with cutting-edge experimental platforms, accelerating the path from ideas to validated new therapies.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q.&amp;nbsp;&lt;/strong&gt;Your research has led to many advances in designing novel antibiotics, using generative AI and deep learning. Can you talk about some of the advances you‚Äôve been a part of in the development of drugs that can battle multi-drug-resistant pathogens, and what you see on the horizon for breakthroughs in this arena?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;In 2025, our lab published a study in&amp;nbsp;&lt;em&gt;Cell&lt;/em&gt; demonstrating how generative AI can be used to design completely new antibiotics from scratch. We used genetic algorithms and variational autoencoders to generate millions of candidate molecules, exploring both fragment-based designs and entirely unconstrained chemical space. After computational filtering, retrosynthetic modeling, and medicinal chemistry review, we synthesized 24 compounds and tested them experimentally. Seven showed selective antibacterial activity. One lead, NG1, was highly narrow-spectrum, eradicating multi-drug-resistant&amp;nbsp;&lt;em&gt;Neisseria gonorrhoeae&lt;/em&gt;, including strains resistant to first-line therapies, while sparing commensal species. Another, DN1, targeted methicillin-resistant &lt;em&gt;Staphylococcus aureus&lt;/em&gt; (MRSA) and cleared infections in mice through broad membrane disruption. Both were non-toxic and showed low rates of resistance.&lt;/p&gt;&lt;p&gt;Looking ahead, we are using deep learning to design antibiotics with drug-like properties that make them stronger candidates for clinical development. By integrating AI with high-throughput biological testing, we aim to accelerate the discovery and design of antibiotics that are novel, safe, and effective, ready for real-world therapeutic use. This approach could transform how we respond to drug-resistant bacterial pathogens, moving from a reactive to a proactive strategy in antibiotic development.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q.&amp;nbsp;&lt;/strong&gt;You‚Äôre a co-founder of Phare Bio, a nonprofit organization that uses AI to discover new antibiotics, and the Collins Lab has helped to launch the Antibiotics-AI Project in collaboration with Phare Bio. Can you tell us more about what you hope to accomplish with these collaborations, and how they tie back to your research goals?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;We founded Phare Bio as a nonprofit to take the most promising antibiotic candidates emerging from the Antibiotics-AI Project at MIT and advance them toward the clinic. The idea is to bridge the gap between discovery and development by collaborating with biotech companies, pharmaceutical partners, AI companies, philanthropies, other nonprofits, and even nation states. Akhila Kosaraju has been doing a brilliant job leading Phare Bio, coordinating these efforts and moving candidates forward efficiently.&lt;/p&gt;&lt;p&gt;Recently, we received a grant from ARPA-H to use generative AI to design 15 new antibiotics and develop them as pre-clinical candidates. This project builds directly on our lab‚Äôs research, combining computational design with experimental testing to create novel antibiotics that are ready for further development. By integrating generative AI, biology, and translational partnerships, we hope to create a pipeline that can respond more rapidly to the global threat of antibiotic resistance, ultimately delivering new therapies to patients who need them most.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/3-questions-using-ai-to-accelerate-discovery-design-therapeutic-drugs-0204</guid><pubDate>Wed, 04 Feb 2026 18:00:00 +0000</pubDate></item><item><title>Tinder looks to AI to help fight ‚Äòswipe fatigue‚Äô and dating app burnout (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/tinder-looks-to-ai-to-help-fight-swipe-fatigue-and-dating-app-burnout/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2197117084.jpg?resize=1200,843" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Tinder is turning to a new AI-powered feature, Chemistry, to help it reduce so-called ‚Äúswipe fatigue,‚Äù a growing problem among online dating users who are feeling burned out and are in search of better outcomes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced last quarter, the Match-owned dating app said that Chemistry leverages AI to get to know users through questions and, with permission, accesses their Camera Roll on their phone to learn more about their interests and personality.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Match‚Äôs Q4 2026 earnings call, one analyst from Morgan Stanley asked for an update on the product‚Äôs success so far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Match CEO Spencer Rascoff noted that Chemistry was still only being tested in Australia for the time being, but said that the feature offered users an ‚ÄúAI way to interact with Tinder.‚Äù He explained that users could choose to answer questions to then ‚Äúget just a single drop or two, rather than swiping through many, many profiles.‚Äù &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to Chemistry‚Äôs Q&amp;amp;A and Camera Roll features, the company plans to use the AI feature in other ways going forward, the CEO also hinted.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Most importantly, Rascoff said the feature is designed to combat swipe fatigue ‚Äî a complaint from users who say they have to swipe through too many profiles to find a potential match. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company‚Äôs turn toward AI comes as Tinder and other dating apps have been experiencing paying subscriber declines, user burnout, and declines in new sign-ups.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In the fourth quarter, new registrations on Tinder were still down 5% year-over-year, and its monthly active users were down 9%. These numbers show some slight improvements over prior quarters, which Match attributes to AI-driven recommendations that change the order of profiles shown to women, and other product experiments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Match said that this year, it aims to address common Gen Z pain points, including better relevance, authenticity, and trust. To do so, the company said it is redesigning discovery to make it less repetitive and is using other features, like Face Check ‚Äî a facial recognition verification system ‚Äî to cut down on bad actors. On Tinder, the latter led to a more than 50% reduction in interactions with bad actors, Match noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tinder‚Äôs decision to start moving away from the swipe toward more targeted, AI-powered recommendations could have a significant impact on the dating app. Today, the swipe method, which was popularized by Tinder, encourages users to think that they‚Äôre choosing a match from an endless number of profiles. But in reality, the app presents the illusion of choice, since matches have to be two-way to connect, and even then, a spark is not guaranteed.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company delivered an earnings beat in the fourth quarter, with revenue of $878 million and EPS of 83 cents per share, above Wall Street estimates. But weak guidance saw the stock decline on Tuesday, before rising again in premarket trading on Wednesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond AI, Match will also increase its product marketing to help boost Tinder engagement. The company is committing to $50 million in Tinder marketing spend, which will include creator campaigns on TikTok and Instagram, where users will make claims that ‚ÄúTinder is cool again,‚Äù Rascoff noted. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2197117084.jpg?resize=1200,843" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Tinder is turning to a new AI-powered feature, Chemistry, to help it reduce so-called ‚Äúswipe fatigue,‚Äù a growing problem among online dating users who are feeling burned out and are in search of better outcomes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced last quarter, the Match-owned dating app said that Chemistry leverages AI to get to know users through questions and, with permission, accesses their Camera Roll on their phone to learn more about their interests and personality.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Match‚Äôs Q4 2026 earnings call, one analyst from Morgan Stanley asked for an update on the product‚Äôs success so far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Match CEO Spencer Rascoff noted that Chemistry was still only being tested in Australia for the time being, but said that the feature offered users an ‚ÄúAI way to interact with Tinder.‚Äù He explained that users could choose to answer questions to then ‚Äúget just a single drop or two, rather than swiping through many, many profiles.‚Äù &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to Chemistry‚Äôs Q&amp;amp;A and Camera Roll features, the company plans to use the AI feature in other ways going forward, the CEO also hinted.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Most importantly, Rascoff said the feature is designed to combat swipe fatigue ‚Äî a complaint from users who say they have to swipe through too many profiles to find a potential match. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company‚Äôs turn toward AI comes as Tinder and other dating apps have been experiencing paying subscriber declines, user burnout, and declines in new sign-ups.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In the fourth quarter, new registrations on Tinder were still down 5% year-over-year, and its monthly active users were down 9%. These numbers show some slight improvements over prior quarters, which Match attributes to AI-driven recommendations that change the order of profiles shown to women, and other product experiments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Match said that this year, it aims to address common Gen Z pain points, including better relevance, authenticity, and trust. To do so, the company said it is redesigning discovery to make it less repetitive and is using other features, like Face Check ‚Äî a facial recognition verification system ‚Äî to cut down on bad actors. On Tinder, the latter led to a more than 50% reduction in interactions with bad actors, Match noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tinder‚Äôs decision to start moving away from the swipe toward more targeted, AI-powered recommendations could have a significant impact on the dating app. Today, the swipe method, which was popularized by Tinder, encourages users to think that they‚Äôre choosing a match from an endless number of profiles. But in reality, the app presents the illusion of choice, since matches have to be two-way to connect, and even then, a spark is not guaranteed.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company delivered an earnings beat in the fourth quarter, with revenue of $878 million and EPS of 83 cents per share, above Wall Street estimates. But weak guidance saw the stock decline on Tuesday, before rising again in premarket trading on Wednesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond AI, Match will also increase its product marketing to help boost Tinder engagement. The company is committing to $50 million in Tinder marketing spend, which will include creator campaigns on TikTok and Instagram, where users will make claims that ‚ÄúTinder is cool again,‚Äù Rascoff noted. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/tinder-looks-to-ai-to-help-fight-swipe-fatigue-and-dating-app-burnout/</guid><pubDate>Wed, 04 Feb 2026 18:08:00 +0000</pubDate></item><item><title>Antonio Torralba, three MIT alumni named 2025 ACM fellows (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/antonio-torralba-three-mit-alumni-named-acm-fellows-0204</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202601/mit-eecs-Torralba-Antonio.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Antonio Torralba, Delta Electronics Professor of Electrical Engineering and Computer Science and faculty head of artificial intelligence and decision-making at MIT, has been named to the 2025 cohort of Association for Computing Machinery (ACM) Fellows. He shares the honor of an ACM Fellowship with three MIT alumni: Eytan Adar ‚Äô97, MEng ‚Äô98; George Candea ‚Äô97, MEng ‚Äô98; and Gookwon Edward Suh SM ‚Äô01, PhD ‚Äô05.&lt;/p&gt;&lt;p dir="ltr"&gt;A principal investigator within both the Computer Science and Artificial Intelligence Laboratory and the Center for Brains, Minds, and Machines, Torralba received his BS in telecommunications engineering from Telecom BCN, Spain, in 1994, and a PhD in signal, image, and speech processing from the Institut National Polytechnique de Grenoble, France, in 2000. At different points in his MIT career, he has been director of both the MIT Quest for Intelligence (now the MIT Siegel Family Quest for Intelligence) and the MIT-IBM Watson AI Lab.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Torralba‚Äôs research focuses on computer vision, machine learning, and human visual perception; as he puts it, ‚ÄúI am interested in building systems that can perceive the world like humans do.‚Äù Alongside Phillip Isola and William Freeman, he recently co-authored&amp;nbsp;‚ÄúFoundations of Computer Vision,‚Äù an 800-plus page textbook exploring the foundations and core principles of the field.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Among other awards and recognitions, he is the recipient of the 2008 National Science Foundation Career award; the 2010 J. K. Aggarwal Prize from the International Association for Pattern Recognition; the 2017 Frank Quick Faculty Research Innovation Fellowship; the Louis D. Smullin (‚Äô39) Award for Teaching Excellence; and the 2020 PAMI Mark Everingham Prize. In 2021, he was awarded the inaugural Thomas Huang Memorial Prize by the Pattern Analysis and Machine Intelligence Technical Committee and was named a fellow of the Association for the Advancement of Artificial Intelligence. In 2022, he received an honorary doctoral degree from the Universitat Polit√®cnica de Catalunya ‚Äî BarcelonaTech (UPC).&amp;nbsp;&lt;/p&gt;&lt;p&gt;ACM fellows, the highest honor bestowed by the professional organization, are registered members of the society selected by their peers for&amp;nbsp;outstanding accomplishments in computing and information technology and/or outstanding service to ACM and the larger computing community.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202601/mit-eecs-Torralba-Antonio.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Antonio Torralba, Delta Electronics Professor of Electrical Engineering and Computer Science and faculty head of artificial intelligence and decision-making at MIT, has been named to the 2025 cohort of Association for Computing Machinery (ACM) Fellows. He shares the honor of an ACM Fellowship with three MIT alumni: Eytan Adar ‚Äô97, MEng ‚Äô98; George Candea ‚Äô97, MEng ‚Äô98; and Gookwon Edward Suh SM ‚Äô01, PhD ‚Äô05.&lt;/p&gt;&lt;p dir="ltr"&gt;A principal investigator within both the Computer Science and Artificial Intelligence Laboratory and the Center for Brains, Minds, and Machines, Torralba received his BS in telecommunications engineering from Telecom BCN, Spain, in 1994, and a PhD in signal, image, and speech processing from the Institut National Polytechnique de Grenoble, France, in 2000. At different points in his MIT career, he has been director of both the MIT Quest for Intelligence (now the MIT Siegel Family Quest for Intelligence) and the MIT-IBM Watson AI Lab.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Torralba‚Äôs research focuses on computer vision, machine learning, and human visual perception; as he puts it, ‚ÄúI am interested in building systems that can perceive the world like humans do.‚Äù Alongside Phillip Isola and William Freeman, he recently co-authored&amp;nbsp;‚ÄúFoundations of Computer Vision,‚Äù an 800-plus page textbook exploring the foundations and core principles of the field.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Among other awards and recognitions, he is the recipient of the 2008 National Science Foundation Career award; the 2010 J. K. Aggarwal Prize from the International Association for Pattern Recognition; the 2017 Frank Quick Faculty Research Innovation Fellowship; the Louis D. Smullin (‚Äô39) Award for Teaching Excellence; and the 2020 PAMI Mark Everingham Prize. In 2021, he was awarded the inaugural Thomas Huang Memorial Prize by the Pattern Analysis and Machine Intelligence Technical Committee and was named a fellow of the Association for the Advancement of Artificial Intelligence. In 2022, he received an honorary doctoral degree from the Universitat Polit√®cnica de Catalunya ‚Äî BarcelonaTech (UPC).&amp;nbsp;&lt;/p&gt;&lt;p&gt;ACM fellows, the highest honor bestowed by the professional organization, are registered members of the society selected by their peers for&amp;nbsp;outstanding accomplishments in computing and information technology and/or outstanding service to ACM and the larger computing community.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/antonio-torralba-three-mit-alumni-named-acm-fellows-0204</guid><pubDate>Wed, 04 Feb 2026 18:15:00 +0000</pubDate></item><item><title>Brian Hedden named co-associate dean of Social and Ethical Responsibilities of Computing (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/brian-hedden-named-co-associate-dean-serc-0204</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/brian-hedden-mit-00.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Brian Hedden PhD ‚Äô12 has been appointed co-associate dean of the Social and Ethical Responsibilities of Computing (SERC) at MIT, a cross-cutting initiative in the MIT Schwarzman College of Computing, effective Jan. 16.&lt;/p&gt;&lt;p&gt;Hedden is a professor in the Department of Linguistics and Philosophy, holding an MIT Schwarzman College of Computing shared position with the Department of Electrical Engineering and Computer Science (EECS). He joined the MIT faculty last fall from the Australian National University and the University of Sydney, where he previously served as a faculty member. He earned his BA from Princeton University and his PhD from MIT, both in philosophy.&lt;/p&gt;&lt;p&gt;‚ÄúBrian is a natural and compelling choice for SERC, as a philosopher whose work speaks directly to the intellectual challenges facing education and research today, particularly in computing and AI. His expertise in epistemology, decision theory, and ethics addresses questions that have become increasingly urgent in an era defined by information abundance and artificial intelligence. His scholarship exemplifies the kind of interdisciplinary inquiry that SERC exists to advance,‚Äù says Dan Huttenlocher, dean of the MIT Schwarzman College of Computing and the Henry Ellis Warren Professor of Electrical Engineering and Computer Science.&lt;/p&gt;&lt;p&gt;Hedden‚Äôs research focuses on how we ought to form beliefs and make decisions, and it explores how philosophical thinking about rationality can yield insights into contemporary ethical issues, including ethics of AI. He is the author of ‚ÄúReasons without Persons: Rationality, Identity, and Time‚Äù (Oxford University Press, 2015) and articles on topics such as collective action problems, legal standards of proof, algorithmic fairness, and political polarization.&lt;/p&gt;&lt;p&gt;Joining co-associate dean Nikos Trichakis, the J.C. Penney Professor of Management at the MIT Sloan School of Management, Hedden will help lead SERC and advance the initiative‚Äôs ongoing research, teaching, and engagement efforts. He succeeds professor of philosophy Caspar Hare, who stepped down at the conclusion of his three-year term on Sept. 1, 2025.&lt;/p&gt;&lt;p&gt;Since its inception in 2020, SERC has launched a range of programs and activities designed to cultivate responsible ‚Äúhabits of mind and action‚Äù among those who create and deploy computing technologies, while fostering the development of technologies in the public interest.&lt;/p&gt;&lt;p&gt;The SERC Scholars Program invites undergraduate and graduate students to work alongside postdoctoral mentors to explore interdisciplinary ethical challenges in computing. The initiative also hosts an annual prize competition that challenges MIT students to envision the future of computing, publishes a twice-yearly series of case studies, and collaborates on coordinated curricular materials, including active-learning projects, homework assignments, and in-class demonstrations. In 2024, SERC introduced a new seed grant program to support MIT researchers investigating ethical technology development; to date, two rounds of grants have been awarded to 24 projects.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/brian-hedden-mit-00.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Brian Hedden PhD ‚Äô12 has been appointed co-associate dean of the Social and Ethical Responsibilities of Computing (SERC) at MIT, a cross-cutting initiative in the MIT Schwarzman College of Computing, effective Jan. 16.&lt;/p&gt;&lt;p&gt;Hedden is a professor in the Department of Linguistics and Philosophy, holding an MIT Schwarzman College of Computing shared position with the Department of Electrical Engineering and Computer Science (EECS). He joined the MIT faculty last fall from the Australian National University and the University of Sydney, where he previously served as a faculty member. He earned his BA from Princeton University and his PhD from MIT, both in philosophy.&lt;/p&gt;&lt;p&gt;‚ÄúBrian is a natural and compelling choice for SERC, as a philosopher whose work speaks directly to the intellectual challenges facing education and research today, particularly in computing and AI. His expertise in epistemology, decision theory, and ethics addresses questions that have become increasingly urgent in an era defined by information abundance and artificial intelligence. His scholarship exemplifies the kind of interdisciplinary inquiry that SERC exists to advance,‚Äù says Dan Huttenlocher, dean of the MIT Schwarzman College of Computing and the Henry Ellis Warren Professor of Electrical Engineering and Computer Science.&lt;/p&gt;&lt;p&gt;Hedden‚Äôs research focuses on how we ought to form beliefs and make decisions, and it explores how philosophical thinking about rationality can yield insights into contemporary ethical issues, including ethics of AI. He is the author of ‚ÄúReasons without Persons: Rationality, Identity, and Time‚Äù (Oxford University Press, 2015) and articles on topics such as collective action problems, legal standards of proof, algorithmic fairness, and political polarization.&lt;/p&gt;&lt;p&gt;Joining co-associate dean Nikos Trichakis, the J.C. Penney Professor of Management at the MIT Sloan School of Management, Hedden will help lead SERC and advance the initiative‚Äôs ongoing research, teaching, and engagement efforts. He succeeds professor of philosophy Caspar Hare, who stepped down at the conclusion of his three-year term on Sept. 1, 2025.&lt;/p&gt;&lt;p&gt;Since its inception in 2020, SERC has launched a range of programs and activities designed to cultivate responsible ‚Äúhabits of mind and action‚Äù among those who create and deploy computing technologies, while fostering the development of technologies in the public interest.&lt;/p&gt;&lt;p&gt;The SERC Scholars Program invites undergraduate and graduate students to work alongside postdoctoral mentors to explore interdisciplinary ethical challenges in computing. The initiative also hosts an annual prize competition that challenges MIT students to envision the future of computing, publishes a twice-yearly series of case studies, and collaborates on coordinated curricular materials, including active-learning projects, homework assignments, and in-class demonstrations. In 2024, SERC introduced a new seed grant program to support MIT researchers investigating ethical technology development; to date, two rounds of grants have been awarded to 24 projects.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/brian-hedden-named-co-associate-dean-serc-0204</guid><pubDate>Wed, 04 Feb 2026 18:25:00 +0000</pubDate></item><item><title>[NEW] What a16z is¬†actually funding¬†(and what¬†it‚Äôs¬†ignoring) when it comes to AI¬†infra (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/what-a16z-is-actually-funding-and-what-its-ignoring-when-it-comes-to-ai-infra/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/a16z-Andreessen-Horowitz.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Andreessen Horowitz just raised&amp;nbsp;a whopping&amp;nbsp;new&amp;nbsp;$15 billion&amp;nbsp;in funding.&amp;nbsp;And a&amp;nbsp;$1.7 billion&amp;nbsp;chunk&amp;nbsp;of that is going to&amp;nbsp;its&amp;nbsp;infrastructure&amp;nbsp;team,&amp;nbsp;the one responsible for some of its biggest, most prominent&amp;nbsp;AI investments, including&amp;nbsp;Black Forest Labs, Cursor, OpenAI,&amp;nbsp;ElevenLabs, Ideogram,&amp;nbsp;Fal,&amp;nbsp;and dozens of others.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;a16z&amp;nbsp;general partner&amp;nbsp;with the infra team Jennifer Li&amp;nbsp;(who oversees such&amp;nbsp;investments&amp;nbsp;as&amp;nbsp;ElevenLabs&amp;nbsp;‚Äî just valued at&amp;nbsp;$11 billion) has a clear thesis on where&amp;nbsp;the team is looking to spend&amp;nbsp;it‚Äôs&amp;nbsp;latest chunk of cash.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;These investors are no strangers to billion-dollar budgets. When the firm raised&amp;nbsp;$7.2 billion&amp;nbsp;in 2024, the infra team&amp;nbsp;was handed&amp;nbsp;$1.25&amp;nbsp;billion&amp;nbsp;at the time, more than any other&amp;nbsp;vertical team.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;So what‚Äôs so exciting about infrastructure, especially in 2026? It covers everything from chip design all the way up to any software stack used by developers. This is the heartbeat of AI development and it is undergoing a never-before-seen transformation process, both in how AI is being used in these areas (AI coding) and the AI available to devs (ElevenLabs voice models, and Fal‚Äôs multi-modal model marketplace).&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;So Li is on the front lines of where AI is today, where it is going in 2026, and what is and likely may never be possible. She‚Äôs, for instance, skeptical about some of the industry‚Äôs biggest assumptions, including the idea that AI will replace human creativity anytime soon.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch‚Äôs&amp;nbsp;Equity&amp;nbsp;podcast,&amp;nbsp;Venture and&amp;nbsp;Startups Editor&amp;nbsp;Julie Bort talked with Li&amp;nbsp;about where a16z sees this&amp;nbsp;AI super cycle&amp;nbsp;going next,&amp;nbsp;including&amp;nbsp;the talent crunch hitting AI-native startups, why search infrastructure matters more than people think, and what kinds of companies are&amp;nbsp;actually getting&amp;nbsp;funded right now.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Where Li thinks the gaps still are when it comes to startups building an AI stack&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;What makes the most successful AI portfolio companies different&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How tools like voice AI are rising in importance (yet still a bit uncomfortable to witness)&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;The AI startups she‚Äôs still searching for and is ready to fund&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on YouTube, Apple Podcasts, Overcast, Spotify and all the casts. You also can follow Equity on X and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/a16z-Andreessen-Horowitz.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Andreessen Horowitz just raised&amp;nbsp;a whopping&amp;nbsp;new&amp;nbsp;$15 billion&amp;nbsp;in funding.&amp;nbsp;And a&amp;nbsp;$1.7 billion&amp;nbsp;chunk&amp;nbsp;of that is going to&amp;nbsp;its&amp;nbsp;infrastructure&amp;nbsp;team,&amp;nbsp;the one responsible for some of its biggest, most prominent&amp;nbsp;AI investments, including&amp;nbsp;Black Forest Labs, Cursor, OpenAI,&amp;nbsp;ElevenLabs, Ideogram,&amp;nbsp;Fal,&amp;nbsp;and dozens of others.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;a16z&amp;nbsp;general partner&amp;nbsp;with the infra team Jennifer Li&amp;nbsp;(who oversees such&amp;nbsp;investments&amp;nbsp;as&amp;nbsp;ElevenLabs&amp;nbsp;‚Äî just valued at&amp;nbsp;$11 billion) has a clear thesis on where&amp;nbsp;the team is looking to spend&amp;nbsp;it‚Äôs&amp;nbsp;latest chunk of cash.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;These investors are no strangers to billion-dollar budgets. When the firm raised&amp;nbsp;$7.2 billion&amp;nbsp;in 2024, the infra team&amp;nbsp;was handed&amp;nbsp;$1.25&amp;nbsp;billion&amp;nbsp;at the time, more than any other&amp;nbsp;vertical team.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;So what‚Äôs so exciting about infrastructure, especially in 2026? It covers everything from chip design all the way up to any software stack used by developers. This is the heartbeat of AI development and it is undergoing a never-before-seen transformation process, both in how AI is being used in these areas (AI coding) and the AI available to devs (ElevenLabs voice models, and Fal‚Äôs multi-modal model marketplace).&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;So Li is on the front lines of where AI is today, where it is going in 2026, and what is and likely may never be possible. She‚Äôs, for instance, skeptical about some of the industry‚Äôs biggest assumptions, including the idea that AI will replace human creativity anytime soon.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch‚Äôs&amp;nbsp;Equity&amp;nbsp;podcast,&amp;nbsp;Venture and&amp;nbsp;Startups Editor&amp;nbsp;Julie Bort talked with Li&amp;nbsp;about where a16z sees this&amp;nbsp;AI super cycle&amp;nbsp;going next,&amp;nbsp;including&amp;nbsp;the talent crunch hitting AI-native startups, why search infrastructure matters more than people think, and what kinds of companies are&amp;nbsp;actually getting&amp;nbsp;funded right now.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Where Li thinks the gaps still are when it comes to startups building an AI stack&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;What makes the most successful AI portfolio companies different&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How tools like voice AI are rising in importance (yet still a bit uncomfortable to witness)&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;The AI startups she‚Äôs still searching for and is ready to fund&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on YouTube, Apple Podcasts, Overcast, Spotify and all the casts. You also can follow Equity on X and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/what-a16z-is-actually-funding-and-what-its-ignoring-when-it-comes-to-ai-infra/</guid><pubDate>Wed, 04 Feb 2026 20:19:12 +0000</pubDate></item><item><title>[NEW] A16z just raised $1.7B for AI infrastructure. Here‚Äôs where it‚Äôs going. (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/video/a16z-just-raised-1-7b-for-ai-infrastructure-heres-where-its-going/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2018/02/tc-backlight-e1689786273147.png?w=1200" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30896071"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player‚Ä¶&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Andreessen Horowitz just raised&amp;nbsp;a whopping&amp;nbsp;‚Å†&lt;u&gt;new&amp;nbsp;$15 billion&amp;nbsp;in funding&lt;/u&gt;‚Å†.&amp;nbsp;And a&amp;nbsp;$1.7 billion&amp;nbsp;chunk&amp;nbsp;of that is going to&amp;nbsp;its&amp;nbsp;‚Å†&lt;u&gt;infrastructure&amp;nbsp;team&lt;/u&gt;‚Å†,&amp;nbsp;the one responsible for some of its biggest, most prominent&amp;nbsp;AI investments including&amp;nbsp;Black Forrest Labs, Cursor, OpenAI,&amp;nbsp;‚Å†ElevenLabs‚Å†, Ideogram,&amp;nbsp;‚Å†Fal‚Å†&amp;nbsp;and dozens of others.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;A16z&amp;nbsp;‚Å†general partner&amp;nbsp;with the infra team Jennifer Li‚Å†&amp;nbsp;(who oversees such&amp;nbsp;investments&amp;nbsp;as&amp;nbsp;ElevenLabs&amp;nbsp;‚Äì just valued at&amp;nbsp;$11 billion);&amp;nbsp;Ideagram&amp;nbsp;and Fal, has a clear thesis on where&amp;nbsp;the team is looking to spend&amp;nbsp;it‚Äôs&amp;nbsp;latest chunk of cash.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Watch as&amp;nbsp;Venture and&amp;nbsp;Startups editor&amp;nbsp;Julie Bort talks with Li&amp;nbsp;on ‚Å†Equity‚Å†&amp;nbsp;about where&amp;nbsp;a16z&amp;nbsp;sees this&amp;nbsp;AI super cycle&amp;nbsp;going next,&amp;nbsp;including&amp;nbsp;the talent crunch hitting AI-native startups, why search infrastructure matters more than people think, and what kinds of companies are&amp;nbsp;actually getting&amp;nbsp;funded right now.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2018/02/tc-backlight-e1689786273147.png?w=1200" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30896071"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player‚Ä¶&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Andreessen Horowitz just raised&amp;nbsp;a whopping&amp;nbsp;‚Å†&lt;u&gt;new&amp;nbsp;$15 billion&amp;nbsp;in funding&lt;/u&gt;‚Å†.&amp;nbsp;And a&amp;nbsp;$1.7 billion&amp;nbsp;chunk&amp;nbsp;of that is going to&amp;nbsp;its&amp;nbsp;‚Å†&lt;u&gt;infrastructure&amp;nbsp;team&lt;/u&gt;‚Å†,&amp;nbsp;the one responsible for some of its biggest, most prominent&amp;nbsp;AI investments including&amp;nbsp;Black Forrest Labs, Cursor, OpenAI,&amp;nbsp;‚Å†ElevenLabs‚Å†, Ideogram,&amp;nbsp;‚Å†Fal‚Å†&amp;nbsp;and dozens of others.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;A16z&amp;nbsp;‚Å†general partner&amp;nbsp;with the infra team Jennifer Li‚Å†&amp;nbsp;(who oversees such&amp;nbsp;investments&amp;nbsp;as&amp;nbsp;ElevenLabs&amp;nbsp;‚Äì just valued at&amp;nbsp;$11 billion);&amp;nbsp;Ideagram&amp;nbsp;and Fal, has a clear thesis on where&amp;nbsp;the team is looking to spend&amp;nbsp;it‚Äôs&amp;nbsp;latest chunk of cash.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Watch as&amp;nbsp;Venture and&amp;nbsp;Startups editor&amp;nbsp;Julie Bort talks with Li&amp;nbsp;on ‚Å†Equity‚Å†&amp;nbsp;about where&amp;nbsp;a16z&amp;nbsp;sees this&amp;nbsp;AI super cycle&amp;nbsp;going next,&amp;nbsp;including&amp;nbsp;the talent crunch hitting AI-native startups, why search infrastructure matters more than people think, and what kinds of companies are&amp;nbsp;actually getting&amp;nbsp;funded right now.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/video/a16z-just-raised-1-7b-for-ai-infrastructure-heres-where-its-going/</guid><pubDate>Wed, 04 Feb 2026 20:24:12 +0000</pubDate></item><item><title>[NEW] Should AI chatbots have ads? Anthropic says no. (AI - Ars Technica)</title><link>https://arstechnica.com/ai/2026/02/should-ai-chatbots-have-ads-anthropic-says-no/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        ChatGPT competitor comes out swinging with Super Bowl ad mocking AI product pitches.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An illustration of a hand holding a shape created by Anthropic." class="absolute inset-0 w-full h-full object-cover hidden" height="356" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/claude-no-ads-640x356.png" width="640" /&gt;
                  &lt;img alt="An illustration of a hand holding a shape created by Anthropic." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/claude-no-ads-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday, Anthropic announced that its AI chatbot, Claude, will remain free of advertisements, drawing a sharp line between itself and rival OpenAI, which began testing ads in a low-cost tier of ChatGPT last month. The announcement comes alongside a Super Bowl ad campaign that mocks AI assistants that interrupt personal conversations with product pitches.&lt;/p&gt;
&lt;p&gt;‚ÄúThere are many good places for advertising. A conversation with Claude is not one of them,‚Äù Anthropic wrote in a blog post. The company argued that including ads in AI conversations would be ‚Äúincompatible‚Äù with what it wants Claude to be: ‚Äúa genuinely helpful assistant for work and for deep thinking.‚Äù&lt;/p&gt;
&lt;p&gt;The stance contrasts with OpenAI‚Äôs January announcement that it would begin testing banner ads for free users and ChatGPT Go subscribers in the US. OpenAI said those ads would appear at the bottom of responses and would not influence the chatbot‚Äôs actual answers. Paid subscribers on Plus, Pro, Business, and Enterprise tiers will not see ads on ChatGPT.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Anthropic‚Äôs 2026 Super Bowl commercial.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;‚ÄúWe want Claude to act unambiguously in our users‚Äô interests,‚Äù Anthropic wrote. ‚ÄúSo we‚Äôve made a choice: Claude will remain ad-free. Our users won‚Äôt see ‚Äòsponsored‚Äô links adjacent to their conversations with Claude; nor will Claude‚Äôs responses be influenced by advertisers or include third-party product placements our users did not ask for.‚Äù&lt;/p&gt;
&lt;p&gt;Competition between OpenAI and Anthropic has been fierce of late, due to the rise of AI coding agents. Claude Code, Anthropic‚Äôs coding tool, and OpenAI‚Äôs Codex have similar capabilities, but Claude Code has been widely popular among developers and is closing in on OpenAI‚Äôs turf. Last month, The Verge reported that many developers inside long-time OpenAI benefactor Microsoft have been adopting Claude Code, choosing Anthropic products over Microsoft‚Äôs Copilot, which is powered by tech that originated at OpenAI.&lt;/p&gt;
&lt;p&gt;In this climate, Anthropic could not resist taking a dig at OpenAI. In its Super Bowl commercial, we see a thin man struggling to do a pull-up beside a buff fitness instructor, who is a stand-in for an AI assistant. The man asks the ‚Äúassistant‚Äù for help making a workout plan, but the assistant slips in an advertisement for a supplement, confusing the man. The commercial doesn‚Äôt name any names, and OpenAI has said it will not include ads in chat text itself, but Anthropic‚Äôs implications are clear.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Different incentives, different futures&lt;/h2&gt;
&lt;p&gt;In its blog post, Anthropic describes internal analysis it conducted that suggests many Claude conversations involve topics that are ‚Äúsensitive or deeply personal‚Äù or require sustained focus on complex tasks. In these contexts, Anthropic wrote, ‚ÄúThe appearance of ads would feel incongruous‚Äîand, in many cases, inappropriate.‚Äù&lt;/p&gt;
&lt;p&gt;The company also argued that advertising introduces&amp;nbsp;incentives that could conflict with providing genuinely helpful advice. It gave the example of a user mentioning trouble sleeping: an ad-free assistant would explore various causes, while an ad-supported one might steer the conversation toward a transaction.&lt;/p&gt;
&lt;p&gt;‚ÄúUsers shouldn‚Äôt have to second-guess whether an AI is genuinely helping them or subtly steering the conversation towards something monetizable,‚Äù Anthropic wrote.&lt;/p&gt;
&lt;p&gt;Currently, OpenAI does not plan to include paid product recommendations within a ChatGPT conversation. Instead, the ads appear as banners alongside the conversation text.&lt;/p&gt;
&lt;p&gt;OpenAI CEO Sam Altman has previously expressed reservations about mixing ads and AI conversations. In a 2024 interview at Harvard University, he described the combination as ‚Äúuniquely unsettling‚Äù and said he would not like having to ‚Äúfigure out exactly how much was who paying here to influence what I‚Äôm being shown.‚Äù&lt;/p&gt;
&lt;p&gt;A key part of Altman‚Äôs partial change of heart is that OpenAI faces enormous financial pressure. The company made more than $1.4 trillion worth of infrastructure deals in 2025, and according to documents obtained by The Wall Street Journal, it expects to burn through roughly $9 billion this year while generating $13 billion in revenue. Only about 5 percent of ChatGPT‚Äôs 800 million weekly users pay for subscriptions.&lt;/p&gt;
&lt;p&gt;Much like OpenAI, Anthropic is not yet profitable, but it is expected to get there much faster. Anthropic has not attempted to span the world with massive datacenters, and its business model largely relies on enterprise contracts and paid subscriptions. The company says Claude Code and Cowork have already brought in at least $1 billion in revenue, according to Axios.&lt;/p&gt;
&lt;p&gt;‚ÄúOur business model is straightforward,‚Äù Anthropic wrote. ‚ÄúThis is a choice with tradeoffs, and we respect that other AI companies might reasonably reach different conclusions.‚Äù&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        ChatGPT competitor comes out swinging with Super Bowl ad mocking AI product pitches.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An illustration of a hand holding a shape created by Anthropic." class="absolute inset-0 w-full h-full object-cover hidden" height="356" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/claude-no-ads-640x356.png" width="640" /&gt;
                  &lt;img alt="An illustration of a hand holding a shape created by Anthropic." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/claude-no-ads-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday, Anthropic announced that its AI chatbot, Claude, will remain free of advertisements, drawing a sharp line between itself and rival OpenAI, which began testing ads in a low-cost tier of ChatGPT last month. The announcement comes alongside a Super Bowl ad campaign that mocks AI assistants that interrupt personal conversations with product pitches.&lt;/p&gt;
&lt;p&gt;‚ÄúThere are many good places for advertising. A conversation with Claude is not one of them,‚Äù Anthropic wrote in a blog post. The company argued that including ads in AI conversations would be ‚Äúincompatible‚Äù with what it wants Claude to be: ‚Äúa genuinely helpful assistant for work and for deep thinking.‚Äù&lt;/p&gt;
&lt;p&gt;The stance contrasts with OpenAI‚Äôs January announcement that it would begin testing banner ads for free users and ChatGPT Go subscribers in the US. OpenAI said those ads would appear at the bottom of responses and would not influence the chatbot‚Äôs actual answers. Paid subscribers on Plus, Pro, Business, and Enterprise tiers will not see ads on ChatGPT.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Anthropic‚Äôs 2026 Super Bowl commercial.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;‚ÄúWe want Claude to act unambiguously in our users‚Äô interests,‚Äù Anthropic wrote. ‚ÄúSo we‚Äôve made a choice: Claude will remain ad-free. Our users won‚Äôt see ‚Äòsponsored‚Äô links adjacent to their conversations with Claude; nor will Claude‚Äôs responses be influenced by advertisers or include third-party product placements our users did not ask for.‚Äù&lt;/p&gt;
&lt;p&gt;Competition between OpenAI and Anthropic has been fierce of late, due to the rise of AI coding agents. Claude Code, Anthropic‚Äôs coding tool, and OpenAI‚Äôs Codex have similar capabilities, but Claude Code has been widely popular among developers and is closing in on OpenAI‚Äôs turf. Last month, The Verge reported that many developers inside long-time OpenAI benefactor Microsoft have been adopting Claude Code, choosing Anthropic products over Microsoft‚Äôs Copilot, which is powered by tech that originated at OpenAI.&lt;/p&gt;
&lt;p&gt;In this climate, Anthropic could not resist taking a dig at OpenAI. In its Super Bowl commercial, we see a thin man struggling to do a pull-up beside a buff fitness instructor, who is a stand-in for an AI assistant. The man asks the ‚Äúassistant‚Äù for help making a workout plan, but the assistant slips in an advertisement for a supplement, confusing the man. The commercial doesn‚Äôt name any names, and OpenAI has said it will not include ads in chat text itself, but Anthropic‚Äôs implications are clear.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Different incentives, different futures&lt;/h2&gt;
&lt;p&gt;In its blog post, Anthropic describes internal analysis it conducted that suggests many Claude conversations involve topics that are ‚Äúsensitive or deeply personal‚Äù or require sustained focus on complex tasks. In these contexts, Anthropic wrote, ‚ÄúThe appearance of ads would feel incongruous‚Äîand, in many cases, inappropriate.‚Äù&lt;/p&gt;
&lt;p&gt;The company also argued that advertising introduces&amp;nbsp;incentives that could conflict with providing genuinely helpful advice. It gave the example of a user mentioning trouble sleeping: an ad-free assistant would explore various causes, while an ad-supported one might steer the conversation toward a transaction.&lt;/p&gt;
&lt;p&gt;‚ÄúUsers shouldn‚Äôt have to second-guess whether an AI is genuinely helping them or subtly steering the conversation towards something monetizable,‚Äù Anthropic wrote.&lt;/p&gt;
&lt;p&gt;Currently, OpenAI does not plan to include paid product recommendations within a ChatGPT conversation. Instead, the ads appear as banners alongside the conversation text.&lt;/p&gt;
&lt;p&gt;OpenAI CEO Sam Altman has previously expressed reservations about mixing ads and AI conversations. In a 2024 interview at Harvard University, he described the combination as ‚Äúuniquely unsettling‚Äù and said he would not like having to ‚Äúfigure out exactly how much was who paying here to influence what I‚Äôm being shown.‚Äù&lt;/p&gt;
&lt;p&gt;A key part of Altman‚Äôs partial change of heart is that OpenAI faces enormous financial pressure. The company made more than $1.4 trillion worth of infrastructure deals in 2025, and according to documents obtained by The Wall Street Journal, it expects to burn through roughly $9 billion this year while generating $13 billion in revenue. Only about 5 percent of ChatGPT‚Äôs 800 million weekly users pay for subscriptions.&lt;/p&gt;
&lt;p&gt;Much like OpenAI, Anthropic is not yet profitable, but it is expected to get there much faster. Anthropic has not attempted to span the world with massive datacenters, and its business model largely relies on enterprise contracts and paid subscriptions. The company says Claude Code and Cowork have already brought in at least $1 billion in revenue, according to Axios.&lt;/p&gt;
&lt;p&gt;‚ÄúOur business model is straightforward,‚Äù Anthropic wrote. ‚ÄúThis is a choice with tradeoffs, and we respect that other AI companies might reasonably reach different conclusions.‚Äù&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2026/02/should-ai-chatbots-have-ads-anthropic-says-no/</guid><pubDate>Wed, 04 Feb 2026 21:15:07 +0000</pubDate></item><item><title>[NEW] Amazon to begin testing AI tools for film and TV production next month (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/amazon-to-begin-testing-ai-tools-for-film-and-tv-production-next-month/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Amazon-HouseofDavid.jpg?w=1080" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last summer, Amazon MGM Studios launched a dedicated AI Studio to develop proprietary AI tools to streamline TV and film production, with a focus on areas like improving character consistency across shots and supporting pre- and post-production.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to a report from Reuters, those tools are now ready to move beyond internal testing. Amazon will begin a closed beta program in March, inviting industry partners to try out its AI tools.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Amazon said it anticipates sharing initial outcomes from the program by May. The company chose not to provide further details on the developments when approached by TechCrunch for a comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI Studio is collaborating with notable producers like Robert Stromberg, known for ‚ÄúMaleficent,‚Äù Kunal Nayyar from ‚ÄúThe Big Bang Theory,‚Äù and former animator Colin Brady from Pixar to learn the best way to implement these tools. Amazon is also tapping Amazon Web Services for support and intends to work with several LLM providers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Albert Cheng, who heads the AI Studios initiative, emphasized that the goal is to support creative teams, not to replace them. The focus is on improving efficiency and reducing costs while ensuring that intellectual property is protected and AI-generated content isn‚Äôt absorbed into other AI models. One example used is Amazon‚Äôs ‚ÄúHouse of David‚Äù series, which featured 350 AI-generated shots in season two.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄãHowever, the rise of adoption of AI in Hollywood has stirred up plenty of debate. Many people in the industry worry about what it means for jobs, creativity, and the future of filmmaking.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The conversations around AI are only getting louder as more companies experiment with these new tools. For instance, Netflix has also jumped on the AI bandwagon, with co-CEO Ted Sarandos revealing that its series ‚ÄúThe Eternaut‚Äù used generative AI to create a building collapse scene.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent years, Amazon has cited its success with AI as a factor in layoffs. The company recently eliminated 16,000 jobs in January, following 14,000 layoffs last October.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Amazon-HouseofDavid.jpg?w=1080" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last summer, Amazon MGM Studios launched a dedicated AI Studio to develop proprietary AI tools to streamline TV and film production, with a focus on areas like improving character consistency across shots and supporting pre- and post-production.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to a report from Reuters, those tools are now ready to move beyond internal testing. Amazon will begin a closed beta program in March, inviting industry partners to try out its AI tools.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Amazon said it anticipates sharing initial outcomes from the program by May. The company chose not to provide further details on the developments when approached by TechCrunch for a comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI Studio is collaborating with notable producers like Robert Stromberg, known for ‚ÄúMaleficent,‚Äù Kunal Nayyar from ‚ÄúThe Big Bang Theory,‚Äù and former animator Colin Brady from Pixar to learn the best way to implement these tools. Amazon is also tapping Amazon Web Services for support and intends to work with several LLM providers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Albert Cheng, who heads the AI Studios initiative, emphasized that the goal is to support creative teams, not to replace them. The focus is on improving efficiency and reducing costs while ensuring that intellectual property is protected and AI-generated content isn‚Äôt absorbed into other AI models. One example used is Amazon‚Äôs ‚ÄúHouse of David‚Äù series, which featured 350 AI-generated shots in season two.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄãHowever, the rise of adoption of AI in Hollywood has stirred up plenty of debate. Many people in the industry worry about what it means for jobs, creativity, and the future of filmmaking.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The conversations around AI are only getting louder as more companies experiment with these new tools. For instance, Netflix has also jumped on the AI bandwagon, with co-CEO Ted Sarandos revealing that its series ‚ÄúThe Eternaut‚Äù used generative AI to create a building collapse scene.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent years, Amazon has cited its success with AI as a factor in layoffs. The company recently eliminated 16,000 jobs in January, following 14,000 layoffs last October.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/amazon-to-begin-testing-ai-tools-for-film-and-tv-production-next-month/</guid><pubDate>Wed, 04 Feb 2026 21:26:43 +0000</pubDate></item><item><title>[NEW] AI SRE Resolve AI confirms $125M raise, unicorn valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/ai-sre-resolve-ai-confirms-125m-raise-unicorn-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/03/GettyImages-1319917047.jpg?resize=1200,764" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Resolve AI, a startup automating the work of system reliability engineering (SRE), aka troubleshooting system&amp;nbsp;failures, has announced a $125 million Series A at a $1 billion valuation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by Lightspeed Venture Partners, with participation of existing investors including Greylock Partners, Unusual Ventures, Artisanal Ventures, and A*.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The announcement confirms TechCrunch‚Äôs December report that the startup was raising at a billion-dollar valuation led by Lightspeed. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sources told TechCrunch at the time that the round may have consisted of multiple tranches, at different prices, which could have put the company‚Äôs actual blended valuation below $1 billion. A spokesperson for Resolve denied that there were multiple tranches in the round, saying that 100% of the equity was purchased at a valuation of $1 billion. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As we previously reported, this kind of structure allows certain investors, often the lead, to purchase a significant portion of equity at a lower price.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Resolve was co-founded in early 2024 by two former Splunk executives, Spiros Xanthos and Mayank Agarwal. Their previous startup, Omnition, was acquired by Splunk in 2019.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another startup applying AI to identify and resolve system outages is the Sequoia-backed Traversal. The emerging category is known as AI SRE. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/03/GettyImages-1319917047.jpg?resize=1200,764" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Resolve AI, a startup automating the work of system reliability engineering (SRE), aka troubleshooting system&amp;nbsp;failures, has announced a $125 million Series A at a $1 billion valuation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by Lightspeed Venture Partners, with participation of existing investors including Greylock Partners, Unusual Ventures, Artisanal Ventures, and A*.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The announcement confirms TechCrunch‚Äôs December report that the startup was raising at a billion-dollar valuation led by Lightspeed. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sources told TechCrunch at the time that the round may have consisted of multiple tranches, at different prices, which could have put the company‚Äôs actual blended valuation below $1 billion. A spokesperson for Resolve denied that there were multiple tranches in the round, saying that 100% of the equity was purchased at a valuation of $1 billion. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As we previously reported, this kind of structure allows certain investors, often the lead, to purchase a significant portion of equity at a lower price.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Resolve was co-founded in early 2024 by two former Splunk executives, Spiros Xanthos and Mayank Agarwal. Their previous startup, Omnition, was acquired by Splunk in 2019.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another startup applying AI to identify and resolve system outages is the Sequoia-backed Traversal. The emerging category is known as AI SRE. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/ai-sre-resolve-ai-confirms-125m-raise-unicorn-valuation/</guid><pubDate>Wed, 04 Feb 2026 21:39:26 +0000</pubDate></item><item><title>[NEW] Meet Gizmo: A TikTok for interactive, vibe-coded mini apps (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/meet-gizmo-a-tiktok-for-interactive-vibe-coded-mini-apps/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Gizmo, a TikTok-like app for vibe-coded mini applications, is offering a new way to create interactive media. The relatively new mobile app from the startup Atma Sciences lets anyone create experiences using text, photos, sound, and touch, which are then displayed in a vertical feed, similar to TikTok or Reels. But unlike traditional short-form video apps, you don‚Äôt just watch and scroll in Gizmo ‚Äî you play.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Depending on the type of ‚ÄúGizmo‚Äù you encounter, you might poke the screen, swipe, tap, draw, drag, and more to interact with the mini app. These Gizmos aren‚Äôt just games but are more like digital toys ‚Äî things that could include interactive puzzles, memes, art, animation, or anything else a creator can dream up.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3089592" height="363" src="https://techcrunch.com/wp-content/uploads/2026/02/gizmo-app-store.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Gizmo&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The result is an engaging, playful feed, where you can like and comment on the tiny creations and even remix existing Gizmos to create your own version, if you choose.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What‚Äôs more, you don‚Äôt need to know how to code or even vibe code to get started. Instead, you can simply type out an AI prompt to explain your idea using natural language.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app then leverages AI coding technology to turn your idea into an interactive experience by generating the code that makes it work. As part of this process, Gizmo will also render your idea visually to ensure that each app functions properly and runs smoothly. Apps are also vetted using AI and human moderation to ensure user safety, a company FAQ notes.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-tiktok wp-block-embed-tiktok"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Gizmo hails from a New York-based startup called Atma Sciences, co-founded by Rudd Fawcett and Brandon Francis, along with CEO Josh Siegel and CTO Daniel Amitay. The company last year raised a $5.49 million seed round from First Round Capital and others, according to data from PitchBook. On the company‚Äôs website (which is also silly and interactive), the team explains their focus is on combining ‚Äúpowerful technology with simple, elegant foundations,‚Äù starting with their creativity app, Gizmo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;None of the company‚Äôs founders responded to requests for an interview when TechCrunch reached out through multiple emails, requests to investors, and via LinkedIn. We were told by one investor that the team isn‚Äôt yet ready to do press. (Sorry!)&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch was drawn to Gizmo because of the app‚Äôs potential for growth and its unique approach to the vibe-coding space (and a rare recommendation from my teen). The company is envisioning a world where anyone can create apps for fun, not just for a purpose, as with other vibe-coding app platforms for micro apps, like Anything, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite being relatively new, Gizmo‚Äôs feed isn‚Äôt repetitive. It‚Äôs filled with creative mini apps, leading to an experience that feels somewhat like a mash-up between TikTok and the interactive 3D-space designer, Rooms. But while Rooms introduced the programming language Lua to those who wanted more advanced controls over their creations, Gizmo keeps things prompt-based and simple.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app is incredibly easy to use. You simply type out your prompt and then see how it turns out, and then modify as needed. In one test, the AI quickly coded a mini quiz, but we had to instruct it to edit the title, which was cut off at the top of the screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The resulting creation can be shared to the app‚Äôs feed, messaged to a friend, or posted to social media using a unique URL.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to data from market intelligence firm Appfigures, Gizmo has roughly 600,000 installs, with around half coming from the U.S., after being introduced with little fanfare less than six months ago. Around 235,000 of its downloads came in December alone, representing 39% of its total count.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gizmo‚Äôs growth from October to December was 312%, with December installs up 50% month-over-month and November installs up 180% from October. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app is available on both iOS and Android.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Gizmo, a TikTok-like app for vibe-coded mini applications, is offering a new way to create interactive media. The relatively new mobile app from the startup Atma Sciences lets anyone create experiences using text, photos, sound, and touch, which are then displayed in a vertical feed, similar to TikTok or Reels. But unlike traditional short-form video apps, you don‚Äôt just watch and scroll in Gizmo ‚Äî you play.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Depending on the type of ‚ÄúGizmo‚Äù you encounter, you might poke the screen, swipe, tap, draw, drag, and more to interact with the mini app. These Gizmos aren‚Äôt just games but are more like digital toys ‚Äî things that could include interactive puzzles, memes, art, animation, or anything else a creator can dream up.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3089592" height="363" src="https://techcrunch.com/wp-content/uploads/2026/02/gizmo-app-store.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Gizmo&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The result is an engaging, playful feed, where you can like and comment on the tiny creations and even remix existing Gizmos to create your own version, if you choose.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What‚Äôs more, you don‚Äôt need to know how to code or even vibe code to get started. Instead, you can simply type out an AI prompt to explain your idea using natural language.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app then leverages AI coding technology to turn your idea into an interactive experience by generating the code that makes it work. As part of this process, Gizmo will also render your idea visually to ensure that each app functions properly and runs smoothly. Apps are also vetted using AI and human moderation to ensure user safety, a company FAQ notes.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-tiktok wp-block-embed-tiktok"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Gizmo hails from a New York-based startup called Atma Sciences, co-founded by Rudd Fawcett and Brandon Francis, along with CEO Josh Siegel and CTO Daniel Amitay. The company last year raised a $5.49 million seed round from First Round Capital and others, according to data from PitchBook. On the company‚Äôs website (which is also silly and interactive), the team explains their focus is on combining ‚Äúpowerful technology with simple, elegant foundations,‚Äù starting with their creativity app, Gizmo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;None of the company‚Äôs founders responded to requests for an interview when TechCrunch reached out through multiple emails, requests to investors, and via LinkedIn. We were told by one investor that the team isn‚Äôt yet ready to do press. (Sorry!)&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch was drawn to Gizmo because of the app‚Äôs potential for growth and its unique approach to the vibe-coding space (and a rare recommendation from my teen). The company is envisioning a world where anyone can create apps for fun, not just for a purpose, as with other vibe-coding app platforms for micro apps, like Anything, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite being relatively new, Gizmo‚Äôs feed isn‚Äôt repetitive. It‚Äôs filled with creative mini apps, leading to an experience that feels somewhat like a mash-up between TikTok and the interactive 3D-space designer, Rooms. But while Rooms introduced the programming language Lua to those who wanted more advanced controls over their creations, Gizmo keeps things prompt-based and simple.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app is incredibly easy to use. You simply type out your prompt and then see how it turns out, and then modify as needed. In one test, the AI quickly coded a mini quiz, but we had to instruct it to edit the title, which was cut off at the top of the screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The resulting creation can be shared to the app‚Äôs feed, messaged to a friend, or posted to social media using a unique URL.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to data from market intelligence firm Appfigures, Gizmo has roughly 600,000 installs, with around half coming from the U.S., after being introduced with little fanfare less than six months ago. Around 235,000 of its downloads came in December alone, representing 39% of its total count.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gizmo‚Äôs growth from October to December was 312%, with December installs up 50% month-over-month and November installs up 180% from October. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app is available on both iOS and Android.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/meet-gizmo-a-tiktok-for-interactive-vibe-coded-mini-apps/</guid><pubDate>Wed, 04 Feb 2026 21:45:32 +0000</pubDate></item><item><title>[NEW] Google‚Äôs Gemini app has surpassed 750M monthly active users (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/googles-gemini-app-has-surpassed-750m-monthly-active-users/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/google-gemini-jagmeet-singh-techcrunch.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google‚Äôs AI chatbot Gemini has surpassed 750 million monthly active users (MAUs), according to the company‚Äôs fourth-quarter 2025 earnings. This figure illustrates the rapid consumer adoption of Gemini, which has quickly become a prominent player in the AI space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last quarter, Google reported 650 million monthly active users for Gemini, indicating substantial growth in a short period. In comparison, Meta AI has reported nearly 500 million monthly users. However, while Gemini is gaining traction, it still trails behind its biggest rival, ChatGPT, which is estimated to have around 810 million MAUs in late 2025.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The recently revealed number comes on the heels of the launch of Gemini 3, which showcases the company‚Äôs most advanced model yet, providing responses that the company claims exhibit an unprecedented level of depth and nuance. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CEO Sundar Pichai highlighted that the introduction of Gemini 3 in AI mode was a ‚Äúpositive driver‚Äù for the company‚Äôs growth and emphasized that continued investment and iteration will maintain this momentum.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google recently rolled out a more affordable plan, the Google AI Plus, priced at $7.99 per month. The plan is expected to drive further growth by appealing to budget-conscious consumers, although it was made available too recently to have any influence on the quarterly figures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe are focused on a free tier and subscriptions and seeing great growth,‚Äù Philipp Schindler, Google‚Äôs chief business officer, said during the call to investors. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini‚Äôs growth is particularly notable given Alphabet‚Äôs overall financial performance. The company has surpassed $400 billion in annual revenue for the first time this quarter. Google attributes the achievement to the expansion of its AI division, which has seen increased demand. Recently, Google introduced the latest generation of its TPU AI accelerator chip, called Ironwood, to compete with Nvidia.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThe launch of Gemini 3 was a major milestone, and we have great momentum. Our first party models, like Gemini, now process over 10 billion tokens per minute via direct API use by our customers, and the Gemini App has grown to over 750 million monthly active users. Search saw more usage than ever before, with AI continuing to drive an expansionary moment,‚Äù Pichai stated in today‚Äôs release.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/google-gemini-jagmeet-singh-techcrunch.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google‚Äôs AI chatbot Gemini has surpassed 750 million monthly active users (MAUs), according to the company‚Äôs fourth-quarter 2025 earnings. This figure illustrates the rapid consumer adoption of Gemini, which has quickly become a prominent player in the AI space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last quarter, Google reported 650 million monthly active users for Gemini, indicating substantial growth in a short period. In comparison, Meta AI has reported nearly 500 million monthly users. However, while Gemini is gaining traction, it still trails behind its biggest rival, ChatGPT, which is estimated to have around 810 million MAUs in late 2025.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The recently revealed number comes on the heels of the launch of Gemini 3, which showcases the company‚Äôs most advanced model yet, providing responses that the company claims exhibit an unprecedented level of depth and nuance. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CEO Sundar Pichai highlighted that the introduction of Gemini 3 in AI mode was a ‚Äúpositive driver‚Äù for the company‚Äôs growth and emphasized that continued investment and iteration will maintain this momentum.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google recently rolled out a more affordable plan, the Google AI Plus, priced at $7.99 per month. The plan is expected to drive further growth by appealing to budget-conscious consumers, although it was made available too recently to have any influence on the quarterly figures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe are focused on a free tier and subscriptions and seeing great growth,‚Äù Philipp Schindler, Google‚Äôs chief business officer, said during the call to investors. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini‚Äôs growth is particularly notable given Alphabet‚Äôs overall financial performance. The company has surpassed $400 billion in annual revenue for the first time this quarter. Google attributes the achievement to the expansion of its AI division, which has seen increased demand. Recently, Google introduced the latest generation of its TPU AI accelerator chip, called Ironwood, to compete with Nvidia.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThe launch of Gemini 3 was a major milestone, and we have great momentum. Our first party models, like Gemini, now process over 10 billion tokens per minute via direct API use by our customers, and the Gemini App has grown to over 750 million monthly active users. Search saw more usage than ever before, with AI continuing to drive an expansionary moment,‚Äù Pichai stated in today‚Äôs release.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/googles-gemini-app-has-surpassed-750m-monthly-active-users/</guid><pubDate>Wed, 04 Feb 2026 22:53:46 +0000</pubDate></item><item><title>[NEW] Alphabet won‚Äôt talk about the Google-Apple AI deal, even to investors (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/alphabet-wont-talk-about-the-google-apple-ai-deal-even-to-investors/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/imgi_38_mp840x830mattef8f8f8t-pad1000x1000f8f8f8.jpg?w=1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Alphabet declined to answer one of its investors during questions about Google‚Äôs AI deal with Apple on Wednesday‚Äôs fourth-quarter earnings call. Instead of responding to an analyst‚Äôs question about how the tech giant is thinking about AI partnerships, such as the one with Apple to power AI for Siri, the question was completely ignored.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That decision tells us something, though ‚Äî Alphabet isn‚Äôt ready to talk about how this partnership will impact its core business, which is increasingly focused on AI. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Over the years, the Google-Apple relationship has been mutually beneficial. The two companies‚Äô search partnership saw the search giant paying the iPhone maker $20 billion to be the default search engine on Apple devices, filings from the Department of Justice‚Äôs lawsuit against the search giant revealed. In turn, Google gained access to Apple‚Äôs massive customer base ‚Äî the iPhone maker last quarter announced it has 2.5 billion active devices worldwide, to give you an idea of scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest Apple AI deal is rumored to cost Apple roughly $1 billion per year, but the payoff beyond that for Google isn‚Äôt as immediately obvious as it is with search. In Google Search, consumers see links to advertisers‚Äô websites at the top of their search results. Ads in AI Mode, which could one day represent the future of Google‚Äôs search business, are still an ‚Äúexperiment‚Äù for now.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company first announced last May that it would bring ads to AI Mode, the chatbot-style interface for Google Search, but these tests see the ads‚Äô placement below or integrated into the chatbot‚Äôs responses. Google is also trying out agentic shopping, including Shop with AI Mode, to guide consumers with product-related queries to a seamless checkout experience from the AI interface. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Google‚Äôs AI competitor Anthropic is taking aim at ad-supported AI with its forthcoming Super Bowl ad, which challenges the business model being adopted by ChatGPT maker OpenAI and Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;How this will all play out longer-term is still an open question ‚Äî and for today, an unanswered one, apparently. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, the Apple Siri deal barely received any mention during Alphabet‚Äôs earnings call on Wednesday. Sundar Pichai only noted he was pleased that Apple‚Äôs ‚Äúpreferred cloud provider‚Äù and would be helping to develop ‚Äúthe next generation of Apple foundation models based on Gemini technology.‚Äù Google‚Äôs Chief Business Officer Philipp Schindler used the exact same wording when mentioning Apple, as well.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/imgi_38_mp840x830mattef8f8f8t-pad1000x1000f8f8f8.jpg?w=1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Alphabet declined to answer one of its investors during questions about Google‚Äôs AI deal with Apple on Wednesday‚Äôs fourth-quarter earnings call. Instead of responding to an analyst‚Äôs question about how the tech giant is thinking about AI partnerships, such as the one with Apple to power AI for Siri, the question was completely ignored.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That decision tells us something, though ‚Äî Alphabet isn‚Äôt ready to talk about how this partnership will impact its core business, which is increasingly focused on AI. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Over the years, the Google-Apple relationship has been mutually beneficial. The two companies‚Äô search partnership saw the search giant paying the iPhone maker $20 billion to be the default search engine on Apple devices, filings from the Department of Justice‚Äôs lawsuit against the search giant revealed. In turn, Google gained access to Apple‚Äôs massive customer base ‚Äî the iPhone maker last quarter announced it has 2.5 billion active devices worldwide, to give you an idea of scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest Apple AI deal is rumored to cost Apple roughly $1 billion per year, but the payoff beyond that for Google isn‚Äôt as immediately obvious as it is with search. In Google Search, consumers see links to advertisers‚Äô websites at the top of their search results. Ads in AI Mode, which could one day represent the future of Google‚Äôs search business, are still an ‚Äúexperiment‚Äù for now.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company first announced last May that it would bring ads to AI Mode, the chatbot-style interface for Google Search, but these tests see the ads‚Äô placement below or integrated into the chatbot‚Äôs responses. Google is also trying out agentic shopping, including Shop with AI Mode, to guide consumers with product-related queries to a seamless checkout experience from the AI interface. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Google‚Äôs AI competitor Anthropic is taking aim at ad-supported AI with its forthcoming Super Bowl ad, which challenges the business model being adopted by ChatGPT maker OpenAI and Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;How this will all play out longer-term is still an open question ‚Äî and for today, an unanswered one, apparently. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, the Apple Siri deal barely received any mention during Alphabet‚Äôs earnings call on Wednesday. Sundar Pichai only noted he was pleased that Apple‚Äôs ‚Äúpreferred cloud provider‚Äù and would be helping to develop ‚Äúthe next generation of Apple foundation models based on Gemini technology.‚Äù Google‚Äôs Chief Business Officer Philipp Schindler used the exact same wording when mentioning Apple, as well.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/alphabet-wont-talk-about-the-google-apple-ai-deal-even-to-investors/</guid><pubDate>Wed, 04 Feb 2026 23:28:31 +0000</pubDate></item><item><title>[NEW] Sam Altman got exceptionally testy over Claude Super Bowl ads (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/sam-altman-got-exceptionally-testy-over-claude-super-bowl-ads/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/10/Sam-Altman-OpenAI-DSC02881.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic‚Äôs Super Bowl commercial, one of four ads the AI lab dropped on Wednesday, begins with the word ‚ÄúBETRAYAL‚Äù splashed boldly across the screen. The camera pans to a man earnestly asking a chatbot (obviously intended to depict ChatGPT) for advice on how to talk to his mom.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bot, portrayed by a blonde woman, offers some classic bits of advice. Start by listening. Try a nature walk! And then twists into an ad for a fictitious (we hope!) cougar-dating site called Golden Encounters. Anthropic finishes the spot by saying that while ads are coming to AI, they won‚Äôt be coming to its own chatbot, Claude.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Another commercial features a slight young man looking for advice on building a six pack. After offering his height, age, and weight, the bot serves him an ad for height-boosting insoles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Anthropic commercials are cleverly aimed at OpenAI‚Äôs users, after that company‚Äôs recent announcement that ads will be coming to ChatGPT‚Äôs free tier. And they caused an immediate stir, spawning headlines that Anthropic ‚Äúmocks,‚Äù ‚Äúskewers,‚Äù and ‚Äúdunks on‚Äù OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They are funny enough that even Sam Altman admitted on X that he laughed at them. But he clearly didn‚Äôt really find them funny. They inspired him to write a novella-sized rant that devolved into calling his rival ‚Äúdishonest‚Äù and ‚Äúauthoritarian.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;First, the good part of the Anthropic ads: they are funny, and I laughed.&lt;/p&gt;&lt;p&gt;But I wonder why Anthropic would go for something so clearly dishonest. Our most important principle for ads says that we won‚Äôt do exactly this; we would obviously never run ads in the way Anthropic‚Ä¶&lt;/p&gt;‚Äî Sam Altman (@sama) February 4, 2026&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In that post, Altman explains that an ad-supported tier is intended to shoulder the burden of offering free ChatGPT to many of its millions of users. ChatGPT is still the most popular chatbot by a large margin.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the OpenAI CEO insisted the ads were ‚Äúdishonest‚Äù in implying that ChatGPT will twist a conversation to insert an ad (and possibly for an off-color product, to boot).‚ÄùWe would obviously never run ads in the way Anthropic depicts them,‚Äù Altman wrote in the social media post. ‚ÄúWe are not stupid and we know our users would reject that.‚Äù&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Indeed, OpenAI has promised ads will be separate, labeled, and will never influence a chat. But the company has also said it is planning on making them conversation-specific ‚Äî which is the central allegation of Anthropic‚Äôs ads. As OpenAI explained on its blog, ‚ÄúWe plan to test ads at the bottom of answers in ChatGPT when there‚Äôs a relevant sponsored product or service based on your current conversation.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman then went on to fling some equally questionable assertions at his rival. ‚ÄúAnthropic serves an expensive product to rich people,‚Äù he wrote. ‚ÄúWe also feel strongly that we need to bring AI to billions of people who can‚Äôt pay for subscriptions.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Claude has a free chat tier, too, with subscriptions at $0, $17, $100, and $200. ChatGPT‚Äôs tiers are $0, $8, $20, and $200. One could argue the subscription tiers are fairly equivalent.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Altman also alleged in his post that ‚ÄúAnthropic wants to control what people do with AI.‚Äù He argues it blocks usage of Claude Code from ‚Äúcompanies they don‚Äôt like,‚Äù like OpenAI, and said Anthropic tells people what they can and can‚Äôt use AI for.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;True, Anthropic‚Äôs whole marketing deal since day one has been ‚Äúresponsible AI.‚Äù The company was founded by two former OpenAI alums, after all, who claimed they grew alarmed about AI safety when they worked there.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, both chatbot companies have usage policies, AI guardrails, and talk about AI safety. And while OpenAI allows ChatGPT to be used for erotica while Anthropic does not, OpenAI, like Anthropic, has determined that some content should be blocked, particularly in regards to mental health.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yet Altman took this Anthropic-tells-you-what-to-do argument to an extreme level when he accused Anthropic of being ‚Äúauthoritarian.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúOne authoritarian company won‚Äôt get us there on their own, to say nothing of the other obvious risks. It is a dark path,‚Äù he wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Using ‚Äúauthoritarian‚Äù in a rant over a cheeky Super Bowl ad is misplaced, at best. It‚Äôs particularly tactless when considering the current geopolitical environment in which protesters around the world have been killed by agents of their own government. While business rivals have been duking it out in ads since the beginning of time, clearly Anthropic hit a nerve.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/10/Sam-Altman-OpenAI-DSC02881.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic‚Äôs Super Bowl commercial, one of four ads the AI lab dropped on Wednesday, begins with the word ‚ÄúBETRAYAL‚Äù splashed boldly across the screen. The camera pans to a man earnestly asking a chatbot (obviously intended to depict ChatGPT) for advice on how to talk to his mom.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bot, portrayed by a blonde woman, offers some classic bits of advice. Start by listening. Try a nature walk! And then twists into an ad for a fictitious (we hope!) cougar-dating site called Golden Encounters. Anthropic finishes the spot by saying that while ads are coming to AI, they won‚Äôt be coming to its own chatbot, Claude.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Another commercial features a slight young man looking for advice on building a six pack. After offering his height, age, and weight, the bot serves him an ad for height-boosting insoles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Anthropic commercials are cleverly aimed at OpenAI‚Äôs users, after that company‚Äôs recent announcement that ads will be coming to ChatGPT‚Äôs free tier. And they caused an immediate stir, spawning headlines that Anthropic ‚Äúmocks,‚Äù ‚Äúskewers,‚Äù and ‚Äúdunks on‚Äù OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They are funny enough that even Sam Altman admitted on X that he laughed at them. But he clearly didn‚Äôt really find them funny. They inspired him to write a novella-sized rant that devolved into calling his rival ‚Äúdishonest‚Äù and ‚Äúauthoritarian.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;First, the good part of the Anthropic ads: they are funny, and I laughed.&lt;/p&gt;&lt;p&gt;But I wonder why Anthropic would go for something so clearly dishonest. Our most important principle for ads says that we won‚Äôt do exactly this; we would obviously never run ads in the way Anthropic‚Ä¶&lt;/p&gt;‚Äî Sam Altman (@sama) February 4, 2026&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In that post, Altman explains that an ad-supported tier is intended to shoulder the burden of offering free ChatGPT to many of its millions of users. ChatGPT is still the most popular chatbot by a large margin.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the OpenAI CEO insisted the ads were ‚Äúdishonest‚Äù in implying that ChatGPT will twist a conversation to insert an ad (and possibly for an off-color product, to boot).‚ÄùWe would obviously never run ads in the way Anthropic depicts them,‚Äù Altman wrote in the social media post. ‚ÄúWe are not stupid and we know our users would reject that.‚Äù&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Indeed, OpenAI has promised ads will be separate, labeled, and will never influence a chat. But the company has also said it is planning on making them conversation-specific ‚Äî which is the central allegation of Anthropic‚Äôs ads. As OpenAI explained on its blog, ‚ÄúWe plan to test ads at the bottom of answers in ChatGPT when there‚Äôs a relevant sponsored product or service based on your current conversation.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman then went on to fling some equally questionable assertions at his rival. ‚ÄúAnthropic serves an expensive product to rich people,‚Äù he wrote. ‚ÄúWe also feel strongly that we need to bring AI to billions of people who can‚Äôt pay for subscriptions.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Claude has a free chat tier, too, with subscriptions at $0, $17, $100, and $200. ChatGPT‚Äôs tiers are $0, $8, $20, and $200. One could argue the subscription tiers are fairly equivalent.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Altman also alleged in his post that ‚ÄúAnthropic wants to control what people do with AI.‚Äù He argues it blocks usage of Claude Code from ‚Äúcompanies they don‚Äôt like,‚Äù like OpenAI, and said Anthropic tells people what they can and can‚Äôt use AI for.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;True, Anthropic‚Äôs whole marketing deal since day one has been ‚Äúresponsible AI.‚Äù The company was founded by two former OpenAI alums, after all, who claimed they grew alarmed about AI safety when they worked there.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, both chatbot companies have usage policies, AI guardrails, and talk about AI safety. And while OpenAI allows ChatGPT to be used for erotica while Anthropic does not, OpenAI, like Anthropic, has determined that some content should be blocked, particularly in regards to mental health.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yet Altman took this Anthropic-tells-you-what-to-do argument to an extreme level when he accused Anthropic of being ‚Äúauthoritarian.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúOne authoritarian company won‚Äôt get us there on their own, to say nothing of the other obvious risks. It is a dark path,‚Äù he wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Using ‚Äúauthoritarian‚Äù in a rant over a cheeky Super Bowl ad is misplaced, at best. It‚Äôs particularly tactless when considering the current geopolitical environment in which protesters around the world have been killed by agents of their own government. While business rivals have been duking it out in ads since the beginning of time, clearly Anthropic hit a nerve.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/sam-altman-got-exceptionally-testy-over-claude-super-bowl-ads/</guid><pubDate>Thu, 05 Feb 2026 00:45:11 +0000</pubDate></item></channel></rss>