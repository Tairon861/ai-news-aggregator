<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 05 Feb 2026 07:02:50 +0000</lastBuildDate><item><title>What a16z is actually funding (and what it’s ignoring) when it comes to AI infra (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/what-a16z-is-actually-funding-and-what-its-ignoring-when-it-comes-to-ai-infra/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/a16z-Andreessen-Horowitz.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Andreessen Horowitz just raised&amp;nbsp;a whopping&amp;nbsp;new&amp;nbsp;$15 billion&amp;nbsp;in funding.&amp;nbsp;And a&amp;nbsp;$1.7 billion&amp;nbsp;chunk&amp;nbsp;of that is going to&amp;nbsp;its&amp;nbsp;infrastructure&amp;nbsp;team,&amp;nbsp;the one responsible for some of its biggest, most prominent&amp;nbsp;AI investments, including&amp;nbsp;Black Forest Labs, Cursor, OpenAI,&amp;nbsp;ElevenLabs, Ideogram,&amp;nbsp;Fal,&amp;nbsp;and dozens of others.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;a16z&amp;nbsp;general partner&amp;nbsp;with the infra team Jennifer Li&amp;nbsp;(who oversees such&amp;nbsp;investments&amp;nbsp;as&amp;nbsp;ElevenLabs&amp;nbsp;— just valued at&amp;nbsp;$11 billion) has a clear thesis on where&amp;nbsp;the team is looking to spend&amp;nbsp;it’s&amp;nbsp;latest chunk of cash.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;These investors are no strangers to billion-dollar budgets. When the firm raised&amp;nbsp;$7.2 billion&amp;nbsp;in 2024, the infra team&amp;nbsp;was handed&amp;nbsp;$1.25&amp;nbsp;billion&amp;nbsp;at the time, more than any other&amp;nbsp;vertical team.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;So what’s so exciting about infrastructure, especially in 2026? It covers everything from chip design all the way up to any software stack used by developers. This is the heartbeat of AI development and it is undergoing a never-before-seen transformation process, both in how AI is being used in these areas (AI coding) and the AI available to devs (ElevenLabs voice models, and Fal’s multi-modal model marketplace).&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;So Li is on the front lines of where AI is today, where it is going in 2026, and what is and likely may never be possible. She’s, for instance, skeptical about some of the industry’s biggest assumptions, including the idea that AI will replace human creativity anytime soon.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast,&amp;nbsp;Venture and&amp;nbsp;Startups Editor&amp;nbsp;Julie Bort talked with Li&amp;nbsp;about where a16z sees this&amp;nbsp;AI super cycle&amp;nbsp;going next,&amp;nbsp;including&amp;nbsp;the talent crunch hitting AI-native startups, why search infrastructure matters more than people think, and what kinds of companies are&amp;nbsp;actually getting&amp;nbsp;funded right now.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Where Li thinks the gaps still are when it comes to startups building an AI stack&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;What makes the most successful AI portfolio companies different&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How tools like voice AI are rising in importance (yet still a bit uncomfortable to witness)&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;The AI startups she’s still searching for and is ready to fund&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on YouTube, Apple Podcasts, Overcast, Spotify and all the casts. You also can follow Equity on X and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/a16z-Andreessen-Horowitz.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Andreessen Horowitz just raised&amp;nbsp;a whopping&amp;nbsp;new&amp;nbsp;$15 billion&amp;nbsp;in funding.&amp;nbsp;And a&amp;nbsp;$1.7 billion&amp;nbsp;chunk&amp;nbsp;of that is going to&amp;nbsp;its&amp;nbsp;infrastructure&amp;nbsp;team,&amp;nbsp;the one responsible for some of its biggest, most prominent&amp;nbsp;AI investments, including&amp;nbsp;Black Forest Labs, Cursor, OpenAI,&amp;nbsp;ElevenLabs, Ideogram,&amp;nbsp;Fal,&amp;nbsp;and dozens of others.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;a16z&amp;nbsp;general partner&amp;nbsp;with the infra team Jennifer Li&amp;nbsp;(who oversees such&amp;nbsp;investments&amp;nbsp;as&amp;nbsp;ElevenLabs&amp;nbsp;— just valued at&amp;nbsp;$11 billion) has a clear thesis on where&amp;nbsp;the team is looking to spend&amp;nbsp;it’s&amp;nbsp;latest chunk of cash.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;These investors are no strangers to billion-dollar budgets. When the firm raised&amp;nbsp;$7.2 billion&amp;nbsp;in 2024, the infra team&amp;nbsp;was handed&amp;nbsp;$1.25&amp;nbsp;billion&amp;nbsp;at the time, more than any other&amp;nbsp;vertical team.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;So what’s so exciting about infrastructure, especially in 2026? It covers everything from chip design all the way up to any software stack used by developers. This is the heartbeat of AI development and it is undergoing a never-before-seen transformation process, both in how AI is being used in these areas (AI coding) and the AI available to devs (ElevenLabs voice models, and Fal’s multi-modal model marketplace).&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;So Li is on the front lines of where AI is today, where it is going in 2026, and what is and likely may never be possible. She’s, for instance, skeptical about some of the industry’s biggest assumptions, including the idea that AI will replace human creativity anytime soon.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast,&amp;nbsp;Venture and&amp;nbsp;Startups Editor&amp;nbsp;Julie Bort talked with Li&amp;nbsp;about where a16z sees this&amp;nbsp;AI super cycle&amp;nbsp;going next,&amp;nbsp;including&amp;nbsp;the talent crunch hitting AI-native startups, why search infrastructure matters more than people think, and what kinds of companies are&amp;nbsp;actually getting&amp;nbsp;funded right now.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Where Li thinks the gaps still are when it comes to startups building an AI stack&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;What makes the most successful AI portfolio companies different&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How tools like voice AI are rising in importance (yet still a bit uncomfortable to witness)&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;The AI startups she’s still searching for and is ready to fund&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on YouTube, Apple Podcasts, Overcast, Spotify and all the casts. You also can follow Equity on X and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/what-a16z-is-actually-funding-and-what-its-ignoring-when-it-comes-to-ai-infra/</guid><pubDate>Wed, 04 Feb 2026 20:19:12 +0000</pubDate></item><item><title>A16z just raised $1.7B for AI infrastructure. Here’s where it’s going. (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/video/a16z-just-raised-1-7b-for-ai-infrastructure-heres-where-its-going/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2018/02/tc-backlight-e1689786273147.png?w=1200" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30896071"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Andreessen Horowitz just raised&amp;nbsp;a whopping&amp;nbsp;⁠&lt;u&gt;new&amp;nbsp;$15 billion&amp;nbsp;in funding&lt;/u&gt;⁠.&amp;nbsp;And a&amp;nbsp;$1.7 billion&amp;nbsp;chunk&amp;nbsp;of that is going to&amp;nbsp;its&amp;nbsp;⁠&lt;u&gt;infrastructure&amp;nbsp;team&lt;/u&gt;⁠,&amp;nbsp;the one responsible for some of its biggest, most prominent&amp;nbsp;AI investments including&amp;nbsp;Black Forrest Labs, Cursor, OpenAI,&amp;nbsp;⁠ElevenLabs⁠, Ideogram,&amp;nbsp;⁠Fal⁠&amp;nbsp;and dozens of others.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;A16z&amp;nbsp;⁠general partner&amp;nbsp;with the infra team Jennifer Li⁠&amp;nbsp;(who oversees such&amp;nbsp;investments&amp;nbsp;as&amp;nbsp;ElevenLabs&amp;nbsp;– just valued at&amp;nbsp;$11 billion);&amp;nbsp;Ideagram&amp;nbsp;and Fal, has a clear thesis on where&amp;nbsp;the team is looking to spend&amp;nbsp;it’s&amp;nbsp;latest chunk of cash.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Watch as&amp;nbsp;Venture and&amp;nbsp;Startups editor&amp;nbsp;Julie Bort talks with Li&amp;nbsp;on ⁠Equity⁠&amp;nbsp;about where&amp;nbsp;a16z&amp;nbsp;sees this&amp;nbsp;AI super cycle&amp;nbsp;going next,&amp;nbsp;including&amp;nbsp;the talent crunch hitting AI-native startups, why search infrastructure matters more than people think, and what kinds of companies are&amp;nbsp;actually getting&amp;nbsp;funded right now.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2018/02/tc-backlight-e1689786273147.png?w=1200" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30896071"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Andreessen Horowitz just raised&amp;nbsp;a whopping&amp;nbsp;⁠&lt;u&gt;new&amp;nbsp;$15 billion&amp;nbsp;in funding&lt;/u&gt;⁠.&amp;nbsp;And a&amp;nbsp;$1.7 billion&amp;nbsp;chunk&amp;nbsp;of that is going to&amp;nbsp;its&amp;nbsp;⁠&lt;u&gt;infrastructure&amp;nbsp;team&lt;/u&gt;⁠,&amp;nbsp;the one responsible for some of its biggest, most prominent&amp;nbsp;AI investments including&amp;nbsp;Black Forrest Labs, Cursor, OpenAI,&amp;nbsp;⁠ElevenLabs⁠, Ideogram,&amp;nbsp;⁠Fal⁠&amp;nbsp;and dozens of others.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;A16z&amp;nbsp;⁠general partner&amp;nbsp;with the infra team Jennifer Li⁠&amp;nbsp;(who oversees such&amp;nbsp;investments&amp;nbsp;as&amp;nbsp;ElevenLabs&amp;nbsp;– just valued at&amp;nbsp;$11 billion);&amp;nbsp;Ideagram&amp;nbsp;and Fal, has a clear thesis on where&amp;nbsp;the team is looking to spend&amp;nbsp;it’s&amp;nbsp;latest chunk of cash.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Watch as&amp;nbsp;Venture and&amp;nbsp;Startups editor&amp;nbsp;Julie Bort talks with Li&amp;nbsp;on ⁠Equity⁠&amp;nbsp;about where&amp;nbsp;a16z&amp;nbsp;sees this&amp;nbsp;AI super cycle&amp;nbsp;going next,&amp;nbsp;including&amp;nbsp;the talent crunch hitting AI-native startups, why search infrastructure matters more than people think, and what kinds of companies are&amp;nbsp;actually getting&amp;nbsp;funded right now.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/video/a16z-just-raised-1-7b-for-ai-infrastructure-heres-where-its-going/</guid><pubDate>Wed, 04 Feb 2026 20:24:12 +0000</pubDate></item><item><title>Should AI chatbots have ads? Anthropic says no. (AI - Ars Technica)</title><link>https://arstechnica.com/ai/2026/02/should-ai-chatbots-have-ads-anthropic-says-no/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        ChatGPT competitor comes out swinging with Super Bowl ad mocking AI product pitches.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An illustration of a hand holding a shape created by Anthropic." class="absolute inset-0 w-full h-full object-cover hidden" height="356" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/claude-no-ads-640x356.png" width="640" /&gt;
                  &lt;img alt="An illustration of a hand holding a shape created by Anthropic." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/claude-no-ads-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday, Anthropic announced that its AI chatbot, Claude, will remain free of advertisements, drawing a sharp line between itself and rival OpenAI, which began testing ads in a low-cost tier of ChatGPT last month. The announcement comes alongside a Super Bowl ad campaign that mocks AI assistants that interrupt personal conversations with product pitches.&lt;/p&gt;
&lt;p&gt;“There are many good places for advertising. A conversation with Claude is not one of them,” Anthropic wrote in a blog post. The company argued that including ads in AI conversations would be “incompatible” with what it wants Claude to be: “a genuinely helpful assistant for work and for deep thinking.”&lt;/p&gt;
&lt;p&gt;The stance contrasts with OpenAI’s January announcement that it would begin testing banner ads for free users and ChatGPT Go subscribers in the US. OpenAI said those ads would appear at the bottom of responses and would not influence the chatbot’s actual answers. Paid subscribers on Plus, Pro, Business, and Enterprise tiers will not see ads on ChatGPT.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Anthropic’s 2026 Super Bowl commercial.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;“We want Claude to act unambiguously in our users’ interests,” Anthropic wrote. “So we’ve made a choice: Claude will remain ad-free. Our users won’t see ‘sponsored’ links adjacent to their conversations with Claude; nor will Claude’s responses be influenced by advertisers or include third-party product placements our users did not ask for.”&lt;/p&gt;
&lt;p&gt;Competition between OpenAI and Anthropic has been fierce of late, due to the rise of AI coding agents. Claude Code, Anthropic’s coding tool, and OpenAI’s Codex have similar capabilities, but Claude Code has been widely popular among developers and is closing in on OpenAI’s turf. Last month, The Verge reported that many developers inside long-time OpenAI benefactor Microsoft have been adopting Claude Code, choosing Anthropic products over Microsoft’s Copilot, which is powered by tech that originated at OpenAI.&lt;/p&gt;
&lt;p&gt;In this climate, Anthropic could not resist taking a dig at OpenAI. In its Super Bowl commercial, we see a thin man struggling to do a pull-up beside a buff fitness instructor, who is a stand-in for an AI assistant. The man asks the “assistant” for help making a workout plan, but the assistant slips in an advertisement for a supplement, confusing the man. The commercial doesn’t name any names, and OpenAI has said it will not include ads in chat text itself, but Anthropic’s implications are clear.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Different incentives, different futures&lt;/h2&gt;
&lt;p&gt;In its blog post, Anthropic describes internal analysis it conducted that suggests many Claude conversations involve topics that are “sensitive or deeply personal” or require sustained focus on complex tasks. In these contexts, Anthropic wrote, “The appearance of ads would feel incongruous—and, in many cases, inappropriate.”&lt;/p&gt;
&lt;p&gt;The company also argued that advertising introduces&amp;nbsp;incentives that could conflict with providing genuinely helpful advice. It gave the example of a user mentioning trouble sleeping: an ad-free assistant would explore various causes, while an ad-supported one might steer the conversation toward a transaction.&lt;/p&gt;
&lt;p&gt;“Users shouldn’t have to second-guess whether an AI is genuinely helping them or subtly steering the conversation towards something monetizable,” Anthropic wrote.&lt;/p&gt;
&lt;p&gt;Currently, OpenAI does not plan to include paid product recommendations within a ChatGPT conversation. Instead, the ads appear as banners alongside the conversation text.&lt;/p&gt;
&lt;p&gt;OpenAI CEO Sam Altman has previously expressed reservations about mixing ads and AI conversations. In a 2024 interview at Harvard University, he described the combination as “uniquely unsettling” and said he would not like having to “figure out exactly how much was who paying here to influence what I’m being shown.”&lt;/p&gt;
&lt;p&gt;A key part of Altman’s partial change of heart is that OpenAI faces enormous financial pressure. The company made more than $1.4 trillion worth of infrastructure deals in 2025, and according to documents obtained by The Wall Street Journal, it expects to burn through roughly $9 billion this year while generating $13 billion in revenue. Only about 5 percent of ChatGPT’s 800 million weekly users pay for subscriptions.&lt;/p&gt;
&lt;p&gt;Much like OpenAI, Anthropic is not yet profitable, but it is expected to get there much faster. Anthropic has not attempted to span the world with massive datacenters, and its business model largely relies on enterprise contracts and paid subscriptions. The company says Claude Code and Cowork have already brought in at least $1 billion in revenue, according to Axios.&lt;/p&gt;
&lt;p&gt;“Our business model is straightforward,” Anthropic wrote. “This is a choice with tradeoffs, and we respect that other AI companies might reasonably reach different conclusions.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        ChatGPT competitor comes out swinging with Super Bowl ad mocking AI product pitches.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An illustration of a hand holding a shape created by Anthropic." class="absolute inset-0 w-full h-full object-cover hidden" height="356" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/claude-no-ads-640x356.png" width="640" /&gt;
                  &lt;img alt="An illustration of a hand holding a shape created by Anthropic." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/claude-no-ads-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday, Anthropic announced that its AI chatbot, Claude, will remain free of advertisements, drawing a sharp line between itself and rival OpenAI, which began testing ads in a low-cost tier of ChatGPT last month. The announcement comes alongside a Super Bowl ad campaign that mocks AI assistants that interrupt personal conversations with product pitches.&lt;/p&gt;
&lt;p&gt;“There are many good places for advertising. A conversation with Claude is not one of them,” Anthropic wrote in a blog post. The company argued that including ads in AI conversations would be “incompatible” with what it wants Claude to be: “a genuinely helpful assistant for work and for deep thinking.”&lt;/p&gt;
&lt;p&gt;The stance contrasts with OpenAI’s January announcement that it would begin testing banner ads for free users and ChatGPT Go subscribers in the US. OpenAI said those ads would appear at the bottom of responses and would not influence the chatbot’s actual answers. Paid subscribers on Plus, Pro, Business, and Enterprise tiers will not see ads on ChatGPT.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Anthropic’s 2026 Super Bowl commercial.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;“We want Claude to act unambiguously in our users’ interests,” Anthropic wrote. “So we’ve made a choice: Claude will remain ad-free. Our users won’t see ‘sponsored’ links adjacent to their conversations with Claude; nor will Claude’s responses be influenced by advertisers or include third-party product placements our users did not ask for.”&lt;/p&gt;
&lt;p&gt;Competition between OpenAI and Anthropic has been fierce of late, due to the rise of AI coding agents. Claude Code, Anthropic’s coding tool, and OpenAI’s Codex have similar capabilities, but Claude Code has been widely popular among developers and is closing in on OpenAI’s turf. Last month, The Verge reported that many developers inside long-time OpenAI benefactor Microsoft have been adopting Claude Code, choosing Anthropic products over Microsoft’s Copilot, which is powered by tech that originated at OpenAI.&lt;/p&gt;
&lt;p&gt;In this climate, Anthropic could not resist taking a dig at OpenAI. In its Super Bowl commercial, we see a thin man struggling to do a pull-up beside a buff fitness instructor, who is a stand-in for an AI assistant. The man asks the “assistant” for help making a workout plan, but the assistant slips in an advertisement for a supplement, confusing the man. The commercial doesn’t name any names, and OpenAI has said it will not include ads in chat text itself, but Anthropic’s implications are clear.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Different incentives, different futures&lt;/h2&gt;
&lt;p&gt;In its blog post, Anthropic describes internal analysis it conducted that suggests many Claude conversations involve topics that are “sensitive or deeply personal” or require sustained focus on complex tasks. In these contexts, Anthropic wrote, “The appearance of ads would feel incongruous—and, in many cases, inappropriate.”&lt;/p&gt;
&lt;p&gt;The company also argued that advertising introduces&amp;nbsp;incentives that could conflict with providing genuinely helpful advice. It gave the example of a user mentioning trouble sleeping: an ad-free assistant would explore various causes, while an ad-supported one might steer the conversation toward a transaction.&lt;/p&gt;
&lt;p&gt;“Users shouldn’t have to second-guess whether an AI is genuinely helping them or subtly steering the conversation towards something monetizable,” Anthropic wrote.&lt;/p&gt;
&lt;p&gt;Currently, OpenAI does not plan to include paid product recommendations within a ChatGPT conversation. Instead, the ads appear as banners alongside the conversation text.&lt;/p&gt;
&lt;p&gt;OpenAI CEO Sam Altman has previously expressed reservations about mixing ads and AI conversations. In a 2024 interview at Harvard University, he described the combination as “uniquely unsettling” and said he would not like having to “figure out exactly how much was who paying here to influence what I’m being shown.”&lt;/p&gt;
&lt;p&gt;A key part of Altman’s partial change of heart is that OpenAI faces enormous financial pressure. The company made more than $1.4 trillion worth of infrastructure deals in 2025, and according to documents obtained by The Wall Street Journal, it expects to burn through roughly $9 billion this year while generating $13 billion in revenue. Only about 5 percent of ChatGPT’s 800 million weekly users pay for subscriptions.&lt;/p&gt;
&lt;p&gt;Much like OpenAI, Anthropic is not yet profitable, but it is expected to get there much faster. Anthropic has not attempted to span the world with massive datacenters, and its business model largely relies on enterprise contracts and paid subscriptions. The company says Claude Code and Cowork have already brought in at least $1 billion in revenue, according to Axios.&lt;/p&gt;
&lt;p&gt;“Our business model is straightforward,” Anthropic wrote. “This is a choice with tradeoffs, and we respect that other AI companies might reasonably reach different conclusions.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2026/02/should-ai-chatbots-have-ads-anthropic-says-no/</guid><pubDate>Wed, 04 Feb 2026 21:15:07 +0000</pubDate></item><item><title>Amazon to begin testing AI tools for film and TV production next month (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/amazon-to-begin-testing-ai-tools-for-film-and-tv-production-next-month/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Amazon-HouseofDavid.jpg?w=1080" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last summer, Amazon MGM Studios launched a dedicated AI Studio to develop proprietary AI tools to streamline TV and film production, with a focus on areas like improving character consistency across shots and supporting pre- and post-production.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to a report from Reuters, those tools are now ready to move beyond internal testing. Amazon will begin a closed beta program in March, inviting industry partners to try out its AI tools.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Amazon said it anticipates sharing initial outcomes from the program by May. The company chose not to provide further details on the developments when approached by TechCrunch for a comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI Studio is collaborating with notable producers like Robert Stromberg, known for “Maleficent,” Kunal Nayyar from “The Big Bang Theory,” and former animator Colin Brady from Pixar to learn the best way to implement these tools. Amazon is also tapping Amazon Web Services for support and intends to work with several LLM providers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Albert Cheng, who heads the AI Studios initiative, emphasized that the goal is to support creative teams, not to replace them. The focus is on improving efficiency and reducing costs while ensuring that intellectual property is protected and AI-generated content isn’t absorbed into other AI models. One example used is Amazon’s “House of David” series, which featured 350 AI-generated shots in season two.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;​However, the rise of adoption of AI in Hollywood has stirred up plenty of debate. Many people in the industry worry about what it means for jobs, creativity, and the future of filmmaking.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The conversations around AI are only getting louder as more companies experiment with these new tools. For instance, Netflix has also jumped on the AI bandwagon, with co-CEO Ted Sarandos revealing that its series “The Eternaut” used generative AI to create a building collapse scene.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent years, Amazon has cited its success with AI as a factor in layoffs. The company recently eliminated 16,000 jobs in January, following 14,000 layoffs last October.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Amazon-HouseofDavid.jpg?w=1080" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last summer, Amazon MGM Studios launched a dedicated AI Studio to develop proprietary AI tools to streamline TV and film production, with a focus on areas like improving character consistency across shots and supporting pre- and post-production.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to a report from Reuters, those tools are now ready to move beyond internal testing. Amazon will begin a closed beta program in March, inviting industry partners to try out its AI tools.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Amazon said it anticipates sharing initial outcomes from the program by May. The company chose not to provide further details on the developments when approached by TechCrunch for a comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI Studio is collaborating with notable producers like Robert Stromberg, known for “Maleficent,” Kunal Nayyar from “The Big Bang Theory,” and former animator Colin Brady from Pixar to learn the best way to implement these tools. Amazon is also tapping Amazon Web Services for support and intends to work with several LLM providers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Albert Cheng, who heads the AI Studios initiative, emphasized that the goal is to support creative teams, not to replace them. The focus is on improving efficiency and reducing costs while ensuring that intellectual property is protected and AI-generated content isn’t absorbed into other AI models. One example used is Amazon’s “House of David” series, which featured 350 AI-generated shots in season two.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;​However, the rise of adoption of AI in Hollywood has stirred up plenty of debate. Many people in the industry worry about what it means for jobs, creativity, and the future of filmmaking.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The conversations around AI are only getting louder as more companies experiment with these new tools. For instance, Netflix has also jumped on the AI bandwagon, with co-CEO Ted Sarandos revealing that its series “The Eternaut” used generative AI to create a building collapse scene.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent years, Amazon has cited its success with AI as a factor in layoffs. The company recently eliminated 16,000 jobs in January, following 14,000 layoffs last October.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/amazon-to-begin-testing-ai-tools-for-film-and-tv-production-next-month/</guid><pubDate>Wed, 04 Feb 2026 21:26:43 +0000</pubDate></item><item><title>AI SRE Resolve AI confirms $125M raise, unicorn valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/ai-sre-resolve-ai-confirms-125m-raise-unicorn-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/03/GettyImages-1319917047.jpg?resize=1200,764" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Resolve AI, a startup automating the work of system reliability engineering (SRE), aka troubleshooting system&amp;nbsp;failures, has announced a $125 million Series A at a $1 billion valuation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by Lightspeed Venture Partners, with participation of existing investors including Greylock Partners, Unusual Ventures, Artisanal Ventures, and A*.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The announcement confirms TechCrunch’s December report that the startup was raising at a billion-dollar valuation led by Lightspeed. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sources told TechCrunch at the time that the round may have consisted of multiple tranches, at different prices, which could have put the company’s actual blended valuation below $1 billion. A spokesperson for Resolve denied that there were multiple tranches in the round, saying that 100% of the equity was purchased at a valuation of $1 billion. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As we previously reported, this kind of structure allows certain investors, often the lead, to purchase a significant portion of equity at a lower price.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Resolve was co-founded in early 2024 by two former Splunk executives, Spiros Xanthos and Mayank Agarwal. Their previous startup, Omnition, was acquired by Splunk in 2019.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another startup applying AI to identify and resolve system outages is the Sequoia-backed Traversal. The emerging category is known as AI SRE. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/03/GettyImages-1319917047.jpg?resize=1200,764" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Resolve AI, a startup automating the work of system reliability engineering (SRE), aka troubleshooting system&amp;nbsp;failures, has announced a $125 million Series A at a $1 billion valuation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by Lightspeed Venture Partners, with participation of existing investors including Greylock Partners, Unusual Ventures, Artisanal Ventures, and A*.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The announcement confirms TechCrunch’s December report that the startup was raising at a billion-dollar valuation led by Lightspeed. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sources told TechCrunch at the time that the round may have consisted of multiple tranches, at different prices, which could have put the company’s actual blended valuation below $1 billion. A spokesperson for Resolve denied that there were multiple tranches in the round, saying that 100% of the equity was purchased at a valuation of $1 billion. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As we previously reported, this kind of structure allows certain investors, often the lead, to purchase a significant portion of equity at a lower price.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Resolve was co-founded in early 2024 by two former Splunk executives, Spiros Xanthos and Mayank Agarwal. Their previous startup, Omnition, was acquired by Splunk in 2019.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another startup applying AI to identify and resolve system outages is the Sequoia-backed Traversal. The emerging category is known as AI SRE. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/ai-sre-resolve-ai-confirms-125m-raise-unicorn-valuation/</guid><pubDate>Wed, 04 Feb 2026 21:39:26 +0000</pubDate></item><item><title>Meet Gizmo: A TikTok for interactive, vibe-coded mini apps (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/meet-gizmo-a-tiktok-for-interactive-vibe-coded-mini-apps/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Gizmo, a TikTok-like app for vibe-coded mini applications, is offering a new way to create interactive media. The relatively new mobile app from the startup Atma Sciences lets anyone create experiences using text, photos, sound, and touch, which are then displayed in a vertical feed, similar to TikTok or Reels. But unlike traditional short-form video apps, you don’t just watch and scroll in Gizmo — you play.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Depending on the type of “Gizmo” you encounter, you might poke the screen, swipe, tap, draw, drag, and more to interact with the mini app. These Gizmos aren’t just games but are more like digital toys — things that could include interactive puzzles, memes, art, animation, or anything else a creator can dream up.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3089592" height="363" src="https://techcrunch.com/wp-content/uploads/2026/02/gizmo-app-store.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Gizmo&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The result is an engaging, playful feed, where you can like and comment on the tiny creations and even remix existing Gizmos to create your own version, if you choose.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What’s more, you don’t need to know how to code or even vibe code to get started. Instead, you can simply type out an AI prompt to explain your idea using natural language.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app then leverages AI coding technology to turn your idea into an interactive experience by generating the code that makes it work. As part of this process, Gizmo will also render your idea visually to ensure that each app functions properly and runs smoothly. Apps are also vetted using AI and human moderation to ensure user safety, a company FAQ notes.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-tiktok wp-block-embed-tiktok"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Gizmo hails from a New York-based startup called Atma Sciences, co-founded by Rudd Fawcett and Brandon Francis, along with CEO Josh Siegel and CTO Daniel Amitay. The company last year raised a $5.49 million seed round from First Round Capital and others, according to data from PitchBook. On the company’s website (which is also silly and interactive), the team explains their focus is on combining “powerful technology with simple, elegant foundations,” starting with their creativity app, Gizmo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;None of the company’s founders responded to requests for an interview when TechCrunch reached out through multiple emails, requests to investors, and via LinkedIn. We were told by one investor that the team isn’t yet ready to do press. (Sorry!)&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch was drawn to Gizmo because of the app’s potential for growth and its unique approach to the vibe-coding space (and a rare recommendation from my teen). The company is envisioning a world where anyone can create apps for fun, not just for a purpose, as with other vibe-coding app platforms for micro apps, like Anything, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite being relatively new, Gizmo’s feed isn’t repetitive. It’s filled with creative mini apps, leading to an experience that feels somewhat like a mash-up between TikTok and the interactive 3D-space designer, Rooms. But while Rooms introduced the programming language Lua to those who wanted more advanced controls over their creations, Gizmo keeps things prompt-based and simple.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app is incredibly easy to use. You simply type out your prompt and then see how it turns out, and then modify as needed. In one test, the AI quickly coded a mini quiz, but we had to instruct it to edit the title, which was cut off at the top of the screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The resulting creation can be shared to the app’s feed, messaged to a friend, or posted to social media using a unique URL.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to data from market intelligence firm Appfigures, Gizmo has roughly 600,000 installs, with around half coming from the U.S., after being introduced with little fanfare less than six months ago. Around 235,000 of its downloads came in December alone, representing 39% of its total count.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gizmo’s growth from October to December was 312%, with December installs up 50% month-over-month and November installs up 180% from October. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app is available on both iOS and Android.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Gizmo, a TikTok-like app for vibe-coded mini applications, is offering a new way to create interactive media. The relatively new mobile app from the startup Atma Sciences lets anyone create experiences using text, photos, sound, and touch, which are then displayed in a vertical feed, similar to TikTok or Reels. But unlike traditional short-form video apps, you don’t just watch and scroll in Gizmo — you play.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Depending on the type of “Gizmo” you encounter, you might poke the screen, swipe, tap, draw, drag, and more to interact with the mini app. These Gizmos aren’t just games but are more like digital toys — things that could include interactive puzzles, memes, art, animation, or anything else a creator can dream up.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3089592" height="363" src="https://techcrunch.com/wp-content/uploads/2026/02/gizmo-app-store.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Gizmo&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The result is an engaging, playful feed, where you can like and comment on the tiny creations and even remix existing Gizmos to create your own version, if you choose.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What’s more, you don’t need to know how to code or even vibe code to get started. Instead, you can simply type out an AI prompt to explain your idea using natural language.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app then leverages AI coding technology to turn your idea into an interactive experience by generating the code that makes it work. As part of this process, Gizmo will also render your idea visually to ensure that each app functions properly and runs smoothly. Apps are also vetted using AI and human moderation to ensure user safety, a company FAQ notes.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-tiktok wp-block-embed-tiktok"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Gizmo hails from a New York-based startup called Atma Sciences, co-founded by Rudd Fawcett and Brandon Francis, along with CEO Josh Siegel and CTO Daniel Amitay. The company last year raised a $5.49 million seed round from First Round Capital and others, according to data from PitchBook. On the company’s website (which is also silly and interactive), the team explains their focus is on combining “powerful technology with simple, elegant foundations,” starting with their creativity app, Gizmo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;None of the company’s founders responded to requests for an interview when TechCrunch reached out through multiple emails, requests to investors, and via LinkedIn. We were told by one investor that the team isn’t yet ready to do press. (Sorry!)&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch was drawn to Gizmo because of the app’s potential for growth and its unique approach to the vibe-coding space (and a rare recommendation from my teen). The company is envisioning a world where anyone can create apps for fun, not just for a purpose, as with other vibe-coding app platforms for micro apps, like Anything, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite being relatively new, Gizmo’s feed isn’t repetitive. It’s filled with creative mini apps, leading to an experience that feels somewhat like a mash-up between TikTok and the interactive 3D-space designer, Rooms. But while Rooms introduced the programming language Lua to those who wanted more advanced controls over their creations, Gizmo keeps things prompt-based and simple.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app is incredibly easy to use. You simply type out your prompt and then see how it turns out, and then modify as needed. In one test, the AI quickly coded a mini quiz, but we had to instruct it to edit the title, which was cut off at the top of the screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The resulting creation can be shared to the app’s feed, messaged to a friend, or posted to social media using a unique URL.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to data from market intelligence firm Appfigures, Gizmo has roughly 600,000 installs, with around half coming from the U.S., after being introduced with little fanfare less than six months ago. Around 235,000 of its downloads came in December alone, representing 39% of its total count.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gizmo’s growth from October to December was 312%, with December installs up 50% month-over-month and November installs up 180% from October. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app is available on both iOS and Android.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/meet-gizmo-a-tiktok-for-interactive-vibe-coded-mini-apps/</guid><pubDate>Wed, 04 Feb 2026 21:45:32 +0000</pubDate></item><item><title>Google’s Gemini app has surpassed 750M monthly active users (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/googles-gemini-app-has-surpassed-750m-monthly-active-users/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/google-gemini-jagmeet-singh-techcrunch.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s AI chatbot Gemini has surpassed 750 million monthly active users (MAUs), according to the company’s fourth-quarter 2025 earnings. This figure illustrates the rapid consumer adoption of Gemini, which has quickly become a prominent player in the AI space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last quarter, Google reported 650 million monthly active users for Gemini, indicating substantial growth in a short period. In comparison, Meta AI has reported nearly 500 million monthly users. However, while Gemini is gaining traction, it still trails behind its biggest rival, ChatGPT, which is estimated to have around 810 million MAUs in late 2025.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The recently revealed number comes on the heels of the launch of Gemini 3, which showcases the company’s most advanced model yet, providing responses that the company claims exhibit an unprecedented level of depth and nuance. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CEO Sundar Pichai highlighted that the introduction of Gemini 3 in AI mode was a “positive driver” for the company’s growth and emphasized that continued investment and iteration will maintain this momentum.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google recently rolled out a more affordable plan, the Google AI Plus, priced at $7.99 per month. The plan is expected to drive further growth by appealing to budget-conscious consumers, although it was made available too recently to have any influence on the quarterly figures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are focused on a free tier and subscriptions and seeing great growth,” Philipp Schindler, Google’s chief business officer, said during the call to investors. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini’s growth is particularly notable given Alphabet’s overall financial performance. The company has surpassed $400 billion in annual revenue for the first time this quarter. Google attributes the achievement to the expansion of its AI division, which has seen increased demand. Recently, Google introduced the latest generation of its TPU AI accelerator chip, called Ironwood, to compete with Nvidia.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The launch of Gemini 3 was a major milestone, and we have great momentum. Our first party models, like Gemini, now process over 10 billion tokens per minute via direct API use by our customers, and the Gemini App has grown to over 750 million monthly active users. Search saw more usage than ever before, with AI continuing to drive an expansionary moment,” Pichai stated in today’s release.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/google-gemini-jagmeet-singh-techcrunch.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s AI chatbot Gemini has surpassed 750 million monthly active users (MAUs), according to the company’s fourth-quarter 2025 earnings. This figure illustrates the rapid consumer adoption of Gemini, which has quickly become a prominent player in the AI space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last quarter, Google reported 650 million monthly active users for Gemini, indicating substantial growth in a short period. In comparison, Meta AI has reported nearly 500 million monthly users. However, while Gemini is gaining traction, it still trails behind its biggest rival, ChatGPT, which is estimated to have around 810 million MAUs in late 2025.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The recently revealed number comes on the heels of the launch of Gemini 3, which showcases the company’s most advanced model yet, providing responses that the company claims exhibit an unprecedented level of depth and nuance. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CEO Sundar Pichai highlighted that the introduction of Gemini 3 in AI mode was a “positive driver” for the company’s growth and emphasized that continued investment and iteration will maintain this momentum.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google recently rolled out a more affordable plan, the Google AI Plus, priced at $7.99 per month. The plan is expected to drive further growth by appealing to budget-conscious consumers, although it was made available too recently to have any influence on the quarterly figures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are focused on a free tier and subscriptions and seeing great growth,” Philipp Schindler, Google’s chief business officer, said during the call to investors. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini’s growth is particularly notable given Alphabet’s overall financial performance. The company has surpassed $400 billion in annual revenue for the first time this quarter. Google attributes the achievement to the expansion of its AI division, which has seen increased demand. Recently, Google introduced the latest generation of its TPU AI accelerator chip, called Ironwood, to compete with Nvidia.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The launch of Gemini 3 was a major milestone, and we have great momentum. Our first party models, like Gemini, now process over 10 billion tokens per minute via direct API use by our customers, and the Gemini App has grown to over 750 million monthly active users. Search saw more usage than ever before, with AI continuing to drive an expansionary moment,” Pichai stated in today’s release.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/googles-gemini-app-has-surpassed-750m-monthly-active-users/</guid><pubDate>Wed, 04 Feb 2026 22:53:46 +0000</pubDate></item><item><title>Alphabet won’t talk about the Google-Apple AI deal, even to investors (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/alphabet-wont-talk-about-the-google-apple-ai-deal-even-to-investors/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/imgi_38_mp840x830mattef8f8f8t-pad1000x1000f8f8f8.jpg?w=1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Alphabet declined to answer one of its investors during questions about Google’s AI deal with Apple on Wednesday’s fourth-quarter earnings call. Instead of responding to an analyst’s question about how the tech giant is thinking about AI partnerships, such as the one with Apple to power AI for Siri, the question was completely ignored.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That decision tells us something, though — Alphabet isn’t ready to talk about how this partnership will impact its core business, which is increasingly focused on AI. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Over the years, the Google-Apple relationship has been mutually beneficial. The two companies’ search partnership saw the search giant paying the iPhone maker $20 billion to be the default search engine on Apple devices, filings from the Department of Justice’s lawsuit against the search giant revealed. In turn, Google gained access to Apple’s massive customer base — the iPhone maker last quarter announced it has 2.5 billion active devices worldwide, to give you an idea of scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest Apple AI deal is rumored to cost Apple roughly $1 billion per year, but the payoff beyond that for Google isn’t as immediately obvious as it is with search. In Google Search, consumers see links to advertisers’ websites at the top of their search results. Ads in AI Mode, which could one day represent the future of Google’s search business, are still an “experiment” for now.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company first announced last May that it would bring ads to AI Mode, the chatbot-style interface for Google Search, but these tests see the ads’ placement below or integrated into the chatbot’s responses. Google is also trying out agentic shopping, including Shop with AI Mode, to guide consumers with product-related queries to a seamless checkout experience from the AI interface. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Google’s AI competitor Anthropic is taking aim at ad-supported AI with its forthcoming Super Bowl ad, which challenges the business model being adopted by ChatGPT maker OpenAI and Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;How this will all play out longer-term is still an open question — and for today, an unanswered one, apparently. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, the Apple Siri deal barely received any mention during Alphabet’s earnings call on Wednesday. Sundar Pichai only noted he was pleased that Apple’s “preferred cloud provider” and would be helping to develop “the next generation of Apple foundation models based on Gemini technology.” Google’s Chief Business Officer Philipp Schindler used the exact same wording when mentioning Apple, as well.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/imgi_38_mp840x830mattef8f8f8t-pad1000x1000f8f8f8.jpg?w=1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Alphabet declined to answer one of its investors during questions about Google’s AI deal with Apple on Wednesday’s fourth-quarter earnings call. Instead of responding to an analyst’s question about how the tech giant is thinking about AI partnerships, such as the one with Apple to power AI for Siri, the question was completely ignored.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That decision tells us something, though — Alphabet isn’t ready to talk about how this partnership will impact its core business, which is increasingly focused on AI. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Over the years, the Google-Apple relationship has been mutually beneficial. The two companies’ search partnership saw the search giant paying the iPhone maker $20 billion to be the default search engine on Apple devices, filings from the Department of Justice’s lawsuit against the search giant revealed. In turn, Google gained access to Apple’s massive customer base — the iPhone maker last quarter announced it has 2.5 billion active devices worldwide, to give you an idea of scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest Apple AI deal is rumored to cost Apple roughly $1 billion per year, but the payoff beyond that for Google isn’t as immediately obvious as it is with search. In Google Search, consumers see links to advertisers’ websites at the top of their search results. Ads in AI Mode, which could one day represent the future of Google’s search business, are still an “experiment” for now.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company first announced last May that it would bring ads to AI Mode, the chatbot-style interface for Google Search, but these tests see the ads’ placement below or integrated into the chatbot’s responses. Google is also trying out agentic shopping, including Shop with AI Mode, to guide consumers with product-related queries to a seamless checkout experience from the AI interface. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Google’s AI competitor Anthropic is taking aim at ad-supported AI with its forthcoming Super Bowl ad, which challenges the business model being adopted by ChatGPT maker OpenAI and Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;How this will all play out longer-term is still an open question — and for today, an unanswered one, apparently. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, the Apple Siri deal barely received any mention during Alphabet’s earnings call on Wednesday. Sundar Pichai only noted he was pleased that Apple’s “preferred cloud provider” and would be helping to develop “the next generation of Apple foundation models based on Gemini technology.” Google’s Chief Business Officer Philipp Schindler used the exact same wording when mentioning Apple, as well.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/alphabet-wont-talk-about-the-google-apple-ai-deal-even-to-investors/</guid><pubDate>Wed, 04 Feb 2026 23:28:31 +0000</pubDate></item><item><title>Sam Altman got exceptionally testy over Claude Super Bowl ads (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/sam-altman-got-exceptionally-testy-over-claude-super-bowl-ads/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/10/Sam-Altman-OpenAI-DSC02881.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic’s Super Bowl commercial, one of four ads the AI lab dropped on Wednesday, begins with the word “BETRAYAL” splashed boldly across the screen. The camera pans to a man earnestly asking a chatbot (obviously intended to depict ChatGPT) for advice on how to talk to his mom.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bot, portrayed by a blonde woman, offers some classic bits of advice. Start by listening. Try a nature walk! And then twists into an ad for a fictitious (we hope!) cougar-dating site called Golden Encounters. Anthropic finishes the spot by saying that while ads are coming to AI, they won’t be coming to its own chatbot, Claude.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Another commercial features a slight young man looking for advice on building a six pack. After offering his height, age, and weight, the bot serves him an ad for height-boosting insoles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Anthropic commercials are cleverly aimed at OpenAI’s users, after that company’s recent announcement that ads will be coming to ChatGPT’s free tier. And they caused an immediate stir, spawning headlines that Anthropic “mocks,” “skewers,” and “dunks on” OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They are funny enough that even Sam Altman admitted on X that he laughed at them. But he clearly didn’t really find them funny. They inspired him to write a novella-sized rant that devolved into calling his rival “dishonest” and “authoritarian.”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;First, the good part of the Anthropic ads: they are funny, and I laughed.&lt;/p&gt;&lt;p&gt;But I wonder why Anthropic would go for something so clearly dishonest. Our most important principle for ads says that we won’t do exactly this; we would obviously never run ads in the way Anthropic…&lt;/p&gt;— Sam Altman (@sama) February 4, 2026&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In that post, Altman explains that an ad-supported tier is intended to shoulder the burden of offering free ChatGPT to many of its millions of users. ChatGPT is still the most popular chatbot by a large margin.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the OpenAI CEO insisted the ads were “dishonest” in implying that ChatGPT will twist a conversation to insert an ad (and possibly for an off-color product, to boot).”We would obviously never run ads in the way Anthropic depicts them,” Altman wrote in the social media post. “We are not stupid and we know our users would reject that.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Indeed, OpenAI has promised ads will be separate, labeled, and will never influence a chat. But the company has also said it is planning on making them conversation-specific — which is the central allegation of Anthropic’s ads. As OpenAI explained on its blog, “We plan to test ads at the bottom of answers in ChatGPT when there’s a relevant sponsored product or service based on your current conversation.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman then went on to fling some equally questionable assertions at his rival. “Anthropic serves an expensive product to rich people,” he wrote. “We also feel strongly that we need to bring AI to billions of people who can’t pay for subscriptions.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Claude has a free chat tier, too, with subscriptions at $0, $17, $100, and $200. ChatGPT’s tiers are $0, $8, $20, and $200. One could argue the subscription tiers are fairly equivalent.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Altman also alleged in his post that “Anthropic wants to control what people do with AI.” He argues it blocks usage of Claude Code from “companies they don’t like,” like OpenAI, and said Anthropic tells people what they can and can’t use AI for.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;True, Anthropic’s whole marketing deal since day one has been “responsible AI.” The company was founded by two former OpenAI alums, after all, who claimed they grew alarmed about AI safety when they worked there.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, both chatbot companies have usage policies, AI guardrails, and talk about AI safety. And while OpenAI allows ChatGPT to be used for erotica while Anthropic does not, OpenAI, like Anthropic, has determined that some content should be blocked, particularly in regards to mental health.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yet Altman took this Anthropic-tells-you-what-to-do argument to an extreme level when he accused Anthropic of being “authoritarian.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“One authoritarian company won’t get us there on their own, to say nothing of the other obvious risks. It is a dark path,” he wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Using “authoritarian” in a rant over a cheeky Super Bowl ad is misplaced, at best. It’s particularly tactless when considering the current geopolitical environment in which protesters around the world have been killed by agents of their own government. While business rivals have been duking it out in ads since the beginning of time, clearly Anthropic hit a nerve.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/10/Sam-Altman-OpenAI-DSC02881.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic’s Super Bowl commercial, one of four ads the AI lab dropped on Wednesday, begins with the word “BETRAYAL” splashed boldly across the screen. The camera pans to a man earnestly asking a chatbot (obviously intended to depict ChatGPT) for advice on how to talk to his mom.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bot, portrayed by a blonde woman, offers some classic bits of advice. Start by listening. Try a nature walk! And then twists into an ad for a fictitious (we hope!) cougar-dating site called Golden Encounters. Anthropic finishes the spot by saying that while ads are coming to AI, they won’t be coming to its own chatbot, Claude.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Another commercial features a slight young man looking for advice on building a six pack. After offering his height, age, and weight, the bot serves him an ad for height-boosting insoles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Anthropic commercials are cleverly aimed at OpenAI’s users, after that company’s recent announcement that ads will be coming to ChatGPT’s free tier. And they caused an immediate stir, spawning headlines that Anthropic “mocks,” “skewers,” and “dunks on” OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They are funny enough that even Sam Altman admitted on X that he laughed at them. But he clearly didn’t really find them funny. They inspired him to write a novella-sized rant that devolved into calling his rival “dishonest” and “authoritarian.”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;First, the good part of the Anthropic ads: they are funny, and I laughed.&lt;/p&gt;&lt;p&gt;But I wonder why Anthropic would go for something so clearly dishonest. Our most important principle for ads says that we won’t do exactly this; we would obviously never run ads in the way Anthropic…&lt;/p&gt;— Sam Altman (@sama) February 4, 2026&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In that post, Altman explains that an ad-supported tier is intended to shoulder the burden of offering free ChatGPT to many of its millions of users. ChatGPT is still the most popular chatbot by a large margin.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the OpenAI CEO insisted the ads were “dishonest” in implying that ChatGPT will twist a conversation to insert an ad (and possibly for an off-color product, to boot).”We would obviously never run ads in the way Anthropic depicts them,” Altman wrote in the social media post. “We are not stupid and we know our users would reject that.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Indeed, OpenAI has promised ads will be separate, labeled, and will never influence a chat. But the company has also said it is planning on making them conversation-specific — which is the central allegation of Anthropic’s ads. As OpenAI explained on its blog, “We plan to test ads at the bottom of answers in ChatGPT when there’s a relevant sponsored product or service based on your current conversation.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman then went on to fling some equally questionable assertions at his rival. “Anthropic serves an expensive product to rich people,” he wrote. “We also feel strongly that we need to bring AI to billions of people who can’t pay for subscriptions.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Claude has a free chat tier, too, with subscriptions at $0, $17, $100, and $200. ChatGPT’s tiers are $0, $8, $20, and $200. One could argue the subscription tiers are fairly equivalent.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Altman also alleged in his post that “Anthropic wants to control what people do with AI.” He argues it blocks usage of Claude Code from “companies they don’t like,” like OpenAI, and said Anthropic tells people what they can and can’t use AI for.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;True, Anthropic’s whole marketing deal since day one has been “responsible AI.” The company was founded by two former OpenAI alums, after all, who claimed they grew alarmed about AI safety when they worked there.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, both chatbot companies have usage policies, AI guardrails, and talk about AI safety. And while OpenAI allows ChatGPT to be used for erotica while Anthropic does not, OpenAI, like Anthropic, has determined that some content should be blocked, particularly in regards to mental health.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yet Altman took this Anthropic-tells-you-what-to-do argument to an extreme level when he accused Anthropic of being “authoritarian.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“One authoritarian company won’t get us there on their own, to say nothing of the other obvious risks. It is a dark path,” he wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Using “authoritarian” in a rant over a cheeky Super Bowl ad is misplaced, at best. It’s particularly tactless when considering the current geopolitical environment in which protesters around the world have been killed by agents of their own government. While business rivals have been duking it out in ads since the beginning of time, clearly Anthropic hit a nerve.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/sam-altman-got-exceptionally-testy-over-claude-super-bowl-ads/</guid><pubDate>Thu, 05 Feb 2026 00:45:11 +0000</pubDate></item><item><title>[NEW] Paza: Introducing automatic speech recognition benchmarks and models for low resource languages (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/paza-introducing-automatic-speech-recognition-benchmarks-and-models-for-low-resource-languages/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white line icons on a blue‑to‑purple gradient background: a vertical audio waveform on the left, a globe showing Africa and Europe in the center, and a network on the right." class="wp-image-1160744" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Paza-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;div class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section"&gt;
	
	&lt;div class="container"&gt;
		&lt;div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow"&gt;
			&lt;div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h2 class="wp-block-heading h3" id="at-a-glance"&gt;At a glance&lt;/h2&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Microsoft Research releases PazaBench and Paza automatic speech recognition models&lt;/strong&gt;, advancing speech technology for low resource languages.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Human-centered pipeline for low-resource languages: &lt;/strong&gt;Built for and tested by communities, Paza is an end-to-end, continuous pipeline that elevates historically under-represented languages and makes speech models usable in real-world, low-resource contexts.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;First-of-its-kind ASR leaderboard, starting with African languages: &lt;/strong&gt;Pazabench is the first automatic speech recognition (ASR) leaderboard for low-resource languages. Launching with 39 African languages and 51 state-of-the-art models, it tracks three key metrics across leading public and community datasets.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Human-centered&amp;nbsp;Paza&amp;nbsp;ASR&amp;nbsp;models:&lt;/strong&gt;&amp;nbsp;Minimal&amp;nbsp;data, fine-tuned&amp;nbsp;ASR models&amp;nbsp;grounded in&amp;nbsp;real-world&amp;nbsp;testing&amp;nbsp;with farmers on everyday mobile devices, covering&amp;nbsp;six&amp;nbsp;Kenyan languages:&amp;nbsp;Swahili,&amp;nbsp;Dholuo, Kalenjin, Kikuyu, Maasai,&amp;nbsp;and Somali.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;		&lt;/div&gt;
	&lt;/div&gt;

	&lt;/div&gt;



&lt;p&gt;According to the 2025&amp;nbsp;Microsoft AI Diffusion Report&amp;nbsp;approximately one&amp;nbsp;in&amp;nbsp;six&amp;nbsp;people globally had used a generative AI product.&amp;nbsp;Yet for billions of&amp;nbsp;people,&amp;nbsp;the promise of voice interaction still falls short, and&amp;nbsp;whilst&amp;nbsp;AI is becoming increasingly multilingual, a key question&amp;nbsp;remains:&amp;nbsp;&lt;em&gt;&lt;strong&gt;Do&amp;nbsp;these models&amp;nbsp;actually work&amp;nbsp;for all languages and the people who rely on them?&lt;/strong&gt;&lt;/em&gt;&amp;nbsp;This challenge is one we first confronted through&amp;nbsp;Project Gecko—a collaboration between Microsoft Research and&amp;nbsp;Digital&amp;nbsp;Green&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,&amp;nbsp;where&amp;nbsp;field teams across Africa and India focused on building usable AI tools for farmers.&lt;/p&gt;



&lt;p&gt;Gecko revealed how often speech systems fail in real‑world, low‑resource environments—where many languages go&amp;nbsp;unrecognized&amp;nbsp;and non‑Western accents are frequently misunderstood. Yet speech remains the primary medium of communication globally. For communities across Kenya, Africa, and beyond, this mismatch creates cascading challenges: without foundational data&amp;nbsp;representing&amp;nbsp;their languages and cultures, innovation stalls, and the digital and AI divides widen.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Paza addresses this with a human-centered speech models pipeline. Through&amp;nbsp;PazaBench, it benchmarks low-resource languages using both public and community-sourced data, and through Paza&amp;nbsp;models, it&amp;nbsp;fine-tunes&amp;nbsp;speech models&amp;nbsp;to deliver outsized gains in mid- and low-resource languages, evaluating with community testers using real devices in real contexts. Upcoming playbooks complement this work by sharing practical guidance on&amp;nbsp;dataset creation,&amp;nbsp;fine-tuning&amp;nbsp;approaches&amp;nbsp;with minimal data&amp;nbsp;and evaluation considerations, introducing a continuous pipeline that&amp;nbsp;enables&amp;nbsp;researchers&amp;nbsp;and practitioners to build and evaluate systems grounded in real human use.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="how-project-gecko-informed-paza-s-design"&gt;How Project Gecko informed Paza’s design&lt;/h2&gt;



&lt;p&gt;In addition to building cost-effective, adaptable AI systems, the extensive&amp;nbsp;fieldwork on&amp;nbsp;Project Gecko highlighted an important lesson:&amp;nbsp;&lt;strong&gt;&lt;em&gt;Building usable speech&amp;nbsp;models&amp;nbsp;in low‑resource settings is not only a data problem,&amp;nbsp;but also&amp;nbsp;a design and evaluation problem.&lt;/em&gt;&lt;/strong&gt;&amp;nbsp;For AI systems to be useful, they must work in local languages, support hands‑free interaction through voice, text, and video, and deliver information in formats that fit real-world environments, that is, on low-bandwidth&amp;nbsp;mobile devices,&amp;nbsp;in&amp;nbsp;noisy settings, and&amp;nbsp;for&amp;nbsp;varying literacy levels.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These insights shaped the design of Paza, from&amp;nbsp;the&amp;nbsp;Swahili&amp;nbsp;phrase&amp;nbsp;&lt;em&gt;&lt;strong&gt;paza&amp;nbsp;sauti&lt;/strong&gt;&lt;/em&gt;&amp;nbsp;meaning “to project,” or “to raise your voice.” &amp;nbsp;The name reflects our intent: rather than simply adding more languages to existing systems,&lt;strong&gt;&amp;nbsp;Paza is about co-creating speech technologies in partnership with the communities who use them.&lt;/strong&gt;&amp;nbsp;Guided by this principle, Paza puts human use&amp;nbsp;first,&amp;nbsp;which&amp;nbsp;enables&amp;nbsp;model improvement.&amp;nbsp;&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Microsoft research newsletter&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Newsletter&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-newsletter"&gt;Stay connected to the research community at Microsoft.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="pazabench-the-first-asr-leaderboard-for-low-resource-languages"&gt;PazaBench: The first ASR leaderboard for low-resource languages&lt;/h2&gt;



&lt;p&gt;&lt;strong&gt;PazaBench&lt;/strong&gt; is the first automatic speech recognition (ASR) leaderboard dedicated to low‑resource languages. It launches&amp;nbsp;with&amp;nbsp;initial&amp;nbsp;coverage&amp;nbsp;for&amp;nbsp;39 African languages and benchmarks&amp;nbsp;52 state‑of‑the‑art ASR and language models, including newly released Paza ASR models for six Kenyan languages. The platform aggregates leading public and community datasets from diverse styles of speech including conversational, scripted read aloud, unscripted, broadcast news, and domain-specific data—into one easy‑to‑explore platform per language.&amp;nbsp;This makes it easier for&amp;nbsp;researchers, developers, and product teams to easily assess which models perform best across underserved languages and diverse regions, understand trade-offs between speed and accuracy&amp;nbsp;while&amp;nbsp;identifying&amp;nbsp;where gaps persist.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;PazaBench tracks three core metrics:&lt;/strong&gt;&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Character Error Rate (CER)&lt;/strong&gt; which is important for languages with rich word forms, where meaning is built by combining word parts, therefore errors at the character level can significantly impact meaning&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Word Error Rate (WER)&lt;/strong&gt; for word-level transcript accuracy&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;RTFx (Inverse Real‑Time Factor)&lt;/strong&gt; which measures how fast transcription runs relative to real‑time audio duration&lt;em&gt;.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;/figure&gt;



&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;em&gt;More than scores,&amp;nbsp;PazaBench&amp;nbsp;standardizes evaluation to prioritize dataset gaps,&amp;nbsp;identify&amp;nbsp;underperforming languages, and highlight where localized models beat&amp;nbsp;wider coverage ASR models—offering early evidence of&amp;nbsp;the value of African‑centric innovation.&lt;/em&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;







&lt;h2 class="wp-block-heading" id="paza-asr-models-built-with-and-for-kenyan-languages"&gt;Paza ASR Models: Built with and for Kenyan languages&lt;/h2&gt;



&lt;p&gt;The Paza ASR models&amp;nbsp;consist of&amp;nbsp;three fine-tuned ASR models built on top of state‑of‑the‑art model architectures. Each model targets&amp;nbsp;&lt;em&gt;Swahili,&amp;nbsp;&lt;/em&gt;a mid-resource language and five low‑resource Kenyan languages;&amp;nbsp;&lt;em&gt;Dholuo, Kalenjin,&amp;nbsp;Kikuyu,&amp;nbsp;Maasai&amp;nbsp;and Somali&lt;/em&gt;.&amp;nbsp;The models are&amp;nbsp;fine-tuned&amp;nbsp;on&amp;nbsp;public and curated proprietary datasets.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Fine‑tuning the three models allowed us to explore supportive approaches toward a shared goal: building speech recognition systems that are usable for local contexts starting with the six Kenyan languages and bridging the gaps of multi-lingual and multi-modal video question and answering through the MMCT agent.&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/p&gt;



&lt;figure class="wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;figcaption class="wp-element-caption"&gt;See the MMCT agent in action in the field&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Early versions of two models in Kikuyu and Swahili were deployed on mobile devices and tested directly with farmers in real‑world settings, enabling the team to observe how the models performed with everyday use. Farmers provided in‑the‑moment feedback on accuracy, usability, and relevance, highlighting where transcripts broke down, which errors were most disruptive, and what improvements would make the models more helpful in practice. This feedback loop directly informed subsequent fine‑tuning, ensuring model improvements were driven not only by benchmark scores, but by the needs and expectations of the communities they are intended to serve.&lt;/p&gt;







&lt;p&gt;Here is how Paza models compare to&amp;nbsp;three&amp;nbsp;state-of-the-art&amp;nbsp;ASR&amp;nbsp;models&amp;nbsp;today:&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1: Character Error Rate (CER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower CER indicates better transcription performance." class="wp-image-1161323" height="1287" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG1_overall_cer_grouped_sorted_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 1: Character Error Rate (CER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower CER indicates better transcription performance.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2: Word Error Rate (WER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower WER indicates better transcription performance." class="wp-image-1161325" height="1287" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG2_overall_wer_grouped_sorted_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 2: Word Error Rate (WER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower WER indicates better transcription performance.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;&lt;strong&gt;1) Paza‑Phi‑4‑Multimodal‑Instruct&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Microsoft’s Phi‑4 multimodal‑instruct&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; is a next‑generation small language model built to reason across audio, text, and vision. With Paza, we extend its audio capabilities, adapting a powerful multimodal architecture into a high‑quality automatic speech recognition (ASR) system for low‑resource African languages.&lt;/p&gt;



&lt;p&gt;Fine‑tuned on unified multilingual speech datasets, the model was optimized specifically for transcription in the six languages. The model preserves its underlying transformer architecture and multi-modal capabilities, while selectively fine-tuning only the audio‑specific components, enabling strong cross‑lingual generalization.&lt;/p&gt;



&lt;p&gt;As the results below show, this model delivers consistent improvements in transcription quality across all six languages.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3: Character Error Rate (CER) comparison across&amp;nbsp;the six&amp;nbsp;languages for the base&amp;nbsp;model&amp;nbsp;versus the finetuned Paza model.&amp;nbsp;Lower CER&amp;nbsp;indicates&amp;nbsp;better transcription performance." class="wp-image-1161376" height="1063" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG3_phi_cer_comparison_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 3: &lt;em&gt;Character Error Rate (CER) comparison across&amp;nbsp;the six&lt;/em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;em&gt;languages for the base&amp;nbsp;model&amp;nbsp;versus the finetuned Paza model.&amp;nbsp;Lower CER&amp;nbsp;indicates&amp;nbsp;better transcription performance.&lt;/em&gt;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4: Word Error Rate (WER) comparison across the six languages for the base model versus the finetuned Paza model. Lower WER indicates better transcription performance." class="wp-image-1161378" height="1063" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG4_phi_wer_comparison_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 4: Word Error Rate (WER) comparison across the six languages for the base model versus the finetuned Paza model. Lower WER indicates better transcription performance.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;







&lt;p&gt;&lt;strong&gt;2) Paza‑MMS‑1B‑All&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;This model is fine-tuned on Meta’s mms-1b-all model, which employs a large-scale Wav2Vec2.0-style encoder with lightweight language-specific adapters to enable efficient multilingual specialization. For this release, each of the six language adapters was fine‑tuned independently on curated low‑resource datasets, allowing targeted adaptation while keeping the shared encoder largely frozen.&lt;/p&gt;



&lt;p&gt;As shown in the figures below, this model improves transcription accuracy while maintaining the model’s strong cross‑lingual generalization.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 5: Character Error Rate (CER)&amp;nbsp;comparison across the six&amp;nbsp;languages for the base model&amp;nbsp;versus&amp;nbsp;the finetuned Paza model.&amp;nbsp;Lower CER&amp;nbsp;indicates&amp;nbsp;better transcription performance." class="wp-image-1161380" height="1160" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG5_mms_cer_comparison_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 5: &lt;em&gt;Character Error Rate (CER)&amp;nbsp;comparison across the six&lt;/em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;em&gt;languages for the base model&amp;nbsp;versus&amp;nbsp;the finetuned Paza model.&amp;nbsp;Lower CER&amp;nbsp;indicates&amp;nbsp;better transcription performance.&lt;/em&gt;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 6: Word Error Rate (WER)&amp;nbsp;comparison across the six&amp;nbsp;languages for the base model&amp;nbsp;versus the finetuned Paza model.&amp;nbsp;Lower WER indicates better transcription performance." class="wp-image-1161382" height="1160" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG6_mms_wer_comparison_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 6: &lt;em&gt;Word Error Rate (WER)&amp;nbsp;comparison across the six&lt;/em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;em&gt;languages for the base model&amp;nbsp;versus the finetuned Paza model.&amp;nbsp;Lower WER indicates better transcription performance.&lt;/em&gt;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;







&lt;p&gt;&lt;strong&gt;3) Paza‑Whisper‑Large‑v3‑Turbo&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;This model is finetuned on OpenAI’s whisper-large-v3-turbo&amp;nbsp;base model. Whisper is a transformer-based encoder–decoder model which&amp;nbsp;delivers robust automatic speech recognition (ASR)&amp;nbsp;capabilities. This model was fine‑tuned on the entire unified multilingual ASR dataset,&amp;nbsp;on&amp;nbsp;the mentioned six languages, to encourage cross-lingual generalization.&amp;nbsp;In addition, an extra post‑processing step was applied to address the known Whisper hallucination failure modes, improving transcription reliability.&lt;/p&gt;



&lt;p&gt;As shown below, this release achieves improved transcription accuracy while retaining Whisper’s robustness.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 7: Character Error Rate (CER) comparison across the six&amp;nbsp;languages for the base model versus the finetuned Paza model.&amp;nbsp;Lower CER&amp;nbsp;indicates&amp;nbsp;better transcription performance." class="wp-image-1161338" height="1081" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG7whisper_cer_comparison_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 7: &lt;em&gt;Character Error Rate (CER) comparison across the six&lt;/em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;em&gt;languages for the base model versus the finetuned Paza model.&amp;nbsp;Lower CER&amp;nbsp;indicates&amp;nbsp;better transcription performance.&lt;/em&gt;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 8: Word Error Rate (WER) comparison across the six&amp;nbsp;languages for the base model versus the finetuned Paza model.&amp;nbsp;Lower WER indicates better transcription performance." class="wp-image-1161341" height="1081" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG8_whisper_wer_comparison_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 8: &lt;em&gt;Word Error Rate (WER) comparison across the six&lt;/em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;em&gt;languages for the base model versus the finetuned Paza model.&amp;nbsp;Lower WER indicates better transcription performance.&lt;/em&gt;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;







&lt;h2 class="wp-block-heading" id="where-do-we-go-from-here"&gt;Where do we go from here&lt;/h2&gt;



&lt;p&gt;AI is reshaping how the world communicates. Designing with people, not just for them, means looking beyond the languages that are already well‑served. We plan to expand PazaBench beyond African languages and evaluate state‑of‑the‑art ASR models across&amp;nbsp;more low‑resource languages globally. The Paza ASR models are an early step; truly supporting small and under‑represented languages requires dedicated datasets, strong local partnerships, and rigorous evaluation. Meaningful progress depends on sustained collaboration with the communities who speak these languages, and expanding responsibly means prioritizing depth and quality over broad but shallow coverage.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;As we continue this work,&amp;nbsp;we’re&amp;nbsp;distilling our methods into a forthcoming playbook to help the broader ecosystem curate datasets, fine‑tune responsibly, and evaluate models in real‑world conditions. And we’re not stopping at speech—additional&amp;nbsp;playbooks will guide&amp;nbsp;teams&amp;nbsp;building AI tools and applications for multilingual, multicultural contexts, and give them practical recommendations for deploying across diverse communities.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Together, these guides—grounded in technical advances and community‑driven design—share our learnings to help researchers, engineers, and designers build more human‑centered AI systems.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;



&lt;p&gt;The following researchers played an integral role in this work: Najeeb Abdulhamid, Felermino Ali, Liz Ankrah, Kevin Chege, Ogbemi Ekwejunor-Etchie, Ignatius Ezeani, Tanuja Ganu, Antonis Krasakis, Mercy Kwambai, Samuel Maina, Muchai Mercy, Danlami Mohammed, Nick Mumero, Martin Mwiti, Stephanie Nyairo, Millicent Ochieng and Jacki O’Neill.&lt;/p&gt;



&lt;p&gt;We would like to thank the Digital Green&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; team—Rikin Gandhi, Alex Mwaura, Jacqueline Wang’ombe, Kevin Mugambi, Lorraine Nyambura, Juan Pablo, Nereah Okanga, Ramaskanda R.S, Vineet Singh, Nafhtari Wanjiku, Kista Ogot, Samuel Owinya&amp;nbsp;and the community evaluators in Nyeri and Nandi, Kenya — for their valuable contributions to this work.&lt;/p&gt;



&lt;p&gt;We extend our gratitude to the creators, community contributors, and maintainers of African Next Voices Kenya&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, African Next Voices South Africa&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, ALFFA&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Digigreen&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Google FLEURS&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Mozilla Common Voice&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Naija Voices&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; whose efforts have been invaluable in advancing African languages speech data.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white line icons on a blue‑to‑purple gradient background: a vertical audio waveform on the left, a globe showing Africa and Europe in the center, and a network on the right." class="wp-image-1160744" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Paza-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;div class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section"&gt;
	
	&lt;div class="container"&gt;
		&lt;div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow"&gt;
			&lt;div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h2 class="wp-block-heading h3" id="at-a-glance"&gt;At a glance&lt;/h2&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Microsoft Research releases PazaBench and Paza automatic speech recognition models&lt;/strong&gt;, advancing speech technology for low resource languages.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Human-centered pipeline for low-resource languages: &lt;/strong&gt;Built for and tested by communities, Paza is an end-to-end, continuous pipeline that elevates historically under-represented languages and makes speech models usable in real-world, low-resource contexts.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;First-of-its-kind ASR leaderboard, starting with African languages: &lt;/strong&gt;Pazabench is the first automatic speech recognition (ASR) leaderboard for low-resource languages. Launching with 39 African languages and 51 state-of-the-art models, it tracks three key metrics across leading public and community datasets.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Human-centered&amp;nbsp;Paza&amp;nbsp;ASR&amp;nbsp;models:&lt;/strong&gt;&amp;nbsp;Minimal&amp;nbsp;data, fine-tuned&amp;nbsp;ASR models&amp;nbsp;grounded in&amp;nbsp;real-world&amp;nbsp;testing&amp;nbsp;with farmers on everyday mobile devices, covering&amp;nbsp;six&amp;nbsp;Kenyan languages:&amp;nbsp;Swahili,&amp;nbsp;Dholuo, Kalenjin, Kikuyu, Maasai,&amp;nbsp;and Somali.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;		&lt;/div&gt;
	&lt;/div&gt;

	&lt;/div&gt;



&lt;p&gt;According to the 2025&amp;nbsp;Microsoft AI Diffusion Report&amp;nbsp;approximately one&amp;nbsp;in&amp;nbsp;six&amp;nbsp;people globally had used a generative AI product.&amp;nbsp;Yet for billions of&amp;nbsp;people,&amp;nbsp;the promise of voice interaction still falls short, and&amp;nbsp;whilst&amp;nbsp;AI is becoming increasingly multilingual, a key question&amp;nbsp;remains:&amp;nbsp;&lt;em&gt;&lt;strong&gt;Do&amp;nbsp;these models&amp;nbsp;actually work&amp;nbsp;for all languages and the people who rely on them?&lt;/strong&gt;&lt;/em&gt;&amp;nbsp;This challenge is one we first confronted through&amp;nbsp;Project Gecko—a collaboration between Microsoft Research and&amp;nbsp;Digital&amp;nbsp;Green&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,&amp;nbsp;where&amp;nbsp;field teams across Africa and India focused on building usable AI tools for farmers.&lt;/p&gt;



&lt;p&gt;Gecko revealed how often speech systems fail in real‑world, low‑resource environments—where many languages go&amp;nbsp;unrecognized&amp;nbsp;and non‑Western accents are frequently misunderstood. Yet speech remains the primary medium of communication globally. For communities across Kenya, Africa, and beyond, this mismatch creates cascading challenges: without foundational data&amp;nbsp;representing&amp;nbsp;their languages and cultures, innovation stalls, and the digital and AI divides widen.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Paza addresses this with a human-centered speech models pipeline. Through&amp;nbsp;PazaBench, it benchmarks low-resource languages using both public and community-sourced data, and through Paza&amp;nbsp;models, it&amp;nbsp;fine-tunes&amp;nbsp;speech models&amp;nbsp;to deliver outsized gains in mid- and low-resource languages, evaluating with community testers using real devices in real contexts. Upcoming playbooks complement this work by sharing practical guidance on&amp;nbsp;dataset creation,&amp;nbsp;fine-tuning&amp;nbsp;approaches&amp;nbsp;with minimal data&amp;nbsp;and evaluation considerations, introducing a continuous pipeline that&amp;nbsp;enables&amp;nbsp;researchers&amp;nbsp;and practitioners to build and evaluate systems grounded in real human use.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="how-project-gecko-informed-paza-s-design"&gt;How Project Gecko informed Paza’s design&lt;/h2&gt;



&lt;p&gt;In addition to building cost-effective, adaptable AI systems, the extensive&amp;nbsp;fieldwork on&amp;nbsp;Project Gecko highlighted an important lesson:&amp;nbsp;&lt;strong&gt;&lt;em&gt;Building usable speech&amp;nbsp;models&amp;nbsp;in low‑resource settings is not only a data problem,&amp;nbsp;but also&amp;nbsp;a design and evaluation problem.&lt;/em&gt;&lt;/strong&gt;&amp;nbsp;For AI systems to be useful, they must work in local languages, support hands‑free interaction through voice, text, and video, and deliver information in formats that fit real-world environments, that is, on low-bandwidth&amp;nbsp;mobile devices,&amp;nbsp;in&amp;nbsp;noisy settings, and&amp;nbsp;for&amp;nbsp;varying literacy levels.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These insights shaped the design of Paza, from&amp;nbsp;the&amp;nbsp;Swahili&amp;nbsp;phrase&amp;nbsp;&lt;em&gt;&lt;strong&gt;paza&amp;nbsp;sauti&lt;/strong&gt;&lt;/em&gt;&amp;nbsp;meaning “to project,” or “to raise your voice.” &amp;nbsp;The name reflects our intent: rather than simply adding more languages to existing systems,&lt;strong&gt;&amp;nbsp;Paza is about co-creating speech technologies in partnership with the communities who use them.&lt;/strong&gt;&amp;nbsp;Guided by this principle, Paza puts human use&amp;nbsp;first,&amp;nbsp;which&amp;nbsp;enables&amp;nbsp;model improvement.&amp;nbsp;&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Microsoft research newsletter&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Newsletter&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-newsletter"&gt;Stay connected to the research community at Microsoft.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="pazabench-the-first-asr-leaderboard-for-low-resource-languages"&gt;PazaBench: The first ASR leaderboard for low-resource languages&lt;/h2&gt;



&lt;p&gt;&lt;strong&gt;PazaBench&lt;/strong&gt; is the first automatic speech recognition (ASR) leaderboard dedicated to low‑resource languages. It launches&amp;nbsp;with&amp;nbsp;initial&amp;nbsp;coverage&amp;nbsp;for&amp;nbsp;39 African languages and benchmarks&amp;nbsp;52 state‑of‑the‑art ASR and language models, including newly released Paza ASR models for six Kenyan languages. The platform aggregates leading public and community datasets from diverse styles of speech including conversational, scripted read aloud, unscripted, broadcast news, and domain-specific data—into one easy‑to‑explore platform per language.&amp;nbsp;This makes it easier for&amp;nbsp;researchers, developers, and product teams to easily assess which models perform best across underserved languages and diverse regions, understand trade-offs between speed and accuracy&amp;nbsp;while&amp;nbsp;identifying&amp;nbsp;where gaps persist.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;PazaBench tracks three core metrics:&lt;/strong&gt;&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Character Error Rate (CER)&lt;/strong&gt; which is important for languages with rich word forms, where meaning is built by combining word parts, therefore errors at the character level can significantly impact meaning&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Word Error Rate (WER)&lt;/strong&gt; for word-level transcript accuracy&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;RTFx (Inverse Real‑Time Factor)&lt;/strong&gt; which measures how fast transcription runs relative to real‑time audio duration&lt;em&gt;.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;/figure&gt;



&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;em&gt;More than scores,&amp;nbsp;PazaBench&amp;nbsp;standardizes evaluation to prioritize dataset gaps,&amp;nbsp;identify&amp;nbsp;underperforming languages, and highlight where localized models beat&amp;nbsp;wider coverage ASR models—offering early evidence of&amp;nbsp;the value of African‑centric innovation.&lt;/em&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;







&lt;h2 class="wp-block-heading" id="paza-asr-models-built-with-and-for-kenyan-languages"&gt;Paza ASR Models: Built with and for Kenyan languages&lt;/h2&gt;



&lt;p&gt;The Paza ASR models&amp;nbsp;consist of&amp;nbsp;three fine-tuned ASR models built on top of state‑of‑the‑art model architectures. Each model targets&amp;nbsp;&lt;em&gt;Swahili,&amp;nbsp;&lt;/em&gt;a mid-resource language and five low‑resource Kenyan languages;&amp;nbsp;&lt;em&gt;Dholuo, Kalenjin,&amp;nbsp;Kikuyu,&amp;nbsp;Maasai&amp;nbsp;and Somali&lt;/em&gt;.&amp;nbsp;The models are&amp;nbsp;fine-tuned&amp;nbsp;on&amp;nbsp;public and curated proprietary datasets.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Fine‑tuning the three models allowed us to explore supportive approaches toward a shared goal: building speech recognition systems that are usable for local contexts starting with the six Kenyan languages and bridging the gaps of multi-lingual and multi-modal video question and answering through the MMCT agent.&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/p&gt;



&lt;figure class="wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;figcaption class="wp-element-caption"&gt;See the MMCT agent in action in the field&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Early versions of two models in Kikuyu and Swahili were deployed on mobile devices and tested directly with farmers in real‑world settings, enabling the team to observe how the models performed with everyday use. Farmers provided in‑the‑moment feedback on accuracy, usability, and relevance, highlighting where transcripts broke down, which errors were most disruptive, and what improvements would make the models more helpful in practice. This feedback loop directly informed subsequent fine‑tuning, ensuring model improvements were driven not only by benchmark scores, but by the needs and expectations of the communities they are intended to serve.&lt;/p&gt;







&lt;p&gt;Here is how Paza models compare to&amp;nbsp;three&amp;nbsp;state-of-the-art&amp;nbsp;ASR&amp;nbsp;models&amp;nbsp;today:&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1: Character Error Rate (CER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower CER indicates better transcription performance." class="wp-image-1161323" height="1287" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG1_overall_cer_grouped_sorted_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 1: Character Error Rate (CER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower CER indicates better transcription performance.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2: Word Error Rate (WER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower WER indicates better transcription performance." class="wp-image-1161325" height="1287" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG2_overall_wer_grouped_sorted_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 2: Word Error Rate (WER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower WER indicates better transcription performance.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;&lt;strong&gt;1) Paza‑Phi‑4‑Multimodal‑Instruct&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Microsoft’s Phi‑4 multimodal‑instruct&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; is a next‑generation small language model built to reason across audio, text, and vision. With Paza, we extend its audio capabilities, adapting a powerful multimodal architecture into a high‑quality automatic speech recognition (ASR) system for low‑resource African languages.&lt;/p&gt;



&lt;p&gt;Fine‑tuned on unified multilingual speech datasets, the model was optimized specifically for transcription in the six languages. The model preserves its underlying transformer architecture and multi-modal capabilities, while selectively fine-tuning only the audio‑specific components, enabling strong cross‑lingual generalization.&lt;/p&gt;



&lt;p&gt;As the results below show, this model delivers consistent improvements in transcription quality across all six languages.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3: Character Error Rate (CER) comparison across&amp;nbsp;the six&amp;nbsp;languages for the base&amp;nbsp;model&amp;nbsp;versus the finetuned Paza model.&amp;nbsp;Lower CER&amp;nbsp;indicates&amp;nbsp;better transcription performance." class="wp-image-1161376" height="1063" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG3_phi_cer_comparison_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 3: &lt;em&gt;Character Error Rate (CER) comparison across&amp;nbsp;the six&lt;/em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;em&gt;languages for the base&amp;nbsp;model&amp;nbsp;versus the finetuned Paza model.&amp;nbsp;Lower CER&amp;nbsp;indicates&amp;nbsp;better transcription performance.&lt;/em&gt;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4: Word Error Rate (WER) comparison across the six languages for the base model versus the finetuned Paza model. Lower WER indicates better transcription performance." class="wp-image-1161378" height="1063" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG4_phi_wer_comparison_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 4: Word Error Rate (WER) comparison across the six languages for the base model versus the finetuned Paza model. Lower WER indicates better transcription performance.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;







&lt;p&gt;&lt;strong&gt;2) Paza‑MMS‑1B‑All&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;This model is fine-tuned on Meta’s mms-1b-all model, which employs a large-scale Wav2Vec2.0-style encoder with lightweight language-specific adapters to enable efficient multilingual specialization. For this release, each of the six language adapters was fine‑tuned independently on curated low‑resource datasets, allowing targeted adaptation while keeping the shared encoder largely frozen.&lt;/p&gt;



&lt;p&gt;As shown in the figures below, this model improves transcription accuracy while maintaining the model’s strong cross‑lingual generalization.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 5: Character Error Rate (CER)&amp;nbsp;comparison across the six&amp;nbsp;languages for the base model&amp;nbsp;versus&amp;nbsp;the finetuned Paza model.&amp;nbsp;Lower CER&amp;nbsp;indicates&amp;nbsp;better transcription performance." class="wp-image-1161380" height="1160" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG5_mms_cer_comparison_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 5: &lt;em&gt;Character Error Rate (CER)&amp;nbsp;comparison across the six&lt;/em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;em&gt;languages for the base model&amp;nbsp;versus&amp;nbsp;the finetuned Paza model.&amp;nbsp;Lower CER&amp;nbsp;indicates&amp;nbsp;better transcription performance.&lt;/em&gt;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 6: Word Error Rate (WER)&amp;nbsp;comparison across the six&amp;nbsp;languages for the base model&amp;nbsp;versus the finetuned Paza model.&amp;nbsp;Lower WER indicates better transcription performance." class="wp-image-1161382" height="1160" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG6_mms_wer_comparison_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 6: &lt;em&gt;Word Error Rate (WER)&amp;nbsp;comparison across the six&lt;/em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;em&gt;languages for the base model&amp;nbsp;versus the finetuned Paza model.&amp;nbsp;Lower WER indicates better transcription performance.&lt;/em&gt;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;







&lt;p&gt;&lt;strong&gt;3) Paza‑Whisper‑Large‑v3‑Turbo&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;This model is finetuned on OpenAI’s whisper-large-v3-turbo&amp;nbsp;base model. Whisper is a transformer-based encoder–decoder model which&amp;nbsp;delivers robust automatic speech recognition (ASR)&amp;nbsp;capabilities. This model was fine‑tuned on the entire unified multilingual ASR dataset,&amp;nbsp;on&amp;nbsp;the mentioned six languages, to encourage cross-lingual generalization.&amp;nbsp;In addition, an extra post‑processing step was applied to address the known Whisper hallucination failure modes, improving transcription reliability.&lt;/p&gt;



&lt;p&gt;As shown below, this release achieves improved transcription accuracy while retaining Whisper’s robustness.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 7: Character Error Rate (CER) comparison across the six&amp;nbsp;languages for the base model versus the finetuned Paza model.&amp;nbsp;Lower CER&amp;nbsp;indicates&amp;nbsp;better transcription performance." class="wp-image-1161338" height="1081" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG7whisper_cer_comparison_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 7: &lt;em&gt;Character Error Rate (CER) comparison across the six&lt;/em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;em&gt;languages for the base model versus the finetuned Paza model.&amp;nbsp;Lower CER&amp;nbsp;indicates&amp;nbsp;better transcription performance.&lt;/em&gt;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 8: Word Error Rate (WER) comparison across the six&amp;nbsp;languages for the base model versus the finetuned Paza model.&amp;nbsp;Lower WER indicates better transcription performance." class="wp-image-1161341" height="1081" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG8_whisper_wer_comparison_NEW-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Figure 8: &lt;em&gt;Word Error Rate (WER) comparison across the six&lt;/em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;em&gt;languages for the base model versus the finetuned Paza model.&amp;nbsp;Lower WER indicates better transcription performance.&lt;/em&gt;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;







&lt;h2 class="wp-block-heading" id="where-do-we-go-from-here"&gt;Where do we go from here&lt;/h2&gt;



&lt;p&gt;AI is reshaping how the world communicates. Designing with people, not just for them, means looking beyond the languages that are already well‑served. We plan to expand PazaBench beyond African languages and evaluate state‑of‑the‑art ASR models across&amp;nbsp;more low‑resource languages globally. The Paza ASR models are an early step; truly supporting small and under‑represented languages requires dedicated datasets, strong local partnerships, and rigorous evaluation. Meaningful progress depends on sustained collaboration with the communities who speak these languages, and expanding responsibly means prioritizing depth and quality over broad but shallow coverage.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;As we continue this work,&amp;nbsp;we’re&amp;nbsp;distilling our methods into a forthcoming playbook to help the broader ecosystem curate datasets, fine‑tune responsibly, and evaluate models in real‑world conditions. And we’re not stopping at speech—additional&amp;nbsp;playbooks will guide&amp;nbsp;teams&amp;nbsp;building AI tools and applications for multilingual, multicultural contexts, and give them practical recommendations for deploying across diverse communities.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Together, these guides—grounded in technical advances and community‑driven design—share our learnings to help researchers, engineers, and designers build more human‑centered AI systems.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;



&lt;p&gt;The following researchers played an integral role in this work: Najeeb Abdulhamid, Felermino Ali, Liz Ankrah, Kevin Chege, Ogbemi Ekwejunor-Etchie, Ignatius Ezeani, Tanuja Ganu, Antonis Krasakis, Mercy Kwambai, Samuel Maina, Muchai Mercy, Danlami Mohammed, Nick Mumero, Martin Mwiti, Stephanie Nyairo, Millicent Ochieng and Jacki O’Neill.&lt;/p&gt;



&lt;p&gt;We would like to thank the Digital Green&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; team—Rikin Gandhi, Alex Mwaura, Jacqueline Wang’ombe, Kevin Mugambi, Lorraine Nyambura, Juan Pablo, Nereah Okanga, Ramaskanda R.S, Vineet Singh, Nafhtari Wanjiku, Kista Ogot, Samuel Owinya&amp;nbsp;and the community evaluators in Nyeri and Nandi, Kenya — for their valuable contributions to this work.&lt;/p&gt;



&lt;p&gt;We extend our gratitude to the creators, community contributors, and maintainers of African Next Voices Kenya&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, African Next Voices South Africa&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, ALFFA&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Digigreen&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Google FLEURS&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Mozilla Common Voice&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Naija Voices&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; whose efforts have been invaluable in advancing African languages speech data.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/paza-introducing-automatic-speech-recognition-benchmarks-and-models-for-low-resource-languages/</guid><pubDate>Thu, 05 Feb 2026 05:07:55 +0000</pubDate></item></channel></rss>