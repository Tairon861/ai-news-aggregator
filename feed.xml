<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 21 Jan 2026 06:40:26 +0000</lastBuildDate><item><title>Why it’s critical to move beyond overly aggregated machine-learning metrics (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/why-its-critical-to-move-beyond-overly-aggregated-machine-learning-metrics-0120</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202601/mit-lids-Ghassemi.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;MIT researchers have identified significant examples of machine-learning model failure when those models are applied to data other than what they were trained on, raising questions about the need to test whenever a model is deployed in a new setting.&lt;/p&gt;&lt;p&gt;“We demonstrate that even when you train models on large amounts of data, and choose the best average model, in a new setting this ‘best model’ could be the worst model for 6-75 percent of the new data,” says Marzyeh Ghassemi, an associate professor in MIT’s Department of Electrical Engineering and Computer Science (EECS), a member of the Institute for Medical Engineering and Science, and principal investigator at the Laboratory for Information and Decision Systems.&lt;/p&gt;&lt;p&gt;In a paper that was presented at the Neural Information Processing Systems (NeurIPS 2025) conference in December, the researchers point out that models trained to effectively diagnose illness in chest X-rays at one hospital, for example, may be considered effective in a different hospital, on average. The researchers’ performance assessment, however, revealed that some of the best-performing models at the first hospital were the worst-performing on up to 75 percent of patients at the second hospital, even though when all patients are aggregated in the second hospital, high average performance hides this failure.&lt;/p&gt;&lt;p&gt;Their findings demonstrate that although spurious correlations — a simple example of which is when a machine-learning system, not having “seen” many cows pictured at the beach, classifies a photo of a beach-going cow as an orca simply because of its background — are thought to be mitigated by just improving model performance on observed data, they actually still occur and remain a risk to a model’s trustworthiness in new settings. In many instances — including areas examined by the researchers such as chest X-rays, cancer histopathology images, and hate speech detection — such spurious correlations are much harder to detect.&lt;/p&gt;&lt;p&gt;In the case of a medical diagnosis model trained on chest X-rays, for example, the model may have learned to correlate a specific and irrelevant marking on one hospital’s X-rays with a certain pathology. At another hospital where the marking is not used, that pathology could be missed.&lt;/p&gt;&lt;p&gt;Previous research by Ghassemi’s group has shown that models can spuriously correlate such factors as age, gender, and race with medical findings. If, for instance, a model has been trained on more older people’s chest X-rays that have pneumonia and hasn’t “seen” as many X-rays belonging to younger people, it might predict that only older patients have pneumonia.&lt;/p&gt;&lt;p&gt;“We want models to learn how to look at the anatomical features of the patient and then make a decision based on that,” says Olawale Salaudeen, an MIT postdoc and the lead author of the paper, “but really anything that’s in the data that’s correlated with a decision can be used by the model. And those correlations might not actually be robust with changes in the environment, making the model predictions unreliable sources of decision-making.”&lt;/p&gt;&lt;p&gt;Spurious correlations contribute to the risks of biased decision-making. In the NeurIPS conference paper, the researchers showed that, for example, chest X-ray models that improved overall diagnosis performance actually performed worse on patients with pleural conditions or enlarged cardiomediastinum, meaning enlargement of the heart or central chest cavity.&lt;/p&gt;&lt;p&gt;Other authors of the paper included PhD students Haoran Zhang and Kumail Alhamoud, EECS Assistant Professor Sara Beery, and Ghassemi.&lt;/p&gt;&lt;p&gt;While previous work has generally accepted that models ordered best-to-worst by performance will preserve that order when applied in new settings, called accuracy-on-the-line, the researchers were able to demonstrate examples of when the best-performing models in one setting were the worst-performing in another.&lt;/p&gt;&lt;p&gt;Salaudeen devised an algorithm called OODSelect to find examples where accuracy-on-the-line was broken. Basically, he trained thousands of models using in-distribution data, meaning the data were from the first setting, and calculated their accuracy. Then he applied the models to the data from the second setting. When those with the highest accuracy on the first-setting data were wrong when applied to a large percentage of examples in the second setting, this identified the problem subsets, or sub-populations. Salaudeen also emphasizes the dangers of aggregate statistics for evaluation, which can obscure more granular and consequential information about model performance.&lt;/p&gt;&lt;p&gt;In the course of their work, the researchers separated out the “most miscalculated examples” so as not to conflate spurious correlations within a dataset with situations that are simply difficult to classify.&lt;/p&gt;&lt;p&gt;The NeurIPS paper releases the researchers’ code and some identified subsets for future work.&lt;/p&gt;&lt;p&gt;Once a hospital, or any organization employing machine learning, identifies subsets on which a model is performing poorly, that information can be used to improve the model for its particular task and setting. The researchers recommend that future work adopt OODSelect in order to highlight targets for evaluation and design approaches to improving performance more consistently.&lt;/p&gt;&lt;p&gt;“We hope the released code and OODSelect subsets become a steppingstone,” the researchers write, “toward benchmarks and models that confront the adverse effects of spurious correlations.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202601/mit-lids-Ghassemi.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;MIT researchers have identified significant examples of machine-learning model failure when those models are applied to data other than what they were trained on, raising questions about the need to test whenever a model is deployed in a new setting.&lt;/p&gt;&lt;p&gt;“We demonstrate that even when you train models on large amounts of data, and choose the best average model, in a new setting this ‘best model’ could be the worst model for 6-75 percent of the new data,” says Marzyeh Ghassemi, an associate professor in MIT’s Department of Electrical Engineering and Computer Science (EECS), a member of the Institute for Medical Engineering and Science, and principal investigator at the Laboratory for Information and Decision Systems.&lt;/p&gt;&lt;p&gt;In a paper that was presented at the Neural Information Processing Systems (NeurIPS 2025) conference in December, the researchers point out that models trained to effectively diagnose illness in chest X-rays at one hospital, for example, may be considered effective in a different hospital, on average. The researchers’ performance assessment, however, revealed that some of the best-performing models at the first hospital were the worst-performing on up to 75 percent of patients at the second hospital, even though when all patients are aggregated in the second hospital, high average performance hides this failure.&lt;/p&gt;&lt;p&gt;Their findings demonstrate that although spurious correlations — a simple example of which is when a machine-learning system, not having “seen” many cows pictured at the beach, classifies a photo of a beach-going cow as an orca simply because of its background — are thought to be mitigated by just improving model performance on observed data, they actually still occur and remain a risk to a model’s trustworthiness in new settings. In many instances — including areas examined by the researchers such as chest X-rays, cancer histopathology images, and hate speech detection — such spurious correlations are much harder to detect.&lt;/p&gt;&lt;p&gt;In the case of a medical diagnosis model trained on chest X-rays, for example, the model may have learned to correlate a specific and irrelevant marking on one hospital’s X-rays with a certain pathology. At another hospital where the marking is not used, that pathology could be missed.&lt;/p&gt;&lt;p&gt;Previous research by Ghassemi’s group has shown that models can spuriously correlate such factors as age, gender, and race with medical findings. If, for instance, a model has been trained on more older people’s chest X-rays that have pneumonia and hasn’t “seen” as many X-rays belonging to younger people, it might predict that only older patients have pneumonia.&lt;/p&gt;&lt;p&gt;“We want models to learn how to look at the anatomical features of the patient and then make a decision based on that,” says Olawale Salaudeen, an MIT postdoc and the lead author of the paper, “but really anything that’s in the data that’s correlated with a decision can be used by the model. And those correlations might not actually be robust with changes in the environment, making the model predictions unreliable sources of decision-making.”&lt;/p&gt;&lt;p&gt;Spurious correlations contribute to the risks of biased decision-making. In the NeurIPS conference paper, the researchers showed that, for example, chest X-ray models that improved overall diagnosis performance actually performed worse on patients with pleural conditions or enlarged cardiomediastinum, meaning enlargement of the heart or central chest cavity.&lt;/p&gt;&lt;p&gt;Other authors of the paper included PhD students Haoran Zhang and Kumail Alhamoud, EECS Assistant Professor Sara Beery, and Ghassemi.&lt;/p&gt;&lt;p&gt;While previous work has generally accepted that models ordered best-to-worst by performance will preserve that order when applied in new settings, called accuracy-on-the-line, the researchers were able to demonstrate examples of when the best-performing models in one setting were the worst-performing in another.&lt;/p&gt;&lt;p&gt;Salaudeen devised an algorithm called OODSelect to find examples where accuracy-on-the-line was broken. Basically, he trained thousands of models using in-distribution data, meaning the data were from the first setting, and calculated their accuracy. Then he applied the models to the data from the second setting. When those with the highest accuracy on the first-setting data were wrong when applied to a large percentage of examples in the second setting, this identified the problem subsets, or sub-populations. Salaudeen also emphasizes the dangers of aggregate statistics for evaluation, which can obscure more granular and consequential information about model performance.&lt;/p&gt;&lt;p&gt;In the course of their work, the researchers separated out the “most miscalculated examples” so as not to conflate spurious correlations within a dataset with situations that are simply difficult to classify.&lt;/p&gt;&lt;p&gt;The NeurIPS paper releases the researchers’ code and some identified subsets for future work.&lt;/p&gt;&lt;p&gt;Once a hospital, or any organization employing machine learning, identifies subsets on which a model is performing poorly, that information can be used to improve the model for its particular task and setting. The researchers recommend that future work adopt OODSelect in order to highlight targets for evaluation and design approaches to improving performance more consistently.&lt;/p&gt;&lt;p&gt;“We hope the released code and OODSelect subsets become a steppingstone,” the researchers write, “toward benchmarks and models that confront the adverse effects of spurious correlations.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/why-its-critical-to-move-beyond-overly-aggregated-machine-learning-metrics-0120</guid><pubDate>Tue, 20 Jan 2026 21:30:00 +0000</pubDate></item><item><title>Elon Musk says Tesla’s restarted Dojo3 will be for ‘space-based AI compute’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/20/elon-musk-says-teslas-restarted-dojo3-will-be-for-space-based-ai-compute/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/tesla-phone-app.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk said over the long weekend that Tesla aims to restart work on Dojo3, the electric vehicle company’s previously abandoned third-generation AI chip. Only this time, Dojo3 won’t be aimed at training self-driving models on Earth. Instead, Musk says it will be dedicated to “space-based AI compute.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes five months after Tesla effectively shut down its Dojo effort. The company disbanded the team behind its Dojo supercomputer following the departure of Dojo lead Peter Bannon. Around 20 Dojo workers also left to join DensityAI, a new AI infrastructure startup founded by former Dojo head Ganesh Venkataramanan and ex-Tesla employees Bill Chang and Ben Floering.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At the time of Dojo’s shutdown, Bloomberg reported Tesla planned to increase its reliance on Nvidia and other partners like AMD for compute and Samsung for chip manufacturing, rather than continue developing its own custom silicon. Musk’s latest comments suggest the strategy has shifted again.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The billionaire executive and Republican megadonor said in a post on X the decision to revive Dojo was based on the state of its in-house chip roadmap, noting that Tesla’s AI5 chip design was “in good shape.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla’s AI5 chip, made by TSMC, was designed to power the automaker’s automated driving features and Optimus humanoid robots. Last summer, Tesla signed a $16.5 billion deal with Samsung to build its AI6 chips that promise to power Tesla vehicles and Optimus, as well as enable high-performance AI training in data centers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI7/Dojo3 will be for space-based AI compute,” Musk said on Sunday, positioning the resurrected project as more of a moonshot.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To achieve that, Tesla is now gearing up to rebuild the team it dismantled months ago. Musk used the same post to recruit engineers directly, writing:&amp;nbsp;“If you’re interested in working on what will be the highest volume chips in the world, send a note to AI_Chips@Tesla.com with 3 bullet points on the toughest technical problems you’ve solved.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The timing of the announcement is notable. At CES 2026, Nvidia unveiled Alpamayo, an open source AI model for autonomous driving that directly challenges Tesla’s FSD software. Musk commented on X that solving the long tail of rare edge cases in driving is “super hard,” adding: “I honestly hope they succeed.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk and several other AI executives have argued the future of data centers may lie off-planet, since Earth’s power grids are already strained to the max. Axios recently reported Musk rival and OpenAI CEO Sam Altman is also excited by the prospect of putting data centers into orbit. Musk has an edge over his peers because he already controls the launch vehicles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Per Axios, Musk plans to use SpaceX’s upcoming IPO to help finance his vision of using Starship to launch a constellation of compute satellites that can operate in constant sunlight, harvesting solar power 24/7.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still, there are many roadblocks to making AI data centers in space a possibility, not least the challenge of cooling high-power compute in a vacuum. Musk’s comments of Tesla building “space-based AI compute” fit a familiar pattern: float an idea that sounds far-fetched, then try to brute-force it into reality.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/tesla-phone-app.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk said over the long weekend that Tesla aims to restart work on Dojo3, the electric vehicle company’s previously abandoned third-generation AI chip. Only this time, Dojo3 won’t be aimed at training self-driving models on Earth. Instead, Musk says it will be dedicated to “space-based AI compute.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes five months after Tesla effectively shut down its Dojo effort. The company disbanded the team behind its Dojo supercomputer following the departure of Dojo lead Peter Bannon. Around 20 Dojo workers also left to join DensityAI, a new AI infrastructure startup founded by former Dojo head Ganesh Venkataramanan and ex-Tesla employees Bill Chang and Ben Floering.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At the time of Dojo’s shutdown, Bloomberg reported Tesla planned to increase its reliance on Nvidia and other partners like AMD for compute and Samsung for chip manufacturing, rather than continue developing its own custom silicon. Musk’s latest comments suggest the strategy has shifted again.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The billionaire executive and Republican megadonor said in a post on X the decision to revive Dojo was based on the state of its in-house chip roadmap, noting that Tesla’s AI5 chip design was “in good shape.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla’s AI5 chip, made by TSMC, was designed to power the automaker’s automated driving features and Optimus humanoid robots. Last summer, Tesla signed a $16.5 billion deal with Samsung to build its AI6 chips that promise to power Tesla vehicles and Optimus, as well as enable high-performance AI training in data centers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI7/Dojo3 will be for space-based AI compute,” Musk said on Sunday, positioning the resurrected project as more of a moonshot.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To achieve that, Tesla is now gearing up to rebuild the team it dismantled months ago. Musk used the same post to recruit engineers directly, writing:&amp;nbsp;“If you’re interested in working on what will be the highest volume chips in the world, send a note to AI_Chips@Tesla.com with 3 bullet points on the toughest technical problems you’ve solved.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The timing of the announcement is notable. At CES 2026, Nvidia unveiled Alpamayo, an open source AI model for autonomous driving that directly challenges Tesla’s FSD software. Musk commented on X that solving the long tail of rare edge cases in driving is “super hard,” adding: “I honestly hope they succeed.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk and several other AI executives have argued the future of data centers may lie off-planet, since Earth’s power grids are already strained to the max. Axios recently reported Musk rival and OpenAI CEO Sam Altman is also excited by the prospect of putting data centers into orbit. Musk has an edge over his peers because he already controls the launch vehicles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Per Axios, Musk plans to use SpaceX’s upcoming IPO to help finance his vision of using Starship to launch a constellation of compute satellites that can operate in constant sunlight, harvesting solar power 24/7.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still, there are many roadblocks to making AI data centers in space a possibility, not least the challenge of cooling high-power compute in a vacuum. Musk’s comments of Tesla building “space-based AI compute” fit a familiar pattern: float an idea that sounds far-fetched, then try to brute-force it into reality.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/20/elon-musk-says-teslas-restarted-dojo3-will-be-for-space-based-ai-compute/</guid><pubDate>Tue, 20 Jan 2026 22:10:41 +0000</pubDate></item><item><title>In an effort to protect young users, ChatGPT will now predict how old you are (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/20/in-an-effort-to-protect-young-users-chatgpt-will-now-predict-how-old-you-are/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2191707579.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As concern for AI’s impact on young people continues to mount, OpenAI has introduced an “age prediction” feature into ChatGPT that is designed to help identify minors and put sensible content constraints on their conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has been heavily criticized in recent years for the impacts that ChatGPT can have on children. A number of teen suicides have been linked to the chatbot, and, like other AI vendors, OpenAI has also been criticized for allowing ChatGPT to discuss sexual topics with young users. Last April, the company was forced to address a bug that allowed its chatbot to generate erotica for users who were under the age of 18.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company has already been working on its underage user problem for some time, and its new “age prediction” feature merely adds to protections already in place. The new feature leverages an AI algorithm that assesses user accounts for particular “behavioral and account-level signals,” in an effort to identify young users, OpenAI said in a blog post Tuesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those “signals” include things like the user’s stated age, the length of time an account has existed, and the times of day that the account is usually active, the company said. The company already has content filters designed to weed out discussions of sex, violence, and other potentially problematic topics for users who are under age 18. If the age prediction mechanism identifies an account as under 18, those filters are automatically applied.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If a user is mistakenly designated as underage, there is a way for them to reestablish their “adult” account. They can submit a selfie to OpenAI’s ID verification partner Persona, OpenAI says.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2191707579.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As concern for AI’s impact on young people continues to mount, OpenAI has introduced an “age prediction” feature into ChatGPT that is designed to help identify minors and put sensible content constraints on their conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has been heavily criticized in recent years for the impacts that ChatGPT can have on children. A number of teen suicides have been linked to the chatbot, and, like other AI vendors, OpenAI has also been criticized for allowing ChatGPT to discuss sexual topics with young users. Last April, the company was forced to address a bug that allowed its chatbot to generate erotica for users who were under the age of 18.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company has already been working on its underage user problem for some time, and its new “age prediction” feature merely adds to protections already in place. The new feature leverages an AI algorithm that assesses user accounts for particular “behavioral and account-level signals,” in an effort to identify young users, OpenAI said in a blog post Tuesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those “signals” include things like the user’s stated age, the length of time an account has existed, and the times of day that the account is usually active, the company said. The company already has content filters designed to weed out discussions of sex, violence, and other potentially problematic topics for users who are under age 18. If the age prediction mechanism identifies an account as under 18, those filters are automatically applied.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If a user is mistakenly designated as underage, there is a way for them to reestablish their “adult” account. They can submit a selfie to OpenAI’s ID verification partner Persona, OpenAI says.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/20/in-an-effort-to-protect-young-users-chatgpt-will-now-predict-how-old-you-are/</guid><pubDate>Tue, 20 Jan 2026 23:29:56 +0000</pubDate></item><item><title>Anthropic’s CEO stuns Davos with Nvidia criticism (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/20/anthropics-ceo-stuns-davos-with-nvidia-criticism/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2153561878.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last week, after reversing an earlier ban, the U.S. administration officially approved the sale of Nvidia’s H200 chips, along with a chip line by AMD, to approved Chinese customers. Maybe they aren’t these chipmakers’ shiniest, most advanced chips, but they’re high-performance processors used for AI, making the export controversial. And at the World Economic Forum in Davos on Tuesday, Anthropic CEO Dario Amodei unloaded on both the administration and the chip companies over the decision.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The criticism was particularly notable because one of those chipmakers, Nvidia, is a major partner and investor in Anthropic.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The CEOs of these companies say, ‘It’s the embargo on chips that’s holding us back,’” Amodei  said, incredulous, in response to a question about the new rules. The decision is going to come back to bite the U.S., he warned.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are many years ahead of China in terms of our ability to make chips,” he told Bloomberg’s editor-in-chief, who was interviewing him. “So I think it would be a big mistake to ship these chips.” Amodei then painted an alarming picture of what’s at stake. He talked about the “incredible national security implications” of AI models that represent “essentially cognition, that are essentially intelligence.” He likened future AI to a “country of geniuses in a data center,” saying to imagine “100 million people smarter than any Nobel Prize winner,” all under the control of one country or another.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The image underscored why he thinks chip exports matter so much. But then came the biggest blow. “I think this is crazy,” Amodei said of the administration’s latest move. “It’s a bit like selling nuclear weapons to North Korea and [bragging that] Boeing made the casings.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That sound you hear? The team at Nvidia, screaming into their phones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia isn’t just another chip company. While Anthropic runs on the servers of Microsoft and Amazon and Google, Nvidia alone supplies the GPUs that power Anthropic’s AI models (every cloud provider needs Nvidia’s GPUs). Not only does Nvidia sit at the center of everything, but it also recently announced it was investing in Anthropic to the tune of up to $10 billion.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Just two months ago, the companies announced that financial relationship, along with a “deep technology partnership” with cheery promises to optimize each other’s technology. Fast-forward to Davos, and Amodei is comparing his partner to an arms dealer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Maybe it was just an unguarded moment — it’s possible he got swept up in his own rhetoric and blurted out the analogy. But given Anthropic’s strong position in the AI market, it seems more likely he felt comfortable speaking with confidence. The company has raised billions, is valued in the hundreds of billions, and its Claude coding assistant has developed a reputation as a highly beloved and top-tier AI coding tool, particularly among developers working on complex, real-world projects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s also entirely possible that Anthropic genuinely fears Chinese AI labs and wants Washington to act. If you want to get someone’s attention, nuclear proliferation comparisons are probably a pretty effective way to do it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But what’s perhaps most remarkable is that Amodei could sit onstage at Davos, drop a bomb like that, and walk away to some other gathering without fear that he just adversely impacted his business. News cycles move on, sure. Anthropic is also on solid footing right now. But it does feel that the AI race has grown so existential in the minds of its leaders that the usual constraints — investor relations, strategic partnerships, diplomatic niceties — don’t apply anymore. Amodei isn’t concerned about what he can and can’t say. More than anything else he said on that stage, that fearlessness is worth paying attention to.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2153561878.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last week, after reversing an earlier ban, the U.S. administration officially approved the sale of Nvidia’s H200 chips, along with a chip line by AMD, to approved Chinese customers. Maybe they aren’t these chipmakers’ shiniest, most advanced chips, but they’re high-performance processors used for AI, making the export controversial. And at the World Economic Forum in Davos on Tuesday, Anthropic CEO Dario Amodei unloaded on both the administration and the chip companies over the decision.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The criticism was particularly notable because one of those chipmakers, Nvidia, is a major partner and investor in Anthropic.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The CEOs of these companies say, ‘It’s the embargo on chips that’s holding us back,’” Amodei  said, incredulous, in response to a question about the new rules. The decision is going to come back to bite the U.S., he warned.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are many years ahead of China in terms of our ability to make chips,” he told Bloomberg’s editor-in-chief, who was interviewing him. “So I think it would be a big mistake to ship these chips.” Amodei then painted an alarming picture of what’s at stake. He talked about the “incredible national security implications” of AI models that represent “essentially cognition, that are essentially intelligence.” He likened future AI to a “country of geniuses in a data center,” saying to imagine “100 million people smarter than any Nobel Prize winner,” all under the control of one country or another.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The image underscored why he thinks chip exports matter so much. But then came the biggest blow. “I think this is crazy,” Amodei said of the administration’s latest move. “It’s a bit like selling nuclear weapons to North Korea and [bragging that] Boeing made the casings.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That sound you hear? The team at Nvidia, screaming into their phones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia isn’t just another chip company. While Anthropic runs on the servers of Microsoft and Amazon and Google, Nvidia alone supplies the GPUs that power Anthropic’s AI models (every cloud provider needs Nvidia’s GPUs). Not only does Nvidia sit at the center of everything, but it also recently announced it was investing in Anthropic to the tune of up to $10 billion.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Just two months ago, the companies announced that financial relationship, along with a “deep technology partnership” with cheery promises to optimize each other’s technology. Fast-forward to Davos, and Amodei is comparing his partner to an arms dealer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Maybe it was just an unguarded moment — it’s possible he got swept up in his own rhetoric and blurted out the analogy. But given Anthropic’s strong position in the AI market, it seems more likely he felt comfortable speaking with confidence. The company has raised billions, is valued in the hundreds of billions, and its Claude coding assistant has developed a reputation as a highly beloved and top-tier AI coding tool, particularly among developers working on complex, real-world projects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s also entirely possible that Anthropic genuinely fears Chinese AI labs and wants Washington to act. If you want to get someone’s attention, nuclear proliferation comparisons are probably a pretty effective way to do it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But what’s perhaps most remarkable is that Amodei could sit onstage at Davos, drop a bomb like that, and walk away to some other gathering without fear that he just adversely impacted his business. News cycles move on, sure. Anthropic is also on solid footing right now. But it does feel that the AI race has grown so existential in the minds of its leaders that the usual constraints — investor relations, strategic partnerships, diplomatic niceties — don’t apply anymore. Amodei isn’t concerned about what he can and can’t say. More than anything else he said on that stage, that fearlessness is worth paying attention to.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/20/anthropics-ceo-stuns-davos-with-nvidia-criticism/</guid><pubDate>Wed, 21 Jan 2026 01:39:58 +0000</pubDate></item><item><title>[NEW] Bolna nabs $6.3M from General Catalyst for its India-focused voice orchestration platform (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/20/bolna-nabs-6-3-million-from-general-catalyst-for-its-india-focused-voice-orchestration-platform/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Industry reports and the growth of voice model companies in the Indian market suggest that there is a growing demand for voice AI solutions in the country. Voice is a popular medium for communication among people and businesses in India. That’s why enterprises and startups are eager to use voice AI to be more efficient at customer support, sales, customer acquisition, hiring, and training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But recognizing market demand is one thing — proving businesses will pay is another. Y Combinator rejected the application from Bolna, a voice orchestration startup built by Maitreya Wagh and Prateek Sachan, five times before finally accepting it into the fall 2025 batch, skeptical that the founders could turn interest into revenue.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“When we were applying for Y Combinator, the feedback we got was, ‘great to see that you have a product that can create realistic voice agents, but Indian enterprises are not going to pay, and you are not going to make money out of this,’” Wagh told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup applied with the same idea for the fall batch but was able to show it had revenue of more than $25,000 coming in every month for the last few months. At that time, the company was running $100 pilots to help users build voice agents. Now the startup is pricing those pilots at $500.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The momentum has continued. The startup said on Tuesday that it has raised a $6.3 million seed round led by General Catalyst, with participation from Y Combinator, Blume Ventures, Orange Collective, Pioneer Fund, Transpose Capital, and Eight Capital. The round also includes individual investors, including Aarthi Ramamurthy, Arpan Sheth, Sriwatsan Krishnan, Ravi Iyer, and Taro Fukuyama.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-product-and-customers"&gt;The product and customers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Bolna is building an orchestration layer — essentially a platform that connects and manages different AI voice technologies — akin to startups like Vapi, LiveKit, and VoiceRun, to suit the idiosyncrasies of interactions in India, including noise cancellation, getting verification on the caller ID platform Truecaller, and handling mixed languages.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Feature-wise, the company has built specific nuances for Indian users, such as speaking numbers in English regardless of the core language, or allowing for keypad input for longer inputs.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3083843" height="387" src="https://techcrunch.com/wp-content/uploads/2026/01/Bolna-UI.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Bolna&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Wagh noted that the key differentiation of Bolna is that it makes it easy for users to build voice agents by just describing them, even if they don’t know much about the underlying technology, and start using them for calls. The company said that 75% of its revenue is coming from self-serve customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also said that because Bolna is an orchestration layer, it doesn’t depend on a single model, so enterprises can easily switch when there is a better model available.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our platform allows customers to switch models easily or even use different models for different locales to get the best out of them. An orchestration layer is necessary for enterprises to ensure they are getting the best models because one model can be better today and another one can be better tomorrow,” Wagh said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company has a range of clients, including car reselling platform Spinny, on-demand house-help startup Snabbit, beverage companies, and dating apps. Most of these are small to midsize businesses that use Bolna’s self-serve platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Separately, Bolna is pursuing large enterprise deals. For these large enterprises and custom implementations, Bolna has a team of forward-deployed engineers — specialists who work directly with clients on-site or closely with their teams. The startup has signed two large enterprises as paying customers and has four more in the pilot stage. Currently, Bolna employs nine forward-deployed engineers and is adding two to three people to that team every month to support this enterprise push.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bolna has seen steady growth in both call volumes and revenue. It say it’s now handling over 200,000 calls per day and on the verge of crossing $700,000 in annual recurring revenue (ARR). The company noted that while 60% to 70% of call volume is in English or Hindi, other regional languages are steadily rising.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Akarsh Shrivastava, who is part of the investment team at General Catalyst, said that the firm found Bolna impressive because its orchestration layer is flexible for various kinds of customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Bolna allows you the freedom to choose any model and has a stack behind it to mold it according to your requirement. It’s a good option for people who want to own some part of the stack, want flexibility in model picking, and want to be able to maintain those products themselves,” Shrivastava told TechCrunch over a call.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Industry reports and the growth of voice model companies in the Indian market suggest that there is a growing demand for voice AI solutions in the country. Voice is a popular medium for communication among people and businesses in India. That’s why enterprises and startups are eager to use voice AI to be more efficient at customer support, sales, customer acquisition, hiring, and training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But recognizing market demand is one thing — proving businesses will pay is another. Y Combinator rejected the application from Bolna, a voice orchestration startup built by Maitreya Wagh and Prateek Sachan, five times before finally accepting it into the fall 2025 batch, skeptical that the founders could turn interest into revenue.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“When we were applying for Y Combinator, the feedback we got was, ‘great to see that you have a product that can create realistic voice agents, but Indian enterprises are not going to pay, and you are not going to make money out of this,’” Wagh told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup applied with the same idea for the fall batch but was able to show it had revenue of more than $25,000 coming in every month for the last few months. At that time, the company was running $100 pilots to help users build voice agents. Now the startup is pricing those pilots at $500.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The momentum has continued. The startup said on Tuesday that it has raised a $6.3 million seed round led by General Catalyst, with participation from Y Combinator, Blume Ventures, Orange Collective, Pioneer Fund, Transpose Capital, and Eight Capital. The round also includes individual investors, including Aarthi Ramamurthy, Arpan Sheth, Sriwatsan Krishnan, Ravi Iyer, and Taro Fukuyama.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-product-and-customers"&gt;The product and customers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Bolna is building an orchestration layer — essentially a platform that connects and manages different AI voice technologies — akin to startups like Vapi, LiveKit, and VoiceRun, to suit the idiosyncrasies of interactions in India, including noise cancellation, getting verification on the caller ID platform Truecaller, and handling mixed languages.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Feature-wise, the company has built specific nuances for Indian users, such as speaking numbers in English regardless of the core language, or allowing for keypad input for longer inputs.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3083843" height="387" src="https://techcrunch.com/wp-content/uploads/2026/01/Bolna-UI.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Bolna&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Wagh noted that the key differentiation of Bolna is that it makes it easy for users to build voice agents by just describing them, even if they don’t know much about the underlying technology, and start using them for calls. The company said that 75% of its revenue is coming from self-serve customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also said that because Bolna is an orchestration layer, it doesn’t depend on a single model, so enterprises can easily switch when there is a better model available.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our platform allows customers to switch models easily or even use different models for different locales to get the best out of them. An orchestration layer is necessary for enterprises to ensure they are getting the best models because one model can be better today and another one can be better tomorrow,” Wagh said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company has a range of clients, including car reselling platform Spinny, on-demand house-help startup Snabbit, beverage companies, and dating apps. Most of these are small to midsize businesses that use Bolna’s self-serve platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Separately, Bolna is pursuing large enterprise deals. For these large enterprises and custom implementations, Bolna has a team of forward-deployed engineers — specialists who work directly with clients on-site or closely with their teams. The startup has signed two large enterprises as paying customers and has four more in the pilot stage. Currently, Bolna employs nine forward-deployed engineers and is adding two to three people to that team every month to support this enterprise push.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bolna has seen steady growth in both call volumes and revenue. It say it’s now handling over 200,000 calls per day and on the verge of crossing $700,000 in annual recurring revenue (ARR). The company noted that while 60% to 70% of call volume is in English or Hindi, other regional languages are steadily rising.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Akarsh Shrivastava, who is part of the investment team at General Catalyst, said that the firm found Bolna impressive because its orchestration layer is flexible for various kinds of customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Bolna allows you the freedom to choose any model and has a stack behind it to mold it according to your requirement. It’s a good option for people who want to own some part of the stack, want flexibility in model picking, and want to be able to maintain those products themselves,” Shrivastava told TechCrunch over a call.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/20/bolna-nabs-6-3-million-from-general-catalyst-for-its-india-focused-voice-orchestration-platform/</guid><pubDate>Wed, 21 Jan 2026 02:00:00 +0000</pubDate></item><item><title>[NEW] AssetOpsBench: Bridging the Gap Between AI Agent Benchmarks and Industrial Reality (Hugging Face - Blog)</title><link>https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face</link><description>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Zhou's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/e2c7242c5a17937275077b1ee1394644.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
&lt;strong&gt;AssetOpsBench&lt;/strong&gt; is a comprehensive benchmark and evaluation system with six qualitative dimensions that bridges the gap for agentic AI in domain-specific settings, starting with industrial Asset Lifecycle Management.
&lt;div align="center"&gt;
  
    &lt;img alt="AssetOpsBench logo" src="https://cdn-uploads.huggingface.co/production/uploads/64c47f731d44fc06afc80953/1MyQgdHfeaZGtK37dbYuz.png" width="40%" /&gt;
  
&lt;/div&gt;

&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Introduction
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;While existing AI benchmarks excel at isolated tasks such as coding or web navigation, they often fail to capture the complexity of real-world industrial operations. To bridge this gap, we introduce &lt;strong&gt;AssetOpsBench&lt;/strong&gt;, a framework specifically designed to evaluate agent performance across six critical dimensions of industrial applications. Unlike traditional benchmarks, AssetOpsBench emphasizes the need for &lt;strong&gt;multi-agent&lt;/strong&gt; coordination—moving beyond `lone wolf' models to systems that can handle complex failure modes, integrate multiple data streams, and manage intricate work orders. By focusing on these high-stakes, multi-agent dynamics, the benchmark ensures that AI agents are assessed on their ability to navigate the nuances and safety-critical demands of a true industrial environment.&lt;/p&gt;
&lt;p&gt;AssetOpsBench is built for asset operations such as chillers and air handling units. It comprises:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;2.3M&lt;/strong&gt; sensor telemetry points&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;140+&lt;/strong&gt; curated scenarios across 4 agents&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;4.2K&lt;/strong&gt; work orders for diverse scenarios&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;53&lt;/strong&gt; structured failure modes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Experts helped curate &lt;strong&gt;150+&lt;/strong&gt; scenarios. Each scenario includes metadata: task type, output format, category, and sub-agents. The tasks designed span across:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Anomaly detection&lt;/strong&gt; in sensor streams&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Failure mode reasoning&lt;/strong&gt; and diagnostics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KPI forecasting&lt;/strong&gt; and analysis  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Work order&lt;/strong&gt; summarization and prioritization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Evaluation Framework and Overall Feedback
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;AssetOpsBench evaluates agentic systems across six qualitative dimensions designed to reflect real operational constraints in industrial asset management. Rather than optimizing for a single success metric, the benchmark emphasizes decision trace quality, evidence grounding, failure awareness, and actionability under incomplete and noisy data.&lt;/p&gt;
&lt;p&gt;Each agent run is scored across six criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Task Completion&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Retrieval Accuracy&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Result Verification&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sequence Correctness&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clarity and Justification&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hallucination rate&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Across early evaluations, we observe that many general-purpose agents perform well on surface-level reasoning but struggle with sustained multi-step coordination involving work orders, failure semantics, and temporal dependencies. Agents that explicitly model operational context and uncertainty tend to produce more stable and interpretable trajectories, even when final task completion is partial.&lt;/p&gt;
&lt;p&gt;This feedback-oriented evaluation is intentional: in industrial settings, understanding why an agent fails is often more valuable than a binary success signal.&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Failure Modes in Industrial Agentic Workflows
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;A central contribution of AssetOpsBench is the explicit treatment of &lt;strong&gt;failure modes&lt;/strong&gt; as first-class evaluation signals in agentic industrial workflows. Rather than treating failure as a binary outcome, AssetOpsBench analyzes full multi-agent execution trajectories to identify &lt;em&gt;where&lt;/em&gt;, &lt;em&gt;how&lt;/em&gt;, and &lt;em&gt;why&lt;/em&gt; agent behavior breaks down under realistic operational constraints.&lt;/p&gt;
&lt;p&gt;Failure analysis in AssetOpsBench is implemented through a dedicated trajectory-level pipeline (&lt;strong&gt;TrajFM&lt;/strong&gt;), which combines LLM-based reasoning with statistical clustering to surface interpretable failure patterns from agent execution traces. This pipeline operates in three stages: (1) trajectory-level failure extraction using an LLM-guided diagnostic prompt, (2) embedding-based clustering to group recurring failure patterns, and (3) analysis and visualization to support developer feedback and iteration.&lt;/p&gt;
&lt;p&gt;Across industrial scenarios, recurrent failure modes include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Misalignment between sensor telemetry, alerts, and historical work orders  &lt;/li&gt;
&lt;li&gt;Overconfident conclusions drawn under missing, delayed, or insufficient evidence  &lt;/li&gt;
&lt;li&gt;Inconsistent aggregation of heterogeneous data modalities across agents  &lt;/li&gt;
&lt;li&gt;Premature action selection without adequate verification or validation steps  &lt;/li&gt;
&lt;li&gt;Breakdowns in multi-agent coordination, such as ignored inputs or action–reasoning mismatches&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Importantly, AssetOpsBench does not rely solely on a fixed, hand-crafted failure taxonomy. While a structured set of predefined failure categories (e.g., verification errors, step repetition, role violations) is used for consistency, the system is explicitly designed to &lt;strong&gt;discover new failure patterns&lt;/strong&gt; that emerge in practice. Additional failure modes identified by the LLM are embedded and clustered automatically, allowing the taxonomy to evolve as new agent designs and behaviors are evaluated.&lt;/p&gt;
&lt;p&gt;To preserve industrial confidentiality, raw execution traces are never exposed. Instead, agents receive aggregated scores across six evaluation dimensions together with clustered failure-mode summaries that explain &lt;em&gt;why&lt;/em&gt; an agent failed, without revealing sensitive data or intermediate reasoning steps. This feedback-driven design enables developers to diagnose weaknesses, refine agent workflows, and iteratively resubmit improved agents.&lt;/p&gt;
&lt;p&gt;This failure-aware evaluation reflects the realities of industrial asset management, where cautious, degradation-aware reasoning—and the ability to recognize uncertainty, defer action, or escalate appropriately—is often preferable to aggressive but brittle automation.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Submit an Agent for Evaluation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;AssetOpsBench-Live is designed as an open, competition-ready benchmark, and we welcome submissions of agent implementations from the community. Agents are evaluated in a controlled, privacy-preserving environment that reflects real industrial asset management constraints.&lt;/p&gt;
&lt;p&gt;To submit an agent, developers first validate their implementation locally using a provided simulated environment, which includes representative sensor data, work orders, alerts, and failure-mode catalogs. Agents are then containerized and submitted for remote execution on hidden evaluation scenarios.&lt;/p&gt;
&lt;p&gt;Submitted agents are evaluated across six qualitative dimensions—task completion, accuracy, result verification, action sequencing, clarity, and hallucination—using a consistent, reproducible evaluation protocol. Execution traces are not exposed; instead, participants receive aggregated scores and structured failure-mode feedback that highlights where and why an agent’s reasoning or coordination broke down.&lt;/p&gt;
&lt;p&gt;This feedback-driven evaluation loop enables iterative improvement: developers can diagnose failure patterns, refine agent design or workflow structure, and resubmit updated agents for further evaluation. Both planning-focused and execution-focused agents are supported, allowing researchers and practitioners to explore diverse agentic designs within the same benchmark framework.&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Experiment and Observations
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We performed a community evaluation where we tested two tracks: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Planning-oriented&lt;/strong&gt; multi-agent orchestration &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Execution-oriented&lt;/strong&gt; dynamic multi-agent workflow.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Across 225 users and 300+ agents and leading open source models, here are the observations:&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th align="left"&gt;Model Family&lt;/th&gt;
&lt;th align="center"&gt;Best Planning Score&lt;/th&gt;
&lt;th align="center"&gt;Best Execution Score&lt;/th&gt;
&lt;th align="left"&gt;Key Limitation&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td align="left"&gt;&lt;strong&gt;GPT-4.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;68.2&lt;/td&gt;
&lt;td align="center"&gt;72.4&lt;/td&gt;
&lt;td align="left"&gt;Hallucinated completion on complex workflows&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;strong&gt;Mistral-Large&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;64.7&lt;/td&gt;
&lt;td align="center"&gt;69.1&lt;/td&gt;
&lt;td align="left"&gt;Struggled with multi-hop tool sequences&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;strong&gt;LLaMA-4 Maverick&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;66.0&lt;/td&gt;
&lt;td align="center"&gt;70.8&lt;/td&gt;
&lt;td align="left"&gt;Missed clarifying questions (fixable)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;strong&gt;LLaMA-3-70B&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;52.3&lt;/td&gt;
&lt;td align="center"&gt;58.9&lt;/td&gt;
&lt;td align="left"&gt;Collapsed under multi-agent coordination&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; None of the models could pass our evaluation criteria benchmark and get 85 points, which is the threshold for deployment readiness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Distribution of Failures
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Across 881 agent execution traces, failure distribution was as follows: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ineffective Error Recovery:&lt;/strong&gt; 31.2%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overstated Completion:&lt;/strong&gt; 23.8%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formatting Issues:&lt;/strong&gt; 21.4%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unhandled Tool Errors:&lt;/strong&gt; 10.3%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ignored Feedback:&lt;/strong&gt; 8.0%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Other:&lt;/strong&gt; 5.3%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Beyond this, 185 traces had one new failure pattern and 164 had multiple novel failures.&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Key Error Findings
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;"Sounds Right, Is Wrong":&lt;/strong&gt; Agents claim to have completed tasks (23.8%) and output success even after unsuccessful failure recovery (31.2%). AssetOps benchmarking is important to uncover this so that operators do not act upon incorrect information.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tool Usage:&lt;/strong&gt; This is the biggest differentiator between high and low performing agents, with top agents having 94% tool accuracy compared to 61% of low performers. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-agent Multiplies Failures:&lt;/strong&gt; Task accuracy between single agent (68%) vs multi-agent (47%) shows the complexity multi-agent brings with context loss, asynchronous issues, and cascaded failures. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Domain Knowledge:&lt;/strong&gt; Agents with access to failure mode databases and maintenance manuals performed better. However, RAG knowledge wasn’t always used correctly, suggesting a need for structured reasoning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ambiguity:&lt;/strong&gt; Missing sensors, conflicting logs, and vague operator descriptions caused the success rate to drop 34%. Agents must have clarification strategies embedded.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Where to get started?
	&lt;/span&gt;
&lt;/h2&gt;

&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Zhou's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/e2c7242c5a17937275077b1ee1394644.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
&lt;strong&gt;AssetOpsBench&lt;/strong&gt; is a comprehensive benchmark and evaluation system with six qualitative dimensions that bridges the gap for agentic AI in domain-specific settings, starting with industrial Asset Lifecycle Management.
&lt;div align="center"&gt;
  
    &lt;img alt="AssetOpsBench logo" src="https://cdn-uploads.huggingface.co/production/uploads/64c47f731d44fc06afc80953/1MyQgdHfeaZGtK37dbYuz.png" width="40%" /&gt;
  
&lt;/div&gt;

&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Introduction
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;While existing AI benchmarks excel at isolated tasks such as coding or web navigation, they often fail to capture the complexity of real-world industrial operations. To bridge this gap, we introduce &lt;strong&gt;AssetOpsBench&lt;/strong&gt;, a framework specifically designed to evaluate agent performance across six critical dimensions of industrial applications. Unlike traditional benchmarks, AssetOpsBench emphasizes the need for &lt;strong&gt;multi-agent&lt;/strong&gt; coordination—moving beyond `lone wolf' models to systems that can handle complex failure modes, integrate multiple data streams, and manage intricate work orders. By focusing on these high-stakes, multi-agent dynamics, the benchmark ensures that AI agents are assessed on their ability to navigate the nuances and safety-critical demands of a true industrial environment.&lt;/p&gt;
&lt;p&gt;AssetOpsBench is built for asset operations such as chillers and air handling units. It comprises:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;2.3M&lt;/strong&gt; sensor telemetry points&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;140+&lt;/strong&gt; curated scenarios across 4 agents&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;4.2K&lt;/strong&gt; work orders for diverse scenarios&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;53&lt;/strong&gt; structured failure modes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Experts helped curate &lt;strong&gt;150+&lt;/strong&gt; scenarios. Each scenario includes metadata: task type, output format, category, and sub-agents. The tasks designed span across:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Anomaly detection&lt;/strong&gt; in sensor streams&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Failure mode reasoning&lt;/strong&gt; and diagnostics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KPI forecasting&lt;/strong&gt; and analysis  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Work order&lt;/strong&gt; summarization and prioritization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Evaluation Framework and Overall Feedback
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;AssetOpsBench evaluates agentic systems across six qualitative dimensions designed to reflect real operational constraints in industrial asset management. Rather than optimizing for a single success metric, the benchmark emphasizes decision trace quality, evidence grounding, failure awareness, and actionability under incomplete and noisy data.&lt;/p&gt;
&lt;p&gt;Each agent run is scored across six criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Task Completion&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Retrieval Accuracy&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Result Verification&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sequence Correctness&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clarity and Justification&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hallucination rate&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Across early evaluations, we observe that many general-purpose agents perform well on surface-level reasoning but struggle with sustained multi-step coordination involving work orders, failure semantics, and temporal dependencies. Agents that explicitly model operational context and uncertainty tend to produce more stable and interpretable trajectories, even when final task completion is partial.&lt;/p&gt;
&lt;p&gt;This feedback-oriented evaluation is intentional: in industrial settings, understanding why an agent fails is often more valuable than a binary success signal.&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Failure Modes in Industrial Agentic Workflows
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;A central contribution of AssetOpsBench is the explicit treatment of &lt;strong&gt;failure modes&lt;/strong&gt; as first-class evaluation signals in agentic industrial workflows. Rather than treating failure as a binary outcome, AssetOpsBench analyzes full multi-agent execution trajectories to identify &lt;em&gt;where&lt;/em&gt;, &lt;em&gt;how&lt;/em&gt;, and &lt;em&gt;why&lt;/em&gt; agent behavior breaks down under realistic operational constraints.&lt;/p&gt;
&lt;p&gt;Failure analysis in AssetOpsBench is implemented through a dedicated trajectory-level pipeline (&lt;strong&gt;TrajFM&lt;/strong&gt;), which combines LLM-based reasoning with statistical clustering to surface interpretable failure patterns from agent execution traces. This pipeline operates in three stages: (1) trajectory-level failure extraction using an LLM-guided diagnostic prompt, (2) embedding-based clustering to group recurring failure patterns, and (3) analysis and visualization to support developer feedback and iteration.&lt;/p&gt;
&lt;p&gt;Across industrial scenarios, recurrent failure modes include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Misalignment between sensor telemetry, alerts, and historical work orders  &lt;/li&gt;
&lt;li&gt;Overconfident conclusions drawn under missing, delayed, or insufficient evidence  &lt;/li&gt;
&lt;li&gt;Inconsistent aggregation of heterogeneous data modalities across agents  &lt;/li&gt;
&lt;li&gt;Premature action selection without adequate verification or validation steps  &lt;/li&gt;
&lt;li&gt;Breakdowns in multi-agent coordination, such as ignored inputs or action–reasoning mismatches&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Importantly, AssetOpsBench does not rely solely on a fixed, hand-crafted failure taxonomy. While a structured set of predefined failure categories (e.g., verification errors, step repetition, role violations) is used for consistency, the system is explicitly designed to &lt;strong&gt;discover new failure patterns&lt;/strong&gt; that emerge in practice. Additional failure modes identified by the LLM are embedded and clustered automatically, allowing the taxonomy to evolve as new agent designs and behaviors are evaluated.&lt;/p&gt;
&lt;p&gt;To preserve industrial confidentiality, raw execution traces are never exposed. Instead, agents receive aggregated scores across six evaluation dimensions together with clustered failure-mode summaries that explain &lt;em&gt;why&lt;/em&gt; an agent failed, without revealing sensitive data or intermediate reasoning steps. This feedback-driven design enables developers to diagnose weaknesses, refine agent workflows, and iteratively resubmit improved agents.&lt;/p&gt;
&lt;p&gt;This failure-aware evaluation reflects the realities of industrial asset management, where cautious, degradation-aware reasoning—and the ability to recognize uncertainty, defer action, or escalate appropriately—is often preferable to aggressive but brittle automation.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Submit an Agent for Evaluation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;AssetOpsBench-Live is designed as an open, competition-ready benchmark, and we welcome submissions of agent implementations from the community. Agents are evaluated in a controlled, privacy-preserving environment that reflects real industrial asset management constraints.&lt;/p&gt;
&lt;p&gt;To submit an agent, developers first validate their implementation locally using a provided simulated environment, which includes representative sensor data, work orders, alerts, and failure-mode catalogs. Agents are then containerized and submitted for remote execution on hidden evaluation scenarios.&lt;/p&gt;
&lt;p&gt;Submitted agents are evaluated across six qualitative dimensions—task completion, accuracy, result verification, action sequencing, clarity, and hallucination—using a consistent, reproducible evaluation protocol. Execution traces are not exposed; instead, participants receive aggregated scores and structured failure-mode feedback that highlights where and why an agent’s reasoning or coordination broke down.&lt;/p&gt;
&lt;p&gt;This feedback-driven evaluation loop enables iterative improvement: developers can diagnose failure patterns, refine agent design or workflow structure, and resubmit updated agents for further evaluation. Both planning-focused and execution-focused agents are supported, allowing researchers and practitioners to explore diverse agentic designs within the same benchmark framework.&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Experiment and Observations
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We performed a community evaluation where we tested two tracks: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Planning-oriented&lt;/strong&gt; multi-agent orchestration &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Execution-oriented&lt;/strong&gt; dynamic multi-agent workflow.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Across 225 users and 300+ agents and leading open source models, here are the observations:&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th align="left"&gt;Model Family&lt;/th&gt;
&lt;th align="center"&gt;Best Planning Score&lt;/th&gt;
&lt;th align="center"&gt;Best Execution Score&lt;/th&gt;
&lt;th align="left"&gt;Key Limitation&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td align="left"&gt;&lt;strong&gt;GPT-4.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;68.2&lt;/td&gt;
&lt;td align="center"&gt;72.4&lt;/td&gt;
&lt;td align="left"&gt;Hallucinated completion on complex workflows&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;strong&gt;Mistral-Large&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;64.7&lt;/td&gt;
&lt;td align="center"&gt;69.1&lt;/td&gt;
&lt;td align="left"&gt;Struggled with multi-hop tool sequences&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;strong&gt;LLaMA-4 Maverick&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;66.0&lt;/td&gt;
&lt;td align="center"&gt;70.8&lt;/td&gt;
&lt;td align="left"&gt;Missed clarifying questions (fixable)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;strong&gt;LLaMA-3-70B&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;52.3&lt;/td&gt;
&lt;td align="center"&gt;58.9&lt;/td&gt;
&lt;td align="left"&gt;Collapsed under multi-agent coordination&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; None of the models could pass our evaluation criteria benchmark and get 85 points, which is the threshold for deployment readiness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Distribution of Failures
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Across 881 agent execution traces, failure distribution was as follows: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ineffective Error Recovery:&lt;/strong&gt; 31.2%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overstated Completion:&lt;/strong&gt; 23.8%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formatting Issues:&lt;/strong&gt; 21.4%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unhandled Tool Errors:&lt;/strong&gt; 10.3%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ignored Feedback:&lt;/strong&gt; 8.0%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Other:&lt;/strong&gt; 5.3%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Beyond this, 185 traces had one new failure pattern and 164 had multiple novel failures.&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Key Error Findings
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;"Sounds Right, Is Wrong":&lt;/strong&gt; Agents claim to have completed tasks (23.8%) and output success even after unsuccessful failure recovery (31.2%). AssetOps benchmarking is important to uncover this so that operators do not act upon incorrect information.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tool Usage:&lt;/strong&gt; This is the biggest differentiator between high and low performing agents, with top agents having 94% tool accuracy compared to 61% of low performers. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-agent Multiplies Failures:&lt;/strong&gt; Task accuracy between single agent (68%) vs multi-agent (47%) shows the complexity multi-agent brings with context loss, asynchronous issues, and cascaded failures. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Domain Knowledge:&lt;/strong&gt; Agents with access to failure mode databases and maintenance manuals performed better. However, RAG knowledge wasn’t always used correctly, suggesting a need for structured reasoning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ambiguity:&lt;/strong&gt; Missing sensors, conflicting logs, and vague operator descriptions caused the success rate to drop 34%. Agents must have clarification strategies embedded.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Where to get started?
	&lt;/span&gt;
&lt;/h2&gt;

&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face</guid><pubDate>Wed, 21 Jan 2026 06:25:31 +0000</pubDate></item></channel></rss>