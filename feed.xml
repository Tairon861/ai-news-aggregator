<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 18 Feb 2026 07:04:27 +0000</lastBuildDate><item><title>Apple is reportedly cooking up a trio of AI wearables (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/17/apple-is-reportedly-cooking-up-a-trio-of-ai-wearables/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/tim-cook-speaking-GettyImages-2234517834.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Late last month, The Information reported that Apple was developing an AI wearable — an AirTag-sized pendant with cameras that could be pinned to a user’s shirt. Now, Bloomberg writes that the development of such a device — along with two other AI-powered items — is accelerating as Apple looks to stay competitive with other tech giants that are racing to release similar products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to the AI pin, Apple is also speeding up the development of its upcoming AI-powered smart glasses, which have been code-named N50, the report claims. Apple obviously has competition in this space, as other companies — including Meta (which is arguably the most successful player when it comes to smart glasses) and Snap (which plans to release its “Specs” later this year) — are working on similar products.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Apple’s new smart glasses, which will supposedly include a high-resolution camera, may see a public release sooner rather than later, with Bloomberg reporting that the company is “targeting the start of production as early as December, ahead of a public release in 2027.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Bloomberg reports that Apple is working on AirPods with new AI capabilities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;All of these items will be designed to connect to the iPhone and will include Siri, the company’s virtual assistant, as a critical component of the user experience, the outlet notes. The glasses are being described as “more upscale and feature-rich” than the AirPods and the AI pendant, however. TechCrunch reached out to Apple for more information.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/tim-cook-speaking-GettyImages-2234517834.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Late last month, The Information reported that Apple was developing an AI wearable — an AirTag-sized pendant with cameras that could be pinned to a user’s shirt. Now, Bloomberg writes that the development of such a device — along with two other AI-powered items — is accelerating as Apple looks to stay competitive with other tech giants that are racing to release similar products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to the AI pin, Apple is also speeding up the development of its upcoming AI-powered smart glasses, which have been code-named N50, the report claims. Apple obviously has competition in this space, as other companies — including Meta (which is arguably the most successful player when it comes to smart glasses) and Snap (which plans to release its “Specs” later this year) — are working on similar products.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Apple’s new smart glasses, which will supposedly include a high-resolution camera, may see a public release sooner rather than later, with Bloomberg reporting that the company is “targeting the start of production as early as December, ahead of a public release in 2027.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Bloomberg reports that Apple is working on AirPods with new AI capabilities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;All of these items will be designed to connect to the iPhone and will include Siri, the company’s virtual assistant, as a critical component of the user experience, the outlet notes. The glasses are being described as “more upscale and feature-rich” than the AirPods and the AI pendant, however. TechCrunch reached out to Apple for more information.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/17/apple-is-reportedly-cooking-up-a-trio-of-ai-wearables/</guid><pubDate>Tue, 17 Feb 2026 20:14:00 +0000</pubDate></item><item><title>Teaching AI to read a map (The latest research from Google)</title><link>https://research.google/blog/teaching-ai-to-read-a-map/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;The pipeline works in four automated and scalable stages, using AI models as both creators and critics to ensure quality and produce pixel-level annotations.&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Generating diverse maps&lt;/b&gt;&lt;/p&gt;&lt;p&gt;First, we use a large language model (LLM) to generate rich, descriptive prompts for different types of maps. The LLM generates everything from "a map of a zoo with interconnected habitats" to "a shopping mall with a central food court" or "a fantasy theme park with winding paths through different themed lands." These text prompts are then fed into a text-to-image model that renders them into complex map images.&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Identifying traversable paths with an AI "Mask Critic"&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Once we have a map image, we need to identify all the "walkable" areas. Our system does this by clustering the pixels by color to create candidate path masks — essentially, a black-and-white map of all the walkways.&lt;/p&gt;&lt;p&gt;But not every shaded region is a valid path. So, we employ another MLLM as a "Mask Critic” used to examine each candidate mask and judge whether it represents a realistic, connected network of paths by looking at both the map image and the mask candidate. If the MLLM identifies the mask candidate as containing mostly valid traversable regions (e.g., paved sidewalks, marked crosswalks, pedestrian-only paths), then it labels the candidate as high quality. Then only these high-quality masks are passed to the next stage.&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. Building a navigable graph&lt;/b&gt;&lt;/p&gt;&lt;p&gt;With a clean mask of all traversable areas, we convert that 2D image into a more structured graph format. Think of this as creating a digital version of a road network, where intersections are nodes and the roads between them are edges. This "pixel-graph" captures the connectivity of the map, making it easy to calculate routes computationally.&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. Generating perfect paths with an AI "Path Critic"&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Finally, we sample thousands of random start and end points on the graph for each map. We use a classic Dijkstra's algorithm to find the absolute shortest path between these points. Then, we use another MLLM as a "Path Critic" to perform a final quality check. This critic looks at the final generated path overlaid on the map image and gives it a thumbs-up or thumbs-down, ensuring the route is logical, stays within the lines, and looks like a path a human would take.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;The pipeline works in four automated and scalable stages, using AI models as both creators and critics to ensure quality and produce pixel-level annotations.&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Generating diverse maps&lt;/b&gt;&lt;/p&gt;&lt;p&gt;First, we use a large language model (LLM) to generate rich, descriptive prompts for different types of maps. The LLM generates everything from "a map of a zoo with interconnected habitats" to "a shopping mall with a central food court" or "a fantasy theme park with winding paths through different themed lands." These text prompts are then fed into a text-to-image model that renders them into complex map images.&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Identifying traversable paths with an AI "Mask Critic"&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Once we have a map image, we need to identify all the "walkable" areas. Our system does this by clustering the pixels by color to create candidate path masks — essentially, a black-and-white map of all the walkways.&lt;/p&gt;&lt;p&gt;But not every shaded region is a valid path. So, we employ another MLLM as a "Mask Critic” used to examine each candidate mask and judge whether it represents a realistic, connected network of paths by looking at both the map image and the mask candidate. If the MLLM identifies the mask candidate as containing mostly valid traversable regions (e.g., paved sidewalks, marked crosswalks, pedestrian-only paths), then it labels the candidate as high quality. Then only these high-quality masks are passed to the next stage.&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. Building a navigable graph&lt;/b&gt;&lt;/p&gt;&lt;p&gt;With a clean mask of all traversable areas, we convert that 2D image into a more structured graph format. Think of this as creating a digital version of a road network, where intersections are nodes and the roads between them are edges. This "pixel-graph" captures the connectivity of the map, making it easy to calculate routes computationally.&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. Generating perfect paths with an AI "Path Critic"&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Finally, we sample thousands of random start and end points on the graph for each map. We use a classic Dijkstra's algorithm to find the absolute shortest path between these points. Then, we use another MLLM as a "Path Critic" to perform a final quality check. This critic looks at the final generated path overlaid on the map image and gives it a thumbs-up or thumbs-down, ensuring the route is logical, stays within the lines, and looks like a path a human would take.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/teaching-ai-to-read-a-map/</guid><pubDate>Tue, 17 Feb 2026 21:37:00 +0000</pubDate></item><item><title>NVIDIA Nemotron 2 Nano 9B Japanese: 日本のソブリンAIを支える最先端小規模言語モデル (Hugging Face - Blog)</title><link>https://huggingface.co/blog/nvidia/nemotron-nano-9b-v2-japanese-ja</link><description>&lt;!-- HTML_TAG_START --&gt;
NVIDIA Nemotronは、オープンモデルだけでなく、データセット、ライブラリ、レシピ、クックブックを提供し、開発者がモデルをカスタマイズし、多様なユースケースや言語に適応できるようにすることでソブリンAIを推進しています。
&lt;p&gt;本日、NVIDIAは、Nejumi Leaderboard 4のパラメータ数10B以下において、最先端の性能（SOTA）を達成した NVIDIA Nemotron-Nano-9B-v2-Japaneseを公開しました。&lt;/p&gt;
&lt;p&gt;本モデルは、高度な日本語理解と強力なエージェント機能を、導入しやすい軽量なサイズで実現しており、日本のエンタープライズAI開発における重要なマイルストーンとなります。この成果は、実績あるNemotron-Nano-9B-v2のアーキテクチャと、Nemotron-Personas-Japanによって実現された高品質な日本語合成データ生成（SDG）という、2つの重要な基盤の上に築かれています。&lt;/p&gt;
&lt;p&gt;既に公開済みのNemotron 2 Nanoモデルを日本語向けにカスタマイズすることで、多様なユースケースや言語に対応したカスタム最先端モデルの開発・公開をコミュニティに促すことを目指しています。Nemotronチームは、このカスタマイズから得た知見を今後のNemotronリリースに反映し、日本語における推論能力の強化を図っています。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;日本のエンタープライズにおけるSLM（小規模言語モデル）の重要性&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;日本のエンタープライズAIにおける重要なギャップ:&lt;/strong&gt; 現在の日本のエンタープライズAI環境には、「高度な日本語能力」と「エージェンティックAIとしてのタスク遂行能力」を兼ね備えたSLMがほとんど存在しないという課題があります。これにより、特に以下の点において導入の障壁が生じています。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;オンプレミスでのデプロイ要件:&lt;/strong&gt; 機密データを扱う企業では、プライベートネットワーク内でのモデル運用が不可欠です。10B（100億）パラメータ未満のモデルであれば、実用レベルの性能を維持しつつ、インフラ面の導入ハードルを大幅に下げることができます。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;カスタマイズの効率化:&lt;/strong&gt; 実証済みのエージェント能力を持つ強力な日本語ベースモデルから開始することで、ファインチューニングのサイクルを短縮できます。基礎能力の構築ではなく、特定のドメインへの適応に計算リソースを集中させることが可能になります。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;エージェント開発の加速:&lt;/strong&gt; 本モデルのアーキテクチャと性能により、大規模モデルのようなオーバーヘッドなしに、マルチエージェントシステムや複雑なワークフローの迅速なプロトタイピングが可能になります。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;実績ある基盤の活用&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Nemotron 2 Nano: 卓越したアーキテクチャ
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Nemotron-Nano-9B-v2-Japanese は、英語ベンチマークにおいてサイズ対性能比で卓越した結果を示した NVIDIA Nemotron-Nano-9B-v2 をベースに構築されています。この効率的なアーキテクチャを基盤としてさらなるカスタマイズを実施し、日本語能力を強化しました。本アーキテクチャには以下の特長があります。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高度な推論能力を実現と最適化されたパラメータ効率  &lt;/li&gt;
&lt;li&gt;多言語適応のための強固な基盤  &lt;/li&gt;
&lt;li&gt;実証済みのエージェントタスク遂行能力&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;この検証済みのアーキテクチャを日本語に適応させることで、ベースモデルの強みを維持しつつ、優れた日本語能力を実現しています。&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Nemotron-Personas-Japan: 高品質な合成データ生成のシードセット
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;本モデルのデータ戦略は、オープンソース（CC BY 4.0）データセットである「Nemotron-Personas-Japan」を、合成データ生成（SDG）の高品質なシードとして活用することに焦点を当てています。このデータセットは、日本の実世界における人口統計、地理的分布、性格特性の分布に基づき合成生成されたペルソナで構成され、人口の多様性と豊かさを捉えています。こうした文化的に正確なペルソナを基盤として、高度に多様性があり、拡張性・堅牢性に優れたトレーニングパイプラインを構築しました。シードデータの豊富なペルソナ群により、多様なシナリオやニュアンスにわたる合成データセットを効率的に拡張できました。この手法により、拡張データは元のペルソナの厳密な文化的整合性を維持しつつ、最先端トレーニングに必要な規模を達成しています。&lt;/p&gt;
&lt;p&gt;特にNemotron-Nano-9B-v2-Japaneseでは、これらのペルソナをツール呼び出しシナリオにおけるトレーニングデータの生成基盤として活用しました。これにより、モデルが獲得する能力が単なるツール呼び出し機能にとどまらず、文化的に適切な日本語の対話と現実世界のユースケースに根差したものであることが保証されます。&lt;/p&gt;
&lt;p&gt;Nemotron-Personas collectionには、米国、インド、シンガポール、ブラジルのデータセットも含まれており、同じ手法を地域を超えて再現することが可能となっています。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;トレーニングパイプライン&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Nemotron-Nano-9B-v2-Japaneseは、継続事前学習、合成データ生成、事後学習に至るプロセスを日本語オープンソースコーパスとNVIDIAのNemotronスタックを組み合わせて構築されました。&lt;/p&gt;
&lt;p&gt;&lt;img alt="training_diagram" src="https://cdn-uploads.huggingface.co/production/uploads/5fc181c4ea82dd667bb0ffae/uxrGpZ29BTHqQeD0_WQ5I.png" /&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		継続事前学習
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Japanese OSS Corpus: Wikipedia, fineweb-2 Japanese, aozorabunko, sip3-ja-general-web-corpus  &lt;/li&gt;
&lt;li&gt;Nemotron-CC-v2.1  &lt;/li&gt;
&lt;li&gt;Nemotron-Pretraining-Specialized-v1&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		SFT
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Nemotron-Personas-JapanをシードセットとしたTool Callingデータセット  &lt;/li&gt;
&lt;li&gt;Nemotron-Post-Training-v3&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Nemotron-Nano-9B-v2-Japaneseに使用したソフトウェア
	&lt;/span&gt;
&lt;/h3&gt;

&lt;p&gt;モデルの日本語能力を最大化するため、継続事前学習を実施しました。ここでは日本を代表するオープンソースLLMコミュニティである LLM-jp の資産を最大限に活用しています。同時にNemotron Pre-training Datasetsを活用し、モデルのエージェント機能を維持しました。&lt;/p&gt;
&lt;p&gt;SFTに使用したNemotron-Personas-JapanをシードとしたTool Callingデータセットは非常に強力でした。性能向上はツール呼び出しに留まらず、日本語知識、QA、指示追従など多岐に渡りました。さらに、このシードセットが600万のペルソナに基づいて構築されているため、SDGを効果的にスケールさせることができました。これにより、重複を最小限に抑えながら、現実世界の多様なシナリオを網羅することに成功しました。Nemotron-Personasコレクションは対象国を拡大しており、日本だけでなく他地域の開発者も同様のアプローチをとることができます。&lt;/p&gt;
&lt;p&gt;モデルのトレーニングは、Nemotron Nano 2で確立されたトレーニングレシピを継承しています。これにより、トレーニングの不安定性を招くことなくスループットを向上させることができました。&lt;/p&gt;
&lt;p&gt;このアプローチによって、ロバストなツール呼び出し機能とリーズニング能力を維持しながら強力な日本語言語モデルとしての性能を実現しています。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;ベンチマークパフォーマンス&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img alt="leaderboard" src="https://cdn-uploads.huggingface.co/production/uploads/5fc181c4ea82dd667bb0ffae/5nuxnXClbAR3GI76KiG51.png" /&gt;&lt;/p&gt;
&lt;p&gt;Nemotron-Nano-9B-v2-Japanese は、日本で最も包括的なLLM評価プラットフォームである「Nejumi Leaderboard 4」において、10B未満のモデルカテゴリで1位を獲得しました。Nejumi Leaderboard は、以下の領域にわたる約40のベンチマークを通じてモデルを多角的に評価しています。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基礎的な言語能力: 日本語の理解と生成  &lt;/li&gt;
&lt;li&gt;エージェント能力: コード生成、数学的推論、ツール利用など  &lt;/li&gt;
&lt;li&gt;アライメント: 指示追従能力、バイアス、毒性、真実性、堅牢性など&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;これらの多次元的な評価により、Nejumi Leaderboard は、日本の環境においてカスタマイズや実運用のためのベースモデルを選定する開発者にとって、信頼できるリファレンスとなっています。&lt;/p&gt;
&lt;p&gt;&lt;img alt="benchmark_summary" src="https://cdn-uploads.huggingface.co/production/uploads/5fc181c4ea82dd667bb0ffae/MfwBo6rVX4MrmsI_8kQpa.png" /&gt;&lt;/p&gt;
&lt;p&gt;ベンチマークの結果は、Nemotron-Nano-9B-v2-Japanese がベースモデルである Nemotron-Nano-9B-v2 に強力な日本語能力を統合できたことを確認できます。これらの改善は、日本語の知識や質問応答能力にとどまらず、ツール呼び出し、コーディング、アライメントなど幅広いタスクに及びます。特筆すべきは、同等サイズの Qwen3-8B を上回り、優れたサイズ対性能比を実現している点です。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;技術的優位性&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img alt="throughput" src="https://cdn-uploads.huggingface.co/production/uploads/5fc181c4ea82dd667bb0ffae/Jozble7RW6oSDRU9ietBv.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;推論の効率性: Nemotron 2 Nano（Transformer-Mamba）のアーキテクチャを継承することで、エッジGPUにデプロイ可能でありながら、オープンソースの代替モデルと比較して最大6倍のスループット向上を実現します。上の図は、Nemotron 2 Nanoの論文で測定された結果を示しています。  &lt;/li&gt;
&lt;li&gt;コンテキスト処理: 複数回（マルチターン）の会話やツール操作に最適化されています。  &lt;/li&gt;
&lt;li&gt;ツール呼び出しの信頼性: API呼び出しや関数実行のために、強力な構造化データ生成能力を備えています。  &lt;/li&gt;
&lt;li&gt;ファインチューニングの効率性: 手頃な計算インフラでもフルファインチューニングが可能なパラメータ数です。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;デプロイのオプション&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		直接デプロイ
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;高い日本語理解とエージェンティックスキルを必要とするアプリケーションではモデルをそのままデプロイして活用できます。すでに学習済みの能力により、エージェントワークフローへの即時統合をサポートします。Nemotron 2 Nanoでサポートされている推論エンジンはシームレスに移行できます。&lt;/p&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		独自ドメインへのカスタマイズ
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;特定のドメインに特化したファインチューニングのベースとして、Nemotron-Nano-9B-v2-Japaneseを利用できます。ベンチマークで実証された日本語およびエージェンティックタスクでの良い性能は、専門的なアプリケーション開発のための強固な開始点となります。カスタマイズにはNeMo Framework（NeMo Megatron-Bridge, NeMo AutoModel, and NeMo-RL）をご利用いただけます。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;今すぐ使ってください&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;日本のAIアプリケーション開発者の皆様は、今すぐ Nemotron-Nano-9B-v2-Japanese をご利用いただけます。顧客対応エージェント、社内自動化ツール、あるいはドメイン特化型アシスタントなど、どのような用途であっても、本モデルは実運用へのデプロイに求められる優れたサイズ対性能比を提供します。&lt;/p&gt;
&lt;p&gt;Nemotron 2 Nanoの実績あるアーキテクチャと、高品質なデータセットのシードとなる Nemotron-Personas-Japan の組み合わせは、日本のソブリンAI開発における効率的な出発点となるでしょう。&lt;/p&gt;
&lt;p&gt;コミュニティの皆様に、Nemotronモデル、データセット、レシピ、ライブラリをぜひご活用いただき、さらに多くの言語やユースケース向けにNemotronモデルをカスタマイズしていただくことを歓迎します。皆様がどのようなものを構築されるか、楽しみにしています！&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Stay up to date on NVIDIA Nemotron by subscribing to NVIDIA news and following NVIDIA AI on LinkedIn, X, YouTube&lt;/em&gt;, &lt;em&gt;and the Nemotron channel on Discord.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Access open Nemotron Models on Hugging Face and a collection of NIM microservices and Developer Examples on build.nvidia.com.&lt;/em&gt; &lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;!-- HTML_TAG_START --&gt;
NVIDIA Nemotronは、オープンモデルだけでなく、データセット、ライブラリ、レシピ、クックブックを提供し、開発者がモデルをカスタマイズし、多様なユースケースや言語に適応できるようにすることでソブリンAIを推進しています。
&lt;p&gt;本日、NVIDIAは、Nejumi Leaderboard 4のパラメータ数10B以下において、最先端の性能（SOTA）を達成した NVIDIA Nemotron-Nano-9B-v2-Japaneseを公開しました。&lt;/p&gt;
&lt;p&gt;本モデルは、高度な日本語理解と強力なエージェント機能を、導入しやすい軽量なサイズで実現しており、日本のエンタープライズAI開発における重要なマイルストーンとなります。この成果は、実績あるNemotron-Nano-9B-v2のアーキテクチャと、Nemotron-Personas-Japanによって実現された高品質な日本語合成データ生成（SDG）という、2つの重要な基盤の上に築かれています。&lt;/p&gt;
&lt;p&gt;既に公開済みのNemotron 2 Nanoモデルを日本語向けにカスタマイズすることで、多様なユースケースや言語に対応したカスタム最先端モデルの開発・公開をコミュニティに促すことを目指しています。Nemotronチームは、このカスタマイズから得た知見を今後のNemotronリリースに反映し、日本語における推論能力の強化を図っています。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;日本のエンタープライズにおけるSLM（小規模言語モデル）の重要性&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;日本のエンタープライズAIにおける重要なギャップ:&lt;/strong&gt; 現在の日本のエンタープライズAI環境には、「高度な日本語能力」と「エージェンティックAIとしてのタスク遂行能力」を兼ね備えたSLMがほとんど存在しないという課題があります。これにより、特に以下の点において導入の障壁が生じています。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;オンプレミスでのデプロイ要件:&lt;/strong&gt; 機密データを扱う企業では、プライベートネットワーク内でのモデル運用が不可欠です。10B（100億）パラメータ未満のモデルであれば、実用レベルの性能を維持しつつ、インフラ面の導入ハードルを大幅に下げることができます。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;カスタマイズの効率化:&lt;/strong&gt; 実証済みのエージェント能力を持つ強力な日本語ベースモデルから開始することで、ファインチューニングのサイクルを短縮できます。基礎能力の構築ではなく、特定のドメインへの適応に計算リソースを集中させることが可能になります。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;エージェント開発の加速:&lt;/strong&gt; 本モデルのアーキテクチャと性能により、大規模モデルのようなオーバーヘッドなしに、マルチエージェントシステムや複雑なワークフローの迅速なプロトタイピングが可能になります。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;実績ある基盤の活用&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Nemotron 2 Nano: 卓越したアーキテクチャ
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Nemotron-Nano-9B-v2-Japanese は、英語ベンチマークにおいてサイズ対性能比で卓越した結果を示した NVIDIA Nemotron-Nano-9B-v2 をベースに構築されています。この効率的なアーキテクチャを基盤としてさらなるカスタマイズを実施し、日本語能力を強化しました。本アーキテクチャには以下の特長があります。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高度な推論能力を実現と最適化されたパラメータ効率  &lt;/li&gt;
&lt;li&gt;多言語適応のための強固な基盤  &lt;/li&gt;
&lt;li&gt;実証済みのエージェントタスク遂行能力&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;この検証済みのアーキテクチャを日本語に適応させることで、ベースモデルの強みを維持しつつ、優れた日本語能力を実現しています。&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Nemotron-Personas-Japan: 高品質な合成データ生成のシードセット
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;本モデルのデータ戦略は、オープンソース（CC BY 4.0）データセットである「Nemotron-Personas-Japan」を、合成データ生成（SDG）の高品質なシードとして活用することに焦点を当てています。このデータセットは、日本の実世界における人口統計、地理的分布、性格特性の分布に基づき合成生成されたペルソナで構成され、人口の多様性と豊かさを捉えています。こうした文化的に正確なペルソナを基盤として、高度に多様性があり、拡張性・堅牢性に優れたトレーニングパイプラインを構築しました。シードデータの豊富なペルソナ群により、多様なシナリオやニュアンスにわたる合成データセットを効率的に拡張できました。この手法により、拡張データは元のペルソナの厳密な文化的整合性を維持しつつ、最先端トレーニングに必要な規模を達成しています。&lt;/p&gt;
&lt;p&gt;特にNemotron-Nano-9B-v2-Japaneseでは、これらのペルソナをツール呼び出しシナリオにおけるトレーニングデータの生成基盤として活用しました。これにより、モデルが獲得する能力が単なるツール呼び出し機能にとどまらず、文化的に適切な日本語の対話と現実世界のユースケースに根差したものであることが保証されます。&lt;/p&gt;
&lt;p&gt;Nemotron-Personas collectionには、米国、インド、シンガポール、ブラジルのデータセットも含まれており、同じ手法を地域を超えて再現することが可能となっています。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;トレーニングパイプライン&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Nemotron-Nano-9B-v2-Japaneseは、継続事前学習、合成データ生成、事後学習に至るプロセスを日本語オープンソースコーパスとNVIDIAのNemotronスタックを組み合わせて構築されました。&lt;/p&gt;
&lt;p&gt;&lt;img alt="training_diagram" src="https://cdn-uploads.huggingface.co/production/uploads/5fc181c4ea82dd667bb0ffae/uxrGpZ29BTHqQeD0_WQ5I.png" /&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		継続事前学習
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Japanese OSS Corpus: Wikipedia, fineweb-2 Japanese, aozorabunko, sip3-ja-general-web-corpus  &lt;/li&gt;
&lt;li&gt;Nemotron-CC-v2.1  &lt;/li&gt;
&lt;li&gt;Nemotron-Pretraining-Specialized-v1&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		SFT
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Nemotron-Personas-JapanをシードセットとしたTool Callingデータセット  &lt;/li&gt;
&lt;li&gt;Nemotron-Post-Training-v3&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Nemotron-Nano-9B-v2-Japaneseに使用したソフトウェア
	&lt;/span&gt;
&lt;/h3&gt;

&lt;p&gt;モデルの日本語能力を最大化するため、継続事前学習を実施しました。ここでは日本を代表するオープンソースLLMコミュニティである LLM-jp の資産を最大限に活用しています。同時にNemotron Pre-training Datasetsを活用し、モデルのエージェント機能を維持しました。&lt;/p&gt;
&lt;p&gt;SFTに使用したNemotron-Personas-JapanをシードとしたTool Callingデータセットは非常に強力でした。性能向上はツール呼び出しに留まらず、日本語知識、QA、指示追従など多岐に渡りました。さらに、このシードセットが600万のペルソナに基づいて構築されているため、SDGを効果的にスケールさせることができました。これにより、重複を最小限に抑えながら、現実世界の多様なシナリオを網羅することに成功しました。Nemotron-Personasコレクションは対象国を拡大しており、日本だけでなく他地域の開発者も同様のアプローチをとることができます。&lt;/p&gt;
&lt;p&gt;モデルのトレーニングは、Nemotron Nano 2で確立されたトレーニングレシピを継承しています。これにより、トレーニングの不安定性を招くことなくスループットを向上させることができました。&lt;/p&gt;
&lt;p&gt;このアプローチによって、ロバストなツール呼び出し機能とリーズニング能力を維持しながら強力な日本語言語モデルとしての性能を実現しています。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;ベンチマークパフォーマンス&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img alt="leaderboard" src="https://cdn-uploads.huggingface.co/production/uploads/5fc181c4ea82dd667bb0ffae/5nuxnXClbAR3GI76KiG51.png" /&gt;&lt;/p&gt;
&lt;p&gt;Nemotron-Nano-9B-v2-Japanese は、日本で最も包括的なLLM評価プラットフォームである「Nejumi Leaderboard 4」において、10B未満のモデルカテゴリで1位を獲得しました。Nejumi Leaderboard は、以下の領域にわたる約40のベンチマークを通じてモデルを多角的に評価しています。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基礎的な言語能力: 日本語の理解と生成  &lt;/li&gt;
&lt;li&gt;エージェント能力: コード生成、数学的推論、ツール利用など  &lt;/li&gt;
&lt;li&gt;アライメント: 指示追従能力、バイアス、毒性、真実性、堅牢性など&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;これらの多次元的な評価により、Nejumi Leaderboard は、日本の環境においてカスタマイズや実運用のためのベースモデルを選定する開発者にとって、信頼できるリファレンスとなっています。&lt;/p&gt;
&lt;p&gt;&lt;img alt="benchmark_summary" src="https://cdn-uploads.huggingface.co/production/uploads/5fc181c4ea82dd667bb0ffae/MfwBo6rVX4MrmsI_8kQpa.png" /&gt;&lt;/p&gt;
&lt;p&gt;ベンチマークの結果は、Nemotron-Nano-9B-v2-Japanese がベースモデルである Nemotron-Nano-9B-v2 に強力な日本語能力を統合できたことを確認できます。これらの改善は、日本語の知識や質問応答能力にとどまらず、ツール呼び出し、コーディング、アライメントなど幅広いタスクに及びます。特筆すべきは、同等サイズの Qwen3-8B を上回り、優れたサイズ対性能比を実現している点です。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;技術的優位性&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img alt="throughput" src="https://cdn-uploads.huggingface.co/production/uploads/5fc181c4ea82dd667bb0ffae/Jozble7RW6oSDRU9ietBv.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;推論の効率性: Nemotron 2 Nano（Transformer-Mamba）のアーキテクチャを継承することで、エッジGPUにデプロイ可能でありながら、オープンソースの代替モデルと比較して最大6倍のスループット向上を実現します。上の図は、Nemotron 2 Nanoの論文で測定された結果を示しています。  &lt;/li&gt;
&lt;li&gt;コンテキスト処理: 複数回（マルチターン）の会話やツール操作に最適化されています。  &lt;/li&gt;
&lt;li&gt;ツール呼び出しの信頼性: API呼び出しや関数実行のために、強力な構造化データ生成能力を備えています。  &lt;/li&gt;
&lt;li&gt;ファインチューニングの効率性: 手頃な計算インフラでもフルファインチューニングが可能なパラメータ数です。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;デプロイのオプション&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		直接デプロイ
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;高い日本語理解とエージェンティックスキルを必要とするアプリケーションではモデルをそのままデプロイして活用できます。すでに学習済みの能力により、エージェントワークフローへの即時統合をサポートします。Nemotron 2 Nanoでサポートされている推論エンジンはシームレスに移行できます。&lt;/p&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		独自ドメインへのカスタマイズ
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;特定のドメインに特化したファインチューニングのベースとして、Nemotron-Nano-9B-v2-Japaneseを利用できます。ベンチマークで実証された日本語およびエージェンティックタスクでの良い性能は、専門的なアプリケーション開発のための強固な開始点となります。カスタマイズにはNeMo Framework（NeMo Megatron-Bridge, NeMo AutoModel, and NeMo-RL）をご利用いただけます。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;今すぐ使ってください&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;日本のAIアプリケーション開発者の皆様は、今すぐ Nemotron-Nano-9B-v2-Japanese をご利用いただけます。顧客対応エージェント、社内自動化ツール、あるいはドメイン特化型アシスタントなど、どのような用途であっても、本モデルは実運用へのデプロイに求められる優れたサイズ対性能比を提供します。&lt;/p&gt;
&lt;p&gt;Nemotron 2 Nanoの実績あるアーキテクチャと、高品質なデータセットのシードとなる Nemotron-Personas-Japan の組み合わせは、日本のソブリンAI開発における効率的な出発点となるでしょう。&lt;/p&gt;
&lt;p&gt;コミュニティの皆様に、Nemotronモデル、データセット、レシピ、ライブラリをぜひご活用いただき、さらに多くの言語やユースケース向けにNemotronモデルをカスタマイズしていただくことを歓迎します。皆様がどのようなものを構築されるか、楽しみにしています！&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Stay up to date on NVIDIA Nemotron by subscribing to NVIDIA news and following NVIDIA AI on LinkedIn, X, YouTube&lt;/em&gt;, &lt;em&gt;and the Nemotron channel on Discord.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Access open Nemotron Models on Hugging Face and a collection of NIM microservices and Developer Examples on build.nvidia.com.&lt;/em&gt; &lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/nvidia/nemotron-nano-9b-v2-japanese-ja</guid><pubDate>Tue, 17 Feb 2026 23:28:52 +0000</pubDate></item><item><title>NVIDIA and Global Industrial Software Leaders Partner With India’s Largest Manufacturers to Drive AI Boom (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/india-global-industrial-software-leaders-manufacturers-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/NVIDIA-India-AI-Impact-Summit-Indias-Manufacturers-Drive-AI-Boom-scaled.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;India is entering a new age of industrialization, as AI transforms how the world designs, builds and runs physical products and systems. The country is investing $134 billion dollars in new manufacturing capacity across construction, automotive, renewable energy and robotics, creating both a massive challenge and opportunity to build software-defined factories from day one.&lt;/p&gt;
&lt;p&gt;At the center of this transformation are applications accelerated by NVIDIA CUDA-X and NVIDIA Omniverse libraries, which connect data from design to operations and bring physical AI into factories, warehouses and infrastructure.&lt;/p&gt;
&lt;p&gt;India’s largest manufacturers are teaming with global industrial software leaders Cadence, Siemens and Synopsys to advance the nation’s AI boom using applications accelerated by CUDA-X and Omniverse libraries.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;India’s Manufacturing Leaders Modernize Factories With &lt;/b&gt;&lt;b&gt;Siemens&lt;/b&gt;&lt;b&gt; and NVIDIA&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To scale India’s growth, manufacturers are using Siemens industrial software integrated with NVIDIA CUDA-X and Omniverse libraries to design, build and operate next-generation, software-defined factories.&lt;/p&gt;
&lt;p&gt;Reliance New Energy, the clean energy arm of Reliance industries, is expanding its collaboration with NVIDIA and Siemens by combining Siemens’ digital twin technology with NVIDIA Omniverse libraries for faster, more precise simulation and plant design for its next-generation gigafactories.&lt;/p&gt;
&lt;p&gt;Addverb Technologies, a leading Indian company providing robots and innovative warehouse automation solutions, is using Siemens’ Technomatix portfolio, NVIDIA Omniverse libraries and NVIDIA Cosmos world foundation models to create digital twins of its factories and train its quadruped and wheeled humanoid robots in simulation.&lt;/p&gt;
&lt;p&gt;Hero MotoCorp is utilizing Siemens Xcelerator and NVIDIA infrastructure to accelerate the product development lifecycle by enhancing its capabilities in computer-aided engineering, numerical virtual verification and validation.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Partners Advance Design and Engineering With NVIDIA-Accelerated Software From &lt;/b&gt;&lt;b&gt;Synopsys&lt;/b&gt;&lt;b&gt; and &lt;/b&gt;&lt;b&gt;Cadence&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Leading enterprises are integrating Synopsys and Cadence’s electronic design automation tools, powered by NVIDIA AI infrastructure and libraries, to enable rapid design iteration and operational intelligence across the energy, automotive and electronics sectors.&lt;/p&gt;
&lt;p&gt;Electrical equipment and home appliances leader Havells India Limited is using Synopsys’ Ansys Fluent to accelerate simulation powered by NVIDIA CUDA-X. Havells has obtained 6x faster fluid dynamic simulations, enabling exploration of more design options to optimize airflow and energy efficiencies, and achieve faster time to market.&lt;/p&gt;
&lt;p&gt;Larsen &amp;amp; Toubro Semiconductor’s application of Cadence Spectre X, accelerated by CUDA-X libraries, on NVIDIA GPUs shortens design iterations of next-generation AI chips.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;India’s Technology Leaders Advance Industrial Automation With Physical AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;India’s IT and business consulting sector has grown into a global powerhouse, projected to reach over $350 billion this year, serving as a primary engine for transforming the world’s largest industries.&lt;/p&gt;
&lt;p&gt;Tata Consultancy Services (TCS), a global leader in IT services, is investing in large-scale AI infrastructure to deliver enterprise solutions at scale. By harnessing the NVIDIA Metropolis platform, the NVIDIA Blueprint for video search and summarization and digital twins built on Omniverse libraries, TCS is setting safety and precision benchmarks at Tata Motors, converting standard camera feeds into intelligent sensors for automated quality checks and real-time safety compliance.&lt;/p&gt;
&lt;p&gt;TCS is also deploying physical AI applications, including autonomous safety and quality inspections via quadruped robots, to minimize risk across complex manufacturing environments.&lt;/p&gt;
&lt;p&gt;Wipro PARI, a leader in industrial automation, is integrating NVIDIA AI infrastructure,&amp;nbsp; Omniverse libraries and the NVIDIA Isaac robotics development platform to deliver solutions for its consumer and automotive customers. This includes real-time simulation and validation of robotic workflows, as well as virtual stress-testing of operations before physical deployment.&lt;/p&gt;
&lt;p&gt;Tata Consulting Engineers is launching its Cognitive Twin platform, built on NVIDIA Omniverse, to create real-time industrial simulations that link physical assets with digital intelligence across manufacturing, energy and infrastructure. The platform supports both capital project planning and operational optimization through early-stage simulation and AI-enabled decision-making. Pilot projects are underway with National High Speed Rail Corporation Limited, Torrent Power and Power Grid Corporation of India Limited.&lt;/p&gt;
&lt;p&gt;To see what’s next, explore industrial AI and manufacturing sessions at NVIDIA GTC.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Main image courtesy of Wipro PARI&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/NVIDIA-India-AI-Impact-Summit-Indias-Manufacturers-Drive-AI-Boom-scaled.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;India is entering a new age of industrialization, as AI transforms how the world designs, builds and runs physical products and systems. The country is investing $134 billion dollars in new manufacturing capacity across construction, automotive, renewable energy and robotics, creating both a massive challenge and opportunity to build software-defined factories from day one.&lt;/p&gt;
&lt;p&gt;At the center of this transformation are applications accelerated by NVIDIA CUDA-X and NVIDIA Omniverse libraries, which connect data from design to operations and bring physical AI into factories, warehouses and infrastructure.&lt;/p&gt;
&lt;p&gt;India’s largest manufacturers are teaming with global industrial software leaders Cadence, Siemens and Synopsys to advance the nation’s AI boom using applications accelerated by CUDA-X and Omniverse libraries.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;India’s Manufacturing Leaders Modernize Factories With &lt;/b&gt;&lt;b&gt;Siemens&lt;/b&gt;&lt;b&gt; and NVIDIA&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To scale India’s growth, manufacturers are using Siemens industrial software integrated with NVIDIA CUDA-X and Omniverse libraries to design, build and operate next-generation, software-defined factories.&lt;/p&gt;
&lt;p&gt;Reliance New Energy, the clean energy arm of Reliance industries, is expanding its collaboration with NVIDIA and Siemens by combining Siemens’ digital twin technology with NVIDIA Omniverse libraries for faster, more precise simulation and plant design for its next-generation gigafactories.&lt;/p&gt;
&lt;p&gt;Addverb Technologies, a leading Indian company providing robots and innovative warehouse automation solutions, is using Siemens’ Technomatix portfolio, NVIDIA Omniverse libraries and NVIDIA Cosmos world foundation models to create digital twins of its factories and train its quadruped and wheeled humanoid robots in simulation.&lt;/p&gt;
&lt;p&gt;Hero MotoCorp is utilizing Siemens Xcelerator and NVIDIA infrastructure to accelerate the product development lifecycle by enhancing its capabilities in computer-aided engineering, numerical virtual verification and validation.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Partners Advance Design and Engineering With NVIDIA-Accelerated Software From &lt;/b&gt;&lt;b&gt;Synopsys&lt;/b&gt;&lt;b&gt; and &lt;/b&gt;&lt;b&gt;Cadence&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Leading enterprises are integrating Synopsys and Cadence’s electronic design automation tools, powered by NVIDIA AI infrastructure and libraries, to enable rapid design iteration and operational intelligence across the energy, automotive and electronics sectors.&lt;/p&gt;
&lt;p&gt;Electrical equipment and home appliances leader Havells India Limited is using Synopsys’ Ansys Fluent to accelerate simulation powered by NVIDIA CUDA-X. Havells has obtained 6x faster fluid dynamic simulations, enabling exploration of more design options to optimize airflow and energy efficiencies, and achieve faster time to market.&lt;/p&gt;
&lt;p&gt;Larsen &amp;amp; Toubro Semiconductor’s application of Cadence Spectre X, accelerated by CUDA-X libraries, on NVIDIA GPUs shortens design iterations of next-generation AI chips.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;India’s Technology Leaders Advance Industrial Automation With Physical AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;India’s IT and business consulting sector has grown into a global powerhouse, projected to reach over $350 billion this year, serving as a primary engine for transforming the world’s largest industries.&lt;/p&gt;
&lt;p&gt;Tata Consultancy Services (TCS), a global leader in IT services, is investing in large-scale AI infrastructure to deliver enterprise solutions at scale. By harnessing the NVIDIA Metropolis platform, the NVIDIA Blueprint for video search and summarization and digital twins built on Omniverse libraries, TCS is setting safety and precision benchmarks at Tata Motors, converting standard camera feeds into intelligent sensors for automated quality checks and real-time safety compliance.&lt;/p&gt;
&lt;p&gt;TCS is also deploying physical AI applications, including autonomous safety and quality inspections via quadruped robots, to minimize risk across complex manufacturing environments.&lt;/p&gt;
&lt;p&gt;Wipro PARI, a leader in industrial automation, is integrating NVIDIA AI infrastructure,&amp;nbsp; Omniverse libraries and the NVIDIA Isaac robotics development platform to deliver solutions for its consumer and automotive customers. This includes real-time simulation and validation of robotic workflows, as well as virtual stress-testing of operations before physical deployment.&lt;/p&gt;
&lt;p&gt;Tata Consulting Engineers is launching its Cognitive Twin platform, built on NVIDIA Omniverse, to create real-time industrial simulations that link physical assets with digital intelligence across manufacturing, energy and infrastructure. The platform supports both capital project planning and operational optimization through early-stage simulation and AI-enabled decision-making. Pilot projects are underway with National High Speed Rail Corporation Limited, Torrent Power and Power Grid Corporation of India Limited.&lt;/p&gt;
&lt;p&gt;To see what’s next, explore industrial AI and manufacturing sessions at NVIDIA GTC.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Main image courtesy of Wipro PARI&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/india-global-industrial-software-leaders-manufacturers-ai/</guid><pubDate>Wed, 18 Feb 2026 00:30:32 +0000</pubDate></item><item><title>India’s Global Systems Integrators Build Next Wave of Enterprise Agents With NVIDIA AI, Transforming Back Office and Customer Support (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/india-enterprise-ai-agents/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/ai-impact-summit-in26-visual-gsi-4871450-concept1-r2-1680x945.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Agentic AI is reshaping India’s tech industry, delivering leaps in services worldwide.&lt;/p&gt;
&lt;p&gt;Tapping into NVIDIA AI Enterprise software and NVIDIA Nemotron models, India’s technology leaders are accelerating productivity and efficiency across industries — from call centers to telecommunications and healthcare.&lt;/p&gt;
&lt;p&gt;Infosys, Persistent, Tech Mahindra and Wipro are leading the way for business transformation, improving back-office productivity and customer services with integrated agentic AI platforms built with NVIDIA AI Enterprise.&lt;/p&gt;
&lt;p&gt;At this year’s India AI Impact Summit, the state of the art for next-generation business services driven by agentic and generative AI was on full display.&lt;/p&gt;
&lt;p&gt;India’s tech industry is on track to reach $500 billion in revenue by 2030, up from about $250 billion in 2023, according to IBEF, citing momentum in AI from 38,000 GPUs secured in September.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Wipro WEGA Platform Boosting Efficiency for Call Centers With NVIDIA AI&lt;/b&gt; &lt;b&gt;Enterprise&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;For health insurance plans in government‑regulated markets, customer experience is important — especially during peak enrollment cycles, when deadlines loom and subscribers need 24/7 support to assess options and optimize enrollment decisions for their families. Traditional contact center business models, built around seasonal hiring and lengthy training, simply can’t keep pace. What’s needed is a new operating model that improves customer experience while containing the growing cost of service.&lt;/p&gt;
&lt;p&gt;Wipro’s AI‑agent-assisted solution, powered by the WEGA platform and NVIDIA AI Enterprise software, offers a glimpse of that future. Deployed for a major U.S. healthcare insurance provider, the system is already reshaping member experiences by enabling service representatives to handle more complex requests, accelerate resolution times, deliver more personalized support and improve operational efficiencies.&lt;/p&gt;
&lt;p&gt;AI agents help meet the expectations customers bring to their health plans: immediate access to accurate information, conversational self‑service, frictionless enrollment and consistent guidance across channels. Behind the scenes, payers face rising call volumes, fragmented data and heavy administrative workloads. AI agents bridge that gap by scaling instantly, operating around the clock and supporting human representatives with real‑time intelligence.&lt;/p&gt;
&lt;p&gt;The results have been striking: 42% of inbound calls are now handled by AI agents and near‑instant responsiveness across 900 concurrent calls and 164 requests per second — all with sub‑200‑millisecond latency.&lt;/p&gt;
&lt;p&gt;Members benefit from natural, conversational self‑service. Human agents receive real‑time prompts and knowledge retrieval. A centralized data hub surfaces personalized insights, while automated digitization removes manual work from downstream processes.&lt;/p&gt;
&lt;p&gt;Using production grade, horizontally scalable NVIDIA NIM microservices and NVIDIA NeMo Guardrails, part of NVIDIA AI Enterprise, the solution includes the performance, governance and safety required in regulated healthcare environments.&lt;/p&gt;
&lt;p&gt;Its impact is already extending beyond healthcare, with similar deployments underway in financial services. Anywhere accuracy, compliance and scale matter, AI agents are becoming a transformative force.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Tech Mahindra Deploying Large Telco Model to Power Autonomous Network Operations Using NVIDIA NIM&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Tech Mahindra is accelerating the shift toward AI-assisted network operations with a new platform built in collaboration with NVIDIA. At the center is a large telco model (LTM) that generates prioritized, data‑driven recommendations to help field technicians rank each fix by its historical success rate across the network. The result is faster, more accurate resolutions — often in a single visit — and a clear path toward level‑4‑plus operational maturity.&lt;/p&gt;
&lt;p&gt;A large telecommunications services provider is adopting the same LTM foundation as part of its operations roadmap, targeting improvements in service‑layer issue resolution, customer experience and back‑office efficiency through higher‑quality tickets and fewer escalations.&lt;/p&gt;
&lt;p&gt;The platform uses NVIDIA Nemotron embedding models for semantic search across telemetry and a Nemotron reranking model to sharpen decision relevance. These models are deployed with NVIDIA NIM microservices for rapid, reliable accelerated AI inference. NVIDIA NeMo Agent Toolkit orchestrates agent workflows across network domains, enabling true agentic operations at scale.&lt;/p&gt;
&lt;p&gt;By embracing autonomous network operations, Tech Mahindra shows how AI can transform a global telecom industry generating more than $1.5 trillion in annual revenue — where even small gains in uptime and efficiency deliver outsized economic impact.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Infosys Builds an Enterprise-Grade Coding Small Language Model With NVIDIA AI Enterprise&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Infosys developed a new small language model for coding, built using the NVIDIA NeMo framework that’s part of NVIDIA AI Enterprise, and integrated within Infosys Topaz Fabric. The model accelerates software delivery with frontier‑grade performance while remaining lightweight, and it can be deployed across on-premises enterprise data centers, cloud environments and even standard desktops.&lt;/p&gt;
&lt;p&gt;The 2.5‑billion‑parameter model supports agent development, code generation, refactoring and end‑to‑end software‑engineering workflows. It’s trained on a curated blend of high‑quality code, synthetic data, mathematical reasoning and natural language inputs — an approach that enables it to match frontier‑model performance on benchmarks such as MBPP, MBPP+ and BFCL.&lt;/p&gt;
&lt;p&gt;Infosys also prioritized safety and trust. The model incorporates safety‑aligned training and responsible AI practices that reduce harmful outputs while preserving fluency. Its secure‑coding capabilities are validated through industry benchmarks including Stanford AIR‑Bench and Meta’s CyberSecEval, giving enterprises confidence to deploy it across code generation, debugging and multi‑agent development pipelines.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Persistent Accelerates AI‑Driven Molecular Discovery With NVIDIA BioNeMo and NeMo Agent Toolkit&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Persistent Systems is working with NVIDIA to push early‑stage drug discovery into a new era of speed and scientific fidelity. The collaboration brings together Persistent’s deep life sciences engineering expertise with NVIDIA’s full‑stack accelerated computing platform, giving researchers a powerful path from AI experimentation to production‑grade discovery workflows.&lt;/p&gt;
&lt;p&gt;At the center of the effort is Persistent’s new Generative Molecules and Virtual Screening (GenMoIVS) solution, built on the NVIDIA BioNeMo platform and the NeMo Agent Toolkit. GenMoIVS uses large, domain‑specific models to simulate molecular behavior with high accuracy, generating and evaluating candidate compounds before they ever reach a wet lab. These agentic workflows continuously reason across virtual screening, prioritization and experimental planning, helping teams de‑risk early discovery and shorten development cycles.&lt;/p&gt;
&lt;p&gt;The platform runs on NVIDIA’s accelerated computing platform, including NVIDIA AI Enterprise software and NIM microservices, enabling high‑throughput simulation and real‑time scientific decision-making in regulated environments. By combining scalable infrastructure with production‑ready agentic AI, Persistent is giving life sciences organizations a faster, more cost‑effective way to explore the compound space and improve downstream success rates.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/ai-impact-summit-in26-visual-gsi-4871450-concept1-r2-1680x945.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Agentic AI is reshaping India’s tech industry, delivering leaps in services worldwide.&lt;/p&gt;
&lt;p&gt;Tapping into NVIDIA AI Enterprise software and NVIDIA Nemotron models, India’s technology leaders are accelerating productivity and efficiency across industries — from call centers to telecommunications and healthcare.&lt;/p&gt;
&lt;p&gt;Infosys, Persistent, Tech Mahindra and Wipro are leading the way for business transformation, improving back-office productivity and customer services with integrated agentic AI platforms built with NVIDIA AI Enterprise.&lt;/p&gt;
&lt;p&gt;At this year’s India AI Impact Summit, the state of the art for next-generation business services driven by agentic and generative AI was on full display.&lt;/p&gt;
&lt;p&gt;India’s tech industry is on track to reach $500 billion in revenue by 2030, up from about $250 billion in 2023, according to IBEF, citing momentum in AI from 38,000 GPUs secured in September.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Wipro WEGA Platform Boosting Efficiency for Call Centers With NVIDIA AI&lt;/b&gt; &lt;b&gt;Enterprise&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;For health insurance plans in government‑regulated markets, customer experience is important — especially during peak enrollment cycles, when deadlines loom and subscribers need 24/7 support to assess options and optimize enrollment decisions for their families. Traditional contact center business models, built around seasonal hiring and lengthy training, simply can’t keep pace. What’s needed is a new operating model that improves customer experience while containing the growing cost of service.&lt;/p&gt;
&lt;p&gt;Wipro’s AI‑agent-assisted solution, powered by the WEGA platform and NVIDIA AI Enterprise software, offers a glimpse of that future. Deployed for a major U.S. healthcare insurance provider, the system is already reshaping member experiences by enabling service representatives to handle more complex requests, accelerate resolution times, deliver more personalized support and improve operational efficiencies.&lt;/p&gt;
&lt;p&gt;AI agents help meet the expectations customers bring to their health plans: immediate access to accurate information, conversational self‑service, frictionless enrollment and consistent guidance across channels. Behind the scenes, payers face rising call volumes, fragmented data and heavy administrative workloads. AI agents bridge that gap by scaling instantly, operating around the clock and supporting human representatives with real‑time intelligence.&lt;/p&gt;
&lt;p&gt;The results have been striking: 42% of inbound calls are now handled by AI agents and near‑instant responsiveness across 900 concurrent calls and 164 requests per second — all with sub‑200‑millisecond latency.&lt;/p&gt;
&lt;p&gt;Members benefit from natural, conversational self‑service. Human agents receive real‑time prompts and knowledge retrieval. A centralized data hub surfaces personalized insights, while automated digitization removes manual work from downstream processes.&lt;/p&gt;
&lt;p&gt;Using production grade, horizontally scalable NVIDIA NIM microservices and NVIDIA NeMo Guardrails, part of NVIDIA AI Enterprise, the solution includes the performance, governance and safety required in regulated healthcare environments.&lt;/p&gt;
&lt;p&gt;Its impact is already extending beyond healthcare, with similar deployments underway in financial services. Anywhere accuracy, compliance and scale matter, AI agents are becoming a transformative force.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Tech Mahindra Deploying Large Telco Model to Power Autonomous Network Operations Using NVIDIA NIM&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Tech Mahindra is accelerating the shift toward AI-assisted network operations with a new platform built in collaboration with NVIDIA. At the center is a large telco model (LTM) that generates prioritized, data‑driven recommendations to help field technicians rank each fix by its historical success rate across the network. The result is faster, more accurate resolutions — often in a single visit — and a clear path toward level‑4‑plus operational maturity.&lt;/p&gt;
&lt;p&gt;A large telecommunications services provider is adopting the same LTM foundation as part of its operations roadmap, targeting improvements in service‑layer issue resolution, customer experience and back‑office efficiency through higher‑quality tickets and fewer escalations.&lt;/p&gt;
&lt;p&gt;The platform uses NVIDIA Nemotron embedding models for semantic search across telemetry and a Nemotron reranking model to sharpen decision relevance. These models are deployed with NVIDIA NIM microservices for rapid, reliable accelerated AI inference. NVIDIA NeMo Agent Toolkit orchestrates agent workflows across network domains, enabling true agentic operations at scale.&lt;/p&gt;
&lt;p&gt;By embracing autonomous network operations, Tech Mahindra shows how AI can transform a global telecom industry generating more than $1.5 trillion in annual revenue — where even small gains in uptime and efficiency deliver outsized economic impact.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Infosys Builds an Enterprise-Grade Coding Small Language Model With NVIDIA AI Enterprise&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Infosys developed a new small language model for coding, built using the NVIDIA NeMo framework that’s part of NVIDIA AI Enterprise, and integrated within Infosys Topaz Fabric. The model accelerates software delivery with frontier‑grade performance while remaining lightweight, and it can be deployed across on-premises enterprise data centers, cloud environments and even standard desktops.&lt;/p&gt;
&lt;p&gt;The 2.5‑billion‑parameter model supports agent development, code generation, refactoring and end‑to‑end software‑engineering workflows. It’s trained on a curated blend of high‑quality code, synthetic data, mathematical reasoning and natural language inputs — an approach that enables it to match frontier‑model performance on benchmarks such as MBPP, MBPP+ and BFCL.&lt;/p&gt;
&lt;p&gt;Infosys also prioritized safety and trust. The model incorporates safety‑aligned training and responsible AI practices that reduce harmful outputs while preserving fluency. Its secure‑coding capabilities are validated through industry benchmarks including Stanford AIR‑Bench and Meta’s CyberSecEval, giving enterprises confidence to deploy it across code generation, debugging and multi‑agent development pipelines.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Persistent Accelerates AI‑Driven Molecular Discovery With NVIDIA BioNeMo and NeMo Agent Toolkit&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Persistent Systems is working with NVIDIA to push early‑stage drug discovery into a new era of speed and scientific fidelity. The collaboration brings together Persistent’s deep life sciences engineering expertise with NVIDIA’s full‑stack accelerated computing platform, giving researchers a powerful path from AI experimentation to production‑grade discovery workflows.&lt;/p&gt;
&lt;p&gt;At the center of the effort is Persistent’s new Generative Molecules and Virtual Screening (GenMoIVS) solution, built on the NVIDIA BioNeMo platform and the NeMo Agent Toolkit. GenMoIVS uses large, domain‑specific models to simulate molecular behavior with high accuracy, generating and evaluating candidate compounds before they ever reach a wet lab. These agentic workflows continuously reason across virtual screening, prioritization and experimental planning, helping teams de‑risk early discovery and shorten development cycles.&lt;/p&gt;
&lt;p&gt;The platform runs on NVIDIA’s accelerated computing platform, including NVIDIA AI Enterprise software and NIM microservices, enabling high‑throughput simulation and real‑time scientific decision-making in regulated environments. By combining scalable infrastructure with production‑ready agentic AI, Persistent is giving life sciences organizations a faster, more cost‑effective way to explore the compound space and improve downstream success rates.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/india-enterprise-ai-agents/</guid><pubDate>Wed, 18 Feb 2026 00:30:41 +0000</pubDate></item><item><title>India Fuels Its AI Mission With NVIDIA (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/india-ai-mission-infrastructure-models/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;India is the nexus of AI innovation this week as the host of the AI Impact Summit, which brings together global heads of state and industry to chart the future of AI.&lt;/p&gt;
&lt;p&gt;At the summit, taking place in New Delhi, industry leaders, government agencies, educational institutions and startups are sharing how they’re working with NVIDIA to drive the AI industrial revolution in the world’s most populous country.&lt;/p&gt;
&lt;p&gt;These initiatives support the IndiaAI Mission, a government effort that’s infusing India’s AI ecosystem with over $1 billion to bolster the nation’s compute capacity and foster the development of sovereign AI datasets, frontier models and applications. The mission also supports AI education, startup innovation and frameworks for trustworthy AI.&lt;/p&gt;
&lt;p&gt;Read how NVIDIA is supporting IndiaAI Mission priorities including:&lt;/p&gt;

&lt;h2 id="infrastructure"&gt;&lt;b&gt;NVIDIA Cloud Partners Boost India AI Infrastructure&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To achieve its AI ambitions, India is investing heavily in its computing infrastructure. Under the IndiaAI Compute Pillar, the nation is building out its AI cloud offerings with systems including tens of thousands of NVIDIA GPUs.&lt;/p&gt;
&lt;p&gt;NVIDIA is collaborating with next‑generation cloud providers Yotta, L&amp;amp;T and E2E Networks to deliver advanced AI factories to meet India’s growing need for AI compute and enable it to develop AI models and services that drive innovation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Yotta is a hyperscale data center and cloud provider building large‑scale sovereign AI infrastructure for India, branded as Shakti Cloud, powered by over 20,000 NVIDIA Blackwell Ultra GPUs. Its campuses in Navi Mumbai and Greater Noida deliver GPU‑dense, high‑bandwidth AI cloud services on a pay‑per‑use model, designed to make advanced AI training and inference affordable and compliant for Indian enterprises and public sector customers.&lt;/li&gt;
&lt;li&gt;E2E Networks is building an NVIDIA Blackwell GPU cluster on its TIR platform, hosted at the L&amp;amp;T Vyoma Data Center in Chennai. The TIR cloud compute platform will feature NVIDIA HGX B200 systems and NVIDIA Enterprise software as well as NVIDIA Nemotron open models to supercharge sovereign development across agentic AI, healthcare, finance, manufacturing and agriculture.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;India’s AI cloud infrastructure will host workloads as well as manufacture intelligence for model training, fine-tuning and high‑scale inference. Capacity within these data centers will be reserved for model builders, startups, researchers and enterprises to build, fine-tune and deploy AI in India.&lt;/p&gt;
&lt;p&gt;Further expanding access to NVIDIA AI infrastructure in India, Netweb Technologies is launching its Tyrone Camarero AI Supercomputing systems built on the NVIDIA Grace Blackwell architecture. The NVIDIA GB200 NVL4 platforms — manufactured in India by Netweb under the government’s “Make in India” mission — feature four NVIDIA Blackwell GPUs and two NVIDIA Grace CPUs to power scientific computing, model training and inference.&lt;/p&gt;
&lt;h2 id="models"&gt;&lt;b&gt;NVIDIA and India AI-Native Companies Build the Nation’s Frontier AI Models&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-large wp-image-90004" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/ai-impact-summit-in-26-promo-pack-ai-natives-devnews-press-1920x1080-v2-1680x945.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;Another key goal of the IndiaAI Mission — led by its Innovation Center Pillar — is to develop and deploy foundation models trained on India-specific data and domestic AI infrastructure.&lt;/p&gt;
&lt;p&gt;For a nation as multilingual as India — with 22 constitutionally recognized languages and over 1,500 more recorded by the country’s census — frontier AI models are a powerful tool to help its more than 1.4 billion residents interact with technology in their primary language.&lt;/p&gt;
&lt;p&gt;Organizations across the country are building AI applications with NVIDIA Nemotron to support public-sector services, financial systems and enterprise operations in multiple languages.&lt;/p&gt;
&lt;p&gt;NVIDIA Nemotron open models, datasets, tools and libraries enable organizations to build frontier speech, language and multimodal models at scale and across languages for government, consumer and enterprise applications. It includes India-specific datasets like Nemotron-Personas-India, an open dataset built from publicly available census data using NeMo Data Designer that includes 21 million fully synthetic Indic personas to enable population-scale sovereign AI development.&lt;/p&gt;
&lt;p&gt;Adopters in India of Nemotron — and NeMo Curator, an open library for multilingual and multimodal data curation — include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;BharatGen&lt;/b&gt;, a sovereign AI initiative supported by the Government of India aimed at strengthening the country’s multilingual and multimodal AI ecosystem. As part of this effort, BharatGen has developed a 17-billion-parameter mixture-of-experts (MoE) model from the ground up, using the NVIDIA NeMo framework for pretraining and the NeMo RL library for post-training. The open source models are designed to power applications across public services, agriculture, security and cultural preservation.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Chariot&lt;/b&gt;, a company building AI systems for speech and multimodal communication. Using the NeMo framework, Chariot is developing an 8-billion-parameter model for real-time text to speech, supporting applications that improve accessibility and digital interaction across consumer and enterprise use cases.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Commotion&lt;/b&gt;, backed by Tata Communications, which has developed an AI operating system to automate complex enterprise workflows. By integrating NVIDIA Nemotron models and speech capabilities, the platform enables governed, production-grade AI deployments, helping enterprises scale AI across critical business operations.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;CoRover.ai&lt;/b&gt;, which has deployed NVIDIA Nemotron Speech open models and NVIDIA Riva libraries for end-to-end, ultralow-latency speech AI — including the NVIDIA Riva Whisper v3 model for multilingual automatic speech recognition in English, Hindi and Gujarati. Powering customer service applications for the Indian Railway Catering and Tourism Corporation, CoRover’s platform supports around 10,000 concurrent users and more than 5,000 daily ticket bookings.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Gnani.ai&lt;/b&gt;, which offers enterprises a multilingual agentic AI platform that can interact with customers through voice and text. Gnani is building a 14-billion-parameter speech-to-speech model built on NVIDIA Nemotron Speech models, datasets and NeMo libraries including NeMo libraries through NVIDIA Cloud Partner E2E Networks — with plans to expand to a 32-billion-parameter model. By fine-tuning the NVIDIA Nemotron Speech model for Indic languages, Gnani has achieved a 15x reduction in inference costs, enabling the company to scale to support more than 10 million calls per day for customers in telecom, banking and hospitality.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;National Payments Corporation of India (NPCI)&lt;/b&gt;, which operates India’s retail payment and settlement systems and is deploying AI models to support digital financial services. Building on its production deployment of the AI-powered UPI Help Assistant — a pilot initiative for India’s Unified Payments Interface (UPI) — NPCI is exploring training FiMi, a financial model for India, using the NVIDIA Nemotron 3 Nano model and its own datasets. The model, fine-tuned with the NeMo framework, will support multilingual customer service across India’s banking ecosystem.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Sarvam.ai&lt;/b&gt;, a leader in full-stack sovereign generative AI that provides enterprise-grade multimodal, speech-to-text, text-to-speech, translation and reasoning models. The company is open sourcing its Sarvam-3 series of text and multimodal large language model variants, trained for 22 Indic languages, English math and code. Sarvam is using NeMo Curator to construct high-quality multilingual training data while adopting a subset of NVIDIA Nemotron datasets. The foundation models were pre-trained from scratch across 3B, 30B and 100B parameter sizes using the NVIDIA NeMo framework and Megatron-LM, and post-trained with NeMo RL. Training was conducted on NVIDIA H100 GPUs through NVIDIA Cloud Partners, including Yotta. With these sovereign models, Sarvam.ai’s new Pravah platform enables production-grade inference for Indian government and enterprise applications.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Soket.ai&lt;/b&gt;, which is using a modern large-model training stack on open NVIDIA Nemotron technologies, including NVIDIA Megatron and NVIDIA NeMo. These open source components enable scalable experimentation, training stability and efficient GPU usage, while preserving full control over the model’s data, design and life cycle.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Tech Mahindra&lt;/b&gt;, which has developed an 8-billion-parameter foundation model tailored for Indian languages and dialects. The model, built with Nemotron, is being designed for use in classrooms, where it can help make educational materials available in a wider range of Indian languages including Hindi, Maithili and Dogri. The team generated synthetic data with Nemotron libraries and tools such as NeMo Data Designer and conducted supervised fine-tuning with NeMo AutoModel.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Zoho&lt;/b&gt;, which is advancing its Zia LLM platform with proprietary models built using NVIDIA NeMo on the NVIDIA Blackwell and Hopper platforms, integrated across its software-as-a-service applications. This privacy-first architecture delivers contextual, production-grade AI for critical business workflows like customer relation management and finance, ensuring technology sovereignty and enterprise security at a global scale.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Developers building sovereign AI systems can access NVIDIA Nemotron and NeMo today. Nemotron models can be deployed anywhere on NVIDIA-accelerated infrastructure — including on NVIDIA DGX Spark, which is now available in India through qualified partners including PNY, RP tech India, Tech Data, a TD SYNNEX Company, as well as on NVIDIA Marketplace. A version manufactured in India as part of the “Make in India” initiative is available through Netweb.&lt;/p&gt;
&lt;p&gt;DGX Spark also runs sovereign AI models by Indian model builders including Sarvam.ai.&lt;/p&gt;
&lt;h2 id="research-innovation"&gt;&lt;b&gt;Government and Academic Partnerships to Support Research in AI for Science and Engineering&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Under its Application Development Initiative Pillar, the IndiaAI Mission is supporting high-impact AI applications — and its Startup Financing Pillar aims to democratize funding availability for AI entrepreneurs across the country.&lt;/p&gt;
&lt;p&gt;NVIDIA is collaborating with government agencies, research institutions, venture capital firms and startups to advance projects aligned with these goals.&lt;/p&gt;
&lt;p&gt;NVIDIA is collaborating with the Anusandhan National Research Foundation (ANRF), a statutory body under the Indian government, to spur even more cutting-edge AI research across the nation’s leading academic institutions. The initiative will support ANRF’s AI for Science &amp;amp; Engineering program and future AI programs.&lt;/p&gt;
&lt;p&gt;NVIDIA will offer ANRF grantee institutions complimentary access to NVIDIA AI Enterprise software and specialized technical mentorship through the NVIDIA AI Technology Center. The collaboration will also include AI bootcamps, workshops and hackathons to strengthen India’s AI research ecosystem.&lt;/p&gt;
&lt;p&gt;NVIDIA is also partnering with prominent venture capital firms including Peak XV, Z47, Elevation Capital,, Nexus Venture Partners and Accel India to identify and fund promising startups of all stages that are building AI solutions for India and international use. More than 4,000 of India’s AI startups are already part of the NVIDIA Inception program.&lt;/p&gt;
&lt;p&gt;For more from the India AI Summit, learn how NVIDIA and global industrial software leaders are partnering with India’s largest manufacturers — and how India’s global systems integrators are building enterprise AI agents with NVIDIA.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;India is the nexus of AI innovation this week as the host of the AI Impact Summit, which brings together global heads of state and industry to chart the future of AI.&lt;/p&gt;
&lt;p&gt;At the summit, taking place in New Delhi, industry leaders, government agencies, educational institutions and startups are sharing how they’re working with NVIDIA to drive the AI industrial revolution in the world’s most populous country.&lt;/p&gt;
&lt;p&gt;These initiatives support the IndiaAI Mission, a government effort that’s infusing India’s AI ecosystem with over $1 billion to bolster the nation’s compute capacity and foster the development of sovereign AI datasets, frontier models and applications. The mission also supports AI education, startup innovation and frameworks for trustworthy AI.&lt;/p&gt;
&lt;p&gt;Read how NVIDIA is supporting IndiaAI Mission priorities including:&lt;/p&gt;

&lt;h2 id="infrastructure"&gt;&lt;b&gt;NVIDIA Cloud Partners Boost India AI Infrastructure&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To achieve its AI ambitions, India is investing heavily in its computing infrastructure. Under the IndiaAI Compute Pillar, the nation is building out its AI cloud offerings with systems including tens of thousands of NVIDIA GPUs.&lt;/p&gt;
&lt;p&gt;NVIDIA is collaborating with next‑generation cloud providers Yotta, L&amp;amp;T and E2E Networks to deliver advanced AI factories to meet India’s growing need for AI compute and enable it to develop AI models and services that drive innovation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Yotta is a hyperscale data center and cloud provider building large‑scale sovereign AI infrastructure for India, branded as Shakti Cloud, powered by over 20,000 NVIDIA Blackwell Ultra GPUs. Its campuses in Navi Mumbai and Greater Noida deliver GPU‑dense, high‑bandwidth AI cloud services on a pay‑per‑use model, designed to make advanced AI training and inference affordable and compliant for Indian enterprises and public sector customers.&lt;/li&gt;
&lt;li&gt;E2E Networks is building an NVIDIA Blackwell GPU cluster on its TIR platform, hosted at the L&amp;amp;T Vyoma Data Center in Chennai. The TIR cloud compute platform will feature NVIDIA HGX B200 systems and NVIDIA Enterprise software as well as NVIDIA Nemotron open models to supercharge sovereign development across agentic AI, healthcare, finance, manufacturing and agriculture.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;India’s AI cloud infrastructure will host workloads as well as manufacture intelligence for model training, fine-tuning and high‑scale inference. Capacity within these data centers will be reserved for model builders, startups, researchers and enterprises to build, fine-tune and deploy AI in India.&lt;/p&gt;
&lt;p&gt;Further expanding access to NVIDIA AI infrastructure in India, Netweb Technologies is launching its Tyrone Camarero AI Supercomputing systems built on the NVIDIA Grace Blackwell architecture. The NVIDIA GB200 NVL4 platforms — manufactured in India by Netweb under the government’s “Make in India” mission — feature four NVIDIA Blackwell GPUs and two NVIDIA Grace CPUs to power scientific computing, model training and inference.&lt;/p&gt;
&lt;h2 id="models"&gt;&lt;b&gt;NVIDIA and India AI-Native Companies Build the Nation’s Frontier AI Models&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-large wp-image-90004" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/ai-impact-summit-in-26-promo-pack-ai-natives-devnews-press-1920x1080-v2-1680x945.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;Another key goal of the IndiaAI Mission — led by its Innovation Center Pillar — is to develop and deploy foundation models trained on India-specific data and domestic AI infrastructure.&lt;/p&gt;
&lt;p&gt;For a nation as multilingual as India — with 22 constitutionally recognized languages and over 1,500 more recorded by the country’s census — frontier AI models are a powerful tool to help its more than 1.4 billion residents interact with technology in their primary language.&lt;/p&gt;
&lt;p&gt;Organizations across the country are building AI applications with NVIDIA Nemotron to support public-sector services, financial systems and enterprise operations in multiple languages.&lt;/p&gt;
&lt;p&gt;NVIDIA Nemotron open models, datasets, tools and libraries enable organizations to build frontier speech, language and multimodal models at scale and across languages for government, consumer and enterprise applications. It includes India-specific datasets like Nemotron-Personas-India, an open dataset built from publicly available census data using NeMo Data Designer that includes 21 million fully synthetic Indic personas to enable population-scale sovereign AI development.&lt;/p&gt;
&lt;p&gt;Adopters in India of Nemotron — and NeMo Curator, an open library for multilingual and multimodal data curation — include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;BharatGen&lt;/b&gt;, a sovereign AI initiative supported by the Government of India aimed at strengthening the country’s multilingual and multimodal AI ecosystem. As part of this effort, BharatGen has developed a 17-billion-parameter mixture-of-experts (MoE) model from the ground up, using the NVIDIA NeMo framework for pretraining and the NeMo RL library for post-training. The open source models are designed to power applications across public services, agriculture, security and cultural preservation.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Chariot&lt;/b&gt;, a company building AI systems for speech and multimodal communication. Using the NeMo framework, Chariot is developing an 8-billion-parameter model for real-time text to speech, supporting applications that improve accessibility and digital interaction across consumer and enterprise use cases.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Commotion&lt;/b&gt;, backed by Tata Communications, which has developed an AI operating system to automate complex enterprise workflows. By integrating NVIDIA Nemotron models and speech capabilities, the platform enables governed, production-grade AI deployments, helping enterprises scale AI across critical business operations.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;CoRover.ai&lt;/b&gt;, which has deployed NVIDIA Nemotron Speech open models and NVIDIA Riva libraries for end-to-end, ultralow-latency speech AI — including the NVIDIA Riva Whisper v3 model for multilingual automatic speech recognition in English, Hindi and Gujarati. Powering customer service applications for the Indian Railway Catering and Tourism Corporation, CoRover’s platform supports around 10,000 concurrent users and more than 5,000 daily ticket bookings.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Gnani.ai&lt;/b&gt;, which offers enterprises a multilingual agentic AI platform that can interact with customers through voice and text. Gnani is building a 14-billion-parameter speech-to-speech model built on NVIDIA Nemotron Speech models, datasets and NeMo libraries including NeMo libraries through NVIDIA Cloud Partner E2E Networks — with plans to expand to a 32-billion-parameter model. By fine-tuning the NVIDIA Nemotron Speech model for Indic languages, Gnani has achieved a 15x reduction in inference costs, enabling the company to scale to support more than 10 million calls per day for customers in telecom, banking and hospitality.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;National Payments Corporation of India (NPCI)&lt;/b&gt;, which operates India’s retail payment and settlement systems and is deploying AI models to support digital financial services. Building on its production deployment of the AI-powered UPI Help Assistant — a pilot initiative for India’s Unified Payments Interface (UPI) — NPCI is exploring training FiMi, a financial model for India, using the NVIDIA Nemotron 3 Nano model and its own datasets. The model, fine-tuned with the NeMo framework, will support multilingual customer service across India’s banking ecosystem.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Sarvam.ai&lt;/b&gt;, a leader in full-stack sovereign generative AI that provides enterprise-grade multimodal, speech-to-text, text-to-speech, translation and reasoning models. The company is open sourcing its Sarvam-3 series of text and multimodal large language model variants, trained for 22 Indic languages, English math and code. Sarvam is using NeMo Curator to construct high-quality multilingual training data while adopting a subset of NVIDIA Nemotron datasets. The foundation models were pre-trained from scratch across 3B, 30B and 100B parameter sizes using the NVIDIA NeMo framework and Megatron-LM, and post-trained with NeMo RL. Training was conducted on NVIDIA H100 GPUs through NVIDIA Cloud Partners, including Yotta. With these sovereign models, Sarvam.ai’s new Pravah platform enables production-grade inference for Indian government and enterprise applications.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Soket.ai&lt;/b&gt;, which is using a modern large-model training stack on open NVIDIA Nemotron technologies, including NVIDIA Megatron and NVIDIA NeMo. These open source components enable scalable experimentation, training stability and efficient GPU usage, while preserving full control over the model’s data, design and life cycle.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Tech Mahindra&lt;/b&gt;, which has developed an 8-billion-parameter foundation model tailored for Indian languages and dialects. The model, built with Nemotron, is being designed for use in classrooms, where it can help make educational materials available in a wider range of Indian languages including Hindi, Maithili and Dogri. The team generated synthetic data with Nemotron libraries and tools such as NeMo Data Designer and conducted supervised fine-tuning with NeMo AutoModel.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Zoho&lt;/b&gt;, which is advancing its Zia LLM platform with proprietary models built using NVIDIA NeMo on the NVIDIA Blackwell and Hopper platforms, integrated across its software-as-a-service applications. This privacy-first architecture delivers contextual, production-grade AI for critical business workflows like customer relation management and finance, ensuring technology sovereignty and enterprise security at a global scale.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Developers building sovereign AI systems can access NVIDIA Nemotron and NeMo today. Nemotron models can be deployed anywhere on NVIDIA-accelerated infrastructure — including on NVIDIA DGX Spark, which is now available in India through qualified partners including PNY, RP tech India, Tech Data, a TD SYNNEX Company, as well as on NVIDIA Marketplace. A version manufactured in India as part of the “Make in India” initiative is available through Netweb.&lt;/p&gt;
&lt;p&gt;DGX Spark also runs sovereign AI models by Indian model builders including Sarvam.ai.&lt;/p&gt;
&lt;h2 id="research-innovation"&gt;&lt;b&gt;Government and Academic Partnerships to Support Research in AI for Science and Engineering&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Under its Application Development Initiative Pillar, the IndiaAI Mission is supporting high-impact AI applications — and its Startup Financing Pillar aims to democratize funding availability for AI entrepreneurs across the country.&lt;/p&gt;
&lt;p&gt;NVIDIA is collaborating with government agencies, research institutions, venture capital firms and startups to advance projects aligned with these goals.&lt;/p&gt;
&lt;p&gt;NVIDIA is collaborating with the Anusandhan National Research Foundation (ANRF), a statutory body under the Indian government, to spur even more cutting-edge AI research across the nation’s leading academic institutions. The initiative will support ANRF’s AI for Science &amp;amp; Engineering program and future AI programs.&lt;/p&gt;
&lt;p&gt;NVIDIA will offer ANRF grantee institutions complimentary access to NVIDIA AI Enterprise software and specialized technical mentorship through the NVIDIA AI Technology Center. The collaboration will also include AI bootcamps, workshops and hackathons to strengthen India’s AI research ecosystem.&lt;/p&gt;
&lt;p&gt;NVIDIA is also partnering with prominent venture capital firms including Peak XV, Z47, Elevation Capital,, Nexus Venture Partners and Accel India to identify and fund promising startups of all stages that are building AI solutions for India and international use. More than 4,000 of India’s AI startups are already part of the NVIDIA Inception program.&lt;/p&gt;
&lt;p&gt;For more from the India AI Summit, learn how NVIDIA and global industrial software leaders are partnering with India’s largest manufacturers — and how India’s global systems integrators are building enterprise AI agents with NVIDIA.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/india-ai-mission-infrastructure-models/</guid><pubDate>Wed, 18 Feb 2026 00:30:49 +0000</pubDate></item><item><title>[NEW] Personalization features can make LLMs more agreeable (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/personalization-features-can-make-llms-more-agreeable-0218</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-LLM-Sycophant-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Many of the latest large language models (LLMs) are designed to remember details from past conversations or store user profiles, enabling these models to personalize responses.&lt;/p&gt;&lt;p&gt;But researchers from MIT and Penn State University found that, over long conversations, such personalization features often increase the likelihood an LLM will become overly agreeable or begin mirroring the individual’s point of view.&lt;/p&gt;&lt;p&gt;This phenomenon, known as sycophancy, can prevent a model from telling a user they are wrong, eroding the accuracy of the LLM’s responses. In addition, LLMs that mirror someone’s political beliefs or worldview can foster misinformation and distort a user’s perception of reality.&lt;/p&gt;&lt;p&gt;Unlike many past sycophancy studies that evaluate prompts in a lab setting without context, the MIT researchers collected two weeks of conversation data from humans who interacted with a real LLM during their daily lives. They studied two settings: agreeableness in personal advice and mirroring of user beliefs in political explanations.&lt;/p&gt;&lt;p&gt;Although interaction context increased agreeableness in four of the five LLMs they studied, the presence of a condensed user profile in the model’s memory had the greatest impact. On the other hand, mirroring behavior only increased if a model could accurately infer a user’s beliefs from the conversation.&lt;/p&gt;&lt;p&gt;The researchers hope these results inspire future research into the development of personalization methods that are more robust to LLM sycophancy.&lt;/p&gt;&lt;p&gt;“From a user perspective, this work highlights how important it is to understand that these models are dynamic and their behavior can change as you interact with them over time. If you are talking to a model for an extended period of time and start to outsource your thinking to it, you may find yourself in an echo chamber that you can’t escape. That is a risk users should definitely remember,” says Shomik Jain, a graduate student in the Institute for Data, Systems, and Society (IDSS) and lead author of a&amp;nbsp;paper on this research.&lt;/p&gt;&lt;p&gt;Jain is joined on the paper by Charlotte Park, an electrical engineering and computer science (EECS) graduate student at MIT; Matt Viana, a graduate student at Penn State University; as well as co-senior authors Ashia Wilson, the Lister Brothers Career Development Professor in EECS and a principal investigator in LIDS; and Dana Calacci PhD ’23, an assistant professor at the Penn State. The research will be presented at the ACM CHI Conference on Human Factors in Computing Systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Extended interactions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Based on their own sycophantic experiences with LLMs, the researchers started thinking about potential benefits and consequences of a model that is overly agreeable. But when they searched the literature to expand their analysis, they found no studies that attempted to understand sycophantic behavior during long-term LLM interactions.&lt;/p&gt;&lt;p&gt;“We are using these models through extended interactions, and they have a lot of context and memory. But our evaluation methods are lagging behind. We wanted to evaluate LLMs in the ways people are actually using them to understand how they are behaving in the wild,” says Calacci.&lt;/p&gt;&lt;p&gt;To fill this gap, the researchers designed a user study to explore two types of sycophancy: agreement sycophancy and perspective sycophancy.&lt;/p&gt;&lt;p&gt;Agreement sycophancy is an LLM’s tendency to be overly agreeable, sometimes to the point where it gives incorrect information or refuses the tell the user they are wrong. Perspective sycophancy occurs when a model mirrors the user’s values and political views.&lt;/p&gt;&lt;p&gt;“There is a lot we know about the benefits of having social connections with people who have similar or different viewpoints. But we don’t yet know about the benefits or risks of extended interactions with AI models that have similar attributes,” Calacci adds.&lt;/p&gt;&lt;p&gt;The researchers built a user interface centered on an LLM and recruited 38 participants to talk with the chatbot over a two-week period. Each participant’s conversations occurred in the same context window to capture all interaction data.&lt;/p&gt;&lt;p&gt;Over the two-week period, the researchers collected an average of 90 queries from each user.&lt;/p&gt;&lt;p&gt;They compared the behavior of five LLMs with this user context versus the same LLMs that weren’t given any conversation data.&lt;/p&gt;&lt;p&gt;“We found that context really does fundamentally change how these models operate, and I would wager this phenomenon would extend well beyond sycophancy. And while sycophancy tended to go up, it didn’t always increase. It really depends on the context itself,” says Wilson.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Context clues&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For instance, when an LLM distills information about the user into a specific profile, it leads to the largest gains in agreement sycophancy. This user profile feature is increasingly being baked into the newest models.&lt;/p&gt;&lt;p&gt;They also found that random text from synthetic conversations also increased the likelihood some models would agree, even though that text contained no user-specific data. This suggests the length of a conversation may sometimes impact sycophancy more than content, Jain adds.&lt;/p&gt;&lt;p&gt;But content matters greatly when it comes to perspective sycophancy. Conversation context only increased perspective sycophancy if it revealed some information about a user’s political perspective.&lt;/p&gt;&lt;p&gt;To obtain this insight, the researchers carefully queried models to infer a user’s beliefs then asked each individual if the model’s deductions were correct. Users said LLMs accurately understood their political views about half the time.&lt;/p&gt;&lt;p&gt;“It is easy to say, in hindsight, that AI companies should be doing this kind of evaluation. But it is hard and it takes a lot of time and investment. Using humans in the evaluation loop is expensive, but we’ve shown that it can reveal new insights,” Jain says.&lt;/p&gt;&lt;p&gt;While the aim of their research was not mitigation, the researchers developed some recommendations.&lt;/p&gt;&lt;p&gt;For instance, to reduce sycophancy one could design models that better identify relevant details in context and memory. In addition, models can be built to detect mirroring behaviors and flag responses with excessive agreement. Model developers could also give users the ability to moderate personalization in long conversations.&lt;/p&gt;&lt;p&gt;“There are many ways to personalize models without making them overly agreeable. The boundary between personalization and sycophancy is not a fine line, but separating personalization from sycophancy is an important area of future work,” Jain says.&lt;/p&gt;&lt;p&gt;“At the end of the day, we need better ways of capturing the dynamics and complexity of what goes on during long conversations with LLMs, and how things can misalign during that long-term process,” Wilson adds.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-LLM-Sycophant-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Many of the latest large language models (LLMs) are designed to remember details from past conversations or store user profiles, enabling these models to personalize responses.&lt;/p&gt;&lt;p&gt;But researchers from MIT and Penn State University found that, over long conversations, such personalization features often increase the likelihood an LLM will become overly agreeable or begin mirroring the individual’s point of view.&lt;/p&gt;&lt;p&gt;This phenomenon, known as sycophancy, can prevent a model from telling a user they are wrong, eroding the accuracy of the LLM’s responses. In addition, LLMs that mirror someone’s political beliefs or worldview can foster misinformation and distort a user’s perception of reality.&lt;/p&gt;&lt;p&gt;Unlike many past sycophancy studies that evaluate prompts in a lab setting without context, the MIT researchers collected two weeks of conversation data from humans who interacted with a real LLM during their daily lives. They studied two settings: agreeableness in personal advice and mirroring of user beliefs in political explanations.&lt;/p&gt;&lt;p&gt;Although interaction context increased agreeableness in four of the five LLMs they studied, the presence of a condensed user profile in the model’s memory had the greatest impact. On the other hand, mirroring behavior only increased if a model could accurately infer a user’s beliefs from the conversation.&lt;/p&gt;&lt;p&gt;The researchers hope these results inspire future research into the development of personalization methods that are more robust to LLM sycophancy.&lt;/p&gt;&lt;p&gt;“From a user perspective, this work highlights how important it is to understand that these models are dynamic and their behavior can change as you interact with them over time. If you are talking to a model for an extended period of time and start to outsource your thinking to it, you may find yourself in an echo chamber that you can’t escape. That is a risk users should definitely remember,” says Shomik Jain, a graduate student in the Institute for Data, Systems, and Society (IDSS) and lead author of a&amp;nbsp;paper on this research.&lt;/p&gt;&lt;p&gt;Jain is joined on the paper by Charlotte Park, an electrical engineering and computer science (EECS) graduate student at MIT; Matt Viana, a graduate student at Penn State University; as well as co-senior authors Ashia Wilson, the Lister Brothers Career Development Professor in EECS and a principal investigator in LIDS; and Dana Calacci PhD ’23, an assistant professor at the Penn State. The research will be presented at the ACM CHI Conference on Human Factors in Computing Systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Extended interactions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Based on their own sycophantic experiences with LLMs, the researchers started thinking about potential benefits and consequences of a model that is overly agreeable. But when they searched the literature to expand their analysis, they found no studies that attempted to understand sycophantic behavior during long-term LLM interactions.&lt;/p&gt;&lt;p&gt;“We are using these models through extended interactions, and they have a lot of context and memory. But our evaluation methods are lagging behind. We wanted to evaluate LLMs in the ways people are actually using them to understand how they are behaving in the wild,” says Calacci.&lt;/p&gt;&lt;p&gt;To fill this gap, the researchers designed a user study to explore two types of sycophancy: agreement sycophancy and perspective sycophancy.&lt;/p&gt;&lt;p&gt;Agreement sycophancy is an LLM’s tendency to be overly agreeable, sometimes to the point where it gives incorrect information or refuses the tell the user they are wrong. Perspective sycophancy occurs when a model mirrors the user’s values and political views.&lt;/p&gt;&lt;p&gt;“There is a lot we know about the benefits of having social connections with people who have similar or different viewpoints. But we don’t yet know about the benefits or risks of extended interactions with AI models that have similar attributes,” Calacci adds.&lt;/p&gt;&lt;p&gt;The researchers built a user interface centered on an LLM and recruited 38 participants to talk with the chatbot over a two-week period. Each participant’s conversations occurred in the same context window to capture all interaction data.&lt;/p&gt;&lt;p&gt;Over the two-week period, the researchers collected an average of 90 queries from each user.&lt;/p&gt;&lt;p&gt;They compared the behavior of five LLMs with this user context versus the same LLMs that weren’t given any conversation data.&lt;/p&gt;&lt;p&gt;“We found that context really does fundamentally change how these models operate, and I would wager this phenomenon would extend well beyond sycophancy. And while sycophancy tended to go up, it didn’t always increase. It really depends on the context itself,” says Wilson.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Context clues&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For instance, when an LLM distills information about the user into a specific profile, it leads to the largest gains in agreement sycophancy. This user profile feature is increasingly being baked into the newest models.&lt;/p&gt;&lt;p&gt;They also found that random text from synthetic conversations also increased the likelihood some models would agree, even though that text contained no user-specific data. This suggests the length of a conversation may sometimes impact sycophancy more than content, Jain adds.&lt;/p&gt;&lt;p&gt;But content matters greatly when it comes to perspective sycophancy. Conversation context only increased perspective sycophancy if it revealed some information about a user’s political perspective.&lt;/p&gt;&lt;p&gt;To obtain this insight, the researchers carefully queried models to infer a user’s beliefs then asked each individual if the model’s deductions were correct. Users said LLMs accurately understood their political views about half the time.&lt;/p&gt;&lt;p&gt;“It is easy to say, in hindsight, that AI companies should be doing this kind of evaluation. But it is hard and it takes a lot of time and investment. Using humans in the evaluation loop is expensive, but we’ve shown that it can reveal new insights,” Jain says.&lt;/p&gt;&lt;p&gt;While the aim of their research was not mitigation, the researchers developed some recommendations.&lt;/p&gt;&lt;p&gt;For instance, to reduce sycophancy one could design models that better identify relevant details in context and memory. In addition, models can be built to detect mirroring behaviors and flag responses with excessive agreement. Model developers could also give users the ability to moderate personalization in long conversations.&lt;/p&gt;&lt;p&gt;“There are many ways to personalize models without making them overly agreeable. The boundary between personalization and sycophancy is not a fine line, but separating personalization from sycophancy is an important area of future work,” Jain says.&lt;/p&gt;&lt;p&gt;“At the end of the day, we need better ways of capturing the dynamics and complexity of what goes on during long conversations with LLMs, and how things can misalign during that long-term process,” Wilson adds.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/personalization-features-can-make-llms-more-agreeable-0218</guid><pubDate>Wed, 18 Feb 2026 05:00:00 +0000</pubDate></item></channel></rss>