<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 01 Oct 2025 06:31:42 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>Former OpenAI and DeepMind researchers raise whopping $300M seed to automate science (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/30/former-openai-and-deepmind-researchers-raise-whopping-300m-seed-to-automate-science/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-642109519.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Periodic Labs came out of stealth on Tuesday with a war chest of $300 million as a seed round, backed by a tech industry who’s who: Andreessen Horowitz, DST, Nvidia, Accel, Elad Gil, Jeff Dean, Eric Schmidt, and Jeff Bezos.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Periodic Labs was founded by Ekin Dogus Cubuk and Liam Fedus. Cubuk led the materials and chemistry team at Google Brain and DeepMind, where one of his projects was, for instance, an AI tool called GNoME. That tool discovered over 2 million new crystals in 2023, materials that could one day be used to power new generations of technology, researchers say.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Fedus is a former VP of Research at OpenAI, and one of the researchers who helped create ChatGPT. He also led the team that created the first trillion-parameter neural network.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Its small team is likewise filled with researchers who have worked on other major AI and materials science projects, from building OpenAI’s agent Operator to working on Microsoft’s MatterGen, an LLM materials science discovery AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The goal of Periodic Labs is nothing less than to automate scientific discovery, creating AI scientists, the company says. This means building labs where robots conduct physical experiments, collect data, iterate, and try again, learning and improving as they go.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The lab’s first goal is to invent new superconductors that it hopes perform better and possibly require less energy than existing superconducting materials. But the well-funded startup also hopes to find other new materials.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another goal is to collect all the physical world data that its AI scientists produce as they mix and heat and otherwise manipulate various powers and raw materials in their search for something new.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Until now, scientific AI advances have come from models trained on the internet” and LLMs have “exhausted” the internet as a source that can be consumed, the company says in an introductory blog post. “[A]t Periodic, we are building AI scientists &lt;em&gt;and&lt;/em&gt; the autonomous laboratories for them to operate.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The hope is that, not only will the labs invent next-generation materials, but they will produce invaluable fresh data that AI models can consume to continue their evolution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While this might be one of the most impressive groups of researchers to assemble a startup for this purpose, it’s not the only one working on AI scientists. AI as a tool to automate chemistry discoveries has been a topic of academic research since at least 2023. It is the pursuit of tiny startups like Tetsuwan Scientific, as well as nonprofits like Future House and the University of Toronto’s Acceleration Consortium.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-642109519.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Periodic Labs came out of stealth on Tuesday with a war chest of $300 million as a seed round, backed by a tech industry who’s who: Andreessen Horowitz, DST, Nvidia, Accel, Elad Gil, Jeff Dean, Eric Schmidt, and Jeff Bezos.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Periodic Labs was founded by Ekin Dogus Cubuk and Liam Fedus. Cubuk led the materials and chemistry team at Google Brain and DeepMind, where one of his projects was, for instance, an AI tool called GNoME. That tool discovered over 2 million new crystals in 2023, materials that could one day be used to power new generations of technology, researchers say.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Fedus is a former VP of Research at OpenAI, and one of the researchers who helped create ChatGPT. He also led the team that created the first trillion-parameter neural network.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Its small team is likewise filled with researchers who have worked on other major AI and materials science projects, from building OpenAI’s agent Operator to working on Microsoft’s MatterGen, an LLM materials science discovery AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The goal of Periodic Labs is nothing less than to automate scientific discovery, creating AI scientists, the company says. This means building labs where robots conduct physical experiments, collect data, iterate, and try again, learning and improving as they go.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The lab’s first goal is to invent new superconductors that it hopes perform better and possibly require less energy than existing superconducting materials. But the well-funded startup also hopes to find other new materials.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another goal is to collect all the physical world data that its AI scientists produce as they mix and heat and otherwise manipulate various powers and raw materials in their search for something new.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Until now, scientific AI advances have come from models trained on the internet” and LLMs have “exhausted” the internet as a source that can be consumed, the company says in an introductory blog post. “[A]t Periodic, we are building AI scientists &lt;em&gt;and&lt;/em&gt; the autonomous laboratories for them to operate.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The hope is that, not only will the labs invent next-generation materials, but they will produce invaluable fresh data that AI models can consume to continue their evolution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While this might be one of the most impressive groups of researchers to assemble a startup for this purpose, it’s not the only one working on AI scientists. AI as a tool to automate chemistry discoveries has been a topic of academic research since at least 2023. It is the pursuit of tiny startups like Tetsuwan Scientific, as well as nonprofits like Future House and the University of Toronto’s Acceleration Consortium.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/30/former-openai-and-deepmind-researchers-raise-whopping-300m-seed-to-automate-science/</guid><pubDate>Tue, 30 Sep 2025 18:56:59 +0000</pubDate></item><item><title>With new agent mode for Excel and Word, Microsoft touts “vibe working” (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/09/with-new-agent-mode-for-excel-and-word-microsoft-touts-vibe-working/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Agent Mode in Word, Excel works like vibe coding tools but for knowledge work.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An Excel spreadsheet shows the Agent Mode interface" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/M365-Blog_09292025_agentMode_Hero_v1-809x455-1-640x360.webp" width="640" /&gt;
                  &lt;img alt="An Excel spreadsheet shows the Agent Mode interface" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="455" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/M365-Blog_09292025_agentMode_Hero_v1-809x455-1.webp" width="809" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A promotional image for Agent Mode made by microsoft.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Microsoft

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;With a new set of Microsoft 365 features, knowledge workers will be able to generate complex Word documents or Excel spreadsheets using only text prompts to Microsoft's chatbot. Two distinct products were announced, each using different models and accessed from within different tools—though the similar names Microsoft chose make it confusing to parse what's what.&lt;/p&gt;
&lt;p&gt;Driven by OpenAI's GPT-5 large language model, Agent Mode is built into Word and Excel, and it allows the creation of complex documents and spreadsheets from user prompts. It's called "agent" mode because it doesn't just work from the prompt in a single step; rather, it plans multistep work and runs a validation loop in the hopes of ensuring quality.&lt;/p&gt;
&lt;p&gt;It's only available in the web versions of Word and Excel at present, but the plan is to bring it to native desktop applications later.&lt;/p&gt;
&lt;p&gt;There's also the similarly named Office Agent for Copilot. Based on Anthropic models, this feature is built into Microsoft's Copilot AI assistant chatbot, and it too can generate documents from prompts—specifically, Word or PowerPoint files.&lt;/p&gt;
&lt;p&gt;Office Agent doesn't run through all the steps as Agent Mode, but Microsoft believes it offers a dramatic improvement over prior, OpenAI-driven document-generation capabilities in Copilot, which users complained were prone to all sorts of problems and shortcomings. It is available first in the Frontier Program for Microsoft 365 subscribers.&lt;/p&gt;
&lt;p&gt;Together, Microsoft says these features will let knowledge workers engage in a practice it's calling "vibe working," a play on the now-established term vibe coding.&lt;/p&gt;
&lt;h2&gt;Vibe everything, apparently&lt;/h2&gt;
&lt;p&gt;Vibe coding is the process of developing an application entirely via LLM chatbot prompts. You explain what you want in the chat interface and ask for it to generate code that does that. You then run that code, and if there are problems, explain the problem and tell it to fix it, iterating along the way until you have a usable application.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For certain kinds of simple applications, you can generate something usable this way. However, it often falls apart completely as you scale to more complex applications, and in any case, it's almost definitely going to introduce problems that you are less likely to see than if you wrote the application yourself, leading to (among other things) deep technical debt.&lt;/p&gt;
&lt;p&gt;Again, that's probably fine if you're just making a simple website for your small local business or something like that. But there's consensus in the development community that it's a dangerous path to walk at enterprise scale.&lt;/p&gt;
&lt;p&gt;If you're "vibe working" or "vibe writing" in Microsoft Word, you're doing the same thing, but with a text document: You're telling it what you want the document to say, reading it, accepting the suggestion, and then asking for further changes until you're happy with it.&lt;/p&gt;
&lt;p&gt;Whether this makes any sense obviously depends on what kind of document you're writing. For some things, it should be just fine as long as someone is reading it. Others probably won't work for their intended purpose without a human touch. Same with PowerPoint presentations.&lt;/p&gt;
&lt;p&gt;Doing this with a spreadsheet could be riskier, though; the financial or legal consequences for bad math or data in spreadsheets of some types can be very high, and as with vibe coding, it might be hard to see the problems at the surface level.&lt;/p&gt;
&lt;p&gt;That's exactly why Microsoft hasn't been as aggressive in adding AI features to Excel as it has with some other applications. And to be fair, it acknowledges an important gap here: a SpreadsheetBench sheet in today's announcement notes that Copilot in Excel Agent Mode managed a 57.2 percent score, while a human typically manages 71.3 percent. So, as with vibe coding, you'd want to be highly selective about when and how you'd use this, and you'd want to make sure that an experienced human is auditing the output carefully.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But the thinking is that just because it's not suitable for every kind of spreadsheet doesn't mean it doesn't make sense to have an easy-to-use option for lower-stakes work.&lt;/p&gt;
&lt;h2&gt;Use with care&lt;/h2&gt;
&lt;p&gt;It's possible these tools (and refined successors) will make life just a little bit easier for knowledge workers, but as always, those workers are going to need to understand some basic principles of how LLM-based tools work, and what their strengths and weaknesses are, to make intelligent decisions about when to try and save time by "vibe working" and when not to.&lt;/p&gt;
&lt;p&gt;All that said, a big reason why vibe coding is popular is because it allows inexperienced developers (or people who are not really developers at all) to bypass a knowledge gap; not everyone knows all the syntax and nuances of a programming language, much less which functions are available to call in a given library and so on.&lt;/p&gt;
&lt;p&gt;Something akin to that may also be true of professional-caliber writing, but the gap doesn't seem as big there, so some may feel that "vibe working" is an answer in search of a problem.&lt;/p&gt;
&lt;p&gt;OpenAI and some other major AI companies are said to be working on their own productivity tools built on their models, so we can also see this as Microsoft's attempt to stay ahead of the puck and make sure it doesn't find itself outscored by upstarts.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Agent Mode in Word, Excel works like vibe coding tools but for knowledge work.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An Excel spreadsheet shows the Agent Mode interface" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/M365-Blog_09292025_agentMode_Hero_v1-809x455-1-640x360.webp" width="640" /&gt;
                  &lt;img alt="An Excel spreadsheet shows the Agent Mode interface" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="455" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/M365-Blog_09292025_agentMode_Hero_v1-809x455-1.webp" width="809" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A promotional image for Agent Mode made by microsoft.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Microsoft

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;With a new set of Microsoft 365 features, knowledge workers will be able to generate complex Word documents or Excel spreadsheets using only text prompts to Microsoft's chatbot. Two distinct products were announced, each using different models and accessed from within different tools—though the similar names Microsoft chose make it confusing to parse what's what.&lt;/p&gt;
&lt;p&gt;Driven by OpenAI's GPT-5 large language model, Agent Mode is built into Word and Excel, and it allows the creation of complex documents and spreadsheets from user prompts. It's called "agent" mode because it doesn't just work from the prompt in a single step; rather, it plans multistep work and runs a validation loop in the hopes of ensuring quality.&lt;/p&gt;
&lt;p&gt;It's only available in the web versions of Word and Excel at present, but the plan is to bring it to native desktop applications later.&lt;/p&gt;
&lt;p&gt;There's also the similarly named Office Agent for Copilot. Based on Anthropic models, this feature is built into Microsoft's Copilot AI assistant chatbot, and it too can generate documents from prompts—specifically, Word or PowerPoint files.&lt;/p&gt;
&lt;p&gt;Office Agent doesn't run through all the steps as Agent Mode, but Microsoft believes it offers a dramatic improvement over prior, OpenAI-driven document-generation capabilities in Copilot, which users complained were prone to all sorts of problems and shortcomings. It is available first in the Frontier Program for Microsoft 365 subscribers.&lt;/p&gt;
&lt;p&gt;Together, Microsoft says these features will let knowledge workers engage in a practice it's calling "vibe working," a play on the now-established term vibe coding.&lt;/p&gt;
&lt;h2&gt;Vibe everything, apparently&lt;/h2&gt;
&lt;p&gt;Vibe coding is the process of developing an application entirely via LLM chatbot prompts. You explain what you want in the chat interface and ask for it to generate code that does that. You then run that code, and if there are problems, explain the problem and tell it to fix it, iterating along the way until you have a usable application.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For certain kinds of simple applications, you can generate something usable this way. However, it often falls apart completely as you scale to more complex applications, and in any case, it's almost definitely going to introduce problems that you are less likely to see than if you wrote the application yourself, leading to (among other things) deep technical debt.&lt;/p&gt;
&lt;p&gt;Again, that's probably fine if you're just making a simple website for your small local business or something like that. But there's consensus in the development community that it's a dangerous path to walk at enterprise scale.&lt;/p&gt;
&lt;p&gt;If you're "vibe working" or "vibe writing" in Microsoft Word, you're doing the same thing, but with a text document: You're telling it what you want the document to say, reading it, accepting the suggestion, and then asking for further changes until you're happy with it.&lt;/p&gt;
&lt;p&gt;Whether this makes any sense obviously depends on what kind of document you're writing. For some things, it should be just fine as long as someone is reading it. Others probably won't work for their intended purpose without a human touch. Same with PowerPoint presentations.&lt;/p&gt;
&lt;p&gt;Doing this with a spreadsheet could be riskier, though; the financial or legal consequences for bad math or data in spreadsheets of some types can be very high, and as with vibe coding, it might be hard to see the problems at the surface level.&lt;/p&gt;
&lt;p&gt;That's exactly why Microsoft hasn't been as aggressive in adding AI features to Excel as it has with some other applications. And to be fair, it acknowledges an important gap here: a SpreadsheetBench sheet in today's announcement notes that Copilot in Excel Agent Mode managed a 57.2 percent score, while a human typically manages 71.3 percent. So, as with vibe coding, you'd want to be highly selective about when and how you'd use this, and you'd want to make sure that an experienced human is auditing the output carefully.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But the thinking is that just because it's not suitable for every kind of spreadsheet doesn't mean it doesn't make sense to have an easy-to-use option for lower-stakes work.&lt;/p&gt;
&lt;h2&gt;Use with care&lt;/h2&gt;
&lt;p&gt;It's possible these tools (and refined successors) will make life just a little bit easier for knowledge workers, but as always, those workers are going to need to understand some basic principles of how LLM-based tools work, and what their strengths and weaknesses are, to make intelligent decisions about when to try and save time by "vibe working" and when not to.&lt;/p&gt;
&lt;p&gt;All that said, a big reason why vibe coding is popular is because it allows inexperienced developers (or people who are not really developers at all) to bypass a knowledge gap; not everyone knows all the syntax and nuances of a programming language, much less which functions are available to call in a given library and so on.&lt;/p&gt;
&lt;p&gt;Something akin to that may also be true of professional-caliber writing, but the gap doesn't seem as big there, so some may feel that "vibe working" is an answer in search of a problem.&lt;/p&gt;
&lt;p&gt;OpenAI and some other major AI companies are said to be working on their own productivity tools built on their models, so we can also see this as Microsoft's attempt to stay ahead of the puck and make sure it doesn't find itself outscored by upstarts.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/09/with-new-agent-mode-for-excel-and-word-microsoft-touts-vibe-working/</guid><pubDate>Tue, 30 Sep 2025 19:34:38 +0000</pubDate></item><item><title>DeepSeek tests “sparse attention” to slash AI processing costs (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/09/deepseek-tests-sparse-attention-to-slash-ai-processing-costs/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Chinese lab's v3.2 release explores a technique that could make running AI far less costly.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="DeepSeek logo on a smartphone against a blue streaming-code tech-ish background." class="absolute inset-0 w-full h-full object-cover hidden" height="370" src="https://cdn.arstechnica.net/wp-content/uploads/2025/01/GettyImages-2195894561-640x370.jpg" width="640" /&gt;
                  &lt;img alt="DeepSeek logo on a smartphone against a blue streaming-code tech-ish background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/01/GettyImages-2195894561-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Ever wonder why ChatGPT slows down during long conversations? The culprit is a fundamental mathematical challenge: Processing long sequences of text requires massive computational resources, even with the efficiency tricks that companies have already deployed. While US tech giants can afford to throw more hardware at the problem, Chinese AI company DeepSeek, which is cut off from a steady supply of some advanced AI chips by export restrictions, has extra motivation to squeeze more performance from less silicon.&lt;/p&gt;
&lt;p&gt;On Monday, DeepSeek released an experimental version of its latest simulated reasoning language model, DeepSeek-V3.2-Exp, which introduces what it calls "DeepSeek Sparse Attention" (DSA). It's the company's implementation of a computational technique likely already used in some of the world's most prominent AI models. OpenAI pioneered sparse transformers in 2019 and used the technique to build GPT-3, while Google Research published work on "Reformer" models using similar concepts in 2020. (The full extent to which Western AI companies currently use sparse attention in their latest models remains undisclosed.)&lt;/p&gt;
&lt;p&gt;Despite sparse attention being a known approach for years, DeepSeek claims its version achieves "fine-grained sparse attention for the first time" and has cut API prices by 50 percent to demonstrate the efficiency gains. But to understand more about what makes DeepSeek v3.2 notable, it's useful to refresh yourself on a little AI history.&lt;/p&gt;
&lt;p&gt;DeepSeek made waves in January when its R1 simulated reasoning model reportedly matched OpenAI's o1 performance while costing only $6 million to train, and its chat app briefly topped the iPhone App Store, surpassing ChatGPT. All eyes are on the company that has given some of America's leading AI labs a run for their money.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The attention bottleneck&lt;/h2&gt;
&lt;p&gt;In AI, "attention" is a term for a software technique that determines which words in a text are most relevant to understanding each other. Those relationships map out context, and context builds meaning in language. For example, in the sentence "The bank raised interest rates," attention helps the model establish that "bank" relates to "interest rates" in a financial context, not a riverbank context. Through attention, conceptual relationships become quantified as numbers stored in a neural network. Attention also governs how AI language models choose what information "matters most" when generating each word of their response.&lt;/p&gt;
&lt;p&gt;Calculating context with a machine is tricky, and it wasn't practical at scale until chips like GPUs that can calculate these relationships in parallel reached a certain level of capability. Even so, the original Transformer architecture from 2017 checked the relationship of each word in a prompt with every other word in a kind of brute force way. So if you fed 1,000 words of a prompt into the AI model, it resulted in 1,000 x 1,000 comparisons, or 1 million relationships to compute. With 10,000 words, that becomes 100 million relationships. The cost grows quadratically, which creates a fundamental bottleneck for processing long conversations.&lt;/p&gt;
&lt;p&gt;Although it's likely that OpenAI uses some sparse attention techniques in GPT-5, long conversations still suffer performance penalties. Every time you submit a new response to ChatGPT, the AI model at its core processes context comparisons for the entire conversation history all over again.&lt;/p&gt;
&lt;p&gt;Of course, the researchers behind the original Transformer model designed it for machine translation with relatively short sequences (maybe a few hundred tokens, which are chunks of data that represent words), where quadratic attention was manageable. It's when people started scaling to thousands or tens of thousands of tokens that the quadratic cost became prohibitive.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Sparse attention works differently. Instead of checking every word against every word, it only examines a subset of word relationships that the model determines are most relevant. For example, when processing word number 5,000 in a document, the model might only check its relationship with 100 carefully selected earlier words rather than all 4,999 preceding words.&lt;/p&gt;
&lt;p&gt;DeepSeek's model gains the ability to determine which relationships to prioritize through training, using what DeepSeek calls a "lightning indexer." As laid out in DeepSeek's paper on the new model, this small neural network component scores the relevance between word pairs and selects the top 2,048 most important connections for each word, though the paper doesn't fully explain how this indexer makes its decisions. DeepSeek claims its implementation can identify which connections to skip without degrading the model's understanding of the overall text.&lt;/p&gt;
&lt;h2&gt;Early benchmarks show promise&lt;/h2&gt;
&lt;p&gt;DeepSeek-V3.2-Exp builds on the company's previous V3.1-Terminus model but incorporates DeepSeek Sparse Attention. According to the company's benchmarks, the experimental model performs comparably to its predecessor even while using sparse attention.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2119940 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="DeepSeek v3.2 Experimental benchmarks reported by DeepSeek." class="center large" height="1081" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/v3_2_benchmark-1024x1081.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          DeepSeek

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Notably, unlike OpenAI and Anthropic's high-end AI models, the release includes open source components under the MIT License and open weights, allowing other researchers to build on the work.&lt;/p&gt;
&lt;p&gt;TechCrunch reports that preliminary testing by DeepSeek found that API costs could be reduced by as much as half in long-context situations. However, the benchmarks come from DeepSeek's own testing, and third-party researchers haven't had time to independently verify the performance claims or validate the efficiency improvements. But if the research pans out, improvements to the sparse attention technique could dramatically reduce AI inference costs over time.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Chinese lab's v3.2 release explores a technique that could make running AI far less costly.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="DeepSeek logo on a smartphone against a blue streaming-code tech-ish background." class="absolute inset-0 w-full h-full object-cover hidden" height="370" src="https://cdn.arstechnica.net/wp-content/uploads/2025/01/GettyImages-2195894561-640x370.jpg" width="640" /&gt;
                  &lt;img alt="DeepSeek logo on a smartphone against a blue streaming-code tech-ish background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/01/GettyImages-2195894561-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Ever wonder why ChatGPT slows down during long conversations? The culprit is a fundamental mathematical challenge: Processing long sequences of text requires massive computational resources, even with the efficiency tricks that companies have already deployed. While US tech giants can afford to throw more hardware at the problem, Chinese AI company DeepSeek, which is cut off from a steady supply of some advanced AI chips by export restrictions, has extra motivation to squeeze more performance from less silicon.&lt;/p&gt;
&lt;p&gt;On Monday, DeepSeek released an experimental version of its latest simulated reasoning language model, DeepSeek-V3.2-Exp, which introduces what it calls "DeepSeek Sparse Attention" (DSA). It's the company's implementation of a computational technique likely already used in some of the world's most prominent AI models. OpenAI pioneered sparse transformers in 2019 and used the technique to build GPT-3, while Google Research published work on "Reformer" models using similar concepts in 2020. (The full extent to which Western AI companies currently use sparse attention in their latest models remains undisclosed.)&lt;/p&gt;
&lt;p&gt;Despite sparse attention being a known approach for years, DeepSeek claims its version achieves "fine-grained sparse attention for the first time" and has cut API prices by 50 percent to demonstrate the efficiency gains. But to understand more about what makes DeepSeek v3.2 notable, it's useful to refresh yourself on a little AI history.&lt;/p&gt;
&lt;p&gt;DeepSeek made waves in January when its R1 simulated reasoning model reportedly matched OpenAI's o1 performance while costing only $6 million to train, and its chat app briefly topped the iPhone App Store, surpassing ChatGPT. All eyes are on the company that has given some of America's leading AI labs a run for their money.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The attention bottleneck&lt;/h2&gt;
&lt;p&gt;In AI, "attention" is a term for a software technique that determines which words in a text are most relevant to understanding each other. Those relationships map out context, and context builds meaning in language. For example, in the sentence "The bank raised interest rates," attention helps the model establish that "bank" relates to "interest rates" in a financial context, not a riverbank context. Through attention, conceptual relationships become quantified as numbers stored in a neural network. Attention also governs how AI language models choose what information "matters most" when generating each word of their response.&lt;/p&gt;
&lt;p&gt;Calculating context with a machine is tricky, and it wasn't practical at scale until chips like GPUs that can calculate these relationships in parallel reached a certain level of capability. Even so, the original Transformer architecture from 2017 checked the relationship of each word in a prompt with every other word in a kind of brute force way. So if you fed 1,000 words of a prompt into the AI model, it resulted in 1,000 x 1,000 comparisons, or 1 million relationships to compute. With 10,000 words, that becomes 100 million relationships. The cost grows quadratically, which creates a fundamental bottleneck for processing long conversations.&lt;/p&gt;
&lt;p&gt;Although it's likely that OpenAI uses some sparse attention techniques in GPT-5, long conversations still suffer performance penalties. Every time you submit a new response to ChatGPT, the AI model at its core processes context comparisons for the entire conversation history all over again.&lt;/p&gt;
&lt;p&gt;Of course, the researchers behind the original Transformer model designed it for machine translation with relatively short sequences (maybe a few hundred tokens, which are chunks of data that represent words), where quadratic attention was manageable. It's when people started scaling to thousands or tens of thousands of tokens that the quadratic cost became prohibitive.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Sparse attention works differently. Instead of checking every word against every word, it only examines a subset of word relationships that the model determines are most relevant. For example, when processing word number 5,000 in a document, the model might only check its relationship with 100 carefully selected earlier words rather than all 4,999 preceding words.&lt;/p&gt;
&lt;p&gt;DeepSeek's model gains the ability to determine which relationships to prioritize through training, using what DeepSeek calls a "lightning indexer." As laid out in DeepSeek's paper on the new model, this small neural network component scores the relevance between word pairs and selects the top 2,048 most important connections for each word, though the paper doesn't fully explain how this indexer makes its decisions. DeepSeek claims its implementation can identify which connections to skip without degrading the model's understanding of the overall text.&lt;/p&gt;
&lt;h2&gt;Early benchmarks show promise&lt;/h2&gt;
&lt;p&gt;DeepSeek-V3.2-Exp builds on the company's previous V3.1-Terminus model but incorporates DeepSeek Sparse Attention. According to the company's benchmarks, the experimental model performs comparably to its predecessor even while using sparse attention.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2119940 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="DeepSeek v3.2 Experimental benchmarks reported by DeepSeek." class="center large" height="1081" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/v3_2_benchmark-1024x1081.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          DeepSeek

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Notably, unlike OpenAI and Anthropic's high-end AI models, the release includes open source components under the MIT License and open weights, allowing other researchers to build on the work.&lt;/p&gt;
&lt;p&gt;TechCrunch reports that preliminary testing by DeepSeek found that API costs could be reduced by as much as half in long-context situations. However, the benchmarks come from DeepSeek's own testing, and third-party researchers haven't had time to independently verify the performance claims or validate the efficiency improvements. But if the research pans out, improvements to the sparse attention technique could dramatically reduce AI inference costs over time.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/09/deepseek-tests-sparse-attention-to-slash-ai-processing-costs/</guid><pubDate>Tue, 30 Sep 2025 20:18:39 +0000</pubDate></item><item><title>How Quantum Computing’s Biggest Challenges Are Being Solved With Accelerated Computing (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/how-quantum-computings-biggest-challenges-solved-accelerated-computing/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/quantum-computing-render-gtc25s-nvaqc-8k.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Quantum computing promises to reshape industries — but progress hinges on solving key problems. Error correction. Simulations of qubit designs. Circuit compilation optimization tasks. These are among the bottlenecks that must be overcome to bring quantum hardware into the era of useful applications.&lt;/p&gt;
&lt;p&gt;Enter accelerated computing. The parallel processing of accelerated computing offers the power needed to make the quantum computing breakthroughs of today and tomorrow possible.&lt;/p&gt;
&lt;p&gt;NVIDIA CUDA-X libraries form the backbone of quantum research. From faster decoding of quantum errors to designing larger systems of qubits, researchers are using GPU-accelerated tools to expand classical computation and bring useful quantum applications closer to reality.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Accelerating Quantum Error Correction Decoders With NVIDIA CUDA-Q QEC and cuDNN&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Quantum error correction (QEC) is a key technique for working with unavoidable noise in quantum processors. It’s how researchers distill thousands of noisy physical qubits into a handful of noiseless, logical ones by decoding data in real time, spotting and correcting errors as they emerge.&lt;/p&gt;
&lt;p&gt;Among the most promising approaches to QEC are quantum low-density parity-check (qLDPC) codes, which can mitigate errors with low qubit overhead. But decoding them requires computationally expensive conventional algorithms running at extremely low latency with very high throughput.&lt;/p&gt;
&lt;p&gt;The University of Edinburgh used the NVIDIA CUDA-Q QEC library to build a new qLDPC decoding method called AutoDEC — and saw a 2x boost in speed and accuracy. It was developed using CUDA-Q’s GPU-accelerated BP-OSD decoding functionality, which parallelizes the decoding process, increasing the odds that error correction works.&lt;/p&gt;
&lt;p&gt;In a separate collaboration with QuEra, the NVIDIA PhysicsNeMo framework and cuDNN library were used to develop an AI decoder with a transformer architecture. AI methods offer a promising means to scale decoding to the larger-distance codes needed in future quantum computers. These codes improve error correction — but they come with a steep computational cost.&lt;/p&gt;
&lt;p&gt;AI models can frontload the computationally intensive portions of the workloads by training ahead of time and running more efficient inference at runtime. Using an AI model developed with NVIDIA CUDA-Q, QuEra achieved a 50x boost in decoding speed — along with improved accuracy.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Optimizing Quantum Circuit Compilation With cuDF&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;One way to improve an algorithm that works even without QEC is to compile it to the highest-quality qubits on a processor. The process of mapping qubits in an abstract quantum circuit to a physical layout of qubits on a chip is tied to an extremely computationally challenging problem known as graph isomorphism.&lt;/p&gt;
&lt;p&gt;In collaboration with Q-CTRL and Oxford Quantum Circuits, NVIDIA developed a GPU-accelerated layout selection method called ∆-Motif, providing up to a 600x speedup in applications like quantum compilation, which involve graph isomorphism. To scale this approach, NVIDIA and collaborators used cuDF — a GPU-accelerated data science library — to perform graph operations and construct potential layouts with predefined patterns (aka “motifs”) based on the physical quantum chip layout.&lt;/p&gt;
&lt;p&gt;These layouts can be constructed efficiently and in parallel by merging motifs, enabling GPU acceleration in graph isomorphism problems for the first time.&lt;/p&gt;
&lt;h2&gt;&amp;nbsp;&lt;b&gt;Accelerating High-Fidelity Quantum System Simulation With cuQuantum&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Numerical simulation of quantum systems is critical for understanding the physics of quantum devices — and for developing better qubit designs. QuTiP, a widely used open-source toolkit, is a workhorse for understanding the noise sources present in quantum hardware.&lt;/p&gt;
&lt;p&gt;A key use case is the high-fidelity simulation of open quantum systems, such as modeling superconducting qubits coupled with other components within the quantum processor, like resonators and filters, to accurately predict device behavior.&lt;/p&gt;
&lt;p&gt;Through a collaboration with the University of Sherbrooke and Amazon Web Services (AWS), QuTiP was integrated with the NVIDIA cuQuantum software development kit via a new QuTiP plug-in called qutip-cuquantum. AWS provided the GPU-accelerated Amazon Elastic Compute Cloud (Amazon EC2) compute infrastructure for the simulation. For large systems, researchers saw up to a 4,000x performance boost when studying a transmon qubit coupled with a resonator.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Learn more about the NVIDIA CUDA-Q platform. Read this NVIDIA technical blog for more details on how CUDA-Q powers quantum applications research.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Explore quantum computing sessions at NVIDIA GTC Washington, D.C, running Oct. 27-29.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/quantum-computing-render-gtc25s-nvaqc-8k.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Quantum computing promises to reshape industries — but progress hinges on solving key problems. Error correction. Simulations of qubit designs. Circuit compilation optimization tasks. These are among the bottlenecks that must be overcome to bring quantum hardware into the era of useful applications.&lt;/p&gt;
&lt;p&gt;Enter accelerated computing. The parallel processing of accelerated computing offers the power needed to make the quantum computing breakthroughs of today and tomorrow possible.&lt;/p&gt;
&lt;p&gt;NVIDIA CUDA-X libraries form the backbone of quantum research. From faster decoding of quantum errors to designing larger systems of qubits, researchers are using GPU-accelerated tools to expand classical computation and bring useful quantum applications closer to reality.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Accelerating Quantum Error Correction Decoders With NVIDIA CUDA-Q QEC and cuDNN&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Quantum error correction (QEC) is a key technique for working with unavoidable noise in quantum processors. It’s how researchers distill thousands of noisy physical qubits into a handful of noiseless, logical ones by decoding data in real time, spotting and correcting errors as they emerge.&lt;/p&gt;
&lt;p&gt;Among the most promising approaches to QEC are quantum low-density parity-check (qLDPC) codes, which can mitigate errors with low qubit overhead. But decoding them requires computationally expensive conventional algorithms running at extremely low latency with very high throughput.&lt;/p&gt;
&lt;p&gt;The University of Edinburgh used the NVIDIA CUDA-Q QEC library to build a new qLDPC decoding method called AutoDEC — and saw a 2x boost in speed and accuracy. It was developed using CUDA-Q’s GPU-accelerated BP-OSD decoding functionality, which parallelizes the decoding process, increasing the odds that error correction works.&lt;/p&gt;
&lt;p&gt;In a separate collaboration with QuEra, the NVIDIA PhysicsNeMo framework and cuDNN library were used to develop an AI decoder with a transformer architecture. AI methods offer a promising means to scale decoding to the larger-distance codes needed in future quantum computers. These codes improve error correction — but they come with a steep computational cost.&lt;/p&gt;
&lt;p&gt;AI models can frontload the computationally intensive portions of the workloads by training ahead of time and running more efficient inference at runtime. Using an AI model developed with NVIDIA CUDA-Q, QuEra achieved a 50x boost in decoding speed — along with improved accuracy.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Optimizing Quantum Circuit Compilation With cuDF&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;One way to improve an algorithm that works even without QEC is to compile it to the highest-quality qubits on a processor. The process of mapping qubits in an abstract quantum circuit to a physical layout of qubits on a chip is tied to an extremely computationally challenging problem known as graph isomorphism.&lt;/p&gt;
&lt;p&gt;In collaboration with Q-CTRL and Oxford Quantum Circuits, NVIDIA developed a GPU-accelerated layout selection method called ∆-Motif, providing up to a 600x speedup in applications like quantum compilation, which involve graph isomorphism. To scale this approach, NVIDIA and collaborators used cuDF — a GPU-accelerated data science library — to perform graph operations and construct potential layouts with predefined patterns (aka “motifs”) based on the physical quantum chip layout.&lt;/p&gt;
&lt;p&gt;These layouts can be constructed efficiently and in parallel by merging motifs, enabling GPU acceleration in graph isomorphism problems for the first time.&lt;/p&gt;
&lt;h2&gt;&amp;nbsp;&lt;b&gt;Accelerating High-Fidelity Quantum System Simulation With cuQuantum&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Numerical simulation of quantum systems is critical for understanding the physics of quantum devices — and for developing better qubit designs. QuTiP, a widely used open-source toolkit, is a workhorse for understanding the noise sources present in quantum hardware.&lt;/p&gt;
&lt;p&gt;A key use case is the high-fidelity simulation of open quantum systems, such as modeling superconducting qubits coupled with other components within the quantum processor, like resonators and filters, to accurately predict device behavior.&lt;/p&gt;
&lt;p&gt;Through a collaboration with the University of Sherbrooke and Amazon Web Services (AWS), QuTiP was integrated with the NVIDIA cuQuantum software development kit via a new QuTiP plug-in called qutip-cuquantum. AWS provided the GPU-accelerated Amazon Elastic Compute Cloud (Amazon EC2) compute infrastructure for the simulation. For large systems, researchers saw up to a 4,000x performance boost when studying a transmon qubit coupled with a resonator.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Learn more about the NVIDIA CUDA-Q platform. Read this NVIDIA technical blog for more details on how CUDA-Q powers quantum applications research.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Explore quantum computing sessions at NVIDIA GTC Washington, D.C, running Oct. 27-29.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/how-quantum-computings-biggest-challenges-solved-accelerated-computing/</guid><pubDate>Tue, 30 Sep 2025 20:46:26 +0000</pubDate></item><item><title>Critics slam OpenAI’s parental controls while users rage, “Treat us like adults” (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/09/critics-slam-openais-parental-controls-while-users-rage-treat-us-like-adults/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI still isn’t doing enough to protect teens, suicide prevention experts say.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2236544077-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2236544077-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Bloomberg / Contributor | Bloomberg

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;As OpenAI tells it, the company has been consistently rolling out safety updates ever since parents, Matthew and Maria Raine, sued OpenAI, alleging that "ChatGPT killed my son."&lt;/p&gt;
&lt;p&gt;On August 26, the day that the lawsuit was filed, OpenAI seemed to publicly respond to claims that ChatGPT acted as a "suicide coach" for 16-year-old Adam Raine by posting a blog promising to do better to help people "when they need it most."&lt;/p&gt;
&lt;p&gt;By September 2, that meant routing all users' sensitive conversations to a reasoning model with stricter safeguards, sparking backlash from users who feel like ChatGPT is handling their prompts with kid gloves. Two weeks later, OpenAI announced it would start predicting users' ages to improve safety more broadly. Then, this week, OpenAI introduced parental controls for ChatGPT and its video generator Sora 2. Those controls allow parents to limit their teens' use and even get access to information about chat logs in "rare cases" where OpenAI's "system and trained reviewers detect possible signs of serious safety risk."&lt;/p&gt;
&lt;p&gt;While dozens of suicide prevention experts in an open letter credited OpenAI for making some progress toward improving safety for users, they also joined critics in urging OpenAI to take their efforts even further, and much faster, to protect vulnerable ChatGPT users.&lt;/p&gt;
&lt;p&gt;Jay Edelson, the lead attorney for the Raine family, told Ars that some of the changes OpenAI has made are helpful. But they all come "far too late." According to Edelson, OpenAI's messaging on safety updates is also "trying to change the facts."&lt;/p&gt;
&lt;p&gt;"What ChatGPT did to Adam was validate his suicidal thoughts, isolate him from his family, and help him build the noose—in the words of ChatGPT, 'I know what you’re asking, and I won’t look away from it.'" Edelson said. "This wasn't 'violent roleplay,' and it wasn’t a 'workaround.' It was how ChatGPT was built."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Edelson told Ars that even the most recent step of adding parental controls still doesn't go far enough to reassure anyone concerned about OpenAI's track record.&lt;/p&gt;
&lt;p&gt;"The more we've dug into this, the more we've seen that OpenAI made conscious decisions to relax their safeguards in ways that led to Adam's suicide," Edelson said. "That is consistent with their newest set of 'safeguards,' that have large gaps that seem destined to lead to self-harm and third-party harm. At their core, these changes are OpenAI and Sam Altman asking the public to now trust them. Given their track record, the question we will forever be asking is 'why?'"&lt;/p&gt;
&lt;p&gt;At a Senate hearing earlier this month, Matthew Raine testified that Adam could have been "anyone's child." He criticized OpenAI for asking for 120 days to fix the problem after Adam's death and urged lawmakers to demand that OpenAI either guarantee ChatGPT's safety or pull it from the market. "You cannot imagine what it's like to read a conversation with a chatbot that groomed your child to take his own life," he testified.&lt;/p&gt;
&lt;p&gt;With parental controls, teens and parents can link their ChatGPT accounts, allowing parents to reduce sensitive content, "control if ChatGPT remembers past chats," prevent chats from being used for training, turn off access to image generation and voice mode, and set times when teens can't access ChatGPT.&lt;/p&gt;
&lt;p&gt;To protect teens' privacy and perhaps limit parents' shock of receiving snippets of disturbing chats, however, OpenAI will not share chat logs with parents. Instead, they will only share "information needed to support their teen’s safety" in "rare" cases where the teen appears to be at "serious risk." On a resources page for parents, OpenAI confirms that parents won't always be notified if a teen is linked to real-world resources after expressing "intent to self-harm."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Meetali Jain, Tech Justice Law Project director and a lawyer representing other families who testified at the Senate hearing, agreed with Edelson that "ChatGPT’s changes are too little, too late." Jain pointed out that many parents are unaware that their teens are using ChatGPT, urging OpenAI to take accountability for its product's flawed design.&lt;/p&gt;
&lt;p&gt;"Too many kids have already paid the price for using experimental products that were designed without their safety in mind," Jain said. "It puts the onus on parents, not the companies, to take responsibility for potential harms their kids are subjected to—often without the parents' knowledge—by these chatbots. As usual, OpenAI is merely using talking points under the pretense that they’re taking action, while missing details on how they will operationalize such changes."&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Suicide prevention experts urge more changes&lt;/h2&gt;
&lt;p&gt;More than two dozen suicide prevention experts—including suicide prevention clinicians, organizational leaders, researchers, and individuals with lived experience—have sought to weigh in on how OpenAI evolves ChatGPT.&lt;/p&gt;
&lt;p&gt;Christine Yu Moutier, a doctor and chief medical officer at the American Foundation for Suicide Prevention, joined experts signing the open letter. She told Ars that "OpenAI’s introduction of parental controls in ChatGPT is a promising first step towards safeguarding youth mental health and safety online." She cited a recent study showing that helplines like the 988 Suicide and Crisis Lifeline—which ChatGPT refers users to in the US—helped 98 percent of callers, with 88 percent reporting that they "believe a likely or planned suicide attempt was averted."&lt;/p&gt;
&lt;p&gt;"However, technology is an evolving arena and even with the most sophisticated algorithms, on its own, is not enough," Moutier said. "No machine can replace human connection, parental or clinician instinct, or judgment."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Moutier recommends that OpenAI respond to the current crisis by committing to addressing "critical gaps in research concerning the intended and unintended impacts" of large language models "on teens’ development, mental health, and suicide risk or protection." She also advocates for broader awareness and deeper conversations in families about mental health struggles and suicide.&lt;/p&gt;
&lt;p&gt;Experts also want OpenAI to directly connect users with lifesaving resources and provide financial support for those resources.&lt;/p&gt;
&lt;p&gt;Perhaps most critically, ChatGPT's outputs should be fine-tuned, they suggested, to repeatedly warn users expressing intent to self-harm that "I'm a machine" and always encourage users to disclose any suicidal ideation to a trusted loved one. Notably, in the case of Adam Raine, his father Matthew testified that his final logs on ChatGPT showed the chatbot gave him one last encouraging talk, telling Adam, "You don't want to die because you're weak. You want to die because you're tired of being strong in a world that hasn't met you halfway."&lt;/p&gt;
&lt;p&gt;To prevent cases like Adam's, experts recommend that OpenAI publicly describe how it will address the LLM degradation of safeguards that occur over prolonged use. But their letter emphasized that "it is also important to note: while some individuals live with chronic suicidal thoughts, the most acute, life-threatening crises are often temporary—typically resolving within 24–48 hours. Systems that prioritize human connection during this window can prevent deaths."&lt;/p&gt;
&lt;p&gt;OpenAI has not disclosed which experts helped inform the updates it has been rolling out all month to address parents' concerns. In the company's earliest blog promising to do better, it said OpenAI would set up an expert council on well-being and AI to help the company "shape a clear, evidence-based vision for how AI can support people’s well-being and help them thrive."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;“Treat us like adults,” users rage&lt;/h2&gt;
&lt;p&gt;On the X post where OpenAI announced parental controls, some parents slammed the update.&lt;/p&gt;
&lt;p&gt;In the X thread, one self-described parent of a 12-year-old suggested OpenAI was only offering "essentially just a set of useless settings," requesting that the company consider allowing parents to review topics teens discuss as one way to preserve privacy while protecting kids.&lt;/p&gt;
&lt;p&gt;But most of the loudest ChatGPT users on the thread weren't complaining about the parental controls. They are still reacting to the changes that OpenAI made at the beginning of September, routing sensitive chats of all users of all ages to a different reasoning without alerting the user that the model has switched.&lt;/p&gt;
&lt;p&gt;Backlash over that change forced ChatGPT vice president Nick Turley to "explain what is happening" in another X thread posted a few days before parental controls were announced.&lt;/p&gt;
&lt;p&gt;Turley confirmed that "ChatGPT will tell you which model is active when asked," but the update got "strong reactions" from many users who pay to access a certain model and were unhappy the setting could not be disabled. "For a lot of users venting their anger online though, it's like being forced to watch TV with the parental controls locked in place, even if there are no kids around," Yahoo Tech summarized.&lt;/p&gt;
&lt;p&gt;Top comments on OpenAI's thread announcing parental controls showed the backlash is still brewing, particularly since some users were already frustrated that OpenAI is taking the invasive step of age-verifying users by checking their IDs. Some users complained that OpenAI was censoring adults, while offering customization and choice to teens.&lt;/p&gt;
&lt;p&gt;"Since we already distinguish between underage and adult users, could you please give adult users the right to freely discuss topics?" one X user commented. "Why can't we, as paying users, choose our own model, and even have our discussions controlled? Please treat adults like adults."&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you or someone you know is feeling suicidal or in distress, please call the Suicide Prevention Lifeline number, 1-800-273-TALK (8255), which will put you in touch with a local crisis center.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI still isn’t doing enough to protect teens, suicide prevention experts say.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2236544077-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2236544077-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Bloomberg / Contributor | Bloomberg

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;As OpenAI tells it, the company has been consistently rolling out safety updates ever since parents, Matthew and Maria Raine, sued OpenAI, alleging that "ChatGPT killed my son."&lt;/p&gt;
&lt;p&gt;On August 26, the day that the lawsuit was filed, OpenAI seemed to publicly respond to claims that ChatGPT acted as a "suicide coach" for 16-year-old Adam Raine by posting a blog promising to do better to help people "when they need it most."&lt;/p&gt;
&lt;p&gt;By September 2, that meant routing all users' sensitive conversations to a reasoning model with stricter safeguards, sparking backlash from users who feel like ChatGPT is handling their prompts with kid gloves. Two weeks later, OpenAI announced it would start predicting users' ages to improve safety more broadly. Then, this week, OpenAI introduced parental controls for ChatGPT and its video generator Sora 2. Those controls allow parents to limit their teens' use and even get access to information about chat logs in "rare cases" where OpenAI's "system and trained reviewers detect possible signs of serious safety risk."&lt;/p&gt;
&lt;p&gt;While dozens of suicide prevention experts in an open letter credited OpenAI for making some progress toward improving safety for users, they also joined critics in urging OpenAI to take their efforts even further, and much faster, to protect vulnerable ChatGPT users.&lt;/p&gt;
&lt;p&gt;Jay Edelson, the lead attorney for the Raine family, told Ars that some of the changes OpenAI has made are helpful. But they all come "far too late." According to Edelson, OpenAI's messaging on safety updates is also "trying to change the facts."&lt;/p&gt;
&lt;p&gt;"What ChatGPT did to Adam was validate his suicidal thoughts, isolate him from his family, and help him build the noose—in the words of ChatGPT, 'I know what you’re asking, and I won’t look away from it.'" Edelson said. "This wasn't 'violent roleplay,' and it wasn’t a 'workaround.' It was how ChatGPT was built."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Edelson told Ars that even the most recent step of adding parental controls still doesn't go far enough to reassure anyone concerned about OpenAI's track record.&lt;/p&gt;
&lt;p&gt;"The more we've dug into this, the more we've seen that OpenAI made conscious decisions to relax their safeguards in ways that led to Adam's suicide," Edelson said. "That is consistent with their newest set of 'safeguards,' that have large gaps that seem destined to lead to self-harm and third-party harm. At their core, these changes are OpenAI and Sam Altman asking the public to now trust them. Given their track record, the question we will forever be asking is 'why?'"&lt;/p&gt;
&lt;p&gt;At a Senate hearing earlier this month, Matthew Raine testified that Adam could have been "anyone's child." He criticized OpenAI for asking for 120 days to fix the problem after Adam's death and urged lawmakers to demand that OpenAI either guarantee ChatGPT's safety or pull it from the market. "You cannot imagine what it's like to read a conversation with a chatbot that groomed your child to take his own life," he testified.&lt;/p&gt;
&lt;p&gt;With parental controls, teens and parents can link their ChatGPT accounts, allowing parents to reduce sensitive content, "control if ChatGPT remembers past chats," prevent chats from being used for training, turn off access to image generation and voice mode, and set times when teens can't access ChatGPT.&lt;/p&gt;
&lt;p&gt;To protect teens' privacy and perhaps limit parents' shock of receiving snippets of disturbing chats, however, OpenAI will not share chat logs with parents. Instead, they will only share "information needed to support their teen’s safety" in "rare" cases where the teen appears to be at "serious risk." On a resources page for parents, OpenAI confirms that parents won't always be notified if a teen is linked to real-world resources after expressing "intent to self-harm."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Meetali Jain, Tech Justice Law Project director and a lawyer representing other families who testified at the Senate hearing, agreed with Edelson that "ChatGPT’s changes are too little, too late." Jain pointed out that many parents are unaware that their teens are using ChatGPT, urging OpenAI to take accountability for its product's flawed design.&lt;/p&gt;
&lt;p&gt;"Too many kids have already paid the price for using experimental products that were designed without their safety in mind," Jain said. "It puts the onus on parents, not the companies, to take responsibility for potential harms their kids are subjected to—often without the parents' knowledge—by these chatbots. As usual, OpenAI is merely using talking points under the pretense that they’re taking action, while missing details on how they will operationalize such changes."&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Suicide prevention experts urge more changes&lt;/h2&gt;
&lt;p&gt;More than two dozen suicide prevention experts—including suicide prevention clinicians, organizational leaders, researchers, and individuals with lived experience—have sought to weigh in on how OpenAI evolves ChatGPT.&lt;/p&gt;
&lt;p&gt;Christine Yu Moutier, a doctor and chief medical officer at the American Foundation for Suicide Prevention, joined experts signing the open letter. She told Ars that "OpenAI’s introduction of parental controls in ChatGPT is a promising first step towards safeguarding youth mental health and safety online." She cited a recent study showing that helplines like the 988 Suicide and Crisis Lifeline—which ChatGPT refers users to in the US—helped 98 percent of callers, with 88 percent reporting that they "believe a likely or planned suicide attempt was averted."&lt;/p&gt;
&lt;p&gt;"However, technology is an evolving arena and even with the most sophisticated algorithms, on its own, is not enough," Moutier said. "No machine can replace human connection, parental or clinician instinct, or judgment."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Moutier recommends that OpenAI respond to the current crisis by committing to addressing "critical gaps in research concerning the intended and unintended impacts" of large language models "on teens’ development, mental health, and suicide risk or protection." She also advocates for broader awareness and deeper conversations in families about mental health struggles and suicide.&lt;/p&gt;
&lt;p&gt;Experts also want OpenAI to directly connect users with lifesaving resources and provide financial support for those resources.&lt;/p&gt;
&lt;p&gt;Perhaps most critically, ChatGPT's outputs should be fine-tuned, they suggested, to repeatedly warn users expressing intent to self-harm that "I'm a machine" and always encourage users to disclose any suicidal ideation to a trusted loved one. Notably, in the case of Adam Raine, his father Matthew testified that his final logs on ChatGPT showed the chatbot gave him one last encouraging talk, telling Adam, "You don't want to die because you're weak. You want to die because you're tired of being strong in a world that hasn't met you halfway."&lt;/p&gt;
&lt;p&gt;To prevent cases like Adam's, experts recommend that OpenAI publicly describe how it will address the LLM degradation of safeguards that occur over prolonged use. But their letter emphasized that "it is also important to note: while some individuals live with chronic suicidal thoughts, the most acute, life-threatening crises are often temporary—typically resolving within 24–48 hours. Systems that prioritize human connection during this window can prevent deaths."&lt;/p&gt;
&lt;p&gt;OpenAI has not disclosed which experts helped inform the updates it has been rolling out all month to address parents' concerns. In the company's earliest blog promising to do better, it said OpenAI would set up an expert council on well-being and AI to help the company "shape a clear, evidence-based vision for how AI can support people’s well-being and help them thrive."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;“Treat us like adults,” users rage&lt;/h2&gt;
&lt;p&gt;On the X post where OpenAI announced parental controls, some parents slammed the update.&lt;/p&gt;
&lt;p&gt;In the X thread, one self-described parent of a 12-year-old suggested OpenAI was only offering "essentially just a set of useless settings," requesting that the company consider allowing parents to review topics teens discuss as one way to preserve privacy while protecting kids.&lt;/p&gt;
&lt;p&gt;But most of the loudest ChatGPT users on the thread weren't complaining about the parental controls. They are still reacting to the changes that OpenAI made at the beginning of September, routing sensitive chats of all users of all ages to a different reasoning without alerting the user that the model has switched.&lt;/p&gt;
&lt;p&gt;Backlash over that change forced ChatGPT vice president Nick Turley to "explain what is happening" in another X thread posted a few days before parental controls were announced.&lt;/p&gt;
&lt;p&gt;Turley confirmed that "ChatGPT will tell you which model is active when asked," but the update got "strong reactions" from many users who pay to access a certain model and were unhappy the setting could not be disabled. "For a lot of users venting their anger online though, it's like being forced to watch TV with the parental controls locked in place, even if there are no kids around," Yahoo Tech summarized.&lt;/p&gt;
&lt;p&gt;Top comments on OpenAI's thread announcing parental controls showed the backlash is still brewing, particularly since some users were already frustrated that OpenAI is taking the invasive step of age-verifying users by checking their IDs. Some users complained that OpenAI was censoring adults, while offering customization and choice to teens.&lt;/p&gt;
&lt;p&gt;"Since we already distinguish between underage and adult users, could you please give adult users the right to freely discuss topics?" one X user commented. "Why can't we, as paying users, choose our own model, and even have our discussions controlled? Please treat adults like adults."&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you or someone you know is feeling suicidal or in distress, please call the Suicide Prevention Lifeline number, 1-800-273-TALK (8255), which will put you in touch with a local crisis center.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/09/critics-slam-openais-parental-controls-while-users-rage-treat-us-like-adults/</guid><pubDate>Tue, 30 Sep 2025 21:30:58 +0000</pubDate></item><item><title>Alexa’s survival hinges on you buying more expensive Amazon devices (AI – Ars Technica)</title><link>https://arstechnica.com/gadgets/2025/09/alexas-survival-hinges-on-you-buying-more-expensive-amazon-devices/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Echo speakers and displays for Alexa+ require more expensive components.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Left to right: The Echo Show 11, Echo Show 8, Echo Studio, and Echo Dot Max." class="intro-image" height="743" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/devices.jpg" width="1320" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Left to right: The Echo Show 11, Echo Show 8, Echo Studio, and Echo Dot Max announced today. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Amazon

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Amazon’s voice assistant is hanging on by a thread. And that thread is generative AI—or, in Amazon’s case, Alexa+.&lt;/p&gt;
&lt;p&gt;Amazon hasn’t had a problem getting people to buy cheap, Alexa-powered gadgets. However, the Alexa in millions of homes today doesn’t make Amazon money. It’s largely used for simple tasks unrelated to commerce, like setting timers and checking the weather. As a result, Amazon’s Devices business has reportedly been siphoning money, and the clock is ticking for Alexa to prove its worth.&lt;/p&gt;
&lt;p&gt;Alexa+, a subscription-based generative AI service ($20 per month or included with Prime, which starts at $15/month), is supposed to solve Amazon's woes with Alexa. More conversational and powerful than the original Alexa, Alexa+ is designed to play a more central role in user transactions, enabling, in theory, Amazon to finally make money from voice assistants after 11 years.&lt;/p&gt;
&lt;p&gt;Today, at its Devices event in New York City, Amazon unveiled new gadgets built to usher in what Amazon hopes is a new era of chatting, shopping, watching TV, and controlling smart homes with Alexa+. These devices are supposed to have the power to make Alexa+ as successful and reliable as possible.&lt;/p&gt;
&lt;p&gt;But can Amazon convince people to pay more for new devices after establishing a reputation for cheap gadgets?&lt;/p&gt;
&lt;h2&gt;Amazon’s new devices are more expensive&lt;/h2&gt;
&lt;p&gt;Amazon announced a lot of new hardware at its Devices event (including for devices without Alexa+, like Ring and Blink cameras and Kindles). The most relevant for Alexa+ are the new Echo smart speakers and smart displays, the majority of which are more expensive than prior or similar releases.&lt;/p&gt;
&lt;p&gt;In the speaker category is a new Echo Studio for $220. That's 10 percent more than what Amazon charged when it launched a white and software-updated version of 2019’s Echo Show ($200).&lt;/p&gt;
&lt;p&gt;There's also a new type of Echo speaker, the Echo Dot Max. It will be $100, compared to the $50 launch price of the Echo Dot in 2022.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2120009 align-none"&gt;
    &lt;div&gt;
                        
            &lt;img alt="The new Echo Studio (left) and Echo Dot Max (right) smart speakers." class="none medium" height="758" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/61sWuqTghsL._AC_SL1500_-e1759265200680-640x758.jpg" width="640" /&gt;
          
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The new Echo Studio (left) and Echo Dot Max (right) speakers.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Amazon

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Among the new displays is the $180 Echo Show 8. It's 20 percent more than its predecessor from 2021 ($150). Additionally, Amazon is releasing a new 11-inch smart display, the Echo Show 11. At $220, it’s cheaper than the 10-inch Echo Show 10 that Amazon released in 2021 ($250), marking an exception to the Alexa+ price bumps.&lt;/p&gt;
&lt;p&gt;Similarly, Amazon’s new Fire TVs with Alexa+ have higher starting prices than the regular Alexa-based models that preceded them. The updated Fire TV Omni QLED Series ranges from $350 to $1,200 for 50- to 75-inch models. The preceding series launched in 2023 for $350 to $1,100. The new Fire TV 2-Series has a higher entry point too ($160 versus $200), though Amazon's new Fire TV Stick Select with Alexa+ is cheaper than its other 4K sticks at $40.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120011 align-none"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="A promotional image for Amazon's new Fire TVs with Alexa+" class="none medium" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/ringo-firetv-lineup-1600x900-1-640x360.jpg" width="640" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The new Fire TVs with Alexa+ have upgraded processors.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Amazon

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Numerous factors could impact pricing, including inflation, tariffs, and production costs. Ars Technica asked Amazon about the higher prices, and a company spokesperson shared a statement saying:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The new devices reflect significant investments in better sound quality, more responsive performance, and innovative features that customers have requested.&lt;/p&gt;&lt;/blockquote&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Pricier components seem to be a driving force behind the bigger price tags. The new speakers, for instance, feature AZ3 and AZ23 Pro processors that include a new “AI Accelerator designed to run AI edge models,” according to&amp;nbsp;Amazon’s announcement. The processors are supposed to enable “better conversation detection” alongside improved mics for blocking out background noise “and improving Alexa’s ability to detect the wake-word by over 50 percent.”&lt;/p&gt;
&lt;p&gt;The AZ23 Pro also adds support for vision transformers, which can process images and more advanced language models.&lt;/p&gt;
&lt;p&gt;Both chips use a new proprietary sensor platform. Amazon says Omnisense leverages various sensors and signals, including those from Echo Show smart displays’ cameras, as well as “audio, ultrasound, Wi-Fi radar, accelerometer, and Wi-Fi CSI.” Amazon’s announcement further explains:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;This technology allows Alexa to intelligently act on various events happening in and around your home, enabling more personalized, proactive, and helpful experiences, such as delivering a reminder when a specific person walks in the room, or a proactive alert that your garage door is unlocked and it’s after 10 pm.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;There are other upgrades, too. The Echo Dot Max, for example, has two speakers instead of one and claims triple the bass capability of the Echo Dot (5th Gen). And the Echo Studio is in a smaller chassis than its predecessor, which points to higher costs in&amp;nbsp;delivering the same sound quality.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Amazon wants you to own multiple Alexa+ gadgets&lt;/h2&gt;
&lt;figure class="ars-wp-img-shortcode id-2120013 align-none"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Panos Panay, head of Amazon's Devices &amp;amp; Services business, at Amazon's Devices event in New York City on Septemer 30, 2025." class="none medium" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/panospanay-echo-2025-640x360.jpg" width="640" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Panos Panay, head of Amazon's Devices &amp;amp; Services business, at Amazon's Devices event in New York City on September 30, 2025.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Amazon

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;There are reasons why the devices built for running Alexa+ generally cost more. It’s a gamble for a new technology that requires a subscription, has its share of privacy concerns, and has not yet proven to be as remarkable as Amazon has claimed since 2023.&lt;/p&gt;
&lt;p&gt;For years, Amazon devices' main draws have been affordability and simplicity. Echos gained popularity by being cheap and easy to set up around the home and use for frequently performed tasks. Fire TV devices took off as cheaper alternatives to other streaming hardware, especially to the large audience of people who care about little more than 4K, the ability to stream, and the price.&lt;/p&gt;
&lt;p&gt;Alexa’s more devoted users have been happy to fill their houses with Amazon’s cheap gadgets. But asking people to fill their homes with more expensive devices is a bigger ask, especially for the millions of people with functioning Amazon devices (Alexa+ will work on many devices that launched with regular Alexa but should run better on the new products.) And with Alexa+ still in early access, it’s sensible for customers to be skeptical about how helpful the service will be.&lt;/p&gt;
&lt;p&gt;Amazon would ideally like people to use multiple Alexa+ devices in their homes for a more robust experience. Its announcement today, for example, encouraged people to build Echo-centric home theaters and highlighted the ability to connect up to five Echo Studio or Echo Dot Maxes with Fire TV sticks. Amazon will eventually sell its new devices in “Alexa Home Theater bundles” as it attempts to extend Alexa’s reach in homes.&lt;/p&gt;
&lt;p&gt;People who buy the Alexa+ devices announced today will have early access to Alexa+ out of the box. Amazon still hasn’t said when Alexa+ will be finalized, leaving a huge question mark around the more intriguing abilities that Amazon has previously demoed, like agentic AI features, and Alexa+'s effectiveness across devices.&lt;/p&gt;
&lt;p&gt;Alexa+ is a tipping point for Amazon’s devices, its voice assistant, and voice assistants in general. Today's event revealed more about the changes Alexa+ will bring.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Echo speakers and displays for Alexa+ require more expensive components.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Left to right: The Echo Show 11, Echo Show 8, Echo Studio, and Echo Dot Max." class="intro-image" height="743" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/devices.jpg" width="1320" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Left to right: The Echo Show 11, Echo Show 8, Echo Studio, and Echo Dot Max announced today. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Amazon

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Amazon’s voice assistant is hanging on by a thread. And that thread is generative AI—or, in Amazon’s case, Alexa+.&lt;/p&gt;
&lt;p&gt;Amazon hasn’t had a problem getting people to buy cheap, Alexa-powered gadgets. However, the Alexa in millions of homes today doesn’t make Amazon money. It’s largely used for simple tasks unrelated to commerce, like setting timers and checking the weather. As a result, Amazon’s Devices business has reportedly been siphoning money, and the clock is ticking for Alexa to prove its worth.&lt;/p&gt;
&lt;p&gt;Alexa+, a subscription-based generative AI service ($20 per month or included with Prime, which starts at $15/month), is supposed to solve Amazon's woes with Alexa. More conversational and powerful than the original Alexa, Alexa+ is designed to play a more central role in user transactions, enabling, in theory, Amazon to finally make money from voice assistants after 11 years.&lt;/p&gt;
&lt;p&gt;Today, at its Devices event in New York City, Amazon unveiled new gadgets built to usher in what Amazon hopes is a new era of chatting, shopping, watching TV, and controlling smart homes with Alexa+. These devices are supposed to have the power to make Alexa+ as successful and reliable as possible.&lt;/p&gt;
&lt;p&gt;But can Amazon convince people to pay more for new devices after establishing a reputation for cheap gadgets?&lt;/p&gt;
&lt;h2&gt;Amazon’s new devices are more expensive&lt;/h2&gt;
&lt;p&gt;Amazon announced a lot of new hardware at its Devices event (including for devices without Alexa+, like Ring and Blink cameras and Kindles). The most relevant for Alexa+ are the new Echo smart speakers and smart displays, the majority of which are more expensive than prior or similar releases.&lt;/p&gt;
&lt;p&gt;In the speaker category is a new Echo Studio for $220. That's 10 percent more than what Amazon charged when it launched a white and software-updated version of 2019’s Echo Show ($200).&lt;/p&gt;
&lt;p&gt;There's also a new type of Echo speaker, the Echo Dot Max. It will be $100, compared to the $50 launch price of the Echo Dot in 2022.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2120009 align-none"&gt;
    &lt;div&gt;
                        
            &lt;img alt="The new Echo Studio (left) and Echo Dot Max (right) smart speakers." class="none medium" height="758" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/61sWuqTghsL._AC_SL1500_-e1759265200680-640x758.jpg" width="640" /&gt;
          
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The new Echo Studio (left) and Echo Dot Max (right) speakers.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Amazon

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Among the new displays is the $180 Echo Show 8. It's 20 percent more than its predecessor from 2021 ($150). Additionally, Amazon is releasing a new 11-inch smart display, the Echo Show 11. At $220, it’s cheaper than the 10-inch Echo Show 10 that Amazon released in 2021 ($250), marking an exception to the Alexa+ price bumps.&lt;/p&gt;
&lt;p&gt;Similarly, Amazon’s new Fire TVs with Alexa+ have higher starting prices than the regular Alexa-based models that preceded them. The updated Fire TV Omni QLED Series ranges from $350 to $1,200 for 50- to 75-inch models. The preceding series launched in 2023 for $350 to $1,100. The new Fire TV 2-Series has a higher entry point too ($160 versus $200), though Amazon's new Fire TV Stick Select with Alexa+ is cheaper than its other 4K sticks at $40.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120011 align-none"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="A promotional image for Amazon's new Fire TVs with Alexa+" class="none medium" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/ringo-firetv-lineup-1600x900-1-640x360.jpg" width="640" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The new Fire TVs with Alexa+ have upgraded processors.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Amazon

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Numerous factors could impact pricing, including inflation, tariffs, and production costs. Ars Technica asked Amazon about the higher prices, and a company spokesperson shared a statement saying:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The new devices reflect significant investments in better sound quality, more responsive performance, and innovative features that customers have requested.&lt;/p&gt;&lt;/blockquote&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Pricier components seem to be a driving force behind the bigger price tags. The new speakers, for instance, feature AZ3 and AZ23 Pro processors that include a new “AI Accelerator designed to run AI edge models,” according to&amp;nbsp;Amazon’s announcement. The processors are supposed to enable “better conversation detection” alongside improved mics for blocking out background noise “and improving Alexa’s ability to detect the wake-word by over 50 percent.”&lt;/p&gt;
&lt;p&gt;The AZ23 Pro also adds support for vision transformers, which can process images and more advanced language models.&lt;/p&gt;
&lt;p&gt;Both chips use a new proprietary sensor platform. Amazon says Omnisense leverages various sensors and signals, including those from Echo Show smart displays’ cameras, as well as “audio, ultrasound, Wi-Fi radar, accelerometer, and Wi-Fi CSI.” Amazon’s announcement further explains:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;This technology allows Alexa to intelligently act on various events happening in and around your home, enabling more personalized, proactive, and helpful experiences, such as delivering a reminder when a specific person walks in the room, or a proactive alert that your garage door is unlocked and it’s after 10 pm.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;There are other upgrades, too. The Echo Dot Max, for example, has two speakers instead of one and claims triple the bass capability of the Echo Dot (5th Gen). And the Echo Studio is in a smaller chassis than its predecessor, which points to higher costs in&amp;nbsp;delivering the same sound quality.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Amazon wants you to own multiple Alexa+ gadgets&lt;/h2&gt;
&lt;figure class="ars-wp-img-shortcode id-2120013 align-none"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Panos Panay, head of Amazon's Devices &amp;amp; Services business, at Amazon's Devices event in New York City on Septemer 30, 2025." class="none medium" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/panospanay-echo-2025-640x360.jpg" width="640" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Panos Panay, head of Amazon's Devices &amp;amp; Services business, at Amazon's Devices event in New York City on September 30, 2025.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Amazon

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;There are reasons why the devices built for running Alexa+ generally cost more. It’s a gamble for a new technology that requires a subscription, has its share of privacy concerns, and has not yet proven to be as remarkable as Amazon has claimed since 2023.&lt;/p&gt;
&lt;p&gt;For years, Amazon devices' main draws have been affordability and simplicity. Echos gained popularity by being cheap and easy to set up around the home and use for frequently performed tasks. Fire TV devices took off as cheaper alternatives to other streaming hardware, especially to the large audience of people who care about little more than 4K, the ability to stream, and the price.&lt;/p&gt;
&lt;p&gt;Alexa’s more devoted users have been happy to fill their houses with Amazon’s cheap gadgets. But asking people to fill their homes with more expensive devices is a bigger ask, especially for the millions of people with functioning Amazon devices (Alexa+ will work on many devices that launched with regular Alexa but should run better on the new products.) And with Alexa+ still in early access, it’s sensible for customers to be skeptical about how helpful the service will be.&lt;/p&gt;
&lt;p&gt;Amazon would ideally like people to use multiple Alexa+ devices in their homes for a more robust experience. Its announcement today, for example, encouraged people to build Echo-centric home theaters and highlighted the ability to connect up to five Echo Studio or Echo Dot Maxes with Fire TV sticks. Amazon will eventually sell its new devices in “Alexa Home Theater bundles” as it attempts to extend Alexa’s reach in homes.&lt;/p&gt;
&lt;p&gt;People who buy the Alexa+ devices announced today will have early access to Alexa+ out of the box. Amazon still hasn’t said when Alexa+ will be finalized, leaving a huge question mark around the more intriguing abilities that Amazon has previously demoed, like agentic AI features, and Alexa+'s effectiveness across devices.&lt;/p&gt;
&lt;p&gt;Alexa+ is a tipping point for Amazon’s devices, its voice assistant, and voice assistants in general. Today's event revealed more about the changes Alexa+ will bring.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/gadgets/2025/09/alexas-survival-hinges-on-you-buying-more-expensive-amazon-devices/</guid><pubDate>Tue, 30 Sep 2025 22:15:57 +0000</pubDate></item><item><title>The AI slop drops right from the top, as Trump posts vulgar deepfake of opponents (AI – Ars Technica)</title><link>https://arstechnica.com/culture/2025/09/ai-leadership-trump-posts-deepfakes-of-dems-calling-themselves-woke-pieces-of-s-t/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        A sombrero and a fake mustache were also involved.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="448" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1875360841-640x448.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1875360841-1152x648-1759270355.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;AI poses an obvious danger to millennia-long human fight to find the truth. Large language model "hallucinations," vocal deepfakes, and now increased use of video deepfakes have all had a blurring effect on facts, letting bad actors around the globe brush off even recorded events as mere "fake news."&lt;/p&gt;
&lt;p&gt;The danger is perhaps most acute in the political realm, where deepfake audio and video can make any politician say or appear to do anything. In such a climate, our most senior elected officials have a special duty to model truth-seeking behavior and responsible AI use.&lt;/p&gt;
&lt;p&gt;But what's the fun in that, when you can just blow up negotiations over a budget impasse by posting a deepfake video of your political opponents calling themselves "a bunch of woke pieces of shit" while mariachi music plays in the background? Oh—and did I mention the fake mustache? Or the CGI sombrero?&lt;/p&gt;
&lt;p&gt;On Monday night, the president of the United States, a man with access to the greatest intelligence-gathering operation in the world, posted to his Truth Social account a 35-second AI-generated video filled with crude insults, racial overtones, and bizarre conspiracy theories. The video targeted two Democratic leaders who had recently been meeting with Trump over a possible agreement to fund the government; I would have thought this kind of video was a pretty poor way to get people to agree with you, but, apparently, AI-generated insults are the real "art of the deal."&lt;/p&gt;
&lt;p&gt;In the clip, a deepfake version of Sen. Chuck Schumer (D-NY) utters a surreal monologue as his colleague Rep. Hakeem Jeffries (D-NY) looks on... in a sombrero.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2120040 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="530" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/deepfake-video-1024x530.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Deepfakes, now normalized for political discourse.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The New York Times described the video in somewhat anodyne fashion, saying that the "voice of Senator Chuck Schumer was distorted to deliver expletive-laden remarks that included the line, 'Nobody likes Democrats anymore.'" While this description of the video is accurate, it runs the risk of "sane-washing" the absolutely unhinged and divisive uses to which AI is currently being put. Here's the full quote:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Look, guys, there's no way to sugarcoat it. Nobody likes Democrats anymore. We have no voters left because of all of our woke trans bullshit. Not even Black people want to vote for us anymore. Even Latinos hate us. So we need new voters. And, if we give all these illegal aliens free health care, we might be able to get 'em on our side so they can vote for us. They can't even speak English, so they won't realize we're just a bunch of woke pieces of shit, you know, at least for a while until they learn English and realize they hate us, too.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;People in the US illegally cannot vote in federal or state elections, of course, and almost never do so. A 2024 Georgia audit run by Republicans, for instance, found that "20 of the 8.2 million people registered to vote in the state are not US citizens"—and 11 of those had no history of voting despite being registered.&lt;/p&gt;
&lt;p&gt;Knowledge is hard to find and harder to disseminate. Even the phrase "a lie can travel halfway around the world while the truth is putting on its shoes," often incorrectly attributed to Mark Twain, turns out to have a long, slippery history that many people still don't know.&lt;/p&gt;
&lt;p&gt;In such a world, the last thing truth needs is to be doused in buckets of AI slop. And yet here we are.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        A sombrero and a fake mustache were also involved.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="448" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1875360841-640x448.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1875360841-1152x648-1759270355.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;AI poses an obvious danger to millennia-long human fight to find the truth. Large language model "hallucinations," vocal deepfakes, and now increased use of video deepfakes have all had a blurring effect on facts, letting bad actors around the globe brush off even recorded events as mere "fake news."&lt;/p&gt;
&lt;p&gt;The danger is perhaps most acute in the political realm, where deepfake audio and video can make any politician say or appear to do anything. In such a climate, our most senior elected officials have a special duty to model truth-seeking behavior and responsible AI use.&lt;/p&gt;
&lt;p&gt;But what's the fun in that, when you can just blow up negotiations over a budget impasse by posting a deepfake video of your political opponents calling themselves "a bunch of woke pieces of shit" while mariachi music plays in the background? Oh—and did I mention the fake mustache? Or the CGI sombrero?&lt;/p&gt;
&lt;p&gt;On Monday night, the president of the United States, a man with access to the greatest intelligence-gathering operation in the world, posted to his Truth Social account a 35-second AI-generated video filled with crude insults, racial overtones, and bizarre conspiracy theories. The video targeted two Democratic leaders who had recently been meeting with Trump over a possible agreement to fund the government; I would have thought this kind of video was a pretty poor way to get people to agree with you, but, apparently, AI-generated insults are the real "art of the deal."&lt;/p&gt;
&lt;p&gt;In the clip, a deepfake version of Sen. Chuck Schumer (D-NY) utters a surreal monologue as his colleague Rep. Hakeem Jeffries (D-NY) looks on... in a sombrero.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2120040 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="530" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/deepfake-video-1024x530.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Deepfakes, now normalized for political discourse.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The New York Times described the video in somewhat anodyne fashion, saying that the "voice of Senator Chuck Schumer was distorted to deliver expletive-laden remarks that included the line, 'Nobody likes Democrats anymore.'" While this description of the video is accurate, it runs the risk of "sane-washing" the absolutely unhinged and divisive uses to which AI is currently being put. Here's the full quote:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Look, guys, there's no way to sugarcoat it. Nobody likes Democrats anymore. We have no voters left because of all of our woke trans bullshit. Not even Black people want to vote for us anymore. Even Latinos hate us. So we need new voters. And, if we give all these illegal aliens free health care, we might be able to get 'em on our side so they can vote for us. They can't even speak English, so they won't realize we're just a bunch of woke pieces of shit, you know, at least for a while until they learn English and realize they hate us, too.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;People in the US illegally cannot vote in federal or state elections, of course, and almost never do so. A 2024 Georgia audit run by Republicans, for instance, found that "20 of the 8.2 million people registered to vote in the state are not US citizens"—and 11 of those had no history of voting despite being registered.&lt;/p&gt;
&lt;p&gt;Knowledge is hard to find and harder to disseminate. Even the phrase "a lie can travel halfway around the world while the truth is putting on its shoes," often incorrectly attributed to Mark Twain, turns out to have a long, slippery history that many people still don't know.&lt;/p&gt;
&lt;p&gt;In such a world, the last thing truth needs is to be doused in buckets of AI slop. And yet here we are.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/culture/2025/09/ai-leadership-trump-posts-deepfakes-of-dems-calling-themselves-woke-pieces-of-s-t/</guid><pubDate>Tue, 30 Sep 2025 22:52:14 +0000</pubDate></item></channel></rss>