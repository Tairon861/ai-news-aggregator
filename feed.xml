<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 09 Jan 2026 18:36:42 +0000</lastBuildDate><item><title>Anthropic adds Allianz to growing list of enterprise wins (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/09/anthropic-adds-allianz-to-growing-list-of-enterprise-wins/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-02-at-12.22.37PM.png?resize=1200,671" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI research lab Anthropic continues to land sizable enterprise deals. Its latest entails bringing its large language models to a legacy German insurance giant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic on Friday announced a deal with Munich, Germany-based global insurance conglomerate Allianz to bring “responsible AI” to the insurance industry. The parties declined to share financial terms of the deal.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The partnership is made up of three specific initiatives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first is making Claude Code, Anthropic’s AI-powered coding tool, available to all of Allianz’s employees. Anthropic and Allianz will also build custom AI agents for Allianz employees that can execute multistep workflows with a human in the loop.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This partnership also includes an AI system that logs all AI interactions to keep the AI transparent and ensure that information is readily available for regulatory or other needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With this partnership, Allianz is taking a decisive step to address critical AI challenges in insurance,” Oliver Bäte, CEO of Allianz SE, said in the company’s press release. “Anthropic’s focus on safety and transparency complements our strong dedication to customer excellence and stakeholder trust. Together, we are building solutions that prioritize what matters most to our customers while setting new standards for innovation and resilience.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is just the latest enterprise deal Anthropic has landed in recent months.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In December, the company inked a $200 million deal to bring its AI models to data cloud company Snowflake and its customers. Shortly after, it announced a multi-year partnership with the consulting firm Accenture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In October, it signed a deal with consulting firm Deloitte to bring its Claude chatbot to the firm’s 500,000 employees. That same month, Anthropic signed a deal with IBM to bring its AI models into the latter’s products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The race for AI enterprise dominance is clearly on, and Anthropic appears to be winning — so far at least.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anthropic holds 40% of enterprise AI market share, according to a December survey from Anthropic investor Menlo Ventures, and 54% of the market share for AI coding. Anthropic market share increased throughout last year. When Menlo’s original survey came out in July, the company held a 32% market share for overall enterprise LLM use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google launched its dedicated enterprise AI product, Gemini Enterprise, in October. At the time, the company touted that the product suite already had customers, including fintech Klarna, design software company Figma, and cruise line operator Virgin Voyages, among others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI launched its enterprise version of ChatGPT, ChatGPT Enterprise, in 2023. Recently, the company reportedly expressed deep concern in an internal memo that Google Gemini’s success was starting to encroach on its business. Shortly after, the company released a report that said enterprise use of ChatGPT had surged 8x in the past year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A recent TechCrunch investor survey found that enterprise-focused VCs overwhelmingly think that 2026 will be the year that enterprises start to see a meaningful return on their investment into AI products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Anthropic seems to be a clear favorite at the moment, this year will likely be telling of what the enterprise AI market — and its competitive landscape — will look like in the future.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-02-at-12.22.37PM.png?resize=1200,671" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI research lab Anthropic continues to land sizable enterprise deals. Its latest entails bringing its large language models to a legacy German insurance giant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic on Friday announced a deal with Munich, Germany-based global insurance conglomerate Allianz to bring “responsible AI” to the insurance industry. The parties declined to share financial terms of the deal.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The partnership is made up of three specific initiatives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first is making Claude Code, Anthropic’s AI-powered coding tool, available to all of Allianz’s employees. Anthropic and Allianz will also build custom AI agents for Allianz employees that can execute multistep workflows with a human in the loop.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This partnership also includes an AI system that logs all AI interactions to keep the AI transparent and ensure that information is readily available for regulatory or other needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With this partnership, Allianz is taking a decisive step to address critical AI challenges in insurance,” Oliver Bäte, CEO of Allianz SE, said in the company’s press release. “Anthropic’s focus on safety and transparency complements our strong dedication to customer excellence and stakeholder trust. Together, we are building solutions that prioritize what matters most to our customers while setting new standards for innovation and resilience.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is just the latest enterprise deal Anthropic has landed in recent months.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In December, the company inked a $200 million deal to bring its AI models to data cloud company Snowflake and its customers. Shortly after, it announced a multi-year partnership with the consulting firm Accenture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In October, it signed a deal with consulting firm Deloitte to bring its Claude chatbot to the firm’s 500,000 employees. That same month, Anthropic signed a deal with IBM to bring its AI models into the latter’s products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The race for AI enterprise dominance is clearly on, and Anthropic appears to be winning — so far at least.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anthropic holds 40% of enterprise AI market share, according to a December survey from Anthropic investor Menlo Ventures, and 54% of the market share for AI coding. Anthropic market share increased throughout last year. When Menlo’s original survey came out in July, the company held a 32% market share for overall enterprise LLM use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google launched its dedicated enterprise AI product, Gemini Enterprise, in October. At the time, the company touted that the product suite already had customers, including fintech Klarna, design software company Figma, and cruise line operator Virgin Voyages, among others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI launched its enterprise version of ChatGPT, ChatGPT Enterprise, in 2023. Recently, the company reportedly expressed deep concern in an internal memo that Google Gemini’s success was starting to encroach on its business. Shortly after, the company released a report that said enterprise use of ChatGPT had surged 8x in the past year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A recent TechCrunch investor survey found that enterprise-focused VCs overwhelmingly think that 2026 will be the year that enterprises start to see a meaningful return on their investment into AI products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Anthropic seems to be a clear favorite at the moment, this year will likely be telling of what the enterprise AI market — and its competitive landscape — will look like in the future.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/09/anthropic-adds-allianz-to-growing-list-of-enterprise-wins/</guid><pubDate>Fri, 09 Jan 2026 09:00:00 +0000</pubDate></item><item><title>A new CRISPR startup is betting regulators will ease up on gene-editing (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/09/1130945/crispr-startup-aurora-betting-regulation-pku/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/260108_GeneEditing_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Here at &lt;em&gt;MIT&lt;/em&gt; &lt;em&gt;Technology Review&lt;/em&gt; we’ve been writing about the gene-editing technology CRISPR since 2013, calling it the biggest biotech breakthrough of the century. Yet so far, there’s been only one gene-editing drug approved. It’s been used commercially on only about 40 patients, all with sickle-cell disease.&lt;/p&gt;  &lt;p&gt;It’s becoming clear that the impact of CRISPR isn’t as big as we all hoped. In fact, there’s a pall of discouragement over the entire field—with some journalists saying the gene-editing revolution has “lost its mojo.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;So what will it take for CRISPR to help more people? A new startup says the answer could be an “umbrella approach” to testing and commercializing treatments. Aurora Therapeutics, which has $16 million from Menlo Ventures and counts CRISPR co-inventor Jennifer Doudna as an advisor, essentially hopes to win approval for gene-editing drugs that can be slightly adjusted, or personalized, without requiring costly new trials or approvals for every new version.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The need to change regulations around gene-editing treatments was endorsed in November by the head of the US Food and Drug Administration, Martin Makary, who said the agency would open a “new” regulatory pathway for “bespoke, personalized therapies” that can’t easily be tested in conventional ways.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Aurora’s first target, the rare inherited disease phenylketonuria, also known as PKU, is a case in point. People with PKU lack a working version of an enzyme needed to use up the amino acid phenylalanine, a component of pretty much all meat and protein. If the amino acid builds up, it causes brain damage. So patients usually go on an onerous “diet for life” of special formula drinks and vegetables.&lt;/p&gt;  &lt;p&gt;In theory, gene editing can fix PKU. In mice, scientists have already restored the gene for the enzyme by rewriting DNA in liver cells, which both make the enzyme and are some of the easiest to reach with a gene-editing drug. The problem is that in human patients, many different mutations can affect the critical gene. According to Cory Harding, a researcher at Oregon Health Sciences University, scientists know about 1,600 different DNA mutations that cause PKU.&lt;/p&gt; 
 &lt;p&gt;There’s no way anyone will develop 1,600 different gene-editing drugs. Instead, Aurora’s goal is to eventually win approval for a single gene editor that, with minor adjustments, could be used to correct several of the most common mutations, including one that’s responsible for about 10% of the estimated 20,000 PKU cases in the US.&lt;/p&gt;  &lt;p&gt;“We can’t have a separate clinical trial for each mutation,” says Edward Kaye, the CEO of Aurora. “The way the FDA approves gene editing has to change, and I think they’ve been very understanding that is the case.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;A gene editor is a special protein that can zero in on a specific location in the genome and change it. To prepare one, Aurora will put genetic code for the editor into a nanoparticle along with a targeting molecule. In total, it will involve about 5,000 gene letters. But only 20 of them need to change in order to redirect the treatment to repair a different mutation.&lt;/p&gt;  &lt;p&gt;“Over 99% of the drug stays the same,” says Johnny Hu, a partner at Menlo Ventures, which put up the funding for the startup.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;The new company came together after Hu met over pizza with Fyodor Urnov, an outspoken gene-editing scientist at the University of California, Berkeley, who is Aurora’s cofounder and sits on its board.&lt;/p&gt;  &lt;p&gt;In 2022, Urnov had written a &lt;em&gt;New York Times&lt;/em&gt; editorial bemoaning the “chasm” between what editing technology can do and the “legal, financial, and organizational” realities preventing researchers from curing people.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;“I went to Fyodor and said, ‘Hey, we’re getting all these great results in the clinic with CRISPR, but why hasn’t it scaled?” says Hu. Part of the reason is that most gene-editing companies are chasing the same few conditions, such as sickle-cell, where (as luck would have it) a single edit works for all patients. But that leaves around 400 million people who have 7,000 other inherited conditions without much hope to get their DNA fixed, Urnov estimated in his editorial.&lt;/p&gt;  &lt;p&gt;Then, last May, came the dramatic demonstration of the first fully “personalized” gene-editing treatment. A team in Philadelphia, assisted by Urnov and others, succeeded in correcting the DNA of a baby, named KJ Muldoon, who had an entirely unique mutation that caused a metabolic disease. Though it didn’t target PKU, the project showed that gene editing could theoretically fix some inherited diseases “on demand.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;It also underscored a big problem. Treating a single child required a large team and cost millions in time, effort, and materials—all to create a drug that would never be used again.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That’s exactly the sort of situation the new “umbrella” trials are supposed to address. Kiran Musunuru, who co-led the team at the University of Pennsylvania, says he’s been in discussions with the FDA to open a study of bespoke gene editors this year focusing on diseases of the type Baby KJ had, called urea cycle disorders. Each time a new patient appears, he says, they’ll try to quickly put together a variant of their gene-editing drug that’s tuned to fix that child’s particular genetic problem.&lt;/p&gt;  &lt;p&gt;Musunuru, who isn’t involved with Aurora, does not think the company’s plans for PKU count as fully personalized editors. “These corporate PKU efforts have nothing whatsoever to do with Baby KJ,” he says. He says his center continues to focus on mutations “so ultra-rare that we don’t see any scenario where a for-profit gene-editing company would find that indication to be commercially viable.”&lt;/p&gt;  &lt;p&gt;Instead, what’s occurring in PKU, says Musunuru, is that researchers have realized they can assemble “a bunch” of the most frequent mutations “into a large enough group of patients to make a platform PKU therapy commercially viable.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;While that would still leave out many patients with extra-rare gene errors, Musunuru says any gene-editing treatment at all would still be “a big improvement over the status quo, which&amp;nbsp; is zero genetic therapies for PKU.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/260108_GeneEditing_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Here at &lt;em&gt;MIT&lt;/em&gt; &lt;em&gt;Technology Review&lt;/em&gt; we’ve been writing about the gene-editing technology CRISPR since 2013, calling it the biggest biotech breakthrough of the century. Yet so far, there’s been only one gene-editing drug approved. It’s been used commercially on only about 40 patients, all with sickle-cell disease.&lt;/p&gt;  &lt;p&gt;It’s becoming clear that the impact of CRISPR isn’t as big as we all hoped. In fact, there’s a pall of discouragement over the entire field—with some journalists saying the gene-editing revolution has “lost its mojo.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;So what will it take for CRISPR to help more people? A new startup says the answer could be an “umbrella approach” to testing and commercializing treatments. Aurora Therapeutics, which has $16 million from Menlo Ventures and counts CRISPR co-inventor Jennifer Doudna as an advisor, essentially hopes to win approval for gene-editing drugs that can be slightly adjusted, or personalized, without requiring costly new trials or approvals for every new version.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The need to change regulations around gene-editing treatments was endorsed in November by the head of the US Food and Drug Administration, Martin Makary, who said the agency would open a “new” regulatory pathway for “bespoke, personalized therapies” that can’t easily be tested in conventional ways.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Aurora’s first target, the rare inherited disease phenylketonuria, also known as PKU, is a case in point. People with PKU lack a working version of an enzyme needed to use up the amino acid phenylalanine, a component of pretty much all meat and protein. If the amino acid builds up, it causes brain damage. So patients usually go on an onerous “diet for life” of special formula drinks and vegetables.&lt;/p&gt;  &lt;p&gt;In theory, gene editing can fix PKU. In mice, scientists have already restored the gene for the enzyme by rewriting DNA in liver cells, which both make the enzyme and are some of the easiest to reach with a gene-editing drug. The problem is that in human patients, many different mutations can affect the critical gene. According to Cory Harding, a researcher at Oregon Health Sciences University, scientists know about 1,600 different DNA mutations that cause PKU.&lt;/p&gt; 
 &lt;p&gt;There’s no way anyone will develop 1,600 different gene-editing drugs. Instead, Aurora’s goal is to eventually win approval for a single gene editor that, with minor adjustments, could be used to correct several of the most common mutations, including one that’s responsible for about 10% of the estimated 20,000 PKU cases in the US.&lt;/p&gt;  &lt;p&gt;“We can’t have a separate clinical trial for each mutation,” says Edward Kaye, the CEO of Aurora. “The way the FDA approves gene editing has to change, and I think they’ve been very understanding that is the case.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;A gene editor is a special protein that can zero in on a specific location in the genome and change it. To prepare one, Aurora will put genetic code for the editor into a nanoparticle along with a targeting molecule. In total, it will involve about 5,000 gene letters. But only 20 of them need to change in order to redirect the treatment to repair a different mutation.&lt;/p&gt;  &lt;p&gt;“Over 99% of the drug stays the same,” says Johnny Hu, a partner at Menlo Ventures, which put up the funding for the startup.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;The new company came together after Hu met over pizza with Fyodor Urnov, an outspoken gene-editing scientist at the University of California, Berkeley, who is Aurora’s cofounder and sits on its board.&lt;/p&gt;  &lt;p&gt;In 2022, Urnov had written a &lt;em&gt;New York Times&lt;/em&gt; editorial bemoaning the “chasm” between what editing technology can do and the “legal, financial, and organizational” realities preventing researchers from curing people.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;“I went to Fyodor and said, ‘Hey, we’re getting all these great results in the clinic with CRISPR, but why hasn’t it scaled?” says Hu. Part of the reason is that most gene-editing companies are chasing the same few conditions, such as sickle-cell, where (as luck would have it) a single edit works for all patients. But that leaves around 400 million people who have 7,000 other inherited conditions without much hope to get their DNA fixed, Urnov estimated in his editorial.&lt;/p&gt;  &lt;p&gt;Then, last May, came the dramatic demonstration of the first fully “personalized” gene-editing treatment. A team in Philadelphia, assisted by Urnov and others, succeeded in correcting the DNA of a baby, named KJ Muldoon, who had an entirely unique mutation that caused a metabolic disease. Though it didn’t target PKU, the project showed that gene editing could theoretically fix some inherited diseases “on demand.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;It also underscored a big problem. Treating a single child required a large team and cost millions in time, effort, and materials—all to create a drug that would never be used again.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That’s exactly the sort of situation the new “umbrella” trials are supposed to address. Kiran Musunuru, who co-led the team at the University of Pennsylvania, says he’s been in discussions with the FDA to open a study of bespoke gene editors this year focusing on diseases of the type Baby KJ had, called urea cycle disorders. Each time a new patient appears, he says, they’ll try to quickly put together a variant of their gene-editing drug that’s tuned to fix that child’s particular genetic problem.&lt;/p&gt;  &lt;p&gt;Musunuru, who isn’t involved with Aurora, does not think the company’s plans for PKU count as fully personalized editors. “These corporate PKU efforts have nothing whatsoever to do with Baby KJ,” he says. He says his center continues to focus on mutations “so ultra-rare that we don’t see any scenario where a for-profit gene-editing company would find that indication to be commercially viable.”&lt;/p&gt;  &lt;p&gt;Instead, what’s occurring in PKU, says Musunuru, is that researchers have realized they can assemble “a bunch” of the most frequent mutations “into a large enough group of patients to make a platform PKU therapy commercially viable.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;While that would still leave out many patients with extra-rare gene errors, Musunuru says any gene-editing treatment at all would still be “a big improvement over the status quo, which&amp;nbsp; is zero genetic therapies for PKU.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/09/1130945/crispr-startup-aurora-betting-regulation-pku/</guid><pubDate>Fri, 09 Jan 2026 11:00:00 +0000</pubDate></item><item><title>[NEW] From cloud to factory – humanoid robots coming to workplaces (AI News)</title><link>https://www.artificialintelligence-news.com/news/from-cloud-to-factory-humanoid-robots-coming-to-workplaces/</link><description>&lt;p&gt; The partnership announced this week between Microsoft and Hexagon Robotics marks an inflection point in the commercialisation of humanoid, AI-powered robots for industrial environments. The two companies will combine Microsoft’s cloud and AI infrastructure with Hexagon’s expertise in robotics, sensors, and spatial intelligence to advance the deployment of physical AI systems in real-world settings.&lt;/p&gt;&lt;p&gt; At the centre of the collaboration is AEON, Hexagon’s industrial humanoid robot, a device designed to operate autonomously in environments like factories, logistics hubs, engineering plants, and inspection sites.&lt;/p&gt;&lt;p&gt; The partnership will focus on multimodal AI training, imitation learning, real-time data management, and integration with existing industrial systems. Initial target sectors include automotive, aerospace, manufacturing, and logistics, the companies say. It’s in these industries where labour shortages and operational complexity are already constraining financial growth.&lt;/p&gt;&lt;p&gt; The announcement is the sign of a maturing ecosystem: cloud platforms, physical AI, and robotics engineering’s convergence, making humanoid automation commercially viable.&lt;/p&gt;&lt;h3&gt;Humanoid robots out of the research lab&lt;/h3&gt;&lt;p&gt; While humanoid robots have been  the subject of work at research institutions, demonstrated proudly at technology events, the last five years have seen a move to practical deployment in real-world, working environments. The main change has been the combination of improved perception, advances in reinforcement and imitation learning, and the availability of scalable cloud infrastructure.&lt;/p&gt;&lt;p&gt; One of the most visible examples is Agility Robotics’ Digit, a bipedal humanoid robot designed for logistics and warehouse operations. Digit has been piloted in live environments by companies like Amazon, where it performs material-handling tasks including tote movement and last-metre logistics. Such deployments tend to focus on augmenting human workers rather than replacing them, with Digit handling more physically demanding tasks.&lt;/p&gt;&lt;p&gt; Similarly, Tesla’s Optimus programme has moved out of the phase where concept videos were all that existed, and is now undergoing factory trials. Optimus robots are being tested on structured tasks like part handling and equipment transport inside Tesla’s automotive manufacturing facilities. While still limited in scope, these pilots demonstrate the pattern of humanoid-like machines chosen over less anthropomorphic form-factors so they can operate in human-designed and -populated spaces.&lt;/p&gt;&lt;h3&gt;Inspection, maintenance, and hazardous environments&lt;/h3&gt;&lt;p&gt; Industrial inspection is emerging as one of the earliest commercially viable use cases for humanoid and quasi-humanoid robots. Boston Dynamics’ Atlas, while not yet a general-purpose commercial product, has been used in live industrial trials for inspection and disaster-response environments. It can navigate uneven terrain, climb stairs, and manipulate tools in places considered unsafe for humans.&lt;/p&gt;&lt;p&gt; Toyota Research Institute has deployed humanoid robotics platforms for remote inspection and manipulation tasks in similar settings. Toyota’s systems rely on multimodal perception and human-in-the-loop control, the latter reinforcing an industry trend: early deployments prioritise reliability and traceability, so need human oversight.&lt;/p&gt;&lt;p&gt; Hexagon’s AEON aligns closely with this trend. Its emphasis on sensor fusion and spatial intelligence is relevant for inspection and quality assurance tasks, where precise understanding of physical environments is more valuable than the conversational abilities most associated with everyday use of AIs.&lt;/p&gt;&lt;h3&gt;Cloud platforms central to robotics strategy&lt;/h3&gt;&lt;p&gt; A defining feature of the Microsoft-Hexagon partnership is the use of cloud infrastructure in the scaling of humanoid robots. Training, updating, and monitoring physical AI systems generates large quantities of data, including video, force feedback from on-device sensors, spatial mapping (such as that derived from LIDAR), and operational telemetry. Managing this data locally has historically been a bottleneck, due to storage and processing constraints.&lt;/p&gt;&lt;p&gt; By using platforms like Azure and Azure IoT Operations, plus real-time intelligence services in the cloud, humanoid robots can be trained fleet-wide, not isolated units. This leads to multiple possibilities in shared learning, improvement by iteration, and greater consistency. For board-level buyers, these IT architecture shifts mean humanoid robots become viable entities that can be treated – in terms of IT requirements – more like enterprise software than machinery.&lt;/p&gt;&lt;h3&gt;Labour shortages drive adoption&lt;/h3&gt;&lt;p&gt; The demographic trends in manufacturing, logistics, and asset-intensive industries are increasingly unfavourable. Ageing workforces, declining interest in manual roles, and persistent skills shortages create skills gaps that conventional automation cannot fully address – at least, not without rebuilding entire facilities to be more suited to a robotic workforce. Fixed robotic systems excel in repetitive, predictable tasks but struggle in dynamic, human environments.&lt;/p&gt;&lt;p&gt; Humanoid robots occupy a middle ground. Not designed to replace workflows, they can stabilise operations where human availability is uncertain. Case studies show early value in night shifts, periods of peak demand, and tasks deemed too hazardous for humans.&lt;/p&gt;&lt;h3&gt;What boards should evaluate before investing&lt;/h3&gt;&lt;p&gt; For decision-makers considering investment in next-generation workplace robots, several issues to note have emerged from existing, real-world deployments:&lt;/p&gt;&lt;p&gt; Task specificity matters more than general intelligence, with the more successful pilots focusing on well-defined activities. Data governance and security continue to have to be placed front and centre when robots are put into play, especially so when it’s necessary to connect them to cloud platforms.&lt;/p&gt;&lt;p&gt; At a human level, workforce integration can be more challenging than sourcing, installing, and running the technology itself. Yet human oversight remains essential at this stage in AI maturity, for safety and regulatory acceptance.&lt;/p&gt;&lt;h3&gt;A measured but irreversible shift&lt;/h3&gt;&lt;p&gt; Humanoid robots won’t replace the human workforce, but an increasing body of evidence from live deployments and prototyping shows such devices are moving into the workplace. As of now, humanoid, AI-powered robots can perform economically-valuable tasks, and integration with existing industrial systems is immensely possible. For boards with the appetite to invest, the question could be when competitors might deploy the technology responsibly and at scale.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: Source: Hexagon Robotics)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt; The partnership announced this week between Microsoft and Hexagon Robotics marks an inflection point in the commercialisation of humanoid, AI-powered robots for industrial environments. The two companies will combine Microsoft’s cloud and AI infrastructure with Hexagon’s expertise in robotics, sensors, and spatial intelligence to advance the deployment of physical AI systems in real-world settings.&lt;/p&gt;&lt;p&gt; At the centre of the collaboration is AEON, Hexagon’s industrial humanoid robot, a device designed to operate autonomously in environments like factories, logistics hubs, engineering plants, and inspection sites.&lt;/p&gt;&lt;p&gt; The partnership will focus on multimodal AI training, imitation learning, real-time data management, and integration with existing industrial systems. Initial target sectors include automotive, aerospace, manufacturing, and logistics, the companies say. It’s in these industries where labour shortages and operational complexity are already constraining financial growth.&lt;/p&gt;&lt;p&gt; The announcement is the sign of a maturing ecosystem: cloud platforms, physical AI, and robotics engineering’s convergence, making humanoid automation commercially viable.&lt;/p&gt;&lt;h3&gt;Humanoid robots out of the research lab&lt;/h3&gt;&lt;p&gt; While humanoid robots have been  the subject of work at research institutions, demonstrated proudly at technology events, the last five years have seen a move to practical deployment in real-world, working environments. The main change has been the combination of improved perception, advances in reinforcement and imitation learning, and the availability of scalable cloud infrastructure.&lt;/p&gt;&lt;p&gt; One of the most visible examples is Agility Robotics’ Digit, a bipedal humanoid robot designed for logistics and warehouse operations. Digit has been piloted in live environments by companies like Amazon, where it performs material-handling tasks including tote movement and last-metre logistics. Such deployments tend to focus on augmenting human workers rather than replacing them, with Digit handling more physically demanding tasks.&lt;/p&gt;&lt;p&gt; Similarly, Tesla’s Optimus programme has moved out of the phase where concept videos were all that existed, and is now undergoing factory trials. Optimus robots are being tested on structured tasks like part handling and equipment transport inside Tesla’s automotive manufacturing facilities. While still limited in scope, these pilots demonstrate the pattern of humanoid-like machines chosen over less anthropomorphic form-factors so they can operate in human-designed and -populated spaces.&lt;/p&gt;&lt;h3&gt;Inspection, maintenance, and hazardous environments&lt;/h3&gt;&lt;p&gt; Industrial inspection is emerging as one of the earliest commercially viable use cases for humanoid and quasi-humanoid robots. Boston Dynamics’ Atlas, while not yet a general-purpose commercial product, has been used in live industrial trials for inspection and disaster-response environments. It can navigate uneven terrain, climb stairs, and manipulate tools in places considered unsafe for humans.&lt;/p&gt;&lt;p&gt; Toyota Research Institute has deployed humanoid robotics platforms for remote inspection and manipulation tasks in similar settings. Toyota’s systems rely on multimodal perception and human-in-the-loop control, the latter reinforcing an industry trend: early deployments prioritise reliability and traceability, so need human oversight.&lt;/p&gt;&lt;p&gt; Hexagon’s AEON aligns closely with this trend. Its emphasis on sensor fusion and spatial intelligence is relevant for inspection and quality assurance tasks, where precise understanding of physical environments is more valuable than the conversational abilities most associated with everyday use of AIs.&lt;/p&gt;&lt;h3&gt;Cloud platforms central to robotics strategy&lt;/h3&gt;&lt;p&gt; A defining feature of the Microsoft-Hexagon partnership is the use of cloud infrastructure in the scaling of humanoid robots. Training, updating, and monitoring physical AI systems generates large quantities of data, including video, force feedback from on-device sensors, spatial mapping (such as that derived from LIDAR), and operational telemetry. Managing this data locally has historically been a bottleneck, due to storage and processing constraints.&lt;/p&gt;&lt;p&gt; By using platforms like Azure and Azure IoT Operations, plus real-time intelligence services in the cloud, humanoid robots can be trained fleet-wide, not isolated units. This leads to multiple possibilities in shared learning, improvement by iteration, and greater consistency. For board-level buyers, these IT architecture shifts mean humanoid robots become viable entities that can be treated – in terms of IT requirements – more like enterprise software than machinery.&lt;/p&gt;&lt;h3&gt;Labour shortages drive adoption&lt;/h3&gt;&lt;p&gt; The demographic trends in manufacturing, logistics, and asset-intensive industries are increasingly unfavourable. Ageing workforces, declining interest in manual roles, and persistent skills shortages create skills gaps that conventional automation cannot fully address – at least, not without rebuilding entire facilities to be more suited to a robotic workforce. Fixed robotic systems excel in repetitive, predictable tasks but struggle in dynamic, human environments.&lt;/p&gt;&lt;p&gt; Humanoid robots occupy a middle ground. Not designed to replace workflows, they can stabilise operations where human availability is uncertain. Case studies show early value in night shifts, periods of peak demand, and tasks deemed too hazardous for humans.&lt;/p&gt;&lt;h3&gt;What boards should evaluate before investing&lt;/h3&gt;&lt;p&gt; For decision-makers considering investment in next-generation workplace robots, several issues to note have emerged from existing, real-world deployments:&lt;/p&gt;&lt;p&gt; Task specificity matters more than general intelligence, with the more successful pilots focusing on well-defined activities. Data governance and security continue to have to be placed front and centre when robots are put into play, especially so when it’s necessary to connect them to cloud platforms.&lt;/p&gt;&lt;p&gt; At a human level, workforce integration can be more challenging than sourcing, installing, and running the technology itself. Yet human oversight remains essential at this stage in AI maturity, for safety and regulatory acceptance.&lt;/p&gt;&lt;h3&gt;A measured but irreversible shift&lt;/h3&gt;&lt;p&gt; Humanoid robots won’t replace the human workforce, but an increasing body of evidence from live deployments and prototyping shows such devices are moving into the workplace. As of now, humanoid, AI-powered robots can perform economically-valuable tasks, and integration with existing industrial systems is immensely possible. For boards with the appetite to invest, the question could be when competitors might deploy the technology responsibly and at scale.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: Source: Hexagon Robotics)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/from-cloud-to-factory-humanoid-robots-coming-to-workplaces/</guid><pubDate>Fri, 09 Jan 2026 13:06:00 +0000</pubDate></item><item><title>[NEW] The Download: the case for AI slop, and helping CRISPR fulfill its promise (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/09/1130953/the-download-the-case-for-ai-slop-and-helping-crispr-fulfill-its-promise/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How I learned to stop worrying and love AI slop&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;If I were to locate the moment AI slop broke through into popular consciousness, I’d pick the video of rabbits bouncing on a trampoline that went viral last summer. For many savvy internet users, myself included, it was the first time we were fooled by an AI video, and it ended up spawning a wave of almost identical generated clips.&lt;/p&gt; 
 &lt;p&gt;My first reaction was that, broadly speaking, all of this sucked. That’s become a familiar refrain, in think pieces and at dinner parties. Everything online is slop now—the internet “enshittified,” with AI taking much of the blame. Initially, I largely agreed. But then friends started sharing AI clips in group chats that were compellingly weird, or funny. Some even had a grain of brilliance.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I had to admit I didn’t fully understand what I was rejecting—what I found so objectionable. To try to get to the bottom of how I felt (and why), I spoke to the people making the videos, a company creating bespoke tools for creators, and experts who study how new media becomes culture. What I found convinced me that maybe generative AI will not end up ruining everything after all. Read the full story.&lt;/p&gt; 
   &lt;p&gt;&lt;strong&gt;A new CRISPR startup is betting regulators will ease up on gene-editing&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Here at &lt;em&gt;MIT Technology Review&lt;/em&gt; we’ve been writing about the gene-editing technology CRISPR since 2013, calling it the biggest biotech breakthrough of the century. Yet so far, there’s been only one gene-editing drug approved, and it’s been used commercially on only about 40 patients, all with sickle-cell disease.&lt;/p&gt;&lt;p&gt;It’s becoming clear that the impact of CRISPR isn’t as big as we all hoped. In fact, there’s a pall of discouragement over the entire field—with some journalists saying the gene-editing revolution has “lost its mojo.”&lt;/p&gt;&lt;p&gt;So what will it take for CRISPR to help more people? A new startup says the answer could be an “umbrella approach” to testing and commercializing treatments which could avoid costly new trials or approvals for every new version. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Antonio Regalado&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;America’s new dietary guidelines ignore decades of scientific research&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The first days of 2026 have brought big news for health. On Wednesday, health secretary Robert F. Kennedy Jr. and his colleagues at the Departments of Health and Human Services and Agriculture unveiled new dietary guidelines for Americans. And they are causing a bit of a stir.&lt;/p&gt;&lt;p&gt;That’s partly because they recommend products like red meat, butter, and beef tallow—foods that have been linked to cardiovascular disease, and that nutrition experts have been recommending people limit in their diets.&lt;/p&gt;&lt;p&gt;These guidelines are a big deal—they influence food assistance programs and school lunches, for example. Let’s take a look at the good, the bad, and the ugly advice being dished up to Americans by their government.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article first appeared in The Checkup, MIT Technology Review’s weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   

 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Grok has switched off its image-generating function for most users&lt;br /&gt;&lt;/strong&gt;Following a global backlash to its sexualized pictures of women and children. (The Guardian)&lt;br /&gt;+ &lt;em&gt;Elon Musk has previously lamented the “guardrails” around the chatbot. &lt;/em&gt;(CNN)&lt;br /&gt;+ &lt;em&gt;XAI has been burning through cash lately. &lt;/em&gt;(Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Online sleuths tried to use AI to unmask the ICE agent who killed a woman&lt;/strong&gt;&lt;br /&gt;The problem is, its results are far from reliable. (WP $)&lt;br /&gt;+ &lt;em&gt;The Trump administration is pushing videos of the incident filmed from a specific angle. &lt;/em&gt;(The Verge)&lt;br /&gt;+ &lt;em&gt;Minneapolis is struggling to make sense of the shooting of Renee Nicole Good. &lt;/em&gt;(WSJ $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 Smartphones and PCs are about to get more expensive&lt;/strong&gt;&lt;br /&gt;You can thank the memory chip shortage sparked by the AI data center boom. (FT $)&lt;br /&gt;+ &lt;em&gt;Expect delays alongside those price rises, too. &lt;/em&gt;(Economist $)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;4 NASA is bringing four of the seven ISS crew members back to Earth&lt;br /&gt;It’s not clear exactly why, but it said one of them experienced a “medical situation” earlier this week. (Ars Technica)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;5 The vast majority of humanoid robots shipped last year were from China&lt;/strong&gt;&lt;br /&gt;The country is dominating early supply for the bipedal machines. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Why a Chinese robot vacuum firm is moving into EVs. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;China’s EV giants are betting big on humanoid robots. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 New Jersey has banned students’ phones in schools&lt;br /&gt;&lt;/strong&gt;It’s the latest in a long line of states to restrict devices during school hours. (NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Are AI coding assistants getting worse?&lt;br /&gt;&lt;/strong&gt;This data scientist certainly seems to think so. (IEEE Spectrum)&lt;br /&gt;+ &lt;em&gt;AI coding is now everywhere. But not everyone is convinced. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 How to save wine from wildfires 🍇&lt;/strong&gt;&lt;br /&gt;Smoke leaves the alcohol with an ashy taste, but a group of scientists are working on a solution. (New Yorker $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 Celebrity Letterboxd accounts are good fun&lt;/strong&gt;&lt;br /&gt;Unsurprisingly, a subset of web users have chosen to hound them. (NY Mag $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 Craigslist refuses to die&lt;/strong&gt;&lt;br /&gt;The old-school classifieds corner of the web still has a legion of diehard fans. (Wired $)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Tools like Grok now risk bringing sexual AI imagery of children into the mainstream. The harms are rippling out.”&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;—Ngaire Alexander, head of the Internet Watch Foundation’s reporting hotline, explains the dangers around low-moderation AI tools like Grok to the Wall Street Journal.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1130956" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/image_af5f4e.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How to measure the returns on R&amp;amp;D spending&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Given the draconian cuts to US federal funding for science, it’s worth asking some hard-nosed money questions: How much should we be spending on R&amp;amp;D? How much value do we get out of such investments, anyway?&lt;/p&gt;&lt;p&gt;To answer that, in several recent papers, economists have approached this issue in clever new ways.&amp;nbsp; And, though they ask slightly different questions, their conclusions share a bottom line: R&amp;amp;D is, in fact, one of the better long-term investments that the government can make. Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—David Rotman&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Bruno Mars is back, baby!&lt;br /&gt;+ Hmm, interesting: Apple’s new &lt;em&gt;Widow’s Bay&lt;/em&gt; show is inspired by both Stephen King and Donald Glover, which is an intriguing combination.&lt;br /&gt;+ Give this man control of the new Lego AI bricks!&lt;br /&gt;+ An iron age war trumpet recently uncovered in Britain is the most complete example discovered anywhere in the world.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How I learned to stop worrying and love AI slop&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;If I were to locate the moment AI slop broke through into popular consciousness, I’d pick the video of rabbits bouncing on a trampoline that went viral last summer. For many savvy internet users, myself included, it was the first time we were fooled by an AI video, and it ended up spawning a wave of almost identical generated clips.&lt;/p&gt; 
 &lt;p&gt;My first reaction was that, broadly speaking, all of this sucked. That’s become a familiar refrain, in think pieces and at dinner parties. Everything online is slop now—the internet “enshittified,” with AI taking much of the blame. Initially, I largely agreed. But then friends started sharing AI clips in group chats that were compellingly weird, or funny. Some even had a grain of brilliance.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I had to admit I didn’t fully understand what I was rejecting—what I found so objectionable. To try to get to the bottom of how I felt (and why), I spoke to the people making the videos, a company creating bespoke tools for creators, and experts who study how new media becomes culture. What I found convinced me that maybe generative AI will not end up ruining everything after all. Read the full story.&lt;/p&gt; 
   &lt;p&gt;&lt;strong&gt;A new CRISPR startup is betting regulators will ease up on gene-editing&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Here at &lt;em&gt;MIT Technology Review&lt;/em&gt; we’ve been writing about the gene-editing technology CRISPR since 2013, calling it the biggest biotech breakthrough of the century. Yet so far, there’s been only one gene-editing drug approved, and it’s been used commercially on only about 40 patients, all with sickle-cell disease.&lt;/p&gt;&lt;p&gt;It’s becoming clear that the impact of CRISPR isn’t as big as we all hoped. In fact, there’s a pall of discouragement over the entire field—with some journalists saying the gene-editing revolution has “lost its mojo.”&lt;/p&gt;&lt;p&gt;So what will it take for CRISPR to help more people? A new startup says the answer could be an “umbrella approach” to testing and commercializing treatments which could avoid costly new trials or approvals for every new version. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Antonio Regalado&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;America’s new dietary guidelines ignore decades of scientific research&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The first days of 2026 have brought big news for health. On Wednesday, health secretary Robert F. Kennedy Jr. and his colleagues at the Departments of Health and Human Services and Agriculture unveiled new dietary guidelines for Americans. And they are causing a bit of a stir.&lt;/p&gt;&lt;p&gt;That’s partly because they recommend products like red meat, butter, and beef tallow—foods that have been linked to cardiovascular disease, and that nutrition experts have been recommending people limit in their diets.&lt;/p&gt;&lt;p&gt;These guidelines are a big deal—they influence food assistance programs and school lunches, for example. Let’s take a look at the good, the bad, and the ugly advice being dished up to Americans by their government.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article first appeared in The Checkup, MIT Technology Review’s weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   

 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Grok has switched off its image-generating function for most users&lt;br /&gt;&lt;/strong&gt;Following a global backlash to its sexualized pictures of women and children. (The Guardian)&lt;br /&gt;+ &lt;em&gt;Elon Musk has previously lamented the “guardrails” around the chatbot. &lt;/em&gt;(CNN)&lt;br /&gt;+ &lt;em&gt;XAI has been burning through cash lately. &lt;/em&gt;(Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Online sleuths tried to use AI to unmask the ICE agent who killed a woman&lt;/strong&gt;&lt;br /&gt;The problem is, its results are far from reliable. (WP $)&lt;br /&gt;+ &lt;em&gt;The Trump administration is pushing videos of the incident filmed from a specific angle. &lt;/em&gt;(The Verge)&lt;br /&gt;+ &lt;em&gt;Minneapolis is struggling to make sense of the shooting of Renee Nicole Good. &lt;/em&gt;(WSJ $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 Smartphones and PCs are about to get more expensive&lt;/strong&gt;&lt;br /&gt;You can thank the memory chip shortage sparked by the AI data center boom. (FT $)&lt;br /&gt;+ &lt;em&gt;Expect delays alongside those price rises, too. &lt;/em&gt;(Economist $)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;4 NASA is bringing four of the seven ISS crew members back to Earth&lt;br /&gt;It’s not clear exactly why, but it said one of them experienced a “medical situation” earlier this week. (Ars Technica)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;5 The vast majority of humanoid robots shipped last year were from China&lt;/strong&gt;&lt;br /&gt;The country is dominating early supply for the bipedal machines. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Why a Chinese robot vacuum firm is moving into EVs. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;China’s EV giants are betting big on humanoid robots. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 New Jersey has banned students’ phones in schools&lt;br /&gt;&lt;/strong&gt;It’s the latest in a long line of states to restrict devices during school hours. (NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Are AI coding assistants getting worse?&lt;br /&gt;&lt;/strong&gt;This data scientist certainly seems to think so. (IEEE Spectrum)&lt;br /&gt;+ &lt;em&gt;AI coding is now everywhere. But not everyone is convinced. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 How to save wine from wildfires 🍇&lt;/strong&gt;&lt;br /&gt;Smoke leaves the alcohol with an ashy taste, but a group of scientists are working on a solution. (New Yorker $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 Celebrity Letterboxd accounts are good fun&lt;/strong&gt;&lt;br /&gt;Unsurprisingly, a subset of web users have chosen to hound them. (NY Mag $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 Craigslist refuses to die&lt;/strong&gt;&lt;br /&gt;The old-school classifieds corner of the web still has a legion of diehard fans. (Wired $)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Tools like Grok now risk bringing sexual AI imagery of children into the mainstream. The harms are rippling out.”&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;—Ngaire Alexander, head of the Internet Watch Foundation’s reporting hotline, explains the dangers around low-moderation AI tools like Grok to the Wall Street Journal.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1130956" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/image_af5f4e.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How to measure the returns on R&amp;amp;D spending&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Given the draconian cuts to US federal funding for science, it’s worth asking some hard-nosed money questions: How much should we be spending on R&amp;amp;D? How much value do we get out of such investments, anyway?&lt;/p&gt;&lt;p&gt;To answer that, in several recent papers, economists have approached this issue in clever new ways.&amp;nbsp; And, though they ask slightly different questions, their conclusions share a bottom line: R&amp;amp;D is, in fact, one of the better long-term investments that the government can make. Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—David Rotman&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Bruno Mars is back, baby!&lt;br /&gt;+ Hmm, interesting: Apple’s new &lt;em&gt;Widow’s Bay&lt;/em&gt; show is inspired by both Stephen King and Donald Glover, which is an intriguing combination.&lt;br /&gt;+ Give this man control of the new Lego AI bricks!&lt;br /&gt;+ An iron age war trumpet recently uncovered in Britain is the most complete example discovered anywhere in the world.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/09/1130953/the-download-the-case-for-ai-slop-and-helping-crispr-fulfill-its-promise/</guid><pubDate>Fri, 09 Jan 2026 13:10:00 +0000</pubDate></item><item><title>[NEW] X restricts Grok’s image generation to paying subscribers only after drawing the world’s ire (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/09/x-restricts-groks-image-generation-to-paying-subscribers-only-after-drawing-the-worlds-ire/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/xAI-Grok-GettyImages-1765893916.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk’s AI company has restricted Grok’s controversial AI image-generation feature to only paying subscribers on X, after the tool invited heavy criticism from across the world for letting users generate sexualized and nude images of women and children. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In replies to users on Friday, Grok said only paying subscribers on X would be able to generate and edit images on the platform. Notably, these limits do not apply to the Grok app, which, at the time of publication was letting anyone generate pictures without having to pay for a subscription.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Initially available to anyone with daily limits, Grok’s image-generation feature allowed users to upload anyone’s picture and ask it to edit it or generate a sexualized or nude version. What ensued was a veritable flood of non-consensual sexualized images of children, actors, models and prominent figures, drawing the ire of multiple nations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;X and Musk have both publicly denounced the use of the tool to produce such images, writing that the company would stick to its policies against posting illegal content on the social media platform. “Anyone using grok to make illegal content will suffer the same consequences as if they upload illegal content,” Musk tweeted last week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The U.K., the European Union, and India have all publicly denounced X and Grok for allowing such use of its capabilities. The EU on Thursday asked xAI to retain all documentation relating to the chatbot, and India’s communications ministry last week ordered X to make immediate changes to stop the image-generation features from being misused or risk its safe harbor protections in the country. The U.K.’s communications watchdog said it’s been in touch with xAI over the issue as well.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/xAI-Grok-GettyImages-1765893916.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk’s AI company has restricted Grok’s controversial AI image-generation feature to only paying subscribers on X, after the tool invited heavy criticism from across the world for letting users generate sexualized and nude images of women and children. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In replies to users on Friday, Grok said only paying subscribers on X would be able to generate and edit images on the platform. Notably, these limits do not apply to the Grok app, which, at the time of publication was letting anyone generate pictures without having to pay for a subscription.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Initially available to anyone with daily limits, Grok’s image-generation feature allowed users to upload anyone’s picture and ask it to edit it or generate a sexualized or nude version. What ensued was a veritable flood of non-consensual sexualized images of children, actors, models and prominent figures, drawing the ire of multiple nations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;X and Musk have both publicly denounced the use of the tool to produce such images, writing that the company would stick to its policies against posting illegal content on the social media platform. “Anyone using grok to make illegal content will suffer the same consequences as if they upload illegal content,” Musk tweeted last week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The U.K., the European Union, and India have all publicly denounced X and Grok for allowing such use of its capabilities. The EU on Thursday asked xAI to retain all documentation relating to the chatbot, and India’s communications ministry last week ordered X to make immediate changes to stop the image-generation features from being misused or risk its safe harbor protections in the country. The U.K.’s communications watchdog said it’s been in touch with xAI over the issue as well.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/09/x-restricts-groks-image-generation-to-paying-subscribers-only-after-drawing-the-worlds-ire/</guid><pubDate>Fri, 09 Jan 2026 13:59:24 +0000</pubDate></item><item><title>[NEW] Autonomy without accountability: The real AI risk (AI News)</title><link>https://www.artificialintelligence-news.com/news/autonomy-without-accountability-the-real-ai-risk/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/Woo-article.jpg" /&gt;&lt;/div&gt;&lt;p&gt;If you have ever taken a self-driving Uber through downtown LA, you might recognise the strange sense of uncertainty that settles in when there is no driver and no conversation, just a quiet car making assumptions about the world around it. The journey feels fine until the car misreads a shadow or slows abruptly for something harmless. In that moment you see the real issue with autonomy. It does not panic when it should, and that gap between confidence and judgement is where trust is either earned or lost. Much of today’s enterprise AI feels remarkably similar. It is competent without being confident, and efficient without being empathetic, which is why the deciding factor in every successful deployment is no longer computing power but trust.&lt;/p&gt;&lt;p&gt;The MLQ State of AI in Business 2025 [PDF] report puts a sharp number on this. 95% of early AI pilots fail to produce measurable ROI, not because the technology is weak but because it is mismatched to the problems organisations are trying to solve. The pattern repeats itself in industries. Leaders get uneasy when they can’t tell if the output is right, teams are unsure whether dashboards can be trusted, and customers quickly lose patience when an interaction feels automated rather than supported. Anyone who has been locked out of their bank account while the automated recovery system insists their answers are wrong knows how quickly confidence evaporates.&lt;/p&gt;&lt;p&gt;Klarna remains the most publicised example of large-scale automation in action. The company has now halved its workforce since 2022 and says internal AI systems are performing the work of 853 full-time roles, up from 700 earlier this year. Revenues have risen 108%, while average employee compensation has increased 60%, funded in part by those operational gains. Yet the picture is more complicated. Klarna still reported a 95 million dollar quarterly loss, and its CEO has warned that further staff reductions are likely. It shows that automation alone does not create stability. Without accountability and structure, the experience breaks down long before the AI does. As Jason Roos, CEO of CCaaS provider Cirrus, puts it, “Any transformation that unsettles confidence, inside or outside the business, carries a cost you cannot ignore. it can leave you worse off.”&lt;/p&gt;&lt;p&gt;We have already seen what happens when autonomy runs ahead of accountability. The UK’s Department for Work and Pensions used an algorithm that wrongly flagged around 200,000 housing-benefit claims as potentially fraudulent, even though the majority were legitimate. The problem wasn’t the technology. It was the absence of clear ownership over its decisions. When an automated system suspends the wrong account, rejects the wrong claim or creates unnecessary fear, the issue is never just “why did the model misfire?” It’s “who owns the outcome?” Without that answer, trust becomes fragile.&lt;/p&gt;&lt;p&gt;“The missing step is always readiness,” says Roos. “If the process, the data and the guardrails aren’t in place, autonomy doesn’t accelerate performance, it amplifies the weaknesses. Accountability has to come first. Start with the outcome, find where effort is being wasted, check your readiness and governance, and only then automate. Skip those steps and accountability disappears just as fast as the efficiency gains arrive.”&lt;/p&gt;&lt;p&gt;Part of the problem is an obsession with scale without the grounding that makes scale sustainable. Many organisations push toward autonomous agents that can act decisively, yet very few pause to consider what happens when those actions drift outside expected boundaries. The Edelman Trust Barometer [PDF] shows a steady decline in public trust in AI over the past five years, and a joint KPMG and University of Melbourne study found that workers prefer more human involvement in almost half the tasks examined. The findings reinforce a simple point. Trust rarely comes from pushing models harder. It comes from people taking the time to understand how decisions are made, and from governance that behaves less like a brake pedal and more like a steering wheel.&lt;/p&gt;&lt;p&gt;The same dynamics appear on the customer side. PwC’s trust research reveals a wide gulf between perception and reality. Most executives believe customers trust their organisation, while only a minority of customers agree. Other surveys show that transparency helps to close this gap, with large majorities of consumers wanting clear disclosure when AI is used in service experiences. Without that clarity, people do not feel reassured. They feel misled, and the relationship becomes strained. Companies that communicate openly about their AI use are not only protecting trust but also normalising the idea that technology and human support can co-exist.&lt;/p&gt;&lt;p&gt;Some of the confusion stems from the term “agentic AI” itself. Much of the market treats it as something unpredictable or self-directing, when in reality it is workflow automation with reasoning and recall. It is a structured way for systems to make modest decisions inside parameters designed by people. The deployments that scale safely all follow the same sequence. They start with the outcome they want to improve, then look at where unnecessary effort sits in the workflow, then assess whether their systems and teams are ready for autonomy, and only then choose the technology. Reversing that order does not speed anything up. It simply creates faster mistakes. As Roos says, AI should expand human judgement, not replace it.&lt;/p&gt;&lt;p&gt;All of this points toward a wider truth. Every wave of automation eventually becomes a social question rather than a purely technical one. Amazon built its dominance through operational consistency, but it also built a level of confidence that the parcel would arrive. When that confidence dips, customers move on. AI follows the same pattern. You can deploy sophisticated, self-correcting systems, but if the customer feels tricked or misled at any point, the trust breaks. Internally, the same pressures apply. The KPMG global study [PDF] highlights how quickly employees disengage when they do not understand how decisions are made or who is accountable for them. Without that clarity, adoption stalls.&lt;/p&gt;&lt;p&gt;As agentic systems take on more conversational roles, the emotional dimension becomes even more significant. Early reviews of autonomous chat interactions show that people now judge their experience not only by whether they were helped but also by whether the interaction felt attentive and respectful. A customer who feels dismissed rarely keeps the frustration to themselves. The emotional tone of AI is becoming a genuine operational factor, and systems that cannot meet that expectation risk becoming liabilities.&lt;/p&gt;&lt;p&gt;The difficult truth is that technology will continue to move faster than people’s instinctive comfort with it. Trust will always lag behind innovation. That is not an argument against progress. It is an argument for maturity. Every AI leader should be asking whether they would trust the system with their own data, whether they can explain its last decision in plain language, and who steps in when something goes wrong. If those answers are unclear, the organisation is not leading transformation. It is preparing an apology.&lt;/p&gt;&lt;p&gt;Roos puts it simply, “Agentic AI is not the concern. Unaccountable AI is.”&lt;/p&gt;&lt;p&gt;When trust goes, adoption goes, and the project that looked transformative becomes another entry in the 95% failure rate. Autonomy is not the enemy. Forgetting who is responsible is. The organisations that keep a human hand on the wheel will be the ones still in control when the self-driving hype eventually fades.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/Woo-article.jpg" /&gt;&lt;/div&gt;&lt;p&gt;If you have ever taken a self-driving Uber through downtown LA, you might recognise the strange sense of uncertainty that settles in when there is no driver and no conversation, just a quiet car making assumptions about the world around it. The journey feels fine until the car misreads a shadow or slows abruptly for something harmless. In that moment you see the real issue with autonomy. It does not panic when it should, and that gap between confidence and judgement is where trust is either earned or lost. Much of today’s enterprise AI feels remarkably similar. It is competent without being confident, and efficient without being empathetic, which is why the deciding factor in every successful deployment is no longer computing power but trust.&lt;/p&gt;&lt;p&gt;The MLQ State of AI in Business 2025 [PDF] report puts a sharp number on this. 95% of early AI pilots fail to produce measurable ROI, not because the technology is weak but because it is mismatched to the problems organisations are trying to solve. The pattern repeats itself in industries. Leaders get uneasy when they can’t tell if the output is right, teams are unsure whether dashboards can be trusted, and customers quickly lose patience when an interaction feels automated rather than supported. Anyone who has been locked out of their bank account while the automated recovery system insists their answers are wrong knows how quickly confidence evaporates.&lt;/p&gt;&lt;p&gt;Klarna remains the most publicised example of large-scale automation in action. The company has now halved its workforce since 2022 and says internal AI systems are performing the work of 853 full-time roles, up from 700 earlier this year. Revenues have risen 108%, while average employee compensation has increased 60%, funded in part by those operational gains. Yet the picture is more complicated. Klarna still reported a 95 million dollar quarterly loss, and its CEO has warned that further staff reductions are likely. It shows that automation alone does not create stability. Without accountability and structure, the experience breaks down long before the AI does. As Jason Roos, CEO of CCaaS provider Cirrus, puts it, “Any transformation that unsettles confidence, inside or outside the business, carries a cost you cannot ignore. it can leave you worse off.”&lt;/p&gt;&lt;p&gt;We have already seen what happens when autonomy runs ahead of accountability. The UK’s Department for Work and Pensions used an algorithm that wrongly flagged around 200,000 housing-benefit claims as potentially fraudulent, even though the majority were legitimate. The problem wasn’t the technology. It was the absence of clear ownership over its decisions. When an automated system suspends the wrong account, rejects the wrong claim or creates unnecessary fear, the issue is never just “why did the model misfire?” It’s “who owns the outcome?” Without that answer, trust becomes fragile.&lt;/p&gt;&lt;p&gt;“The missing step is always readiness,” says Roos. “If the process, the data and the guardrails aren’t in place, autonomy doesn’t accelerate performance, it amplifies the weaknesses. Accountability has to come first. Start with the outcome, find where effort is being wasted, check your readiness and governance, and only then automate. Skip those steps and accountability disappears just as fast as the efficiency gains arrive.”&lt;/p&gt;&lt;p&gt;Part of the problem is an obsession with scale without the grounding that makes scale sustainable. Many organisations push toward autonomous agents that can act decisively, yet very few pause to consider what happens when those actions drift outside expected boundaries. The Edelman Trust Barometer [PDF] shows a steady decline in public trust in AI over the past five years, and a joint KPMG and University of Melbourne study found that workers prefer more human involvement in almost half the tasks examined. The findings reinforce a simple point. Trust rarely comes from pushing models harder. It comes from people taking the time to understand how decisions are made, and from governance that behaves less like a brake pedal and more like a steering wheel.&lt;/p&gt;&lt;p&gt;The same dynamics appear on the customer side. PwC’s trust research reveals a wide gulf between perception and reality. Most executives believe customers trust their organisation, while only a minority of customers agree. Other surveys show that transparency helps to close this gap, with large majorities of consumers wanting clear disclosure when AI is used in service experiences. Without that clarity, people do not feel reassured. They feel misled, and the relationship becomes strained. Companies that communicate openly about their AI use are not only protecting trust but also normalising the idea that technology and human support can co-exist.&lt;/p&gt;&lt;p&gt;Some of the confusion stems from the term “agentic AI” itself. Much of the market treats it as something unpredictable or self-directing, when in reality it is workflow automation with reasoning and recall. It is a structured way for systems to make modest decisions inside parameters designed by people. The deployments that scale safely all follow the same sequence. They start with the outcome they want to improve, then look at where unnecessary effort sits in the workflow, then assess whether their systems and teams are ready for autonomy, and only then choose the technology. Reversing that order does not speed anything up. It simply creates faster mistakes. As Roos says, AI should expand human judgement, not replace it.&lt;/p&gt;&lt;p&gt;All of this points toward a wider truth. Every wave of automation eventually becomes a social question rather than a purely technical one. Amazon built its dominance through operational consistency, but it also built a level of confidence that the parcel would arrive. When that confidence dips, customers move on. AI follows the same pattern. You can deploy sophisticated, self-correcting systems, but if the customer feels tricked or misled at any point, the trust breaks. Internally, the same pressures apply. The KPMG global study [PDF] highlights how quickly employees disengage when they do not understand how decisions are made or who is accountable for them. Without that clarity, adoption stalls.&lt;/p&gt;&lt;p&gt;As agentic systems take on more conversational roles, the emotional dimension becomes even more significant. Early reviews of autonomous chat interactions show that people now judge their experience not only by whether they were helped but also by whether the interaction felt attentive and respectful. A customer who feels dismissed rarely keeps the frustration to themselves. The emotional tone of AI is becoming a genuine operational factor, and systems that cannot meet that expectation risk becoming liabilities.&lt;/p&gt;&lt;p&gt;The difficult truth is that technology will continue to move faster than people’s instinctive comfort with it. Trust will always lag behind innovation. That is not an argument against progress. It is an argument for maturity. Every AI leader should be asking whether they would trust the system with their own data, whether they can explain its last decision in plain language, and who steps in when something goes wrong. If those answers are unclear, the organisation is not leading transformation. It is preparing an apology.&lt;/p&gt;&lt;p&gt;Roos puts it simply, “Agentic AI is not the concern. Unaccountable AI is.”&lt;/p&gt;&lt;p&gt;When trust goes, adoption goes, and the project that looked transformative becomes another entry in the 95% failure rate. Autonomy is not the enemy. Forgetting who is responsible is. The organisations that keep a human hand on the wheel will be the ones still in control when the self-driving hype eventually fades.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/autonomy-without-accountability-the-real-ai-risk/</guid><pubDate>Fri, 09 Jan 2026 14:44:37 +0000</pubDate></item><item><title>[NEW] The future of personal injury law: AI and legal tech in Philadelphia (AI News)</title><link>https://www.artificialintelligence-news.com/news/the-future-of-personal-injury-law-ai-and-legal-tech-in-philadelphia/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/Picture1.png" /&gt;&lt;/div&gt;&lt;p&gt;Artificial intelligence and legal technology are reshaping the landscape of personal injury law in Philadelphia, introducing significant changes. The advancements offer new capabilities for legal professionals, enhancing the strategic approach lawyers take in managing cases.&lt;/p&gt;&lt;p&gt;The integration of AI and legal tech into personal injury law is changing how legal practices operate in Philadelphia. By using advanced technologies, like predictive analytics, law firms can gain valuable insights that were previously unattainable. The innovation aids in case management and empowers attorneys to strategize more effectively. As a Grays Ferry, Philadelphia personal injury lawyer adapts to these changes, you can expect a more data-driven approach to legal proceedings, using AI’s potential in predicting case outcomes.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-ai-s-impact-on-personal-injury-law-practices"&gt;AI’s impact on personal injury law practices&lt;/h3&gt;&lt;p&gt;Artificial intelligence has made significant strides in various industries, and personal injury law is no exception. The incorporation of AI technologies allows for more efficient and precise handling of cases. With AI-driven tools, lawyers can analyze vast amounts of data quickly and accurately. The capability facilitates better decision-making processes and enables legal professionals to offer more tailored services to their clients.&lt;/p&gt;&lt;p&gt;Predictive analytics, a key application of AI, plays a crucial role in this transformation. By processing historical data and identifying patterns, predictive analytics can forecast potential case outcomes with remarkable accuracy. This enables lawyers to assess risks and develop strategies informed by empirical evidence rather than intuition alone. As the field continues to evolve, the reliance on data-driven insights will likely become an integral part of legal practices.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-understanding-predictive-analytics"&gt;Understanding predictive analytics&lt;/h3&gt;&lt;p&gt;Predictive analytics involves analyzing current and historical data to predict future outcomes. In legal practices, this means using data from past cases to anticipate how similar cases might unfold. By examining factors like case details, precedents, and court rulings, AI can generate predictions that guide lawyers in making informed decisions.&lt;/p&gt;&lt;p&gt;The types of data used in predictive analytics range from demographic information to historical court records. Advanced algorithms process this information to identify trends and correlations that may not be immediately apparent to human analysts. Through this process, lawyers gain insights that enhance their understanding of complex legal scenarios, ultimately improving their ability to advocate for their clients effectively.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-applications-in-managing-personal-injury-cases"&gt;Applications in managing personal injury cases&lt;/h3&gt;&lt;p&gt;In personal injury cases, predictive analytics serves as a tool for risk assessment and strategy development. Lawyers can use these insights to estimate the likelihood of winning a case or securing a favorable settlement. By analyzing similar past cases, attorneys can better understand potential challenges and opportunities unique to each situation.&lt;/p&gt;&lt;p&gt;The application of predictive analytics extends beyond mere predictions; it influences how lawyers prepare for negotiations and trials. Knowing the probable outcome allows for more effective resource allocation and client counseling. As legal professionals continue adopting these technologies, they gain a competitive edge in delivering superior service and achieving optimal results for their clients.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-benefits-and-challenges-of-ai-in-law"&gt;Benefits and challenges of AI in law&lt;/h3&gt;&lt;p&gt;For legal professionals, embracing AI-driven analytics offers numerous benefits beyond improved client outcomes. One significant advantage is enhanced decision-making capabilities. By providing clear, evidence-based insights into case probabilities, predictive analytics empowers lawyers to make strategic choices with greater confidence.&lt;/p&gt;&lt;p&gt;The efficiency gains associated with these technologies cannot be overstated. AI streamlines various processes in law firms, reducing time spent on mundane tasks and allowing attorneys to focus on higher-value activities. The efficiency translates into cost savings and improved service delivery, positioning firms that adopt these tools as leaders in the competitive legal market.&lt;/p&gt;&lt;p&gt;While the benefits of integrating AI into personal injury law are substantial, several challenges must be addressed to ensure responsible implementation. Data privacy is a primary concern; ensuring that client information is protected while using these advanced tools is paramount. Legal professionals must navigate these complexities carefully to maintain trust and compliance with regulations.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/Picture1.png" /&gt;&lt;/div&gt;&lt;p&gt;Artificial intelligence and legal technology are reshaping the landscape of personal injury law in Philadelphia, introducing significant changes. The advancements offer new capabilities for legal professionals, enhancing the strategic approach lawyers take in managing cases.&lt;/p&gt;&lt;p&gt;The integration of AI and legal tech into personal injury law is changing how legal practices operate in Philadelphia. By using advanced technologies, like predictive analytics, law firms can gain valuable insights that were previously unattainable. The innovation aids in case management and empowers attorneys to strategize more effectively. As a Grays Ferry, Philadelphia personal injury lawyer adapts to these changes, you can expect a more data-driven approach to legal proceedings, using AI’s potential in predicting case outcomes.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-ai-s-impact-on-personal-injury-law-practices"&gt;AI’s impact on personal injury law practices&lt;/h3&gt;&lt;p&gt;Artificial intelligence has made significant strides in various industries, and personal injury law is no exception. The incorporation of AI technologies allows for more efficient and precise handling of cases. With AI-driven tools, lawyers can analyze vast amounts of data quickly and accurately. The capability facilitates better decision-making processes and enables legal professionals to offer more tailored services to their clients.&lt;/p&gt;&lt;p&gt;Predictive analytics, a key application of AI, plays a crucial role in this transformation. By processing historical data and identifying patterns, predictive analytics can forecast potential case outcomes with remarkable accuracy. This enables lawyers to assess risks and develop strategies informed by empirical evidence rather than intuition alone. As the field continues to evolve, the reliance on data-driven insights will likely become an integral part of legal practices.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-understanding-predictive-analytics"&gt;Understanding predictive analytics&lt;/h3&gt;&lt;p&gt;Predictive analytics involves analyzing current and historical data to predict future outcomes. In legal practices, this means using data from past cases to anticipate how similar cases might unfold. By examining factors like case details, precedents, and court rulings, AI can generate predictions that guide lawyers in making informed decisions.&lt;/p&gt;&lt;p&gt;The types of data used in predictive analytics range from demographic information to historical court records. Advanced algorithms process this information to identify trends and correlations that may not be immediately apparent to human analysts. Through this process, lawyers gain insights that enhance their understanding of complex legal scenarios, ultimately improving their ability to advocate for their clients effectively.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-applications-in-managing-personal-injury-cases"&gt;Applications in managing personal injury cases&lt;/h3&gt;&lt;p&gt;In personal injury cases, predictive analytics serves as a tool for risk assessment and strategy development. Lawyers can use these insights to estimate the likelihood of winning a case or securing a favorable settlement. By analyzing similar past cases, attorneys can better understand potential challenges and opportunities unique to each situation.&lt;/p&gt;&lt;p&gt;The application of predictive analytics extends beyond mere predictions; it influences how lawyers prepare for negotiations and trials. Knowing the probable outcome allows for more effective resource allocation and client counseling. As legal professionals continue adopting these technologies, they gain a competitive edge in delivering superior service and achieving optimal results for their clients.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-benefits-and-challenges-of-ai-in-law"&gt;Benefits and challenges of AI in law&lt;/h3&gt;&lt;p&gt;For legal professionals, embracing AI-driven analytics offers numerous benefits beyond improved client outcomes. One significant advantage is enhanced decision-making capabilities. By providing clear, evidence-based insights into case probabilities, predictive analytics empowers lawyers to make strategic choices with greater confidence.&lt;/p&gt;&lt;p&gt;The efficiency gains associated with these technologies cannot be overstated. AI streamlines various processes in law firms, reducing time spent on mundane tasks and allowing attorneys to focus on higher-value activities. The efficiency translates into cost savings and improved service delivery, positioning firms that adopt these tools as leaders in the competitive legal market.&lt;/p&gt;&lt;p&gt;While the benefits of integrating AI into personal injury law are substantial, several challenges must be addressed to ensure responsible implementation. Data privacy is a primary concern; ensuring that client information is protected while using these advanced tools is paramount. Legal professionals must navigate these complexities carefully to maintain trust and compliance with regulations.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/the-future-of-personal-injury-law-ai-and-legal-tech-in-philadelphia/</guid><pubDate>Fri, 09 Jan 2026 15:01:54 +0000</pubDate></item><item><title>[NEW] X’s half-assed attempt to paywall Grok doesn’t block free image editing (AI - Ars Technica)</title><link>https://arstechnica.com/tech-policy/2026/01/xs-half-assed-attempt-to-paywall-grok-doesnt-block-free-image-editing/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Faced with a ban in the United Kingdom, X pushes flawed fix to CSAM problem.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151277734-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151277734-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Apu Gomes / Stringer | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Once again, people are taking Grok at its word, treating the chatbot as a company spokesperson without questioning what it says.&lt;/p&gt;
&lt;p&gt;On Friday morning, many outlets reported that X had blocked universal access to Grok’s image-editing features after the chatbot began prompting some users to pay $8 to use them. The messages are seemingly in response to reporting that people are using Grok to generate thousands of non-consensual sexualized images of women and children each hour.&lt;/p&gt;
&lt;p&gt;“Image generation and editing are currently limited to paying subscribers,” Grok tells users, dropping a link and urging, “you can subscribe to unlock these features.”&lt;/p&gt;
&lt;p&gt;However, as The Verge pointed out and Ars verified, unsubscribed X users can still use Grok to edit images. X seems to have limited users’ ability to request edits made by replying to Grok while still allowing image edits through the desktop site. App users can access the same feature by long-pressing on any image.&lt;/p&gt;
&lt;p&gt;Using image-editing features without publicly prompting Grok keeps outputs out of the public feed. That means the only issue X has rushed to solve is stopping Grok from directly posting harmful images on the platform.&lt;/p&gt;
&lt;p&gt;X declined to comment on whether it’s working to close those loopholes, but it has a history of pushing janky updates since Elon Musk took over the platform formerly known as Twitter. Still, motivated X users can also continue using the standalone Grok app or website to make abusive content for free.&lt;/p&gt;
&lt;p&gt;Like images X users can edit without publicly asking Grok, these images aren’t posted publicly to an official X account but are likely to be shared by bad actors—some of whom, according to the BBC, are already promoting allegedly Grok-generated child sexual abuse materials (CSAM) on the dark web. That’s especially concerning since Wired reported this week that users of the Grok app and website are generating far more graphic and disturbing images than what X users are creating.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;X risks fines if UK rejects supposed fix&lt;/h2&gt;
&lt;p&gt;It’s unclear how charging for Grok image editing will block controversial outputs, as Grok’s problematic safety guidelines remain intact. The chatbot is still instructed to assume that users have “good intent” when requesting images of “teenage” girls, which xAI says “does not necessarily imply underage.”&lt;/p&gt;
&lt;p&gt;That could lead to Grok continuing to post harmful images of minors. xAI’s other priorities include Grok directives to avoid moralizing users and to place “no restrictions” on “fictional adult sexual content with dark or violent themes.” An AI safety expert told Ars that Grok could be tweaked to be safer, describing the chatbot’s safety guidelines as the kind of policy a platform would design if it “wanted to look safe while still allowing a lot under the hood.”&lt;/p&gt;
&lt;p&gt;Updates to Grok’s X responses came after the platform risked fines and legal action from regulators around the world, including a potential ban in the United Kingdom.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;X seems to hope that forcing users to share identification and credit card information as paying subscribers will make them less likely to use Grok to generate illegal content. But advocates who combat image-based sex abuse note that content like Grok’s “undressing” outputs can cause lasting psychological, financial, and reputational harm, even if the content is not illegal in some states.&lt;/p&gt;
&lt;p&gt;That suggests that paying subscribers could continue using Grok to create harmful images that X may leave unchecked because they’re not technically illegal. In 2024, X agreed to voluntarily moderate all non-consensual intimate images, but Musk’s promotion of revealing bikini images of public and private figures suggests that’s no longer the case.&lt;/p&gt;
&lt;p&gt;It seems likely that Grok will continue to be used to create non-consensual intimate images. So rather than solve the problem, X may at best succeed in limiting public exposure to Grok’s appalling outputs. The company may even profit from the feature, as Wired reported that Grok pushed “nudifying” or “undressing” apps into the mainstream.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;So far, US regulators have been quiet about Grok’s outputs, with the Justice Department generally promising to take all forms of CSAM seriously. On Friday, Democratic senators started shifting those tides, demanding that Google and Apple remove X and Grok from app stores until it improves safeguards to block harmful outputs.&lt;/p&gt;
&lt;p&gt;“There can be no mistake about X’s knowledge, and, at best, negligent response to these trends,” the senators wrote in a letter to Apple Chief Executive Officer Tim Cook and Google Chief Executive Officer Sundar Pichai. “Turning a blind eye to X’s egregious behavior would make a mockery of your moderation practices. Indeed, not taking action would undermine your claims in public and in court that your app stores offer a safer user experience than letting users download apps directly to their phones.”&lt;/p&gt;
&lt;p&gt;A response to the letter is requested by January 23.&lt;/p&gt;
&lt;p&gt;Whether the UK will accept X’s supposed solution is yet to be seen. If UK regulator Ofcom decides to move ahead with a probe into whether Musk’s chatbot violates the UK’s Online Safety Act, X could face a UK ban or fines of up to 10 percent of the company’s global turnover.&lt;/p&gt;
&lt;p&gt;“It’s unlawful,” UK Prime Minister Keir Starmer said of Grok’s worst outputs. “We’re not going to tolerate it. I’ve asked for all options to be on the table. It’s disgusting. X need to get their act together and get this material down. We will take action on this because it’s simply not tolerable.”&lt;/p&gt;
&lt;p&gt;At least one UK parliament member, Jess Asato, told The Guardian that even if X had put up an actual paywall, that isn’t enough to end the scrutiny.&lt;/p&gt;
&lt;p&gt;“While it is a step forward to have removed the universal access to Grok’s disgusting nudifying features, this still means paying users can take images of women without their consent to sexualise and brutalise them,” Asato said. “Paying to put semen, bullet holes, or bikinis on women is still digital sexual assault, and xAI should disable the feature for good.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Faced with a ban in the United Kingdom, X pushes flawed fix to CSAM problem.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151277734-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151277734-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Apu Gomes / Stringer | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Once again, people are taking Grok at its word, treating the chatbot as a company spokesperson without questioning what it says.&lt;/p&gt;
&lt;p&gt;On Friday morning, many outlets reported that X had blocked universal access to Grok’s image-editing features after the chatbot began prompting some users to pay $8 to use them. The messages are seemingly in response to reporting that people are using Grok to generate thousands of non-consensual sexualized images of women and children each hour.&lt;/p&gt;
&lt;p&gt;“Image generation and editing are currently limited to paying subscribers,” Grok tells users, dropping a link and urging, “you can subscribe to unlock these features.”&lt;/p&gt;
&lt;p&gt;However, as The Verge pointed out and Ars verified, unsubscribed X users can still use Grok to edit images. X seems to have limited users’ ability to request edits made by replying to Grok while still allowing image edits through the desktop site. App users can access the same feature by long-pressing on any image.&lt;/p&gt;
&lt;p&gt;Using image-editing features without publicly prompting Grok keeps outputs out of the public feed. That means the only issue X has rushed to solve is stopping Grok from directly posting harmful images on the platform.&lt;/p&gt;
&lt;p&gt;X declined to comment on whether it’s working to close those loopholes, but it has a history of pushing janky updates since Elon Musk took over the platform formerly known as Twitter. Still, motivated X users can also continue using the standalone Grok app or website to make abusive content for free.&lt;/p&gt;
&lt;p&gt;Like images X users can edit without publicly asking Grok, these images aren’t posted publicly to an official X account but are likely to be shared by bad actors—some of whom, according to the BBC, are already promoting allegedly Grok-generated child sexual abuse materials (CSAM) on the dark web. That’s especially concerning since Wired reported this week that users of the Grok app and website are generating far more graphic and disturbing images than what X users are creating.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;X risks fines if UK rejects supposed fix&lt;/h2&gt;
&lt;p&gt;It’s unclear how charging for Grok image editing will block controversial outputs, as Grok’s problematic safety guidelines remain intact. The chatbot is still instructed to assume that users have “good intent” when requesting images of “teenage” girls, which xAI says “does not necessarily imply underage.”&lt;/p&gt;
&lt;p&gt;That could lead to Grok continuing to post harmful images of minors. xAI’s other priorities include Grok directives to avoid moralizing users and to place “no restrictions” on “fictional adult sexual content with dark or violent themes.” An AI safety expert told Ars that Grok could be tweaked to be safer, describing the chatbot’s safety guidelines as the kind of policy a platform would design if it “wanted to look safe while still allowing a lot under the hood.”&lt;/p&gt;
&lt;p&gt;Updates to Grok’s X responses came after the platform risked fines and legal action from regulators around the world, including a potential ban in the United Kingdom.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;X seems to hope that forcing users to share identification and credit card information as paying subscribers will make them less likely to use Grok to generate illegal content. But advocates who combat image-based sex abuse note that content like Grok’s “undressing” outputs can cause lasting psychological, financial, and reputational harm, even if the content is not illegal in some states.&lt;/p&gt;
&lt;p&gt;That suggests that paying subscribers could continue using Grok to create harmful images that X may leave unchecked because they’re not technically illegal. In 2024, X agreed to voluntarily moderate all non-consensual intimate images, but Musk’s promotion of revealing bikini images of public and private figures suggests that’s no longer the case.&lt;/p&gt;
&lt;p&gt;It seems likely that Grok will continue to be used to create non-consensual intimate images. So rather than solve the problem, X may at best succeed in limiting public exposure to Grok’s appalling outputs. The company may even profit from the feature, as Wired reported that Grok pushed “nudifying” or “undressing” apps into the mainstream.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;So far, US regulators have been quiet about Grok’s outputs, with the Justice Department generally promising to take all forms of CSAM seriously. On Friday, Democratic senators started shifting those tides, demanding that Google and Apple remove X and Grok from app stores until it improves safeguards to block harmful outputs.&lt;/p&gt;
&lt;p&gt;“There can be no mistake about X’s knowledge, and, at best, negligent response to these trends,” the senators wrote in a letter to Apple Chief Executive Officer Tim Cook and Google Chief Executive Officer Sundar Pichai. “Turning a blind eye to X’s egregious behavior would make a mockery of your moderation practices. Indeed, not taking action would undermine your claims in public and in court that your app stores offer a safer user experience than letting users download apps directly to their phones.”&lt;/p&gt;
&lt;p&gt;A response to the letter is requested by January 23.&lt;/p&gt;
&lt;p&gt;Whether the UK will accept X’s supposed solution is yet to be seen. If UK regulator Ofcom decides to move ahead with a probe into whether Musk’s chatbot violates the UK’s Online Safety Act, X could face a UK ban or fines of up to 10 percent of the company’s global turnover.&lt;/p&gt;
&lt;p&gt;“It’s unlawful,” UK Prime Minister Keir Starmer said of Grok’s worst outputs. “We’re not going to tolerate it. I’ve asked for all options to be on the table. It’s disgusting. X need to get their act together and get this material down. We will take action on this because it’s simply not tolerable.”&lt;/p&gt;
&lt;p&gt;At least one UK parliament member, Jess Asato, told The Guardian that even if X had put up an actual paywall, that isn’t enough to end the scrutiny.&lt;/p&gt;
&lt;p&gt;“While it is a step forward to have removed the universal access to Grok’s disgusting nudifying features, this still means paying users can take images of women without their consent to sexualise and brutalise them,” Asato said. “Paying to put semen, bullet holes, or bikinis on women is still digital sexual assault, and xAI should disable the feature for good.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2026/01/xs-half-assed-attempt-to-paywall-grok-doesnt-block-free-image-editing/</guid><pubDate>Fri, 09 Jan 2026 16:46:04 +0000</pubDate></item><item><title>[NEW] Datadog: How AI code reviews slash incident risk (AI News)</title><link>https://www.artificialintelligence-news.com/news/datadog-how-ai-code-reviews-slash-incident-risk/</link><description>&lt;p&gt;Integrating AI into code review workflows allows engineering leaders to detect systemic risks that often evade human detection at scale.&lt;/p&gt;&lt;p&gt;For engineering leaders managing distributed systems, the trade-off between deployment speed and operational stability often defines the success of their platform. Datadog, a company responsible for the observability of complex infrastructures worldwide, operates under intense pressure to maintain this balance.&lt;/p&gt;&lt;p&gt;When a client’s systems fail, they rely on Datadog’s platform to diagnose the root cause—meaning reliability must be established well &lt;em&gt;before&lt;/em&gt; software reaches a production environment.&lt;/p&gt;&lt;p&gt;Scaling this reliability is an operational challenge. Code review has traditionally acted as the primary gatekeeper, a high-stakes phase where senior engineers attempt to catch errors. However, as teams expand, relying on human reviewers to maintain deep contextual knowledge of the entire codebase becomes unsustainable.&lt;/p&gt;&lt;p&gt;To address this bottleneck, Datadog’s AI Development Experience (AI DevX) team integrated OpenAI’s Codex, aiming to automate the detection of risks that human reviewers frequently miss.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-static-analysis-falls-short"&gt;Why static analysis falls short&lt;/h3&gt;&lt;p&gt;The enterprise market has long utilised automated tools to assist in code review, but their effectiveness has historically been limited.&lt;/p&gt;&lt;p&gt;Early iterations of AI code review tools often performed like “advanced linters,” identifying superficial syntax issues but failing to grasp the broader system architecture. Because these tools lacked the ability to understand context, engineers at Datadog frequently dismissed their suggestions as noise.&lt;/p&gt;&lt;p&gt;The core issue was not detecting errors in isolation, but understanding how a specific change might ripple through interconnected systems. Datadog required a solution capable of reasoning over the codebase and its dependencies, rather than simply scanning for style violations.&lt;/p&gt;&lt;p&gt;The team integrated the new agent directly into the workflow of one of their most active repositories, allowing it to review every pull request automatically. Unlike static analysis tools, this system compares the developer’s intent with the actual code submission, executing tests to validate behaviour.&lt;/p&gt;&lt;p&gt;For CTOs and CIOs, the difficulty in adopting generative AI often lies in proving its value beyond theoretical efficiency. Datadog bypassed standard productivity metrics by creating an “incident replay harness” to test the tool against historical outages.&lt;/p&gt;&lt;p&gt;Instead of relying on hypothetical test cases, the team reconstructed past pull requests that were known to have caused incidents. They then ran the AI agent against these specific changes to determine if it would have flagged the issues that humans missed in their code reviews.&lt;/p&gt;&lt;p&gt;The results provided a concrete data point for risk mitigation: the agent identified over 10 cases (approximately 22% of the examined incidents) where its feedback would have prevented the error. These were pull requests that had already bypassed human review, demonstrating that the AI surfaced risks invisible to the engineers at the time.&lt;/p&gt;&lt;p&gt;This validation changed the internal conversation regarding the tool’s utility. Brad Carter, who leads the AI DevX team, noted that while efficiency gains are welcome, “preventing incidents is far more compelling at our scale.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-ai-code-reviews-are-changing-engineering-culture"&gt;How AI code reviews are changing engineering culture&lt;/h3&gt;&lt;p&gt;The deployment of this technology to more than 1,000 engineers has influenced the culture of code review within the organisation. Rather than replacing the human element, the AI serves as a partner that handles the cognitive load of cross-service interactions.&lt;/p&gt;&lt;p&gt;Engineers reported that the system consistently flagged issues that were not obvious from the immediate code difference. It identified missing test coverage in areas of cross-service coupling and pointed out interactions with modules that the developer had not touched directly.&lt;/p&gt;&lt;p&gt;This depth of analysis changed how the engineering staff interacted with automated feedback.&lt;/p&gt;&lt;p&gt;“For me, a Codex comment feels like the smartest engineer I’ve worked with and who has infinite time to find bugs. It sees connections my brain doesn’t hold all at once,” explains Carter.&lt;/p&gt;&lt;p&gt;The AI code review system’s ability to contextualise changes allows human reviewers to shift their focus from catching bugs to evaluating architecture and design.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-bug-hunting-to-reliability"&gt;From bug hunting to reliability&lt;/h3&gt;&lt;p&gt;For enterprise leaders, the Datadog case study illustrates a transition in how code review is defined. It is no longer viewed merely as a checkpoint for error detection or a metric for cycle time, but as a core reliability system.&lt;/p&gt;&lt;p&gt;By surfacing risks that exceed individual context, the technology supports a strategy where confidence in shipping code scales alongside the team. This aligns with the priorities of Datadog’s leadership, who view reliability as a fundamental component of customer trust.&lt;/p&gt;&lt;p&gt;“We are the platform companies rely on when everything else is breaking,” says Carter. “Preventing incidents strengthens the trust our customers place in us”.&lt;/p&gt;&lt;p&gt;The successful integration of AI into the code review pipeline suggests that the technology’s highest value in the enterprise may lie in its ability to enforce complex quality standards that protect the bottom line.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Agentic AI scaling requires new memory architecture&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111551" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-2.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Integrating AI into code review workflows allows engineering leaders to detect systemic risks that often evade human detection at scale.&lt;/p&gt;&lt;p&gt;For engineering leaders managing distributed systems, the trade-off between deployment speed and operational stability often defines the success of their platform. Datadog, a company responsible for the observability of complex infrastructures worldwide, operates under intense pressure to maintain this balance.&lt;/p&gt;&lt;p&gt;When a client’s systems fail, they rely on Datadog’s platform to diagnose the root cause—meaning reliability must be established well &lt;em&gt;before&lt;/em&gt; software reaches a production environment.&lt;/p&gt;&lt;p&gt;Scaling this reliability is an operational challenge. Code review has traditionally acted as the primary gatekeeper, a high-stakes phase where senior engineers attempt to catch errors. However, as teams expand, relying on human reviewers to maintain deep contextual knowledge of the entire codebase becomes unsustainable.&lt;/p&gt;&lt;p&gt;To address this bottleneck, Datadog’s AI Development Experience (AI DevX) team integrated OpenAI’s Codex, aiming to automate the detection of risks that human reviewers frequently miss.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-static-analysis-falls-short"&gt;Why static analysis falls short&lt;/h3&gt;&lt;p&gt;The enterprise market has long utilised automated tools to assist in code review, but their effectiveness has historically been limited.&lt;/p&gt;&lt;p&gt;Early iterations of AI code review tools often performed like “advanced linters,” identifying superficial syntax issues but failing to grasp the broader system architecture. Because these tools lacked the ability to understand context, engineers at Datadog frequently dismissed their suggestions as noise.&lt;/p&gt;&lt;p&gt;The core issue was not detecting errors in isolation, but understanding how a specific change might ripple through interconnected systems. Datadog required a solution capable of reasoning over the codebase and its dependencies, rather than simply scanning for style violations.&lt;/p&gt;&lt;p&gt;The team integrated the new agent directly into the workflow of one of their most active repositories, allowing it to review every pull request automatically. Unlike static analysis tools, this system compares the developer’s intent with the actual code submission, executing tests to validate behaviour.&lt;/p&gt;&lt;p&gt;For CTOs and CIOs, the difficulty in adopting generative AI often lies in proving its value beyond theoretical efficiency. Datadog bypassed standard productivity metrics by creating an “incident replay harness” to test the tool against historical outages.&lt;/p&gt;&lt;p&gt;Instead of relying on hypothetical test cases, the team reconstructed past pull requests that were known to have caused incidents. They then ran the AI agent against these specific changes to determine if it would have flagged the issues that humans missed in their code reviews.&lt;/p&gt;&lt;p&gt;The results provided a concrete data point for risk mitigation: the agent identified over 10 cases (approximately 22% of the examined incidents) where its feedback would have prevented the error. These were pull requests that had already bypassed human review, demonstrating that the AI surfaced risks invisible to the engineers at the time.&lt;/p&gt;&lt;p&gt;This validation changed the internal conversation regarding the tool’s utility. Brad Carter, who leads the AI DevX team, noted that while efficiency gains are welcome, “preventing incidents is far more compelling at our scale.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-ai-code-reviews-are-changing-engineering-culture"&gt;How AI code reviews are changing engineering culture&lt;/h3&gt;&lt;p&gt;The deployment of this technology to more than 1,000 engineers has influenced the culture of code review within the organisation. Rather than replacing the human element, the AI serves as a partner that handles the cognitive load of cross-service interactions.&lt;/p&gt;&lt;p&gt;Engineers reported that the system consistently flagged issues that were not obvious from the immediate code difference. It identified missing test coverage in areas of cross-service coupling and pointed out interactions with modules that the developer had not touched directly.&lt;/p&gt;&lt;p&gt;This depth of analysis changed how the engineering staff interacted with automated feedback.&lt;/p&gt;&lt;p&gt;“For me, a Codex comment feels like the smartest engineer I’ve worked with and who has infinite time to find bugs. It sees connections my brain doesn’t hold all at once,” explains Carter.&lt;/p&gt;&lt;p&gt;The AI code review system’s ability to contextualise changes allows human reviewers to shift their focus from catching bugs to evaluating architecture and design.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-bug-hunting-to-reliability"&gt;From bug hunting to reliability&lt;/h3&gt;&lt;p&gt;For enterprise leaders, the Datadog case study illustrates a transition in how code review is defined. It is no longer viewed merely as a checkpoint for error detection or a metric for cycle time, but as a core reliability system.&lt;/p&gt;&lt;p&gt;By surfacing risks that exceed individual context, the technology supports a strategy where confidence in shipping code scales alongside the team. This aligns with the priorities of Datadog’s leadership, who view reliability as a fundamental component of customer trust.&lt;/p&gt;&lt;p&gt;“We are the platform companies rely on when everything else is breaking,” says Carter. “Preventing incidents strengthens the trust our customers place in us”.&lt;/p&gt;&lt;p&gt;The successful integration of AI into the code review pipeline suggests that the technology’s highest value in the enterprise may lie in its ability to enforce complex quality standards that protect the bottom line.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Agentic AI scaling requires new memory architecture&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111551" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-2.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/datadog-how-ai-code-reviews-slash-incident-risk/</guid><pubDate>Fri, 09 Jan 2026 17:39:40 +0000</pubDate></item><item><title>[NEW] Meta signs deals with three nuclear companies for 6-plus GW of power (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/09/meta-signs-deals-with-three-nuclear-companies-for-6-plus-gw-of-power/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/06/GettyImages-129268875.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta today announced three deals to provide its data centers with nuclear power: one from a startup, one from a smaller energy company, and one from a larger company that already operates several nuclear reactors in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Oklo and TerraPower, two companies developing small modular reactors (SMR), each signed agreements with Meta to build multiple reactors, while Vistra is selling capacity from its existing power plants.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nuclear power has become a favored power source for tech companies as their AI ambitions have grown, providing stable 24/7 electricity. Startups and existing reactors have benefited from the race for data center power, though in different ways.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Existing reactors tend to be the cheapest form of baseload capacity, but there are only so many to go around, which has pushed Meta and its peers toward SMR startups. Companies like Oklo and TerraPower are betting that by building a large number of smaller reactors, they’ll be able to bring the cost down through mass manufacturing. It’s a plausible hypothesis, though one that has yet to be tested. Meta’s deal could give SMR startups a chance to prove it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deals are the result of a request for proposals that Meta issued in December 2024, in which Meta sought partners that could add between 1 to 4 gigawatts of generating capacity by the early 2030s. Much of the new power will flow through the PJM interconnection, a grid which covers 13 Mid-Atlantic and Midwestern states and has become saturated with data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The 20-year agreement with Vistra will have the most immediate impact on Meta’s energy needs. The tech company will buy a total of 2.1 gigawatts from two existing nuclear power plants, Perry and Davis-Besse in Ohio.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the deal, Vistra will also add capacity to those power plants and to its Beaver Valley power plant in Pennsylvania. Together, the upgrades will generate an additional 433 MW and are scheduled to come online in the early 2030s.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Meta is also buying 1.2 gigawatts from young provider Oklo. Under its deal with Meta, Oklo is hoping to start supplying power to the grid as early as 2030. The SMR company went public via SPAC in 2023, and while Oklo has landed a large deal with data center operator Switch, it has struggled to get its reactor design approved by the Nuclear Regulatory Commission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If Oklo can deliver on its timeline, the new reactors would be built in Pike County, Ohio. The startup’s Aurora Powerhouse reactors each produce 75 megawatts of electricity, and it will need to build more than a dozen to fulfill Meta’s order.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TerraPower is a startup co-founded by Bill Gates, and it is aiming to start sending electricity to Meta as early as 2032. It has designed a reactor that uses molten sodium to transfer energy from reactor to generator. When demand is low, the superheated salt can be stored in an insulated vat until more power is needed. The reactor can generate 345 megawatts of electricity, while the storage system can provide an additional 100 to 500 megawatts for more than five hours.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company has navigated the NRC process more smoothly, and it is working with GE Hitachi to build its first power plant in Wyoming. Its first two reactors for Meta would provide 690 megawatts, and Meta said it has rights to buy another six units for a total of 2.8 gigawatts of nuclear capacity and 1.2 gigawatts of storage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta did not disclose financial terms of the deals.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The power purchases from Vistra are certain to be the cheapest — electricity from already operating nuclear reactors is among the cheapest on the grid.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Costs for SMRs still have yet to be worked out. Several startups have aggressive cost targets: TerraPower has estimated that it can bring it down to $50 to $60 per megawatt-hour, while Oklo has said it is aiming for $80 to $130 per megawatt-hour. Those figures are for later power plants — the first examples are likely to cost more.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/06/GettyImages-129268875.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta today announced three deals to provide its data centers with nuclear power: one from a startup, one from a smaller energy company, and one from a larger company that already operates several nuclear reactors in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Oklo and TerraPower, two companies developing small modular reactors (SMR), each signed agreements with Meta to build multiple reactors, while Vistra is selling capacity from its existing power plants.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nuclear power has become a favored power source for tech companies as their AI ambitions have grown, providing stable 24/7 electricity. Startups and existing reactors have benefited from the race for data center power, though in different ways.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Existing reactors tend to be the cheapest form of baseload capacity, but there are only so many to go around, which has pushed Meta and its peers toward SMR startups. Companies like Oklo and TerraPower are betting that by building a large number of smaller reactors, they’ll be able to bring the cost down through mass manufacturing. It’s a plausible hypothesis, though one that has yet to be tested. Meta’s deal could give SMR startups a chance to prove it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deals are the result of a request for proposals that Meta issued in December 2024, in which Meta sought partners that could add between 1 to 4 gigawatts of generating capacity by the early 2030s. Much of the new power will flow through the PJM interconnection, a grid which covers 13 Mid-Atlantic and Midwestern states and has become saturated with data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The 20-year agreement with Vistra will have the most immediate impact on Meta’s energy needs. The tech company will buy a total of 2.1 gigawatts from two existing nuclear power plants, Perry and Davis-Besse in Ohio.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the deal, Vistra will also add capacity to those power plants and to its Beaver Valley power plant in Pennsylvania. Together, the upgrades will generate an additional 433 MW and are scheduled to come online in the early 2030s.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Meta is also buying 1.2 gigawatts from young provider Oklo. Under its deal with Meta, Oklo is hoping to start supplying power to the grid as early as 2030. The SMR company went public via SPAC in 2023, and while Oklo has landed a large deal with data center operator Switch, it has struggled to get its reactor design approved by the Nuclear Regulatory Commission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If Oklo can deliver on its timeline, the new reactors would be built in Pike County, Ohio. The startup’s Aurora Powerhouse reactors each produce 75 megawatts of electricity, and it will need to build more than a dozen to fulfill Meta’s order.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TerraPower is a startup co-founded by Bill Gates, and it is aiming to start sending electricity to Meta as early as 2032. It has designed a reactor that uses molten sodium to transfer energy from reactor to generator. When demand is low, the superheated salt can be stored in an insulated vat until more power is needed. The reactor can generate 345 megawatts of electricity, while the storage system can provide an additional 100 to 500 megawatts for more than five hours.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company has navigated the NRC process more smoothly, and it is working with GE Hitachi to build its first power plant in Wyoming. Its first two reactors for Meta would provide 690 megawatts, and Meta said it has rights to buy another six units for a total of 2.8 gigawatts of nuclear capacity and 1.2 gigawatts of storage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta did not disclose financial terms of the deals.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The power purchases from Vistra are certain to be the cheapest — electricity from already operating nuclear reactors is among the cheapest on the grid.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Costs for SMRs still have yet to be worked out. Several startups have aggressive cost targets: TerraPower has estimated that it can bring it down to $50 to $60 per megawatt-hour, while Oklo has said it is aiming for $80 to $130 per megawatt-hour. Those figures are for later power plants — the first examples are likely to cost more.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/09/meta-signs-deals-with-three-nuclear-companies-for-6-plus-gw-of-power/</guid><pubDate>Fri, 09 Jan 2026 17:51:15 +0000</pubDate></item><item><title>[NEW] CES 2026: Follow live for the best, weirdest, most interesting tech as this robot and AI-heavy event wraps up (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/storyline/ces-2026-follow-live-for-the-best-weirdest-most-interesting-tech-as-this-robot-and-ai-heavy-event-wraps/</link><description>&lt;div class="wp-block-techcrunch-post-authors-list"&gt;
	&lt;div class="post-authors-list__authors"&gt;
		&lt;ul class="post-authors-list__author-thumbs"&gt;
							&lt;li&gt;
											&lt;img alt="Lucas Ropek" class="post-authors-list__author-thumb" src="https://secure.gravatar.com/avatar/495cdfd5deaad915a1ad58ab35edcbaa84b90c4ce9b7ded356c5ad1b61884800?s=265&amp;amp;d=identicon&amp;amp;r=g" /&gt;
									&lt;/li&gt;
					&lt;/ul&gt;
		&lt;ul class="post-authors-list__author-list"&gt;
							&lt;li&gt;Lucas Ropek&lt;/li&gt;
					&lt;/ul&gt;
	&lt;/div&gt;
&lt;/div&gt;
							&lt;h3 class="loop-card__title"&gt;
					This new solid-state EV battery can fully charge in just 5 minutes
				&lt;/h3&gt;
			
							
&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;I stopped by the exhibit for Donut Lab, a startup out of Finland that specializes in electric mobility. The company (which gets its name from its flagship donut-shaped in-wheel EV vehicle motor) announced at CES the launch of what it calls the first solid-state battery for vehicle production.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Solid-state batteries differ from lithium-ion batteries (which are used by a majority of EVs) in that they use solid rather than liquid electrolytes. They are supposed to offer much greater energy density (more bang for your buck, so to speak) and better safety, and they degrade less than lithium ion batteries. On top of all that, Donut says its battery can fully charge in a lean five minutes.&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;p class="wp-block-paragraph"&gt;Charging times obviously differ between vehicles and models but five minutes is, you know, pretty damn fast. Donut claims that, with the long-range version of its battery, a rider can get up to 600 kilometers on a single charge. The company also says that its battery quashes many of the causes of battery fires, as the SSB remains stable across extreme temperatures and includes no flammable liquid. As a result, it’s also supposed to operate better in cold environments (chilly weather has been known to reduce the range capacity of many EVs).&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Donut is a subsidiary of Verge Motorcycles. Verge Motorcycle co-founder and former CTO Marko Lehtimaki is the co-founder and CEO of Donut Lab. Lehtimaki isn’t new to the startup scene. He has founded a number of companies, including no-code software startup AppGyver, which was acquired by SAP in 2021.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Donut’s new SSBs will be introduced to Verge’s motorcycles early this year, the companies said this week. The batteries will be incorporated into Verge’s Verge TS Pro and Verge TS Ultra. At its exhibit, the company showed off a number of other partner vehicles that will soon have the batteries incorporated into them.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3081079" height="510" src="https://techcrunch.com/wp-content/uploads/2026/01/donutlab-verge-motorcycles-ces.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;kirsten korosec&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</description><content:encoded>&lt;div class="wp-block-techcrunch-post-authors-list"&gt;
	&lt;div class="post-authors-list__authors"&gt;
		&lt;ul class="post-authors-list__author-thumbs"&gt;
							&lt;li&gt;
											&lt;img alt="Lucas Ropek" class="post-authors-list__author-thumb" src="https://secure.gravatar.com/avatar/495cdfd5deaad915a1ad58ab35edcbaa84b90c4ce9b7ded356c5ad1b61884800?s=265&amp;amp;d=identicon&amp;amp;r=g" /&gt;
									&lt;/li&gt;
					&lt;/ul&gt;
		&lt;ul class="post-authors-list__author-list"&gt;
							&lt;li&gt;Lucas Ropek&lt;/li&gt;
					&lt;/ul&gt;
	&lt;/div&gt;
&lt;/div&gt;
							&lt;h3 class="loop-card__title"&gt;
					This new solid-state EV battery can fully charge in just 5 minutes
				&lt;/h3&gt;
			
							
&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;I stopped by the exhibit for Donut Lab, a startup out of Finland that specializes in electric mobility. The company (which gets its name from its flagship donut-shaped in-wheel EV vehicle motor) announced at CES the launch of what it calls the first solid-state battery for vehicle production.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Solid-state batteries differ from lithium-ion batteries (which are used by a majority of EVs) in that they use solid rather than liquid electrolytes. They are supposed to offer much greater energy density (more bang for your buck, so to speak) and better safety, and they degrade less than lithium ion batteries. On top of all that, Donut says its battery can fully charge in a lean five minutes.&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;p class="wp-block-paragraph"&gt;Charging times obviously differ between vehicles and models but five minutes is, you know, pretty damn fast. Donut claims that, with the long-range version of its battery, a rider can get up to 600 kilometers on a single charge. The company also says that its battery quashes many of the causes of battery fires, as the SSB remains stable across extreme temperatures and includes no flammable liquid. As a result, it’s also supposed to operate better in cold environments (chilly weather has been known to reduce the range capacity of many EVs).&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Donut is a subsidiary of Verge Motorcycles. Verge Motorcycle co-founder and former CTO Marko Lehtimaki is the co-founder and CEO of Donut Lab. Lehtimaki isn’t new to the startup scene. He has founded a number of companies, including no-code software startup AppGyver, which was acquired by SAP in 2021.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Donut’s new SSBs will be introduced to Verge’s motorcycles early this year, the companies said this week. The batteries will be incorporated into Verge’s Verge TS Pro and Verge TS Ultra. At its exhibit, the company showed off a number of other partner vehicles that will soon have the batteries incorporated into them.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3081079" height="510" src="https://techcrunch.com/wp-content/uploads/2026/01/donutlab-verge-motorcycles-ces.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;kirsten korosec&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/storyline/ces-2026-follow-live-for-the-best-weirdest-most-interesting-tech-as-this-robot-and-ai-heavy-event-wraps/</guid><pubDate>Fri, 09 Jan 2026 17:57:00 +0000</pubDate></item><item><title>[NEW] CES 2026 was all about ‘physical AI’ and robots, robots, robots (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/ces-2026-was-all-about-physical-ai-and-robots-robots-robots/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/atlas-announcement.jpg?resize=1200,847" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;After years of chatbots and image generators, AI is finally leaving the screen. At CES 2026, that shift became impossible to ignore.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;The annual tech showcase in Las Vegas was&amp;nbsp;dominated by “physical AI”&amp;nbsp;and robotics, from Boston Dynamic’s newly redesigned Atlas humanoid robot to AI-powered ice makers (yes, really). The companies in attendance clearly want consumers to know: AI&amp;nbsp;isn’t&amp;nbsp;just&amp;nbsp;capable of&amp;nbsp;answering&amp;nbsp;questions&amp;nbsp;anymore.&amp;nbsp;It’s&amp;nbsp;ready to move car parts in factories, catching&amp;nbsp;drones with net guns, and dance&amp;nbsp;in automaker booths.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, hosts Kirsten Korosec, Anthony Ha, and Sean O’Kane break down everything we saw at CES 2026&amp;nbsp;and more deals from the week that caught our eye.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&amp;nbsp;&lt;/p&gt;



















&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/atlas-announcement.jpg?resize=1200,847" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;After years of chatbots and image generators, AI is finally leaving the screen. At CES 2026, that shift became impossible to ignore.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;The annual tech showcase in Las Vegas was&amp;nbsp;dominated by “physical AI”&amp;nbsp;and robotics, from Boston Dynamic’s newly redesigned Atlas humanoid robot to AI-powered ice makers (yes, really). The companies in attendance clearly want consumers to know: AI&amp;nbsp;isn’t&amp;nbsp;just&amp;nbsp;capable of&amp;nbsp;answering&amp;nbsp;questions&amp;nbsp;anymore.&amp;nbsp;It’s&amp;nbsp;ready to move car parts in factories, catching&amp;nbsp;drones with net guns, and dance&amp;nbsp;in automaker booths.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, hosts Kirsten Korosec, Anthony Ha, and Sean O’Kane break down everything we saw at CES 2026&amp;nbsp;and more deals from the week that caught our eye.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&amp;nbsp;&lt;/p&gt;



















&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/ces-2026-was-all-about-physical-ai-and-robots-robots-robots/</guid><pubDate>Fri, 09 Jan 2026 18:02:09 +0000</pubDate></item></channel></rss>