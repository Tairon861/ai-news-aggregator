<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 24 Feb 2026 07:04:01 +0000</lastBuildDate><item><title>Google’s Cloud AI leads on the three frontiers of model capability (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/googles-cloud-ai-lead-on-the-three-frontiers-of-model-capability/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/google-logo.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As a product VP at Google Cloud, Michael Gerstenhaber works mostly on Vertex AI, the company’s unified platform for deploying enterprise AI. It gives him a high-level view of how companies are actually using AI models, and what still needs to be done to unleash the potential of agentic AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When I spoke with Gerstenhaber, I was particularly struck by one idea I hadn’t heard before. As he put it, AI models are pushing against three frontiers at once: raw intelligence, response time, and a third quality that has less to do with raw capability than with cost — whether a model can be deployed cheaply enough to run at massive, unpredictable scale. It’s a new way of thinking about model capabilities, and a particularly valuable one for anyone trying to push frontier models in a new direction.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This interview has been edited for length and clarity.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Why don’t you start by walking us through your experience in AI so far, and what you do at Google.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I’ve been in AI for about two years now. I was at Anthropic for a year and a half, I’ve been at Google almost half a year now. I run Vertex AI, Google’s developer platform. Most of our customers are engineers building their own applications. They want access to agentic patterns. They want access to an agentic platform. They want access to the inference of the smartest models in the world. I provide them that, but I don’t provide the applications themselves. That’s for Shopify, Thomson Reuters, and our various customers to provide in their own domains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;What drew you to Google?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is I think unique in the world in that we have everything from the interface to the infrastructure layer. We can build data centers. We can buy electricity and build power plants. We have our own chips. We have our own model. We have the inference layer that we control. We have the agentic layer we control. We have APIs for memory, for interleaved code writing. We have an agent engine on top of that that ensures compliance and governance. And then we even have the chat interface with Gemini enterprise and Gemini chat for consumers, right? So part of the reason I came here is because I saw Google as uniquely vertically integrated, and that being a strength for us.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;It’s odd because, even with all the differences between companies, it feels like all three of the big labs are really&lt;/strong&gt; &lt;strong&gt;close in capabilities. Is it just a race for more intelligence, or is it more complicated than that?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I see three boundaries. Models like Gemini Pro are tuned for raw intelligence. Think about writing code. You just want the best code you can get, doesn’t matter if it takes 45 minutes, because I have to maintain it, I have to put it in production. I just want the best.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then there’s this other boundary with latency. If I’m doing customer support and I need to know how to apply a policy, you need intelligence to apply that policy. Are you allowed to transact a return? Can I upgrade my seat on an airplane? But it doesn’t matter how right you are if it took 45 minutes to get the answer. So for those cases, you want the most intelligent product within that latency budget, because more intelligence no longer matters once that person gets bored and hangs up the phone.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And then there’s this last bucket, where somebody like Reddit or Meta wants to moderate the entire internet. They have large budgets, but they can’t take an enterprise risk on something if they don’t know how it scales. They don’t know how many poisonous posts there will be today or tomorrow. So they have to restrict their budget to a model at the highest intelligence they can afford, but in a scalable way to an infinite number of subjects. And for that, cost becomes very, very important.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;One of the things I’ve been puzzling about is why agentic systems are taking so long to catch on. It feels like the models are there and I’ve seen incredible demos, but we’re not seeing the kind of major changes I would have expected a year ago. What do you think is holding it back?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This technology is basically two years old, and there’s still a lot of missing infrastructure. We don’t have patterns for auditing what the agents are doing. We don’t have patterns for authorization of data to an agent. There are these patterns that are going to require work to put into production. And production is always a trailing indicator of what the technology is capable of. So two years isn’t long enough to see what the intelligence supports in production, and that’s where people are struggling.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think it’s moved uniquely quickly in software engineering because it fits nicely in the software development lifecycle. We have a dev environment in which it’s safe to break things, and then we promote from the dev environment to the test environment. The process of writing code at Google requires two people to audit that code and both affirm that it’s good enough to put Google’s brand behind and give to our customers. So we have a lot of those human-in-the-loop processes that make the implementation exceptionally low-risk. But we need to produce those patterns in other places and for other professions.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/google-logo.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As a product VP at Google Cloud, Michael Gerstenhaber works mostly on Vertex AI, the company’s unified platform for deploying enterprise AI. It gives him a high-level view of how companies are actually using AI models, and what still needs to be done to unleash the potential of agentic AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When I spoke with Gerstenhaber, I was particularly struck by one idea I hadn’t heard before. As he put it, AI models are pushing against three frontiers at once: raw intelligence, response time, and a third quality that has less to do with raw capability than with cost — whether a model can be deployed cheaply enough to run at massive, unpredictable scale. It’s a new way of thinking about model capabilities, and a particularly valuable one for anyone trying to push frontier models in a new direction.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This interview has been edited for length and clarity.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Why don’t you start by walking us through your experience in AI so far, and what you do at Google.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I’ve been in AI for about two years now. I was at Anthropic for a year and a half, I’ve been at Google almost half a year now. I run Vertex AI, Google’s developer platform. Most of our customers are engineers building their own applications. They want access to agentic patterns. They want access to an agentic platform. They want access to the inference of the smartest models in the world. I provide them that, but I don’t provide the applications themselves. That’s for Shopify, Thomson Reuters, and our various customers to provide in their own domains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;What drew you to Google?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is I think unique in the world in that we have everything from the interface to the infrastructure layer. We can build data centers. We can buy electricity and build power plants. We have our own chips. We have our own model. We have the inference layer that we control. We have the agentic layer we control. We have APIs for memory, for interleaved code writing. We have an agent engine on top of that that ensures compliance and governance. And then we even have the chat interface with Gemini enterprise and Gemini chat for consumers, right? So part of the reason I came here is because I saw Google as uniquely vertically integrated, and that being a strength for us.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;It’s odd because, even with all the differences between companies, it feels like all three of the big labs are really&lt;/strong&gt; &lt;strong&gt;close in capabilities. Is it just a race for more intelligence, or is it more complicated than that?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I see three boundaries. Models like Gemini Pro are tuned for raw intelligence. Think about writing code. You just want the best code you can get, doesn’t matter if it takes 45 minutes, because I have to maintain it, I have to put it in production. I just want the best.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then there’s this other boundary with latency. If I’m doing customer support and I need to know how to apply a policy, you need intelligence to apply that policy. Are you allowed to transact a return? Can I upgrade my seat on an airplane? But it doesn’t matter how right you are if it took 45 minutes to get the answer. So for those cases, you want the most intelligent product within that latency budget, because more intelligence no longer matters once that person gets bored and hangs up the phone.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And then there’s this last bucket, where somebody like Reddit or Meta wants to moderate the entire internet. They have large budgets, but they can’t take an enterprise risk on something if they don’t know how it scales. They don’t know how many poisonous posts there will be today or tomorrow. So they have to restrict their budget to a model at the highest intelligence they can afford, but in a scalable way to an infinite number of subjects. And for that, cost becomes very, very important.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;One of the things I’ve been puzzling about is why agentic systems are taking so long to catch on. It feels like the models are there and I’ve seen incredible demos, but we’re not seeing the kind of major changes I would have expected a year ago. What do you think is holding it back?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This technology is basically two years old, and there’s still a lot of missing infrastructure. We don’t have patterns for auditing what the agents are doing. We don’t have patterns for authorization of data to an agent. There are these patterns that are going to require work to put into production. And production is always a trailing indicator of what the technology is capable of. So two years isn’t long enough to see what the intelligence supports in production, and that’s where people are struggling.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think it’s moved uniquely quickly in software engineering because it fits nicely in the software development lifecycle. We have a dev environment in which it’s safe to break things, and then we promote from the dev environment to the test environment. The process of writing code at Google requires two people to audit that code and both affirm that it’s good enough to put Google’s brand behind and give to our customers. So we have a lot of those human-in-the-loop processes that make the implementation exceptionally low-risk. But we need to produce those patterns in other places and for other professions.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/googles-cloud-ai-lead-on-the-three-frontiers-of-model-capability/</guid><pubDate>Mon, 23 Feb 2026 19:18:42 +0000</pubDate></item><item><title>Anthropic accuses Chinese AI labs of mining Claude as US debates AI chip exports (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/anthropic-accuses-chinese-ai-labs-of-mining-claude-as-us-debates-ai-chip-exports/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2262515136.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic is accusing three Chinese AI companies of setting up more than 24,000 fake accounts with its Claude AI model to improve their own models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The labs — DeepSeek, Moonshot AI, and MiniMax — allegedly generated more than 16 million exchanges with Claude through those accounts using a technique called “distillation.” Anthropic said the labs “targeted Claude’s most differentiated capabilities: agentic reasoning, tool use, and coding.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The accusations come amid debates over how strictly to enforce export controls on advanced AI chips, a policy aimed at curbing China’s AI development.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Distillation is a common training method that AI labs use on their own models to create smaller, cheaper versions, but competitors can use it to essentially copy the homework of other labs. OpenAI sent a memo to House lawmakers earlier this month accusing DeepSeek of using distillation to mimic its products.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek first made waves a year ago when it released its open source R1 reasoning model that nearly matched American frontier labs in performance at a fraction of the cost. DeepSeek is expected to soon release DeepSeek V4, its latest model, which reportedly can outperform Anthropic’s Claude and OpenAI’s ChatGPT in coding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The scale of each attack differed in scope. Anthropic tracked more than 150,000 exchanges from DeepSeek that seemed aimed at improving foundational logic and alignment, specifically around censorship-safe alternatives to policy-sensitive queries.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Moonshot AI had more than 3.4 million exchanges targeting agentic reasoning and tool use, coding and data analysis, computer-use agent development, and computer vision. Last month, the firm released a new open source model Kimi K2.5 and a coding agent.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;MiniMax’s 13 million exchanges targeted agentic coding and tool use and orchestration. Anthropic said it was able to observe MiniMax in action as it redirected nearly half its traffic to siphon capabilities from the latest Claude model when it was launched.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic says it will continue to invest in defenses that make distillation attacks harder to execute and easier to identify, but is calling on “a coordinated response across the AI industry, cloud providers, and policymakers.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The distillation attacks come at a time when American chip exports to China are still hotly debated. Last month, the Trump administration formally allowed U.S. companies like Nvidia to export advanced AI chips (like the H200) to China. Critics have argued that this loosening of export controls increases China’s AI computing capacity at a critical time in the global race for AI dominance.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anthropic says that the scale of extraction DeepSeek, MiniMax, and Moonshot performed “requires access to advanced chips.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Distillation attacks therefore reinforce the rationale for export controls: restricted chip access limits both direct model training and the scale of illicit distillation,” per Anthropic’s blog.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dmitri Alperovitch, chairman of the Silverado Policy Accelerator think-tank and co-founder of CrowdStrike, told TechCrunch he’s not surprised to see these attacks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s been clear for a while now that part of the reason for the rapid progress of Chinese AI models has been theft via distillation of U.S. frontier models. Now we know this for a fact,” Alperovitch said. “This should give us even more compelling reasons to refuse to sell any AI chips to any of these [companies], which would only advantage them further.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic also said distillation doesn’t only threaten to undercut American AI dominance, but could also create national security risks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Anthropic and other U.S. companies build systems that prevent state and non-state actors from using AI to, for example, develop bioweapons or carry out malicious cyber activities,” reads Anthropic’s blog post. “Models built through illicit distillation are unlikely to retain those safeguards, meaning that dangerous capabilities can proliferate with many protections stripped out entirely.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic pointed to authoritarian governments deploying frontier AI for things like “offensive cyber operations, disinformation campaigns, and mass surveillance,” a risk that is multiplied if those models are open sourced.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to DeepSeek, MiniMax, and Moonshot for comment. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2262515136.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic is accusing three Chinese AI companies of setting up more than 24,000 fake accounts with its Claude AI model to improve their own models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The labs — DeepSeek, Moonshot AI, and MiniMax — allegedly generated more than 16 million exchanges with Claude through those accounts using a technique called “distillation.” Anthropic said the labs “targeted Claude’s most differentiated capabilities: agentic reasoning, tool use, and coding.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The accusations come amid debates over how strictly to enforce export controls on advanced AI chips, a policy aimed at curbing China’s AI development.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Distillation is a common training method that AI labs use on their own models to create smaller, cheaper versions, but competitors can use it to essentially copy the homework of other labs. OpenAI sent a memo to House lawmakers earlier this month accusing DeepSeek of using distillation to mimic its products.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek first made waves a year ago when it released its open source R1 reasoning model that nearly matched American frontier labs in performance at a fraction of the cost. DeepSeek is expected to soon release DeepSeek V4, its latest model, which reportedly can outperform Anthropic’s Claude and OpenAI’s ChatGPT in coding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The scale of each attack differed in scope. Anthropic tracked more than 150,000 exchanges from DeepSeek that seemed aimed at improving foundational logic and alignment, specifically around censorship-safe alternatives to policy-sensitive queries.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Moonshot AI had more than 3.4 million exchanges targeting agentic reasoning and tool use, coding and data analysis, computer-use agent development, and computer vision. Last month, the firm released a new open source model Kimi K2.5 and a coding agent.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;MiniMax’s 13 million exchanges targeted agentic coding and tool use and orchestration. Anthropic said it was able to observe MiniMax in action as it redirected nearly half its traffic to siphon capabilities from the latest Claude model when it was launched.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic says it will continue to invest in defenses that make distillation attacks harder to execute and easier to identify, but is calling on “a coordinated response across the AI industry, cloud providers, and policymakers.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The distillation attacks come at a time when American chip exports to China are still hotly debated. Last month, the Trump administration formally allowed U.S. companies like Nvidia to export advanced AI chips (like the H200) to China. Critics have argued that this loosening of export controls increases China’s AI computing capacity at a critical time in the global race for AI dominance.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anthropic says that the scale of extraction DeepSeek, MiniMax, and Moonshot performed “requires access to advanced chips.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Distillation attacks therefore reinforce the rationale for export controls: restricted chip access limits both direct model training and the scale of illicit distillation,” per Anthropic’s blog.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dmitri Alperovitch, chairman of the Silverado Policy Accelerator think-tank and co-founder of CrowdStrike, told TechCrunch he’s not surprised to see these attacks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s been clear for a while now that part of the reason for the rapid progress of Chinese AI models has been theft via distillation of U.S. frontier models. Now we know this for a fact,” Alperovitch said. “This should give us even more compelling reasons to refuse to sell any AI chips to any of these [companies], which would only advantage them further.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic also said distillation doesn’t only threaten to undercut American AI dominance, but could also create national security risks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Anthropic and other U.S. companies build systems that prevent state and non-state actors from using AI to, for example, develop bioweapons or carry out malicious cyber activities,” reads Anthropic’s blog post. “Models built through illicit distillation are unlikely to retain those safeguards, meaning that dangerous capabilities can proliferate with many protections stripped out entirely.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic pointed to authoritarian governments deploying frontier AI for things like “offensive cyber operations, disinformation campaigns, and mass surveillance,” a risk that is multiplied if those models are open sourced.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to DeepSeek, MiniMax, and Moonshot for comment. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/anthropic-accuses-chinese-ai-labs-of-mining-claude-as-us-debates-ai-chip-exports/</guid><pubDate>Mon, 23 Feb 2026 19:57:27 +0000</pubDate></item><item><title>With AI, investor loyalty is (almost) dead: At least a dozen OpenAI VCs now also back Anthropic (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/with-ai-investor-loyalty-is-almost-dead-at-least-a-dozen-openai-vcs-now-also-back-anthropic/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/06/GettyImages-516286525.jpg?resize=1200,1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;With OpenAI on the verge of finalizing a new $100 billion round, and Anthropic just closing its own monster $30 billion raise, one thing is clear: The concept of investor “loyalty” is only hanging on by a thread.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At least a dozen direct investors in OpenAI were announced as backers in Anthropic’s $30 billion raise earlier this month, including Founders Fund, Iconiq, Insight Partners, and Sequoia Capital.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Some dual investments are understandable if they come from the hedge fund or asset manager worlds, where their focus is still largely investing in public stocks (competitors or not). These include D1, Fidelity, and TPG.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of these was a bit shocking. Affiliated funds of BlackRock joined in Anthropic’s $30 billion raise even though BlackRock’s senior managing director and board member Adebayo Ogunlesi is also on OpenAI’s board of directors.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In that world, it’s true that if various BlackRock funds get a chance to own OpenAI stock, they are likely to take it, never mind the personal association of a member of their senior leadership. (BlackRock runs every type of fund, including mutuals, closed-ends, and ETFs).&amp;nbsp;And we all know the history of OpenAI and Microsoft’s relationship and why Microsoft is hedging its bets. Ditto for Nvidia.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But venture capital funds have — until now — operated differently. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;VCs market themselves as “founder friendly” and “helpful,” the idea being that when a VC firm buys a chunk of a startup’s company, the investor will help that startup be successful, particularly against its major rivals. If you are an owner of both OpenAI and Anthropic, who does your loyalty belong to, besides your own investors?&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, startups are private companies. They typically share confidential information with their direct investors on their business status — data that isn’t disclosed publicly the way it is with public companies. In many cases, the VCs also take board seats, which carries another level of fiduciary responsibility to their portfolio companies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What makes this particular case even more interesting is that Sam Altman comes from the world of venture capital, as a former president of Y Combinator. He knows the drill. In 2024, he reportedly gave his investors a list of OpenAI’s rivals that he didn’t want them to back. It largely included companies launched by folks who left OpenAI, including Anthropic, xAI, and Safe Superintelligence.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman later denied that he told OpenAI investors they would be barred from future rounds if they backed his list of perceived rivals. Altman did admit that he said if they “made non-passive investments,” they would no longer receive OpenAI’s confidential business information, according to documents in the lawsuit between Elon Musk and OpenAI, Business Insider reported.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI is also breaking the mold because of the record-breaking amounts of money that the largest AI labs are raising as they experience never-before-seen growth (and never-before-seen data center needs). At some point, when the hat is being passed around, the needs are so great and the possibilities of returns are so large, who can be expected to say no?&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It turns out that not all venture investors have yet slid down the slippery slope. Andreessen Horowitz backs OpenAI but not (yet) Anthropic. Menlo Ventures backs Anthropic but not (yet) OpenAI, for instance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In fact, in our admittedly not exhaustive research, we found a dozen investors that appear to only have direct investments in one of these companies, not both.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Others include Bessemer Venture Partners, General Catalyst, and Greenoaks. (Note: We originally asked Claude to give us the list of dual investors. It got almost as many entries wrong as it got right, so all this for a very cool tech whose work sometimes remains less trustworthy than an intern’s.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, as we previously reported, the fact that this longstanding rule has been tossed by some of the most respected firms in the Valley, like Sequoia, is notable. One investor we reached out to simply shrugged and said that as long as the firm doesn’t have a board seat, no one sees the harm in it anymore.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, conflict-of-interest policies should now become another thing that founders ask about before signing that term sheet, no matter who it’s from.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/06/GettyImages-516286525.jpg?resize=1200,1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;With OpenAI on the verge of finalizing a new $100 billion round, and Anthropic just closing its own monster $30 billion raise, one thing is clear: The concept of investor “loyalty” is only hanging on by a thread.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At least a dozen direct investors in OpenAI were announced as backers in Anthropic’s $30 billion raise earlier this month, including Founders Fund, Iconiq, Insight Partners, and Sequoia Capital.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Some dual investments are understandable if they come from the hedge fund or asset manager worlds, where their focus is still largely investing in public stocks (competitors or not). These include D1, Fidelity, and TPG.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of these was a bit shocking. Affiliated funds of BlackRock joined in Anthropic’s $30 billion raise even though BlackRock’s senior managing director and board member Adebayo Ogunlesi is also on OpenAI’s board of directors.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In that world, it’s true that if various BlackRock funds get a chance to own OpenAI stock, they are likely to take it, never mind the personal association of a member of their senior leadership. (BlackRock runs every type of fund, including mutuals, closed-ends, and ETFs).&amp;nbsp;And we all know the history of OpenAI and Microsoft’s relationship and why Microsoft is hedging its bets. Ditto for Nvidia.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But venture capital funds have — until now — operated differently. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;VCs market themselves as “founder friendly” and “helpful,” the idea being that when a VC firm buys a chunk of a startup’s company, the investor will help that startup be successful, particularly against its major rivals. If you are an owner of both OpenAI and Anthropic, who does your loyalty belong to, besides your own investors?&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, startups are private companies. They typically share confidential information with their direct investors on their business status — data that isn’t disclosed publicly the way it is with public companies. In many cases, the VCs also take board seats, which carries another level of fiduciary responsibility to their portfolio companies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What makes this particular case even more interesting is that Sam Altman comes from the world of venture capital, as a former president of Y Combinator. He knows the drill. In 2024, he reportedly gave his investors a list of OpenAI’s rivals that he didn’t want them to back. It largely included companies launched by folks who left OpenAI, including Anthropic, xAI, and Safe Superintelligence.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman later denied that he told OpenAI investors they would be barred from future rounds if they backed his list of perceived rivals. Altman did admit that he said if they “made non-passive investments,” they would no longer receive OpenAI’s confidential business information, according to documents in the lawsuit between Elon Musk and OpenAI, Business Insider reported.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI is also breaking the mold because of the record-breaking amounts of money that the largest AI labs are raising as they experience never-before-seen growth (and never-before-seen data center needs). At some point, when the hat is being passed around, the needs are so great and the possibilities of returns are so large, who can be expected to say no?&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It turns out that not all venture investors have yet slid down the slippery slope. Andreessen Horowitz backs OpenAI but not (yet) Anthropic. Menlo Ventures backs Anthropic but not (yet) OpenAI, for instance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In fact, in our admittedly not exhaustive research, we found a dozen investors that appear to only have direct investments in one of these companies, not both.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Others include Bessemer Venture Partners, General Catalyst, and Greenoaks. (Note: We originally asked Claude to give us the list of dual investors. It got almost as many entries wrong as it got right, so all this for a very cool tech whose work sometimes remains less trustworthy than an intern’s.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, as we previously reported, the fact that this longstanding rule has been tossed by some of the most respected firms in the Valley, like Sequoia, is notable. One investor we reached out to simply shrugged and said that as long as the firm doesn’t have a board seat, no one sees the harm in it anymore.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, conflict-of-interest policies should now become another thing that founders ask about before signing that term sheet, no matter who it’s from.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/with-ai-investor-loyalty-is-almost-dead-at-least-a-dozen-openai-vcs-now-also-back-anthropic/</guid><pubDate>Mon, 23 Feb 2026 21:46:41 +0000</pubDate></item><item><title>Data center builders thought farmers would willingly sell land, learn otherwise (AI - Ars Technica)</title><link>https://arstechnica.com/tech-policy/2026/02/im-not-for-sale-farmers-refuse-to-take-millions-in-data-center-deals/</link><description>&lt;article class="double-column h-entry post-2142230 post type-post status-publish format-standard has-post-thumbnail hentry category-ai category-tech-policy tag-artificial-intelligence tag-data-centers"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Even in a fragile farm economy, million-dollar offers can’t sway dedicated farmers.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1233733221-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1233733221-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Bloomberg / Contributor | Bloomberg

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;It seems that tech giants eyeing rural zones for data center development have underestimated how attached American farmers have grown to their lands in the decades they’ve been nurturing them.&lt;/p&gt;
&lt;p&gt;Across the country, several farmers have firmly rejected eye-popping offers—sometimes in the tens of millions. These offers dwarf the value of their properties, but farmers have refused to put a price on the lands that they love most.&lt;/p&gt;
&lt;p&gt;In a report on Monday, The Guardian highlighted a handful of cases nationwide where farmers’ refusals have frustrated plans to build data centers in areas long deemed rural.&lt;/p&gt;
&lt;p&gt;It’s unclear how many farmers have received such offers, but rural lands have been increasingly targeted as demand for data centers to power AI has grown—most recently projected to increase by 165 percent by 2030. Globally, 40,000 acres are needed to support data center growth over the next five years, Hines Research estimated.&lt;/p&gt;
&lt;p&gt;For “Silicon Valley executives,” rural areas are likely attractive due to “weak zoning protections, cheap power, and abundant water,” The Guardian reported.&lt;/p&gt;
&lt;p&gt;It likely doesn’t help to sell the farmers these deals when they tend to come out of nowhere, following a knock on the door from a middleman who doesn’t make it clear who wants to buy the land or how the land would be used.&lt;/p&gt;
&lt;p&gt;One 82-year-old Kentucky woman, Ida Huddleston, turned away a “Fortune 500 company” offering $33 million for 650 acres. NBC News reported that several of her neighbors received similar offers. Huddleston joined at least five other residents in the county who refused to move forward after learning they’d have to sign a non-disclosure agreement just to find out who they would be dealing with. Ultimately, Huddleston had to search public records to figure out that a data center was even being planned in the area, The Guardian reported. The lack of transparency is a problem, farmers have said, because what buyers want to do with the land matters.&lt;/p&gt;
&lt;p&gt;“You don’t have enough to buy me out,” Huddleston told the company representatives when rejecting the deal. “I’m not for sale. Leave me alone, I’m satisfied.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Notably, one resident in Huddleston’s county who received an offer, 75-year-old Timothy Grosser, even declined a proposal to “name your price” when a tech company sought to buy his 250-acre farm, The Guardian reported.&lt;/p&gt;
&lt;p&gt;“There is none,” Grosser said.&lt;/p&gt;
&lt;p&gt;The farm is where he “lives, hunts, and raises cattle” and where his grandson hunts a turkey every Christmas for the family feast.&lt;/p&gt;
&lt;p&gt;“The money’s not worth giving up your lifestyle,” Grosser said.&lt;/p&gt;
&lt;p&gt;Another farmer in Wisconsin, Anthony Barta, reportedly fretted about what would happen to his neighbors if he took a deal he was offered—showing the deep bonds of people whose farms have bordered each other for years. In his community, another farmer was offered between $70 million and $80 million for 6,000 acres.&lt;/p&gt;
&lt;p&gt;“Me and my family, we own the farm and run close to 1,000 animals,” Barta said. “What would that do if that’s next to it? Can they even be there? You know, that’s our livelihood—the farm. We’re just concerned what, if it would go through, what would happen to us and our neighbors and farms and our community? What would happen to that?”&lt;/p&gt;
&lt;p&gt;Some tech companies are apparently not taking “no” for an answer. At least one farmer who spent 51 years milking cows in Pennsylvania prior to the AI boom described tech companies as “relentless.”&lt;/p&gt;
&lt;p&gt;Eighty-six-year-old Mervin Raudabaugh, Jr., found a creative solution to end the pressure to sell two contiguous farms. He reportedly staved off developers by turning to “a farmland preservation program dedicating taxpayer dollars toward protecting agricultural resources.”&lt;/p&gt;
&lt;p&gt;By working with the program, Raudabaugh will only receive about one-eighth of what the developers were offering. But he said it’s worth it to know his land would be preserved for farming purposes and out of reach of persistent tech companies.&lt;/p&gt;
&lt;p&gt;“These people have hounded the living daylights out of me,” Raudabaugh said.&lt;/p&gt;
&lt;h2&gt;Data center deals come amid fragile farm economy&lt;/h2&gt;
&lt;p&gt;For people in rural communities, data center fights go beyond concerns about water and electricity consumption—although those are concerns, too. Communities are defending the character of the land, which they don’t want to see suddenly disrupted by extensive construction, data center noise pollution, or untold environmental impacts from massive operations.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;There are also public health concerns, as an attorney with environmental nonprofit Earthjustice, Jonathan Kalmuss-Katz, told The Guardian that the pollution new data centers will emit could possibly proliferate “forever chemicals” known as polyfluoroalkyl substances (PFAS).&lt;/p&gt;
&lt;p&gt;“We know there are PFAS in these centers, and all of that has to go somewhere,” Kalmuss-Katz said. “This issue has been dangerously understudied as we have been building out data centers, and there’s not adequate information on what the long term impacts will be.”&lt;/p&gt;
&lt;p&gt;Some rural communities are fighting to block rezoning requests that would allow developers to build data centers in areas previously zoned only for agricultural lands. But those fights are seemingly hard-won. At least one Michigan community sought to settle after a developer firm working for an unnamed tech company reportedly sued to push through a construction project on farmland.&lt;/p&gt;
&lt;p&gt;For farmers, the data center deals come at a particularly difficult time financially. On Friday, the National Farmers Union issued a statement after the Supreme Court blocked the majority of Trump’s tariffs. In it, NFU President Rob Larew confirmed that “many family farmers and ranchers have already felt the consequences” of Trump’s tariffs, which have raised costs, disrupted sales, and triggered retaliations impacting US agricultural goods.&lt;/p&gt;
&lt;p&gt;“In an already fragile farm economy, uncertainty has hit family operations hardest,” Larew said.&lt;/p&gt;
&lt;p&gt;The deals also arrived at a time when the number of US farms is shrinking, continuing “a long-lasting trend of declining farm numbers,” Farm Journal reported this month. In total, the US lost about 15,000 farms in 2025, with no state reporting an increase in farms.&lt;/p&gt;
&lt;p&gt;So far, not much farmland has been ceded to data centers, the report seemed to indicate, perhaps due to farmers who dig their heels in when developer representatives come knocking.&lt;/p&gt;
&lt;p&gt;Perhaps particularly in this dire climate—where farms are shrinking and existing farms are cash-strapped from unpredictable tariffs—it seems notable that farmers are not being swayed by developers’ offers of what the Guardian described as “unimaginable riches.”&lt;/p&gt;
&lt;p&gt;But Huddleston made it sound easy to turn down $33 million, because her ties to the land run deep. She told The Guardian that “four generations of the Huddleston family have watched the world change from the same fields,” while raising cattle and living off the land.&lt;/p&gt;
&lt;p&gt;“My whole entire life is nothing but the land,” Huddleston said. “It’s provided me with anything and everything that I’ve needed for 82 years.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #b388ff; background-color: #512da8;"&gt;&lt;img alt="scortiusthecharioteer" class="ars-avatar-image" src="https://cdn.arstechnica.net/civis/data/avatars/m/821/821783.jpg?1685642227" /&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              scortiusthecharioteer
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            &lt;blockquote class="xfBb-quote"&gt;On the note of pollution from dtatacenters, infrasound is understudied, especially its impact on wildlife ecology - and in this case, livestock as well.&lt;p&gt;That's aside from the human impacts, which could really use some public awareness and further research.&lt;/p&gt;&lt;/blockquote&gt;Decent piece on it recently by Benn Jordan.  &lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;View: https://youtu.be/_bP80DEAbuo?si=Sm9kNaEj7gJgZdx1&lt;br /&gt;
          &lt;/p&gt;&lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2026-02-23T22:14:33+00:00"&gt;February 23, 2026 at 10:14 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</description><content:encoded>&lt;article class="double-column h-entry post-2142230 post type-post status-publish format-standard has-post-thumbnail hentry category-ai category-tech-policy tag-artificial-intelligence tag-data-centers"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Even in a fragile farm economy, million-dollar offers can’t sway dedicated farmers.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1233733221-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1233733221-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Bloomberg / Contributor | Bloomberg

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;It seems that tech giants eyeing rural zones for data center development have underestimated how attached American farmers have grown to their lands in the decades they’ve been nurturing them.&lt;/p&gt;
&lt;p&gt;Across the country, several farmers have firmly rejected eye-popping offers—sometimes in the tens of millions. These offers dwarf the value of their properties, but farmers have refused to put a price on the lands that they love most.&lt;/p&gt;
&lt;p&gt;In a report on Monday, The Guardian highlighted a handful of cases nationwide where farmers’ refusals have frustrated plans to build data centers in areas long deemed rural.&lt;/p&gt;
&lt;p&gt;It’s unclear how many farmers have received such offers, but rural lands have been increasingly targeted as demand for data centers to power AI has grown—most recently projected to increase by 165 percent by 2030. Globally, 40,000 acres are needed to support data center growth over the next five years, Hines Research estimated.&lt;/p&gt;
&lt;p&gt;For “Silicon Valley executives,” rural areas are likely attractive due to “weak zoning protections, cheap power, and abundant water,” The Guardian reported.&lt;/p&gt;
&lt;p&gt;It likely doesn’t help to sell the farmers these deals when they tend to come out of nowhere, following a knock on the door from a middleman who doesn’t make it clear who wants to buy the land or how the land would be used.&lt;/p&gt;
&lt;p&gt;One 82-year-old Kentucky woman, Ida Huddleston, turned away a “Fortune 500 company” offering $33 million for 650 acres. NBC News reported that several of her neighbors received similar offers. Huddleston joined at least five other residents in the county who refused to move forward after learning they’d have to sign a non-disclosure agreement just to find out who they would be dealing with. Ultimately, Huddleston had to search public records to figure out that a data center was even being planned in the area, The Guardian reported. The lack of transparency is a problem, farmers have said, because what buyers want to do with the land matters.&lt;/p&gt;
&lt;p&gt;“You don’t have enough to buy me out,” Huddleston told the company representatives when rejecting the deal. “I’m not for sale. Leave me alone, I’m satisfied.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Notably, one resident in Huddleston’s county who received an offer, 75-year-old Timothy Grosser, even declined a proposal to “name your price” when a tech company sought to buy his 250-acre farm, The Guardian reported.&lt;/p&gt;
&lt;p&gt;“There is none,” Grosser said.&lt;/p&gt;
&lt;p&gt;The farm is where he “lives, hunts, and raises cattle” and where his grandson hunts a turkey every Christmas for the family feast.&lt;/p&gt;
&lt;p&gt;“The money’s not worth giving up your lifestyle,” Grosser said.&lt;/p&gt;
&lt;p&gt;Another farmer in Wisconsin, Anthony Barta, reportedly fretted about what would happen to his neighbors if he took a deal he was offered—showing the deep bonds of people whose farms have bordered each other for years. In his community, another farmer was offered between $70 million and $80 million for 6,000 acres.&lt;/p&gt;
&lt;p&gt;“Me and my family, we own the farm and run close to 1,000 animals,” Barta said. “What would that do if that’s next to it? Can they even be there? You know, that’s our livelihood—the farm. We’re just concerned what, if it would go through, what would happen to us and our neighbors and farms and our community? What would happen to that?”&lt;/p&gt;
&lt;p&gt;Some tech companies are apparently not taking “no” for an answer. At least one farmer who spent 51 years milking cows in Pennsylvania prior to the AI boom described tech companies as “relentless.”&lt;/p&gt;
&lt;p&gt;Eighty-six-year-old Mervin Raudabaugh, Jr., found a creative solution to end the pressure to sell two contiguous farms. He reportedly staved off developers by turning to “a farmland preservation program dedicating taxpayer dollars toward protecting agricultural resources.”&lt;/p&gt;
&lt;p&gt;By working with the program, Raudabaugh will only receive about one-eighth of what the developers were offering. But he said it’s worth it to know his land would be preserved for farming purposes and out of reach of persistent tech companies.&lt;/p&gt;
&lt;p&gt;“These people have hounded the living daylights out of me,” Raudabaugh said.&lt;/p&gt;
&lt;h2&gt;Data center deals come amid fragile farm economy&lt;/h2&gt;
&lt;p&gt;For people in rural communities, data center fights go beyond concerns about water and electricity consumption—although those are concerns, too. Communities are defending the character of the land, which they don’t want to see suddenly disrupted by extensive construction, data center noise pollution, or untold environmental impacts from massive operations.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;There are also public health concerns, as an attorney with environmental nonprofit Earthjustice, Jonathan Kalmuss-Katz, told The Guardian that the pollution new data centers will emit could possibly proliferate “forever chemicals” known as polyfluoroalkyl substances (PFAS).&lt;/p&gt;
&lt;p&gt;“We know there are PFAS in these centers, and all of that has to go somewhere,” Kalmuss-Katz said. “This issue has been dangerously understudied as we have been building out data centers, and there’s not adequate information on what the long term impacts will be.”&lt;/p&gt;
&lt;p&gt;Some rural communities are fighting to block rezoning requests that would allow developers to build data centers in areas previously zoned only for agricultural lands. But those fights are seemingly hard-won. At least one Michigan community sought to settle after a developer firm working for an unnamed tech company reportedly sued to push through a construction project on farmland.&lt;/p&gt;
&lt;p&gt;For farmers, the data center deals come at a particularly difficult time financially. On Friday, the National Farmers Union issued a statement after the Supreme Court blocked the majority of Trump’s tariffs. In it, NFU President Rob Larew confirmed that “many family farmers and ranchers have already felt the consequences” of Trump’s tariffs, which have raised costs, disrupted sales, and triggered retaliations impacting US agricultural goods.&lt;/p&gt;
&lt;p&gt;“In an already fragile farm economy, uncertainty has hit family operations hardest,” Larew said.&lt;/p&gt;
&lt;p&gt;The deals also arrived at a time when the number of US farms is shrinking, continuing “a long-lasting trend of declining farm numbers,” Farm Journal reported this month. In total, the US lost about 15,000 farms in 2025, with no state reporting an increase in farms.&lt;/p&gt;
&lt;p&gt;So far, not much farmland has been ceded to data centers, the report seemed to indicate, perhaps due to farmers who dig their heels in when developer representatives come knocking.&lt;/p&gt;
&lt;p&gt;Perhaps particularly in this dire climate—where farms are shrinking and existing farms are cash-strapped from unpredictable tariffs—it seems notable that farmers are not being swayed by developers’ offers of what the Guardian described as “unimaginable riches.”&lt;/p&gt;
&lt;p&gt;But Huddleston made it sound easy to turn down $33 million, because her ties to the land run deep. She told The Guardian that “four generations of the Huddleston family have watched the world change from the same fields,” while raising cattle and living off the land.&lt;/p&gt;
&lt;p&gt;“My whole entire life is nothing but the land,” Huddleston said. “It’s provided me with anything and everything that I’ve needed for 82 years.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #b388ff; background-color: #512da8;"&gt;&lt;img alt="scortiusthecharioteer" class="ars-avatar-image" src="https://cdn.arstechnica.net/civis/data/avatars/m/821/821783.jpg?1685642227" /&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              scortiusthecharioteer
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            &lt;blockquote class="xfBb-quote"&gt;On the note of pollution from dtatacenters, infrasound is understudied, especially its impact on wildlife ecology - and in this case, livestock as well.&lt;p&gt;That's aside from the human impacts, which could really use some public awareness and further research.&lt;/p&gt;&lt;/blockquote&gt;Decent piece on it recently by Benn Jordan.  &lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;View: https://youtu.be/_bP80DEAbuo?si=Sm9kNaEj7gJgZdx1&lt;br /&gt;
          &lt;/p&gt;&lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2026-02-23T22:14:33+00:00"&gt;February 23, 2026 at 10:14 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2026/02/im-not-for-sale-farmers-refuse-to-take-millions-in-data-center-deals/</guid><pubDate>Mon, 23 Feb 2026 21:48:44 +0000</pubDate></item><item><title>Deploying Open Source Vision Language Models (VLM) on Jetson (Hugging Face - Blog)</title><link>https://huggingface.co/blog/nvidia/cosmos-on-jetson</link><description>&lt;div class="not-prose mb-6 font-sans lg:hidden"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8"&gt;   &lt;ul class="flex items-center text-gray-600  flex-row  text-base   "&gt;&lt;li class=" -mr-2 h-5 w-5 md:h-6 md:w-6   bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800 " title="kalyanvadrevu"&gt;&lt;img alt="alt" class="overflow-hidden rounded-full" src="https://huggingface.co/avatars/e4e855ad75d5d86959765610d67668b3.svg" /&gt;  &lt;/li&gt; &lt;li class="text-xs hover:text-gray-700 dark:text-gray-400 dark:hover:text-gray-300 order-last ml-3"&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;  &lt;dialog class="shadow-alternate z-40 mx-4 my-auto h-fit select-text overflow-hidden rounded-xl bg-white max-sm:max-w-[calc(100dvw-2rem)] sm:mx-auto lg:mt-26 md:portrait:mt-30 xl:mt-30 2xl:mt-32 w-full sm:w-96 max-w-[calc(100%-4rem)] text-base not-prose"&gt; &lt;/dialog&gt;&lt;/div&gt;&lt;/div&gt;   &lt;!-- HTML_TAG_START --&gt;
Vision-Language Models (VLMs) mark a significant leap in AI by blending visual perception with semantic reasoning. Moving beyond traditional models constrained by fixed labels, VLMs utilize a joint embedding space to interpret and discuss complex, open-ended environments using natural language. 
&lt;p&gt;The rapid evolution of reasoning accuracy and efficiency has made these models ideal for edge devices. The NVIDIA Jetson family, ranging from the high-performance AGX Thor and AGX Orin to the compact Orin Nano Super is purpose-built to drive accelerated applications for physical AI and robotics, providing the optimized runtime necessary for leading open source models. &lt;/p&gt;
&lt;p&gt;In this tutorial, we will demonstrate how to deploy the NVIDIA Cosmos Reasoning 2B model across the Jetson lineup using the vLLM framework. We will also guide you through connecting this model to the Live VLM WebUI, enabling a real-time, webcam-based interface for interactive physical AI. &lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Prerequisites
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Supported Devices:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jetson AGX Thor Developer Kit&lt;/li&gt;
&lt;li&gt;Jetson AGX Orin (64GB / 32GB)&lt;/li&gt;
&lt;li&gt;Jetson Orin Super Nano&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;JetPack Version:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;JetPack 6 (L4T r36.x) — for Orin devices&lt;/li&gt;
&lt;li&gt;JetPack 7 (L4T r38.x) — for Thor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Storage:&lt;/strong&gt; NVMe SSD &lt;strong&gt;required&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;~5 GB for the FP8 model weights&lt;/li&gt;
&lt;li&gt;~8 GB for the vLLM container image&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Accounts:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create NVIDIA NGC account(free) to download both the model and vLLM contanier&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Overview
	&lt;/span&gt;
&lt;/h2&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Jetson AGX Thor&lt;/th&gt;
&lt;th&gt;Jetson AGX Orin&lt;/th&gt;
&lt;th&gt;Orin Super Nano&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;vLLM Container&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nvcr.io/nvidia/vllm:26.01-py3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;FP8 via NGC (volume mount)&lt;/td&gt;
&lt;td&gt;FP8 via NGC (volume mount)&lt;/td&gt;
&lt;td&gt;FP8 via NGC (volume mount)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Max Model Length&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;8192 tokens&lt;/td&gt;
&lt;td&gt;8192 tokens&lt;/td&gt;
&lt;td&gt;256 tokens (memory-constrained)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;GPU Memory Util&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.65&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The workflow is the same for both devices:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Download&lt;/strong&gt; the FP8 model checkpoint via NGC CLI&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pull&lt;/strong&gt; the vLLM Docker image for your device&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Launch&lt;/strong&gt; the container with the model mounted as a volume&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Connect&lt;/strong&gt; Live VLM WebUI to the vLLM endpoint&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 1: Install the NGC CLI
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The NGC CLI lets you download model checkpoints from the NVIDIA NGC Catalog.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Download and install
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p ~/Projects/CosmosReasoning
cd ~/Projects/CosmosReasoning

# Download the NGC CLI for ARM64
# Get the latest installer URL from: https://org.ngc.nvidia.com/setup/installers/cli
wget -O ngccli_arm64.zip https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/4.13.0/files/ngccli_arm64.zip
unzip ngccli_arm64.zip
chmod u+x ngc-cli/ngc

# Add to PATH
export PATH="$PATH:$(pwd)/ngc-cli"
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Configure the CLI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;ngc config set
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will be prompted for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;API Key&lt;/strong&gt; — generate one at NGC API Key setup&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CLI output format&lt;/strong&gt; — choose &lt;code&gt;json&lt;/code&gt; or &lt;code&gt;ascii&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;org&lt;/strong&gt; — press Enter to accept the default&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 2: Download the Model
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Download the &lt;strong&gt;FP8 quantized&lt;/strong&gt; checkpoint. This is used on all Jetson devices:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd ~/Projects/CosmosReasoning
ngc registry model download-version "nim/nvidia/cosmos-reason2-2b:1208-fp8-static-kv8"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a directory called &lt;code&gt;cosmos-reason2-2b_v1208-fp8-static-kv8/&lt;/code&gt; containing the model weights. Note the full path — you will mount it into the Docker container as a volume.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 3: Pull the vLLM Docker Image
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		For Jetson AGX Thor
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;docker pull nvcr.io/nvidia/vllm:26.01-py3
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		For Jetson AGX Orin / Orin Super Nano
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;docker pull ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04
&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 4: Serve Cosmos Reasoning 2B with vLLM
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Option A: Jetson AGX Thor
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Thor has ample GPU memory and can run the model with generous context length.&lt;/p&gt;
&lt;p&gt;Set the path to your downloaded model and free cached memory on the host:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MODEL_PATH="$HOME/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8"
sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Launch the container with the model mounted:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -it \
  --runtime nvidia \
  --network host \
  --ipc host \
  -v "$MODEL_PATH:/models/cosmos-reason2-2b:ro" \
  -e NVIDIA_VISIBLE_DEVICES=all \
  -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
  nvcr.io/nvidia/vllm:26.01-py3 \
  bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Inside the container, activate the environment and serve the model:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vllm serve /models/cosmos-reason2-2b \
  --max-model-len 8192 \
  --media-io-kwargs '{"video": {"num_frames": -1}}' \
  --reasoning-parser qwen3 \
  --gpu-memory-utilization 0.8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The &lt;code&gt;--reasoning-parser qwen3&lt;/code&gt; flag enables chain-of-thought reasoning extraction. The &lt;code&gt;--media-io-kwargs&lt;/code&gt; flag configures video frame handling.&lt;/p&gt;
&lt;p&gt;Wait until you see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO:     Uvicorn running on http://0.0.0.0:8000
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Option B: Jetson AGX Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;AGX Orin has enough memory to run the model with the same generous parameters as Thor.&lt;/p&gt;
&lt;p&gt;Set the path to your downloaded model and free cached memory on the host:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MODEL_PATH="$HOME/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8"
sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1. Launch the container:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -it \
  --runtime nvidia \
  --network host \
  -v "$MODEL_PATH:/models/cosmos-reason2-2b:ro" \
  -e NVIDIA_VISIBLE_DEVICES=all \
  -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
  ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04 \
  bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Inside the container, activate the environment and serve:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /opt/
source venv/bin/activate

vllm serve /models/cosmos-reason2-2b \
  --max-model-len 8192 \
  --media-io-kwargs '{"video": {"num_frames": -1}}' \
  --reasoning-parser qwen3 \
  --gpu-memory-utilization 0.8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait until you see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO:     Uvicorn running on http://0.0.0.0:8000
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Option C: Jetson Orin Super Nano (memory-constrained)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The Orin Super Nano has significantly less RAM, so we need aggressive memory optimization flags.&lt;/p&gt;
&lt;p&gt;Set the path to your downloaded model and free cached memory on the host:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MODEL_PATH="$HOME/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8"
sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1. Launch the container:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -it \
  --runtime nvidia \
  --network host \
  -v "$MODEL_PATH:/models/cosmos-reason2-2b:ro" \
  -e NVIDIA_VISIBLE_DEVICES=all \
  -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
  ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04 \
  bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Inside the container, activate the environment and serve:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /opt/
source venv/bin/activate

vllm serve /models/cosmos-reason2-2b \
  --host 0.0.0.0 \
  --port 8000 \
  --trust-remote-code \
  --enforce-eager \
  --max-model-len 256 \
  --max-num-batched-tokens 256 \
  --gpu-memory-utilization 0.65 \
  --max-num-seqs 1 \
  --enable-chunked-prefill \
  --limit-mm-per-prompt '{"image":1,"video":1}' \
  --mm-processor-kwargs '{"num_frames":2,"max_pixels":150528}'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Key flags explained (Orin Super Nano only):&lt;/strong&gt;&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Flag&lt;/th&gt;
&lt;th&gt;Purpose&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--enforce-eager&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Disables CUDA graphs to save memory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--max-model-len 256&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Limits context to fit in available memory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--max-num-batched-tokens 256&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Matches the model length limit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--gpu-memory-utilization 0.65&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Reserves headroom for system processes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--max-num-seqs 1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Single request at a time to minimize memory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--enable-chunked-prefill&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Processes prefill in chunks for memory efficiency&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--limit-mm-per-prompt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Limits to 1 image and 1 video per prompt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--mm-processor-kwargs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Reduces video frames and image resolution&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--VLLM_SKIP_WARMUP=true&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Skips warmup to save time and memory&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Wait until you see the server is ready:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO:     Uvicorn running on http://0.0.0.0:8000
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Verify the server is running
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;From another terminal on the Jetson:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl http://localhost:8000/v1/models
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should see the model listed in the response.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 5: Test with a Quick API Call
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Before connecting the WebUI, verify the model responds correctly:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -s http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/models/cosmos-reason2-2b",
    "messages": [
      {
        "role": "user",
        "content": "What capabilities do you have?"
      }
    ],
    "max_tokens": 128
  }' | python3 -m json.tool
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; The model name used in the API request must match what vLLM reports. Verify with &lt;code&gt;curl http://localhost:8000/v1/models&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 6: Connect to Live VLM WebUI
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Live VLM WebUI provides a real-time webcam-to-VLM interface. With vLLM serving Cosmos Reasoning 2B, you can stream your webcam and get live AI analysis with reasoning.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Install Live VLM WebUI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The easiest method is pip (Open another terminal):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.local/bin/env
cd ~/Projects/CosmosReasoning
uv venv .live-vlm --python 3.12
source .live-vlm/bin/activate
uv pip install live-vlm-webui
live-vlm-webui
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or use Docker:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/nvidia-ai-iot/live-vlm-webui.git
cd live-vlm-webui
./scripts/start_container.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Configure the WebUI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Open &lt;strong&gt;&lt;code&gt;https://localhost:8090&lt;/code&gt;&lt;/strong&gt; in your browser&lt;/li&gt;
&lt;li&gt;Accept the self-signed certificate (click &lt;strong&gt;Advanced&lt;/strong&gt; → &lt;strong&gt;Proceed&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;In the &lt;strong&gt;VLM API Configuration&lt;/strong&gt; section on the left sidebar:&lt;ul&gt;
&lt;li&gt;Set &lt;strong&gt;API Base URL&lt;/strong&gt; to &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Click the &lt;strong&gt;Refresh&lt;/strong&gt; button to detect the model&lt;/li&gt;
&lt;li&gt;Select the Cosmos Reasoning 2B model from the dropdown&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Select your camera and click &lt;strong&gt;Start&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The WebUI will now stream your webcam frames to Cosmos Reasoning 2B and display the model’s analysis in real-time.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Recommended WebUI settings for Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Since Orin runs with a shorter context length, adjust these settings in the WebUI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Max Tokens&lt;/strong&gt;: Set to &lt;strong&gt;100–150&lt;/strong&gt; (shorter responses complete faster)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frame Processing Interval&lt;/strong&gt;: Set to &lt;strong&gt;60+&lt;/strong&gt; (gives the model time between frames)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Troubleshooting
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Out of memory on Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; vLLM crashes with CUDA out-of-memory errors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Free system memory before starting:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lower &lt;code&gt;--gpu-memory-utilization&lt;/code&gt; (try &lt;code&gt;0.55&lt;/code&gt; or &lt;code&gt;0.50&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reduce &lt;code&gt;--max-model-len&lt;/code&gt; further (try &lt;code&gt;128&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make sure no other GPU-intensive processes are running&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Model not found in WebUI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; The model doesn’t appear in the Live VLM WebUI dropdown.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Verify vLLM is running: &lt;code&gt;curl http://localhost:8000/v1/models&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Make sure the WebUI API Base URL is set to &lt;code&gt;http://localhost:8000/v1&lt;/code&gt; (not &lt;code&gt;https&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;If vLLM and WebUI are in separate containers, use &lt;code&gt;http://&amp;lt;jetson-ip&amp;gt;:8000/v1&lt;/code&gt; instead of &lt;code&gt;localhost&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Slow inference on Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Each response takes a very long time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is expected with the memory-constrained configuration. Cosmos Reasoning 2B FP8 on Orin prioritizes fitting in memory over speed&lt;/li&gt;
&lt;li&gt;Reduce &lt;code&gt;max_tokens&lt;/code&gt; in the WebUI to get shorter, faster responses&lt;/li&gt;
&lt;li&gt;Increase the frame interval so the model isn’t constantly processing new frames&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		vLLM fails to load model
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; vLLM reports that the model path doesn’t exist or can’t be loaded.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify the NGC download completed successfully: &lt;code&gt;ls ~/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Make sure the volume mount path is correct in your &lt;code&gt;docker run&lt;/code&gt; command&lt;/li&gt;
&lt;li&gt;Check that the model directory is mounted as read-only (&lt;code&gt;:ro&lt;/code&gt;) and the path inside the container matches what you pass to &lt;code&gt;vllm serve&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Summary
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;In this tutorial, we showcased how to deploy &lt;strong&gt;NVIDIA Cosmos Reasoning 2B&lt;/strong&gt; model on Jetson family of devices using vLLM. &lt;/p&gt;


&lt;p&gt;The combination of Cosmos Reasoning 2B’s chain-of-thought capabilities with Live VLM WebUI’s real-time streaming makes it ideal to prototype and evaluate vision AI applications at the edge.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Additional Resources
	&lt;/span&gt;
&lt;/h2&gt;

&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose mb-6 font-sans lg:hidden"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8"&gt;   &lt;ul class="flex items-center text-gray-600  flex-row  text-base   "&gt;&lt;li class=" -mr-2 h-5 w-5 md:h-6 md:w-6   bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800 " title="kalyanvadrevu"&gt;&lt;img alt="alt" class="overflow-hidden rounded-full" src="https://huggingface.co/avatars/e4e855ad75d5d86959765610d67668b3.svg" /&gt;  &lt;/li&gt; &lt;li class="text-xs hover:text-gray-700 dark:text-gray-400 dark:hover:text-gray-300 order-last ml-3"&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;  &lt;dialog class="shadow-alternate z-40 mx-4 my-auto h-fit select-text overflow-hidden rounded-xl bg-white max-sm:max-w-[calc(100dvw-2rem)] sm:mx-auto lg:mt-26 md:portrait:mt-30 xl:mt-30 2xl:mt-32 w-full sm:w-96 max-w-[calc(100%-4rem)] text-base not-prose"&gt; &lt;/dialog&gt;&lt;/div&gt;&lt;/div&gt;   &lt;!-- HTML_TAG_START --&gt;
Vision-Language Models (VLMs) mark a significant leap in AI by blending visual perception with semantic reasoning. Moving beyond traditional models constrained by fixed labels, VLMs utilize a joint embedding space to interpret and discuss complex, open-ended environments using natural language. 
&lt;p&gt;The rapid evolution of reasoning accuracy and efficiency has made these models ideal for edge devices. The NVIDIA Jetson family, ranging from the high-performance AGX Thor and AGX Orin to the compact Orin Nano Super is purpose-built to drive accelerated applications for physical AI and robotics, providing the optimized runtime necessary for leading open source models. &lt;/p&gt;
&lt;p&gt;In this tutorial, we will demonstrate how to deploy the NVIDIA Cosmos Reasoning 2B model across the Jetson lineup using the vLLM framework. We will also guide you through connecting this model to the Live VLM WebUI, enabling a real-time, webcam-based interface for interactive physical AI. &lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Prerequisites
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Supported Devices:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jetson AGX Thor Developer Kit&lt;/li&gt;
&lt;li&gt;Jetson AGX Orin (64GB / 32GB)&lt;/li&gt;
&lt;li&gt;Jetson Orin Super Nano&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;JetPack Version:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;JetPack 6 (L4T r36.x) — for Orin devices&lt;/li&gt;
&lt;li&gt;JetPack 7 (L4T r38.x) — for Thor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Storage:&lt;/strong&gt; NVMe SSD &lt;strong&gt;required&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;~5 GB for the FP8 model weights&lt;/li&gt;
&lt;li&gt;~8 GB for the vLLM container image&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Accounts:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create NVIDIA NGC account(free) to download both the model and vLLM contanier&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Overview
	&lt;/span&gt;
&lt;/h2&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Jetson AGX Thor&lt;/th&gt;
&lt;th&gt;Jetson AGX Orin&lt;/th&gt;
&lt;th&gt;Orin Super Nano&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;vLLM Container&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nvcr.io/nvidia/vllm:26.01-py3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;FP8 via NGC (volume mount)&lt;/td&gt;
&lt;td&gt;FP8 via NGC (volume mount)&lt;/td&gt;
&lt;td&gt;FP8 via NGC (volume mount)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Max Model Length&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;8192 tokens&lt;/td&gt;
&lt;td&gt;8192 tokens&lt;/td&gt;
&lt;td&gt;256 tokens (memory-constrained)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;GPU Memory Util&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.65&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The workflow is the same for both devices:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Download&lt;/strong&gt; the FP8 model checkpoint via NGC CLI&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pull&lt;/strong&gt; the vLLM Docker image for your device&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Launch&lt;/strong&gt; the container with the model mounted as a volume&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Connect&lt;/strong&gt; Live VLM WebUI to the vLLM endpoint&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 1: Install the NGC CLI
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The NGC CLI lets you download model checkpoints from the NVIDIA NGC Catalog.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Download and install
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p ~/Projects/CosmosReasoning
cd ~/Projects/CosmosReasoning

# Download the NGC CLI for ARM64
# Get the latest installer URL from: https://org.ngc.nvidia.com/setup/installers/cli
wget -O ngccli_arm64.zip https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/4.13.0/files/ngccli_arm64.zip
unzip ngccli_arm64.zip
chmod u+x ngc-cli/ngc

# Add to PATH
export PATH="$PATH:$(pwd)/ngc-cli"
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Configure the CLI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;ngc config set
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will be prompted for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;API Key&lt;/strong&gt; — generate one at NGC API Key setup&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CLI output format&lt;/strong&gt; — choose &lt;code&gt;json&lt;/code&gt; or &lt;code&gt;ascii&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;org&lt;/strong&gt; — press Enter to accept the default&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 2: Download the Model
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Download the &lt;strong&gt;FP8 quantized&lt;/strong&gt; checkpoint. This is used on all Jetson devices:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd ~/Projects/CosmosReasoning
ngc registry model download-version "nim/nvidia/cosmos-reason2-2b:1208-fp8-static-kv8"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a directory called &lt;code&gt;cosmos-reason2-2b_v1208-fp8-static-kv8/&lt;/code&gt; containing the model weights. Note the full path — you will mount it into the Docker container as a volume.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 3: Pull the vLLM Docker Image
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		For Jetson AGX Thor
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;docker pull nvcr.io/nvidia/vllm:26.01-py3
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		For Jetson AGX Orin / Orin Super Nano
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;docker pull ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04
&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 4: Serve Cosmos Reasoning 2B with vLLM
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Option A: Jetson AGX Thor
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Thor has ample GPU memory and can run the model with generous context length.&lt;/p&gt;
&lt;p&gt;Set the path to your downloaded model and free cached memory on the host:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MODEL_PATH="$HOME/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8"
sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Launch the container with the model mounted:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -it \
  --runtime nvidia \
  --network host \
  --ipc host \
  -v "$MODEL_PATH:/models/cosmos-reason2-2b:ro" \
  -e NVIDIA_VISIBLE_DEVICES=all \
  -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
  nvcr.io/nvidia/vllm:26.01-py3 \
  bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Inside the container, activate the environment and serve the model:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vllm serve /models/cosmos-reason2-2b \
  --max-model-len 8192 \
  --media-io-kwargs '{"video": {"num_frames": -1}}' \
  --reasoning-parser qwen3 \
  --gpu-memory-utilization 0.8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The &lt;code&gt;--reasoning-parser qwen3&lt;/code&gt; flag enables chain-of-thought reasoning extraction. The &lt;code&gt;--media-io-kwargs&lt;/code&gt; flag configures video frame handling.&lt;/p&gt;
&lt;p&gt;Wait until you see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO:     Uvicorn running on http://0.0.0.0:8000
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Option B: Jetson AGX Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;AGX Orin has enough memory to run the model with the same generous parameters as Thor.&lt;/p&gt;
&lt;p&gt;Set the path to your downloaded model and free cached memory on the host:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MODEL_PATH="$HOME/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8"
sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1. Launch the container:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -it \
  --runtime nvidia \
  --network host \
  -v "$MODEL_PATH:/models/cosmos-reason2-2b:ro" \
  -e NVIDIA_VISIBLE_DEVICES=all \
  -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
  ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04 \
  bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Inside the container, activate the environment and serve:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /opt/
source venv/bin/activate

vllm serve /models/cosmos-reason2-2b \
  --max-model-len 8192 \
  --media-io-kwargs '{"video": {"num_frames": -1}}' \
  --reasoning-parser qwen3 \
  --gpu-memory-utilization 0.8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait until you see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO:     Uvicorn running on http://0.0.0.0:8000
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Option C: Jetson Orin Super Nano (memory-constrained)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The Orin Super Nano has significantly less RAM, so we need aggressive memory optimization flags.&lt;/p&gt;
&lt;p&gt;Set the path to your downloaded model and free cached memory on the host:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MODEL_PATH="$HOME/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8"
sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1. Launch the container:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -it \
  --runtime nvidia \
  --network host \
  -v "$MODEL_PATH:/models/cosmos-reason2-2b:ro" \
  -e NVIDIA_VISIBLE_DEVICES=all \
  -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
  ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04 \
  bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Inside the container, activate the environment and serve:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /opt/
source venv/bin/activate

vllm serve /models/cosmos-reason2-2b \
  --host 0.0.0.0 \
  --port 8000 \
  --trust-remote-code \
  --enforce-eager \
  --max-model-len 256 \
  --max-num-batched-tokens 256 \
  --gpu-memory-utilization 0.65 \
  --max-num-seqs 1 \
  --enable-chunked-prefill \
  --limit-mm-per-prompt '{"image":1,"video":1}' \
  --mm-processor-kwargs '{"num_frames":2,"max_pixels":150528}'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Key flags explained (Orin Super Nano only):&lt;/strong&gt;&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Flag&lt;/th&gt;
&lt;th&gt;Purpose&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--enforce-eager&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Disables CUDA graphs to save memory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--max-model-len 256&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Limits context to fit in available memory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--max-num-batched-tokens 256&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Matches the model length limit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--gpu-memory-utilization 0.65&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Reserves headroom for system processes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--max-num-seqs 1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Single request at a time to minimize memory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--enable-chunked-prefill&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Processes prefill in chunks for memory efficiency&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--limit-mm-per-prompt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Limits to 1 image and 1 video per prompt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--mm-processor-kwargs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Reduces video frames and image resolution&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--VLLM_SKIP_WARMUP=true&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Skips warmup to save time and memory&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Wait until you see the server is ready:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO:     Uvicorn running on http://0.0.0.0:8000
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Verify the server is running
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;From another terminal on the Jetson:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl http://localhost:8000/v1/models
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should see the model listed in the response.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 5: Test with a Quick API Call
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Before connecting the WebUI, verify the model responds correctly:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -s http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/models/cosmos-reason2-2b",
    "messages": [
      {
        "role": "user",
        "content": "What capabilities do you have?"
      }
    ],
    "max_tokens": 128
  }' | python3 -m json.tool
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; The model name used in the API request must match what vLLM reports. Verify with &lt;code&gt;curl http://localhost:8000/v1/models&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 6: Connect to Live VLM WebUI
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Live VLM WebUI provides a real-time webcam-to-VLM interface. With vLLM serving Cosmos Reasoning 2B, you can stream your webcam and get live AI analysis with reasoning.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Install Live VLM WebUI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The easiest method is pip (Open another terminal):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.local/bin/env
cd ~/Projects/CosmosReasoning
uv venv .live-vlm --python 3.12
source .live-vlm/bin/activate
uv pip install live-vlm-webui
live-vlm-webui
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or use Docker:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/nvidia-ai-iot/live-vlm-webui.git
cd live-vlm-webui
./scripts/start_container.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Configure the WebUI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Open &lt;strong&gt;&lt;code&gt;https://localhost:8090&lt;/code&gt;&lt;/strong&gt; in your browser&lt;/li&gt;
&lt;li&gt;Accept the self-signed certificate (click &lt;strong&gt;Advanced&lt;/strong&gt; → &lt;strong&gt;Proceed&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;In the &lt;strong&gt;VLM API Configuration&lt;/strong&gt; section on the left sidebar:&lt;ul&gt;
&lt;li&gt;Set &lt;strong&gt;API Base URL&lt;/strong&gt; to &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Click the &lt;strong&gt;Refresh&lt;/strong&gt; button to detect the model&lt;/li&gt;
&lt;li&gt;Select the Cosmos Reasoning 2B model from the dropdown&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Select your camera and click &lt;strong&gt;Start&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The WebUI will now stream your webcam frames to Cosmos Reasoning 2B and display the model’s analysis in real-time.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Recommended WebUI settings for Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Since Orin runs with a shorter context length, adjust these settings in the WebUI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Max Tokens&lt;/strong&gt;: Set to &lt;strong&gt;100–150&lt;/strong&gt; (shorter responses complete faster)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frame Processing Interval&lt;/strong&gt;: Set to &lt;strong&gt;60+&lt;/strong&gt; (gives the model time between frames)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Troubleshooting
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Out of memory on Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; vLLM crashes with CUDA out-of-memory errors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Free system memory before starting:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lower &lt;code&gt;--gpu-memory-utilization&lt;/code&gt; (try &lt;code&gt;0.55&lt;/code&gt; or &lt;code&gt;0.50&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reduce &lt;code&gt;--max-model-len&lt;/code&gt; further (try &lt;code&gt;128&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make sure no other GPU-intensive processes are running&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Model not found in WebUI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; The model doesn’t appear in the Live VLM WebUI dropdown.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Verify vLLM is running: &lt;code&gt;curl http://localhost:8000/v1/models&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Make sure the WebUI API Base URL is set to &lt;code&gt;http://localhost:8000/v1&lt;/code&gt; (not &lt;code&gt;https&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;If vLLM and WebUI are in separate containers, use &lt;code&gt;http://&amp;lt;jetson-ip&amp;gt;:8000/v1&lt;/code&gt; instead of &lt;code&gt;localhost&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Slow inference on Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Each response takes a very long time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is expected with the memory-constrained configuration. Cosmos Reasoning 2B FP8 on Orin prioritizes fitting in memory over speed&lt;/li&gt;
&lt;li&gt;Reduce &lt;code&gt;max_tokens&lt;/code&gt; in the WebUI to get shorter, faster responses&lt;/li&gt;
&lt;li&gt;Increase the frame interval so the model isn’t constantly processing new frames&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		vLLM fails to load model
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; vLLM reports that the model path doesn’t exist or can’t be loaded.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify the NGC download completed successfully: &lt;code&gt;ls ~/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Make sure the volume mount path is correct in your &lt;code&gt;docker run&lt;/code&gt; command&lt;/li&gt;
&lt;li&gt;Check that the model directory is mounted as read-only (&lt;code&gt;:ro&lt;/code&gt;) and the path inside the container matches what you pass to &lt;code&gt;vllm serve&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Summary
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;In this tutorial, we showcased how to deploy &lt;strong&gt;NVIDIA Cosmos Reasoning 2B&lt;/strong&gt; model on Jetson family of devices using vLLM. &lt;/p&gt;


&lt;p&gt;The combination of Cosmos Reasoning 2B’s chain-of-thought capabilities with Live VLM WebUI’s real-time streaming makes it ideal to prototype and evaluate vision AI applications at the edge.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Additional Resources
	&lt;/span&gt;
&lt;/h2&gt;

&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/nvidia/cosmos-on-jetson</guid><pubDate>Tue, 24 Feb 2026 00:00:21 +0000</pubDate></item><item><title>A Meta AI security researcher said an OpenClaw agent ran amok on her inbox (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/a-meta-ai-security-researcher-said-an-openclaw-agent-ran-amok-on-her-inbox/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Y-Combinator-Crab.png?resize=1200,829" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The now-viral X post from Meta AI security researcher Summer Yue reads, at first, like satire. She told her OpenClaw AI agent to check her overstuffed email inbox and suggest what to delete or archive.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agent proceeded to run amok. It started deleting all her email in a “speed run” while ignoring her commands from her phone telling it to stop.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I had to RUN to my Mac mini like I was defusing a bomb,” she wrote, posting images of the ignored stop prompts as receipts.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Mac Mini, an affordable Apple computer that sits flat on a desk and fits in the palm of your hand, has become the favored device these days for running OpenClaw. (The Mini is selling “like hotcakes,” one “confused” Apple employee apparently told famed AI researcher Andrej Karpathy when he bought one to run an OpenClaw alternative called NanoClaw.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenClaw is, of course, the open source AI agent that achieved fame through Moltbook, an AI-only social network. OpenClaw agents were at the center of that now largely debunked episode on Moltbook in which it looked like the AIs were plotting against humans.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But OpenClaw’s mission, according to its GitHub page, is not focused on social networks. It aims to be a personal AI assistant that runs on your own devices.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Silicon Valley in-crowd has fallen so in love with OpenClaw that “claw” and “claws” have become the buzzwords of choice for agents that run on personal hardware. Other such agents include ZeroClaw, IronClaw, and PicoClaw. Y Combinator’s podcast team even appeared on their most recent episode dressed in lobster costumes.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But Yue’s post serves as a warning. As others on X noted, if an AI security researcher could run into this problem, what hope do mere mortals have?&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Were you intentionally testing its guardrails or did you make a rookie mistake?” a software developer asked her on X.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Rookie mistake tbh,” she replied. She had been testing her agent with a smaller “toy” inbox, as she called it, and it had been running well on less important email. It had earned her trust, so she thought she’d let it loose on the real thing.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Yue believes that the large amount of data in her real inbox “triggered compaction,” she wrote. Compaction happens when the context window — the running record of everything the AI has been told and has done in a session — grows too large, causing the agent to begin summarizing, compressing, and managing the conversation.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At that point, the AI may skip over instructions that the human considers quite important.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In this case, it may have skipped her last prompt — where she told it not to act — and reverted back to its instructions from the “toy” inbox.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As several others on X pointed out, prompts can’t be trusted to act as security guardrails. Models may misconstrue or ignore them.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Various people offered suggestions that ranged from the exact syntax Yue should have used to stop the agent, to various methods to ensure better adherence to guardrails, like writing instructions to dedicated files or using other open source tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the interest of full transparency, TechCrunch could not independently verify what happened to Yue’s inbox. (She didn’t respond to our request for comment, though she did respond to many questions and comments sent her way on X.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it doesn’t really matter.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The point of the tale is that agents aimed at knowledge workers, at their current stage of development, are risky. People who say they are using them successfully are cobbling together methods to protect themselves.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;One day, perhaps soon (by 2027? 2028?), they may be ready for widespread use. Goodness knows many of us would love help with email, grocery orders, and scheduling dentist appointments. But that day has not yet come.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Y-Combinator-Crab.png?resize=1200,829" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The now-viral X post from Meta AI security researcher Summer Yue reads, at first, like satire. She told her OpenClaw AI agent to check her overstuffed email inbox and suggest what to delete or archive.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agent proceeded to run amok. It started deleting all her email in a “speed run” while ignoring her commands from her phone telling it to stop.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I had to RUN to my Mac mini like I was defusing a bomb,” she wrote, posting images of the ignored stop prompts as receipts.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Mac Mini, an affordable Apple computer that sits flat on a desk and fits in the palm of your hand, has become the favored device these days for running OpenClaw. (The Mini is selling “like hotcakes,” one “confused” Apple employee apparently told famed AI researcher Andrej Karpathy when he bought one to run an OpenClaw alternative called NanoClaw.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenClaw is, of course, the open source AI agent that achieved fame through Moltbook, an AI-only social network. OpenClaw agents were at the center of that now largely debunked episode on Moltbook in which it looked like the AIs were plotting against humans.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But OpenClaw’s mission, according to its GitHub page, is not focused on social networks. It aims to be a personal AI assistant that runs on your own devices.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Silicon Valley in-crowd has fallen so in love with OpenClaw that “claw” and “claws” have become the buzzwords of choice for agents that run on personal hardware. Other such agents include ZeroClaw, IronClaw, and PicoClaw. Y Combinator’s podcast team even appeared on their most recent episode dressed in lobster costumes.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But Yue’s post serves as a warning. As others on X noted, if an AI security researcher could run into this problem, what hope do mere mortals have?&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Were you intentionally testing its guardrails or did you make a rookie mistake?” a software developer asked her on X.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Rookie mistake tbh,” she replied. She had been testing her agent with a smaller “toy” inbox, as she called it, and it had been running well on less important email. It had earned her trust, so she thought she’d let it loose on the real thing.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Yue believes that the large amount of data in her real inbox “triggered compaction,” she wrote. Compaction happens when the context window — the running record of everything the AI has been told and has done in a session — grows too large, causing the agent to begin summarizing, compressing, and managing the conversation.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At that point, the AI may skip over instructions that the human considers quite important.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In this case, it may have skipped her last prompt — where she told it not to act — and reverted back to its instructions from the “toy” inbox.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As several others on X pointed out, prompts can’t be trusted to act as security guardrails. Models may misconstrue or ignore them.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Various people offered suggestions that ranged from the exact syntax Yue should have used to stop the agent, to various methods to ensure better adherence to guardrails, like writing instructions to dedicated files or using other open source tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the interest of full transparency, TechCrunch could not independently verify what happened to Yue’s inbox. (She didn’t respond to our request for comment, though she did respond to many questions and comments sent her way on X.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it doesn’t really matter.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The point of the tale is that agents aimed at knowledge workers, at their current stage of development, are risky. People who say they are using them successfully are cobbling together methods to protect themselves.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;One day, perhaps soon (by 2027? 2028?), they may be ready for widespread use. Goodness knows many of us would love help with email, grocery orders, and scheduling dentist appointments. But that day has not yet come.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/a-meta-ai-security-researcher-said-an-openclaw-agent-ran-amok-on-her-inbox/</guid><pubDate>Tue, 24 Feb 2026 00:57:14 +0000</pubDate></item></channel></rss>