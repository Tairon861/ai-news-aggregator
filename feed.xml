<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 09 Dec 2025 06:36:14 +0000</lastBuildDate><item><title>Anthropic's Claude Code can now read your Slack messages and write code for you (AI | VentureBeat)</title><link>https://venturebeat.com/ai/anthropics-claude-code-can-now-read-your-slack-messages-and-write-code-for</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt; on Monday launched a beta integration that connects its fast-growing &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; programming agent &lt;a href="https://www.anthropic.com/news/claude-code-and-slack"&gt;&lt;u&gt;directly to Slack&lt;/u&gt;&lt;/a&gt;, allowing software engineers to delegate coding tasks without leaving the workplace messaging platform where much of their daily communication already happens.&lt;/p&gt;&lt;p&gt;The release, which Anthropic describes as a &amp;quot;&lt;a href="https://www.anthropic.com/news/claude-code-and-slack"&gt;&lt;u&gt;research preview&lt;/u&gt;&lt;/a&gt;,&amp;quot; is the AI safety company&amp;#x27;s latest move to embed its technology deeper into enterprise workflows — and comes as Claude Code has emerged as a surprise revenue engine, generating over $1 billion in annualized revenue just six months after its public debut in May.&lt;/p&gt;&lt;p&gt;&amp;quot;The critical context around engineering work often lives in Slack, including bug reports, feature requests, and engineering discussion,&amp;quot; the company wrote in its &lt;a href="https://www.anthropic.com/news/claude-code-and-slack"&gt;&lt;u&gt;announcement blog post&lt;/u&gt;&lt;/a&gt;. &amp;quot;When a bug report appears or a teammate needs a code fix, you can now tag Claude in Slack to automatically spin up a Claude Code session using the surrounding context.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From bug report to pull request: how the new Slack integration actually works&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The mechanics are deceptively simple but address a persistent friction point in software development: the gap between where problems get discussed and where they get fixed.&lt;/p&gt;&lt;p&gt;When a user mentions @Claude in a Slack channel or thread, Claude analyzes the message to determine whether it constitutes a coding task. If it does, the system automatically creates a new &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; session. Users can also explicitly instruct Claude to treat requests as coding tasks.&lt;/p&gt;&lt;p&gt;Claude gathers context from recent channel and thread messages in Slack to feed into the Claude Code session. It will use this context to automatically choose which repository to run the task on based on the repositories you&amp;#x27;ve authenticated to Claude Code on the web.&lt;/p&gt;&lt;p&gt;As the Claude Code session progresses, Claude posts status updates back to the Slack thread. Once complete, users receive a link to the full session where they can review changes, along with a direct link to open a pull request.&lt;/p&gt;&lt;p&gt;The feature builds on Anthropic&amp;#x27;s existing &lt;a href="https://www.claude.com/claude-and-slack"&gt;&lt;u&gt;Claude for Slack&lt;/u&gt;&lt;/a&gt; integration and requires users to have access to Claude Code on the web. In practical terms, a product manager reporting a bug in Slack could tag Claude, which would then analyze the conversation context, identify the relevant code repository, investigate the issue, propose a fix, and post a pull request—all while updating the original Slack thread with its progress.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Anthropic is betting big on enterprise workflow integrations&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The Slack integration arrives at a pivotal moment for Anthropic. Claude Code has already hit &lt;a href="https://www.linkedin.com/posts/mikekrieger_anthropic-acquires-bun-as-claude-code-reaches-activity-7401686833233076224-n3TI/"&gt;&lt;u&gt;$1 billion in revenue six months&lt;/u&gt;&lt;/a&gt; since its public debut in May, according to a LinkedIn post from Anthropic&amp;#x27;s chief product officer, Mike Krieger. The coding agent continues to barrel toward scale with customers like Netflix, Spotify, and Salesforce.&lt;/p&gt;&lt;p&gt;The velocity of that growth helps explain why Anthropic made its &lt;a href="https://bun.com/blog/bun-joins-anthropic"&gt;&lt;u&gt;first-ever acquisition&lt;/u&gt;&lt;/a&gt; earlier this month. Anthropic declined to comment on financial details. The Information earlier reported on &lt;a href="https://www.theinformation.com/articles/anthropic-advanced-talks-buy-developer-tool-startup-first-acquisition"&gt;&lt;u&gt;Anthropic&amp;#x27;s bid to acquire Bun&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a href="https://bun.com/blog/bun-joins-anthropic"&gt;&lt;u&gt;Bun&lt;/u&gt;&lt;/a&gt; is a breakthrough JavaScript runtime that is dramatically faster than the leading competition. As an all-in-one toolkit — combining runtime, package manager, bundler, and test runner — it&amp;#x27;s become essential infrastructure for AI-led software engineering, helping developers build and test applications at unprecedented velocity.&lt;/p&gt;&lt;p&gt;Since becoming generally available in May 2025, &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; has grown from its origins as an internal engineering experiment into a critical tool for many of the world&amp;#x27;s category-leading enterprises, including &lt;a href="https://www.netflix.com/"&gt;&lt;u&gt;Netflix&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://open.spotify.com/"&gt;&lt;u&gt;Spotify&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://kpmg.com/us/en.html"&gt;&lt;u&gt;KPMG&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.loreal.com/en/"&gt;&lt;u&gt;L&amp;#x27;Oreal&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.salesforce.com/"&gt;&lt;u&gt;Salesforce&lt;/u&gt;&lt;/a&gt; — and &lt;a href="https://bun.com/blog/bun-joins-anthropic"&gt;&lt;u&gt;Bun&lt;/u&gt;&lt;/a&gt; has been key in helping scale its infrastructure throughout that evolution.&lt;/p&gt;&lt;p&gt;The acquisition signals that Anthropic views &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; not as a peripheral feature but as a core business line worth substantial investment. The Slack integration extends that bet, positioning Claude Code as an ambient presence in the workspaces where engineering decisions actually get made.&lt;/p&gt;&lt;p&gt;According to an Anthropic spokesperson, companies including &lt;a href="https://www.rakuten.com/"&gt;&lt;u&gt;Rakuten&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.novonordisk.com/"&gt;&lt;u&gt;Novo Nordisk&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.uber.com/"&gt;&lt;u&gt;Uber&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.snowflake.com/en/"&gt;&lt;u&gt;Snowflake&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://ramp.com/business-cards?utm_campaign_id=15309675834&amp;amp;utm_ad_group_id=134786044053&amp;amp;utm_ad_id=737002215776&amp;amp;utm_matchtype=e&amp;amp;utm_term=ramp&amp;amp;utm_campaign=google-search-branded&amp;amp;utm_source=google&amp;amp;utm_medium=cpc&amp;amp;hsa_acc=5740001625&amp;amp;hsa_cam=15309675834&amp;amp;hsa_grp=134786044053&amp;amp;hsa_ad=737002215776&amp;amp;hsa_src=g&amp;amp;hsa_tgt=aud-2197767270940:kwd-27557331&amp;amp;hsa_kw=ramp&amp;amp;hsa_mt=e&amp;amp;hsa_net=adwords&amp;amp;hsa_ver=3&amp;amp;cq_cmp=15309675834&amp;amp;cq_con=134786044053&amp;amp;cq_plac=&amp;amp;cq_net=g&amp;amp;cq_plt=gp&amp;amp;gad_source=1&amp;amp;gad_campaignid=15309675834&amp;amp;gbraid=0AAAAACjAjKPV4x9hvhWDm_aa7C0JiphLP&amp;amp;gclid=Cj0KCQiAi9rJBhCYARIsALyPDts_kz5iHuPDKNlcI4ZU2sHwiscemYdv3EbJXX7uKXC2wnh2yc5C1QAaAi2WEALw_wcB"&gt;&lt;u&gt;Ramp&lt;/u&gt;&lt;/a&gt; now use Claude Code for both professional and novice developers. Rakuten, the Japanese e-commerce giant, has reportedly reduced software development timelines from 24 days to just 5 days using the tool — a 79% reduction that illustrates the productivity claims Anthropic has been making.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Claude Code&amp;#x27;s rapid rise from internal experiment to billion-dollar product&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The Slack launch is the latest in a rapid series of Claude Code expansions. In late November, &lt;a href="https://www.anthropic.com/news/claude-opus-4-5"&gt;&lt;u&gt;Claude Code was added to Anthropic&amp;#x27;s desktop apps&lt;/u&gt;&lt;/a&gt; including the Mac version. Claude Code was previously limited to mobile apps and the web. It allows software engineers to code, research, and update work with multiple local and remote sessions running at the same time.&lt;/p&gt;&lt;p&gt;That release accompanied Anthropic&amp;#x27;s unveiling of Claude &lt;a href="https://www.anthropic.com/news/claude-opus-4-5"&gt;&lt;u&gt;Opus 4.5&lt;/u&gt;&lt;/a&gt;, its newest and most capable model. Claude Opus 4.5 is available today on the company&amp;#x27;s apps, API, and on all three major cloud platforms. Pricing is $5/$25 per million tokens — making Opus-level capabilities accessible to even more users, teams, and enterprises.&lt;/p&gt;&lt;p&gt;The company has also invested heavily in the developer infrastructure that powers Claude Code. In late November, &lt;a href="https://www.anthropic.com/engineering/advanced-tool-use"&gt;&lt;u&gt;Anthropic released three new beta features for tool use&lt;/u&gt;&lt;/a&gt;: Tool Search Tool, which allows Claude to use search tools to access thousands of tools without consuming its context window; Programmatic Tool Calling, which allows Claude to invoke tools in a code execution environment reducing the impact on the model&amp;#x27;s context window; and Tool Use Examples, which provides a universal standard for demonstrating how to effectively use a given tool.&lt;/p&gt;&lt;p&gt;The &lt;a href="https://modelcontextprotocol.io/docs/getting-started/intro"&gt;&lt;u&gt;Model Context Protocol (MCP)&lt;/u&gt;&lt;/a&gt; is an open standard for connecting AI agents to external systems. Connecting agents to tools and data traditionally requires a custom integration for each pairing, creating fragmentation and duplicated effort that makes it difficult to scale truly connected systems. MCP provides a universal protocol — developers implement MCP once in their agent and it unlocks an entire ecosystem of integrations.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside Anthropic&amp;#x27;s own AI transformation: what happens when engineers use Claude all day&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Anthropic has been unusually transparent about how its own engineers use Claude Code — and the findings offer a preview of broader workforce implications. In August 2025, Anthropic&lt;a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic"&gt;&lt;u&gt; surveyed 132 engineers and researchers&lt;/u&gt;&lt;/a&gt;, conducted 53 in-depth qualitative interviews, and studied internal Claude Code usage data to understand how AI use is changing work at the company.&lt;/p&gt;&lt;p&gt;Employees self-reported &lt;a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic"&gt;&lt;u&gt;using Claude in 60% of their work&lt;/u&gt;&lt;/a&gt; and achieving a 50% productivity boost, a 2-3x increase from this time last year. This productivity looks like slightly less time per task category, but considerably more output volume.&lt;/p&gt;&lt;p&gt;Perhaps most notably, 27% of Claude-assisted work consists of tasks that wouldn&amp;#x27;t have been done otherwise, such as scaling projects, making nice-to-have tools like interactive data dashboards, and exploratory work that wouldn&amp;#x27;t be cost-effective if done manually.&lt;/p&gt;&lt;p&gt;The internal research also revealed how Claude is changing the nature of engineering collaboration. The maximum number of consecutive tool calls Claude Code makes per transcript increased by 116%. Claude now chains together 21.2 independent tool calls without need for human intervention versus 9.8 tool calls from six months ago.&lt;/p&gt;&lt;p&gt;The number of human turns decreased by 33%. The average number of human turns decreased from 6.2 to 4.1 per transcript, suggesting that less human input is necessary to accomplish a given task now compared to six months ago.&lt;/p&gt;&lt;p&gt;But the research also surfaced tensions. One prominent theme was that Claude has become the first stop for questions that once went to colleagues. &amp;quot;It has reduced my dependence on [my team] by 80%, [but] the last 20% is crucial and I go and talk to them,&amp;quot; one engineer explained. Several engineers said they &amp;quot;bounce ideas off&amp;quot; Claude, similar to interactions with human collaborators.&lt;/p&gt;&lt;p&gt;Others described experiencing less interaction with colleagues. Some appreciate the reduced social friction, but others resist the change or miss the older way of working: &amp;quot;I like working with people and it is sad that I &amp;#x27;need&amp;#x27; them less now.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How Anthropic stacks up against OpenAI, Google, and Microsoft in the enterprise AI race&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Anthropic is not alone in racing to capture the enterprise coding market. &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.microsoft.com/en-us/"&gt;&lt;u&gt;Microsoft&lt;/u&gt;&lt;/a&gt; (through &lt;a href="https://github.com/features/copilot"&gt;&lt;u&gt;GitHub Copilot&lt;/u&gt;&lt;/a&gt;) are all pursuing similar integrations. The Slack launch gives Anthropic a presence in one of the most widely-used enterprise communication platforms — Slack claims over 750,000 organizations use its software.&lt;/p&gt;&lt;p&gt;The deal comes as Anthropic pursues a more disciplined growth path than rival OpenAI, focusing on enterprise customers and coding workloads. Internal financials reported by The Wall Street Journal show &lt;a href="https://www.wsj.com/tech/ai/openai-anthropic-profitability-e9f5bcd6?gaa_at=eafs&amp;amp;gaa_n=AWEtsqf_yQrKzPR_c7Gjwi4FvVkM-Uc0I6k_V6Kev-0xg4RiUud_8rUKLIBVacKJN5E%3D&amp;amp;gaa_ts=693721bf&amp;amp;gaa_sig=m9SvmJAAB8gRVI39wbvEYOLFa9vd_lQbtYoQE1ROCKxaf6CXTOxXgKn4HmpIwymS0BUt58auzZusgCjq74SM2w%3D%3D"&gt;&lt;u&gt;Anthropic expects to break even by 2028&lt;/u&gt;&lt;/a&gt; — two years earlier than OpenAI, which continues to invest heavily in infrastructure as it expands into video, hardware, and consumer products.&lt;/p&gt;&lt;p&gt;The move also marks an increased push into developer tooling. Anthropic has recently seen backing from some of tech&amp;#x27;s biggest titans. Microsoft and Nvidia pledged up to &lt;a href="https://www.cnbc.com/2025/12/03/anthropic-claude-reportedly-preparing-ipo-race-openai-chatgpt-ft-wilson-sonsini-goodrich-rosati.html"&gt;&lt;u&gt;$15 billion in fresh investment&lt;/u&gt;&lt;/a&gt; in Anthropic last month, alongside a $30 billion commitment from Anthropic to run Claude Code on Microsoft&amp;#x27;s cloud. This is in addition to the &lt;a href="https://www.anthropic.com/news/anthropic-amazon-trainium"&gt;&lt;u&gt;$8 billion invested from Amazon &lt;/u&gt;&lt;/a&gt;and &lt;a href="https://www.cnbc.com/2025/12/04/google-replit-ai-vibe-coding-anthropic-cursor.html"&gt;&lt;u&gt;$3 billion from Google&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The cross-investment from both &lt;a href="https://microsoft.com/"&gt;&lt;u&gt;Microsoft&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt; — fierce competitors in the cloud and AI spaces — highlights how valuable Anthropic&amp;#x27;s enterprise positioning has become. By integrating with Slack (which is owned by Salesforce), Anthropic further embeds itself in the enterprise software ecosystem while remaining platform-agnostic.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What the Slack integration means for developers — and whether they can trust it&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For engineering teams, the Slack integration promises to collapse the distance between problem identification and problem resolution. A bug report in a Slack channel can immediately trigger investigation. A feature request can spawn a prototype. A code review comment can generate a refactor.&lt;/p&gt;&lt;p&gt;But the integration also raises questions about oversight and code quality. Most Anthropic employees use Claude frequently while reporting they can &amp;quot;fully delegate&amp;quot; only 0-20% of their work to it. Claude is a constant collaborator but using it generally involves active supervision and validation, especially in high-stakes work — versus handing off tasks requiring no verification at all.&lt;/p&gt;&lt;p&gt;Some employees are concerned about the atrophy of deeper skillsets required for both writing and critiquing code — &amp;quot;When producing output is so easy and fast, it gets harder and harder to actually take the time to learn something.&amp;quot;&lt;/p&gt;&lt;p&gt;The Slack integration, by making Claude Code invocation as simple as an @mention, may accelerate both the productivity benefits and the skill-atrophy concerns that Anthropic&amp;#x27;s own research has documented.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The future of coding may be conversational—and Anthropic is racing to prove it&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The beta launch marks the beginning of what Anthropic expects will be a broader rollout, with documentation forthcoming for teams looking to deploy the integration and refinements planned based on user feedback during the research preview phase.&lt;/p&gt;&lt;p&gt;For Anthropic, the Slack integration is a calculated bet on a fundamental shift in how software gets written. The company is wagering that the future of coding will be conversational — that the walls between where developers talk about problems and where they solve them will dissolve entirely. The companies that win enterprise AI, in this view, will be the ones that meet developers not in specialized tools but in the chat windows they already have open all day.&lt;/p&gt;&lt;p&gt;Whether that vision becomes reality will depend on whether &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; can deliver enterprise-grade reliability while maintaining the security that organizations demand. The early returns are promising: a billion dollars in revenue, a roster of Fortune 500 customers, and a growing ecosystem of integrations suggest Anthropic is onto something real.&lt;/p&gt;&lt;p&gt;But in one of Anthropic&amp;#x27;s own internal interviews, an engineer offered a more cautious assessment of the transformation underway: &amp;quot;Nobody knows what&amp;#x27;s going to happen… the important thing is to just be really adaptable.&amp;quot;&lt;/p&gt;&lt;p&gt;In the age of AI coding agents, that may be the only career advice that holds up.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt; on Monday launched a beta integration that connects its fast-growing &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; programming agent &lt;a href="https://www.anthropic.com/news/claude-code-and-slack"&gt;&lt;u&gt;directly to Slack&lt;/u&gt;&lt;/a&gt;, allowing software engineers to delegate coding tasks without leaving the workplace messaging platform where much of their daily communication already happens.&lt;/p&gt;&lt;p&gt;The release, which Anthropic describes as a &amp;quot;&lt;a href="https://www.anthropic.com/news/claude-code-and-slack"&gt;&lt;u&gt;research preview&lt;/u&gt;&lt;/a&gt;,&amp;quot; is the AI safety company&amp;#x27;s latest move to embed its technology deeper into enterprise workflows — and comes as Claude Code has emerged as a surprise revenue engine, generating over $1 billion in annualized revenue just six months after its public debut in May.&lt;/p&gt;&lt;p&gt;&amp;quot;The critical context around engineering work often lives in Slack, including bug reports, feature requests, and engineering discussion,&amp;quot; the company wrote in its &lt;a href="https://www.anthropic.com/news/claude-code-and-slack"&gt;&lt;u&gt;announcement blog post&lt;/u&gt;&lt;/a&gt;. &amp;quot;When a bug report appears or a teammate needs a code fix, you can now tag Claude in Slack to automatically spin up a Claude Code session using the surrounding context.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From bug report to pull request: how the new Slack integration actually works&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The mechanics are deceptively simple but address a persistent friction point in software development: the gap between where problems get discussed and where they get fixed.&lt;/p&gt;&lt;p&gt;When a user mentions @Claude in a Slack channel or thread, Claude analyzes the message to determine whether it constitutes a coding task. If it does, the system automatically creates a new &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; session. Users can also explicitly instruct Claude to treat requests as coding tasks.&lt;/p&gt;&lt;p&gt;Claude gathers context from recent channel and thread messages in Slack to feed into the Claude Code session. It will use this context to automatically choose which repository to run the task on based on the repositories you&amp;#x27;ve authenticated to Claude Code on the web.&lt;/p&gt;&lt;p&gt;As the Claude Code session progresses, Claude posts status updates back to the Slack thread. Once complete, users receive a link to the full session where they can review changes, along with a direct link to open a pull request.&lt;/p&gt;&lt;p&gt;The feature builds on Anthropic&amp;#x27;s existing &lt;a href="https://www.claude.com/claude-and-slack"&gt;&lt;u&gt;Claude for Slack&lt;/u&gt;&lt;/a&gt; integration and requires users to have access to Claude Code on the web. In practical terms, a product manager reporting a bug in Slack could tag Claude, which would then analyze the conversation context, identify the relevant code repository, investigate the issue, propose a fix, and post a pull request—all while updating the original Slack thread with its progress.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Anthropic is betting big on enterprise workflow integrations&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The Slack integration arrives at a pivotal moment for Anthropic. Claude Code has already hit &lt;a href="https://www.linkedin.com/posts/mikekrieger_anthropic-acquires-bun-as-claude-code-reaches-activity-7401686833233076224-n3TI/"&gt;&lt;u&gt;$1 billion in revenue six months&lt;/u&gt;&lt;/a&gt; since its public debut in May, according to a LinkedIn post from Anthropic&amp;#x27;s chief product officer, Mike Krieger. The coding agent continues to barrel toward scale with customers like Netflix, Spotify, and Salesforce.&lt;/p&gt;&lt;p&gt;The velocity of that growth helps explain why Anthropic made its &lt;a href="https://bun.com/blog/bun-joins-anthropic"&gt;&lt;u&gt;first-ever acquisition&lt;/u&gt;&lt;/a&gt; earlier this month. Anthropic declined to comment on financial details. The Information earlier reported on &lt;a href="https://www.theinformation.com/articles/anthropic-advanced-talks-buy-developer-tool-startup-first-acquisition"&gt;&lt;u&gt;Anthropic&amp;#x27;s bid to acquire Bun&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a href="https://bun.com/blog/bun-joins-anthropic"&gt;&lt;u&gt;Bun&lt;/u&gt;&lt;/a&gt; is a breakthrough JavaScript runtime that is dramatically faster than the leading competition. As an all-in-one toolkit — combining runtime, package manager, bundler, and test runner — it&amp;#x27;s become essential infrastructure for AI-led software engineering, helping developers build and test applications at unprecedented velocity.&lt;/p&gt;&lt;p&gt;Since becoming generally available in May 2025, &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; has grown from its origins as an internal engineering experiment into a critical tool for many of the world&amp;#x27;s category-leading enterprises, including &lt;a href="https://www.netflix.com/"&gt;&lt;u&gt;Netflix&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://open.spotify.com/"&gt;&lt;u&gt;Spotify&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://kpmg.com/us/en.html"&gt;&lt;u&gt;KPMG&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.loreal.com/en/"&gt;&lt;u&gt;L&amp;#x27;Oreal&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.salesforce.com/"&gt;&lt;u&gt;Salesforce&lt;/u&gt;&lt;/a&gt; — and &lt;a href="https://bun.com/blog/bun-joins-anthropic"&gt;&lt;u&gt;Bun&lt;/u&gt;&lt;/a&gt; has been key in helping scale its infrastructure throughout that evolution.&lt;/p&gt;&lt;p&gt;The acquisition signals that Anthropic views &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; not as a peripheral feature but as a core business line worth substantial investment. The Slack integration extends that bet, positioning Claude Code as an ambient presence in the workspaces where engineering decisions actually get made.&lt;/p&gt;&lt;p&gt;According to an Anthropic spokesperson, companies including &lt;a href="https://www.rakuten.com/"&gt;&lt;u&gt;Rakuten&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.novonordisk.com/"&gt;&lt;u&gt;Novo Nordisk&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.uber.com/"&gt;&lt;u&gt;Uber&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.snowflake.com/en/"&gt;&lt;u&gt;Snowflake&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://ramp.com/business-cards?utm_campaign_id=15309675834&amp;amp;utm_ad_group_id=134786044053&amp;amp;utm_ad_id=737002215776&amp;amp;utm_matchtype=e&amp;amp;utm_term=ramp&amp;amp;utm_campaign=google-search-branded&amp;amp;utm_source=google&amp;amp;utm_medium=cpc&amp;amp;hsa_acc=5740001625&amp;amp;hsa_cam=15309675834&amp;amp;hsa_grp=134786044053&amp;amp;hsa_ad=737002215776&amp;amp;hsa_src=g&amp;amp;hsa_tgt=aud-2197767270940:kwd-27557331&amp;amp;hsa_kw=ramp&amp;amp;hsa_mt=e&amp;amp;hsa_net=adwords&amp;amp;hsa_ver=3&amp;amp;cq_cmp=15309675834&amp;amp;cq_con=134786044053&amp;amp;cq_plac=&amp;amp;cq_net=g&amp;amp;cq_plt=gp&amp;amp;gad_source=1&amp;amp;gad_campaignid=15309675834&amp;amp;gbraid=0AAAAACjAjKPV4x9hvhWDm_aa7C0JiphLP&amp;amp;gclid=Cj0KCQiAi9rJBhCYARIsALyPDts_kz5iHuPDKNlcI4ZU2sHwiscemYdv3EbJXX7uKXC2wnh2yc5C1QAaAi2WEALw_wcB"&gt;&lt;u&gt;Ramp&lt;/u&gt;&lt;/a&gt; now use Claude Code for both professional and novice developers. Rakuten, the Japanese e-commerce giant, has reportedly reduced software development timelines from 24 days to just 5 days using the tool — a 79% reduction that illustrates the productivity claims Anthropic has been making.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Claude Code&amp;#x27;s rapid rise from internal experiment to billion-dollar product&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The Slack launch is the latest in a rapid series of Claude Code expansions. In late November, &lt;a href="https://www.anthropic.com/news/claude-opus-4-5"&gt;&lt;u&gt;Claude Code was added to Anthropic&amp;#x27;s desktop apps&lt;/u&gt;&lt;/a&gt; including the Mac version. Claude Code was previously limited to mobile apps and the web. It allows software engineers to code, research, and update work with multiple local and remote sessions running at the same time.&lt;/p&gt;&lt;p&gt;That release accompanied Anthropic&amp;#x27;s unveiling of Claude &lt;a href="https://www.anthropic.com/news/claude-opus-4-5"&gt;&lt;u&gt;Opus 4.5&lt;/u&gt;&lt;/a&gt;, its newest and most capable model. Claude Opus 4.5 is available today on the company&amp;#x27;s apps, API, and on all three major cloud platforms. Pricing is $5/$25 per million tokens — making Opus-level capabilities accessible to even more users, teams, and enterprises.&lt;/p&gt;&lt;p&gt;The company has also invested heavily in the developer infrastructure that powers Claude Code. In late November, &lt;a href="https://www.anthropic.com/engineering/advanced-tool-use"&gt;&lt;u&gt;Anthropic released three new beta features for tool use&lt;/u&gt;&lt;/a&gt;: Tool Search Tool, which allows Claude to use search tools to access thousands of tools without consuming its context window; Programmatic Tool Calling, which allows Claude to invoke tools in a code execution environment reducing the impact on the model&amp;#x27;s context window; and Tool Use Examples, which provides a universal standard for demonstrating how to effectively use a given tool.&lt;/p&gt;&lt;p&gt;The &lt;a href="https://modelcontextprotocol.io/docs/getting-started/intro"&gt;&lt;u&gt;Model Context Protocol (MCP)&lt;/u&gt;&lt;/a&gt; is an open standard for connecting AI agents to external systems. Connecting agents to tools and data traditionally requires a custom integration for each pairing, creating fragmentation and duplicated effort that makes it difficult to scale truly connected systems. MCP provides a universal protocol — developers implement MCP once in their agent and it unlocks an entire ecosystem of integrations.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside Anthropic&amp;#x27;s own AI transformation: what happens when engineers use Claude all day&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Anthropic has been unusually transparent about how its own engineers use Claude Code — and the findings offer a preview of broader workforce implications. In August 2025, Anthropic&lt;a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic"&gt;&lt;u&gt; surveyed 132 engineers and researchers&lt;/u&gt;&lt;/a&gt;, conducted 53 in-depth qualitative interviews, and studied internal Claude Code usage data to understand how AI use is changing work at the company.&lt;/p&gt;&lt;p&gt;Employees self-reported &lt;a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic"&gt;&lt;u&gt;using Claude in 60% of their work&lt;/u&gt;&lt;/a&gt; and achieving a 50% productivity boost, a 2-3x increase from this time last year. This productivity looks like slightly less time per task category, but considerably more output volume.&lt;/p&gt;&lt;p&gt;Perhaps most notably, 27% of Claude-assisted work consists of tasks that wouldn&amp;#x27;t have been done otherwise, such as scaling projects, making nice-to-have tools like interactive data dashboards, and exploratory work that wouldn&amp;#x27;t be cost-effective if done manually.&lt;/p&gt;&lt;p&gt;The internal research also revealed how Claude is changing the nature of engineering collaboration. The maximum number of consecutive tool calls Claude Code makes per transcript increased by 116%. Claude now chains together 21.2 independent tool calls without need for human intervention versus 9.8 tool calls from six months ago.&lt;/p&gt;&lt;p&gt;The number of human turns decreased by 33%. The average number of human turns decreased from 6.2 to 4.1 per transcript, suggesting that less human input is necessary to accomplish a given task now compared to six months ago.&lt;/p&gt;&lt;p&gt;But the research also surfaced tensions. One prominent theme was that Claude has become the first stop for questions that once went to colleagues. &amp;quot;It has reduced my dependence on [my team] by 80%, [but] the last 20% is crucial and I go and talk to them,&amp;quot; one engineer explained. Several engineers said they &amp;quot;bounce ideas off&amp;quot; Claude, similar to interactions with human collaborators.&lt;/p&gt;&lt;p&gt;Others described experiencing less interaction with colleagues. Some appreciate the reduced social friction, but others resist the change or miss the older way of working: &amp;quot;I like working with people and it is sad that I &amp;#x27;need&amp;#x27; them less now.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How Anthropic stacks up against OpenAI, Google, and Microsoft in the enterprise AI race&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Anthropic is not alone in racing to capture the enterprise coding market. &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.microsoft.com/en-us/"&gt;&lt;u&gt;Microsoft&lt;/u&gt;&lt;/a&gt; (through &lt;a href="https://github.com/features/copilot"&gt;&lt;u&gt;GitHub Copilot&lt;/u&gt;&lt;/a&gt;) are all pursuing similar integrations. The Slack launch gives Anthropic a presence in one of the most widely-used enterprise communication platforms — Slack claims over 750,000 organizations use its software.&lt;/p&gt;&lt;p&gt;The deal comes as Anthropic pursues a more disciplined growth path than rival OpenAI, focusing on enterprise customers and coding workloads. Internal financials reported by The Wall Street Journal show &lt;a href="https://www.wsj.com/tech/ai/openai-anthropic-profitability-e9f5bcd6?gaa_at=eafs&amp;amp;gaa_n=AWEtsqf_yQrKzPR_c7Gjwi4FvVkM-Uc0I6k_V6Kev-0xg4RiUud_8rUKLIBVacKJN5E%3D&amp;amp;gaa_ts=693721bf&amp;amp;gaa_sig=m9SvmJAAB8gRVI39wbvEYOLFa9vd_lQbtYoQE1ROCKxaf6CXTOxXgKn4HmpIwymS0BUt58auzZusgCjq74SM2w%3D%3D"&gt;&lt;u&gt;Anthropic expects to break even by 2028&lt;/u&gt;&lt;/a&gt; — two years earlier than OpenAI, which continues to invest heavily in infrastructure as it expands into video, hardware, and consumer products.&lt;/p&gt;&lt;p&gt;The move also marks an increased push into developer tooling. Anthropic has recently seen backing from some of tech&amp;#x27;s biggest titans. Microsoft and Nvidia pledged up to &lt;a href="https://www.cnbc.com/2025/12/03/anthropic-claude-reportedly-preparing-ipo-race-openai-chatgpt-ft-wilson-sonsini-goodrich-rosati.html"&gt;&lt;u&gt;$15 billion in fresh investment&lt;/u&gt;&lt;/a&gt; in Anthropic last month, alongside a $30 billion commitment from Anthropic to run Claude Code on Microsoft&amp;#x27;s cloud. This is in addition to the &lt;a href="https://www.anthropic.com/news/anthropic-amazon-trainium"&gt;&lt;u&gt;$8 billion invested from Amazon &lt;/u&gt;&lt;/a&gt;and &lt;a href="https://www.cnbc.com/2025/12/04/google-replit-ai-vibe-coding-anthropic-cursor.html"&gt;&lt;u&gt;$3 billion from Google&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The cross-investment from both &lt;a href="https://microsoft.com/"&gt;&lt;u&gt;Microsoft&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt; — fierce competitors in the cloud and AI spaces — highlights how valuable Anthropic&amp;#x27;s enterprise positioning has become. By integrating with Slack (which is owned by Salesforce), Anthropic further embeds itself in the enterprise software ecosystem while remaining platform-agnostic.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What the Slack integration means for developers — and whether they can trust it&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For engineering teams, the Slack integration promises to collapse the distance between problem identification and problem resolution. A bug report in a Slack channel can immediately trigger investigation. A feature request can spawn a prototype. A code review comment can generate a refactor.&lt;/p&gt;&lt;p&gt;But the integration also raises questions about oversight and code quality. Most Anthropic employees use Claude frequently while reporting they can &amp;quot;fully delegate&amp;quot; only 0-20% of their work to it. Claude is a constant collaborator but using it generally involves active supervision and validation, especially in high-stakes work — versus handing off tasks requiring no verification at all.&lt;/p&gt;&lt;p&gt;Some employees are concerned about the atrophy of deeper skillsets required for both writing and critiquing code — &amp;quot;When producing output is so easy and fast, it gets harder and harder to actually take the time to learn something.&amp;quot;&lt;/p&gt;&lt;p&gt;The Slack integration, by making Claude Code invocation as simple as an @mention, may accelerate both the productivity benefits and the skill-atrophy concerns that Anthropic&amp;#x27;s own research has documented.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The future of coding may be conversational—and Anthropic is racing to prove it&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The beta launch marks the beginning of what Anthropic expects will be a broader rollout, with documentation forthcoming for teams looking to deploy the integration and refinements planned based on user feedback during the research preview phase.&lt;/p&gt;&lt;p&gt;For Anthropic, the Slack integration is a calculated bet on a fundamental shift in how software gets written. The company is wagering that the future of coding will be conversational — that the walls between where developers talk about problems and where they solve them will dissolve entirely. The companies that win enterprise AI, in this view, will be the ones that meet developers not in specialized tools but in the chat windows they already have open all day.&lt;/p&gt;&lt;p&gt;Whether that vision becomes reality will depend on whether &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; can deliver enterprise-grade reliability while maintaining the security that organizations demand. The early returns are promising: a billion dollars in revenue, a roster of Fortune 500 customers, and a growing ecosystem of integrations suggest Anthropic is onto something real.&lt;/p&gt;&lt;p&gt;But in one of Anthropic&amp;#x27;s own internal interviews, an engineer offered a more cautious assessment of the transformation underway: &amp;quot;Nobody knows what&amp;#x27;s going to happen… the important thing is to just be really adaptable.&amp;quot;&lt;/p&gt;&lt;p&gt;In the age of AI coding agents, that may be the only career advice that holds up.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/anthropics-claude-code-can-now-read-your-slack-messages-and-write-code-for</guid><pubDate>Mon, 08 Dec 2025 19:00:00 +0000</pubDate></item><item><title>Claude Code is coming to Slack, and that’s a bigger deal than it sounds (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/08/claude-code-is-coming-to-slack-and-thats-a-bigger-deal-than-it-sounds/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/claude-code-slack.png?resize=1200,685" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic is launching Claude Code in Slack, allowing developers to delegate coding tasks directly from chat threads. The beta feature, available Monday as a research preview, builds on Anthropic’s existing Slack integration by adding full workflow automation. The rollout signals that the next frontier in coding assistants isn’t the model; it’s the workflow.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Previously, developers could only get lightweight coding help via Claude in Slack — like writing snippets, debugging, and explanations. Now they can tag @Claude to spin up a complete coding session using Slack context like bug reports or feature requests. Claude analyzes recent messages to determine the right repository, posts progress updates in threads, and shares links to review work and open pull requests.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move reflects a broader industry shift: AI coding assistants are migrating from IDEs (integrated development environment, where software development happens) into collaboration tools where teams already work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor offers Slack integration for drafting and debugging code in threads, while GitHub Copilot recently added features to generate pull requests from chat. OpenAI’s Codex is accessible via custom Slack bots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Slack, positioning itself as an “agentic hub” where AI meets workplace context creates a strategic advantage: Whichever AI tool dominates Slack — the center of engineering communication — could shape how software teams work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By letting developers move seamlessly from conversation to code without switching apps, Claude Code and similar tools represent a shift toward AI-embedded collaboration that could fundamentally change developer workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Anthropic has not yet confirmed when it would make a broader rollout available, the timing is strategic. The AI coding market is getting more competitive, and differentiation is starting to depend more on integration depth and distribution than model capability alone.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;That said, the integration raises questions about code security and IP protection, as it adds another platform through which sensitive repository access must be managed and audited — while also introducing new dependencies where outages or rate limits in either Slack or Claude’s API could disrupt development workflows that teams previously controlled locally.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Anthropic and Slack for more information.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/claude-code-slack.png?resize=1200,685" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic is launching Claude Code in Slack, allowing developers to delegate coding tasks directly from chat threads. The beta feature, available Monday as a research preview, builds on Anthropic’s existing Slack integration by adding full workflow automation. The rollout signals that the next frontier in coding assistants isn’t the model; it’s the workflow.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Previously, developers could only get lightweight coding help via Claude in Slack — like writing snippets, debugging, and explanations. Now they can tag @Claude to spin up a complete coding session using Slack context like bug reports or feature requests. Claude analyzes recent messages to determine the right repository, posts progress updates in threads, and shares links to review work and open pull requests.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move reflects a broader industry shift: AI coding assistants are migrating from IDEs (integrated development environment, where software development happens) into collaboration tools where teams already work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor offers Slack integration for drafting and debugging code in threads, while GitHub Copilot recently added features to generate pull requests from chat. OpenAI’s Codex is accessible via custom Slack bots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Slack, positioning itself as an “agentic hub” where AI meets workplace context creates a strategic advantage: Whichever AI tool dominates Slack — the center of engineering communication — could shape how software teams work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By letting developers move seamlessly from conversation to code without switching apps, Claude Code and similar tools represent a shift toward AI-embedded collaboration that could fundamentally change developer workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Anthropic has not yet confirmed when it would make a broader rollout available, the timing is strategic. The AI coding market is getting more competitive, and differentiation is starting to depend more on integration depth and distribution than model capability alone.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;That said, the integration raises questions about code security and IP protection, as it adds another platform through which sensitive repository access must be managed and audited — while also introducing new dependencies where outages or rate limits in either Slack or Claude’s API could disrupt development workflows that teams previously controlled locally.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Anthropic and Slack for more information.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/08/claude-code-is-coming-to-slack-and-thats-a-bigger-deal-than-it-sounds/</guid><pubDate>Mon, 08 Dec 2025 19:18:05 +0000</pubDate></item><item><title>MIT affiliates named 2025 Schmidt Sciences AI2050 Fellows (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/mit-affiliates-named-schmidt-sciences-ai2050-fellows-1208</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/ai2050-fellows-2025_0.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Two current MIT affiliates and seven additional alumni are among those named to the 2025 cohort of&amp;nbsp;AI2050 Fellows. &amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Zongyi Li, a postdoc in the MIT Computer Science and Artificial Intelligence Lab, and Tess Smidt ’12, an associate professor of electrical engineering and computer science (EECS), were both named as AI2050 Early Career Fellows.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Seven additional MIT alumni were also honored. AI2050 Early Career Fellows include Brian Hie SM '19, PhD '21; Natasha Mary Jaques PhD '20; Martin Anton Schrimpf PhD '22; Lindsey Raymond SM '19, PhD '24, who will join the MIT faculty in EECS, the Department of Economics, and the MIT Schwarzman College of Computing in 2026; and Ellen Dee Zhong PhD ’22. AI2050 Senior Fellows include Surya Ganguli ’98, MNG ’98; and Luke Zettlemoyer SM ’03, PhD ’09.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;AI2050 Fellows are announced annually by Schmidt Sciences, a nonprofit organization founded in 2024 by Eric and Wendy Schmidt that works to accelerate scientific knowledge and breakthroughs with the most promising, advanced tools to support a thriving planet. The organization prioritizes research in areas poised for impact including AI and advanced computing, astrophysics, biosciences, climate, and space — as well as supporting researchers in a variety of disciplines through its science systems program.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Li is postdoc in CSAIL working with associate professor of EECS Kaiming He. Li's research focuses on developing neural operator methods to accelerate scientific computing. He received his PhD in computing and mathematical sciences from Caltech, where he was advised by Anima Anandkumar and Andrew Stuart. He holds undergraduate degrees in computer science and mathematics from Washington University in St. Louis.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Li's work has been supported by a Kortschak Scholarship, PIMCO Fellowship, Amazon AI4Science Fellowship, Nvidia Fellowship, and MIT-Novo Nordisk AI Fellowship. He has also completed three summer internships at Nvidia. Li will join the NYU Courant Institute of Mathematical Sciences as an assistant professor of mathematics and data science in fall 2026.&lt;/p&gt;&lt;p dir="ltr"&gt;Smidt, associate professor of electrical engineering and computer science (EECS), is the principal investigator of the Atomic Architects group at the Research Laboratory of Electronics (RLE), where she works at the intersection of physics, geometry, and machine learning to design algorithms that aid in the understanding of physical systems under physical and geometric constraints, with applications to the design both of new materials and new molecules. She has a particular focus on symmetries present in 3D physical systems, such as rotation, translation, and reflection.&lt;/p&gt;&lt;p dir="ltr"&gt;Smidt earned her BS in physics from MIT in 2012 and her PhD in physics from the University of California at Berkeley in 2018. Prior to joining the MIT EECS faculty in 2021, she was the 2018 Alvarez Postdoctoral Fellow in Computing Sciences at Lawrence Berkeley National Laboratory, and a software engineering intern on the Google Accelerated Sciences team, where she developed Euclidean symmetry equivariant neural networks that naturally handle 3D geometry and geometric tensor data. Besides the AI2050 fellowship, she has received an Air Force Office of Scientific Research Young Investigator Program award, the EECS Outstanding Educator Award, and a Transformative Research Fund award.&lt;/p&gt;&lt;p&gt;Conceived and co-chaired by Eric Schmidt and James Manyika, AI2050 is a philanthropic initiative aimed at helping to solve&amp;nbsp;hard problems in AI. Within their research, each fellow will contend with the central motivating question of AI2050: “It’s 2050. AI has turned out to be hugely beneficial to society. What happened? What are the most important problems we solved and the opportunities and possibilities we realized to ensure this outcome?”&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/ai2050-fellows-2025_0.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Two current MIT affiliates and seven additional alumni are among those named to the 2025 cohort of&amp;nbsp;AI2050 Fellows. &amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Zongyi Li, a postdoc in the MIT Computer Science and Artificial Intelligence Lab, and Tess Smidt ’12, an associate professor of electrical engineering and computer science (EECS), were both named as AI2050 Early Career Fellows.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Seven additional MIT alumni were also honored. AI2050 Early Career Fellows include Brian Hie SM '19, PhD '21; Natasha Mary Jaques PhD '20; Martin Anton Schrimpf PhD '22; Lindsey Raymond SM '19, PhD '24, who will join the MIT faculty in EECS, the Department of Economics, and the MIT Schwarzman College of Computing in 2026; and Ellen Dee Zhong PhD ’22. AI2050 Senior Fellows include Surya Ganguli ’98, MNG ’98; and Luke Zettlemoyer SM ’03, PhD ’09.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;AI2050 Fellows are announced annually by Schmidt Sciences, a nonprofit organization founded in 2024 by Eric and Wendy Schmidt that works to accelerate scientific knowledge and breakthroughs with the most promising, advanced tools to support a thriving planet. The organization prioritizes research in areas poised for impact including AI and advanced computing, astrophysics, biosciences, climate, and space — as well as supporting researchers in a variety of disciplines through its science systems program.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Li is postdoc in CSAIL working with associate professor of EECS Kaiming He. Li's research focuses on developing neural operator methods to accelerate scientific computing. He received his PhD in computing and mathematical sciences from Caltech, where he was advised by Anima Anandkumar and Andrew Stuart. He holds undergraduate degrees in computer science and mathematics from Washington University in St. Louis.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Li's work has been supported by a Kortschak Scholarship, PIMCO Fellowship, Amazon AI4Science Fellowship, Nvidia Fellowship, and MIT-Novo Nordisk AI Fellowship. He has also completed three summer internships at Nvidia. Li will join the NYU Courant Institute of Mathematical Sciences as an assistant professor of mathematics and data science in fall 2026.&lt;/p&gt;&lt;p dir="ltr"&gt;Smidt, associate professor of electrical engineering and computer science (EECS), is the principal investigator of the Atomic Architects group at the Research Laboratory of Electronics (RLE), where she works at the intersection of physics, geometry, and machine learning to design algorithms that aid in the understanding of physical systems under physical and geometric constraints, with applications to the design both of new materials and new molecules. She has a particular focus on symmetries present in 3D physical systems, such as rotation, translation, and reflection.&lt;/p&gt;&lt;p dir="ltr"&gt;Smidt earned her BS in physics from MIT in 2012 and her PhD in physics from the University of California at Berkeley in 2018. Prior to joining the MIT EECS faculty in 2021, she was the 2018 Alvarez Postdoctoral Fellow in Computing Sciences at Lawrence Berkeley National Laboratory, and a software engineering intern on the Google Accelerated Sciences team, where she developed Euclidean symmetry equivariant neural networks that naturally handle 3D geometry and geometric tensor data. Besides the AI2050 fellowship, she has received an Air Force Office of Scientific Research Young Investigator Program award, the EECS Outstanding Educator Award, and a Transformative Research Fund award.&lt;/p&gt;&lt;p&gt;Conceived and co-chaired by Eric Schmidt and James Manyika, AI2050 is a philanthropic initiative aimed at helping to solve&amp;nbsp;hard problems in AI. Within their research, each fellow will contend with the central motivating question of AI2050: “It’s 2050. AI has turned out to be hugely beneficial to society. What happened? What are the most important problems we solved and the opportunities and possibilities we realized to ensure this outcome?”&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/mit-affiliates-named-schmidt-sciences-ai2050-fellows-1208</guid><pubDate>Mon, 08 Dec 2025 20:15:00 +0000</pubDate></item><item><title>Google’s AI try-on app Doppl adds a shoppable discovery feed (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/08/googles-ai-try-on-app-doppl-adds-a-shoppable-discovery-feed/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/doppl.png?resize=1200,665" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Monday that it’s introducing a shoppable discovery feed in Doppl, its experimental app that uses AI to visualize how different outfits might look on you.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant says the idea behind the new feed is to display recommendations so users can discover and virtually try on new items. Nearly everything in the feed is shoppable, with direct links to merchants. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The discovery feed features AI-generated videos of real products and suggests outfits based on your personalized style. Google determines your style by analyzing the preferences you share with Doppl and the items you interact with.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as short-form video feeds, particularly on TikTok and Instagram, have conditioned users to scroll visual feeds and buy what they see. However, unlike on TikTok and Instagram, where real influencers showcase products, Google’s new feed only consists of AI-generated content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While some may not be fond of an AI-generated feed, Google likely sees it as a way to surface products in a format that people are already used to. Plus, it makes sense for the company to try a new e-commerce strategy, especially as it continues to lose ground to companies like Amazon and social media platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth noting that AI-generated videos aren’t new to Doppl. While the app creates images of a virtual version of yourself wearing different outfits, it can turn these static images and convert them into AI-generated videos. The purpose of this is to give you a better sense of how the outfit would look on you in real life. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new discovery feed is rolling out to Doppl on iOS and Android in the U.S. for users 18 and above. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Although a feed consisting solely of AI-generated content would have seemed strange a year ago, the idea is now gaining traction. For example, OpenAI in September launched Sora, a social media platform of just AI videos. Meta also has a short-form video feed of AI-generated videos called “Vibes” in the Meta AI app.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/doppl.png?resize=1200,665" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Monday that it’s introducing a shoppable discovery feed in Doppl, its experimental app that uses AI to visualize how different outfits might look on you.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant says the idea behind the new feed is to display recommendations so users can discover and virtually try on new items. Nearly everything in the feed is shoppable, with direct links to merchants. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The discovery feed features AI-generated videos of real products and suggests outfits based on your personalized style. Google determines your style by analyzing the preferences you share with Doppl and the items you interact with.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as short-form video feeds, particularly on TikTok and Instagram, have conditioned users to scroll visual feeds and buy what they see. However, unlike on TikTok and Instagram, where real influencers showcase products, Google’s new feed only consists of AI-generated content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While some may not be fond of an AI-generated feed, Google likely sees it as a way to surface products in a format that people are already used to. Plus, it makes sense for the company to try a new e-commerce strategy, especially as it continues to lose ground to companies like Amazon and social media platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth noting that AI-generated videos aren’t new to Doppl. While the app creates images of a virtual version of yourself wearing different outfits, it can turn these static images and convert them into AI-generated videos. The purpose of this is to give you a better sense of how the outfit would look on you in real life. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new discovery feed is rolling out to Doppl on iOS and Android in the U.S. for users 18 and above. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Although a feed consisting solely of AI-generated content would have seemed strange a year ago, the idea is now gaining traction. For example, OpenAI in September launched Sora, a social media platform of just AI videos. Meta also has a short-form video feed of AI-generated videos called “Vibes” in the Meta AI app.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/08/googles-ai-try-on-app-doppl-adds-a-shoppable-discovery-feed/</guid><pubDate>Mon, 08 Dec 2025 21:29:03 +0000</pubDate></item><item><title>Department of Commerce approves Nvidia H200 chip exports to China (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/08/department-of-commerce-may-approve-nvidia-h200-chip-exports-to-china/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2219673294.jpg?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Advanced Nvidia AI chips can head back to China after all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Department of Commerce will allow Nvidia to ship H200 chips to China, as originally reported by Semafor, to approved customers in the country. The U.S. will take a 25% cut of these sales, CNBC reported. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;H200 chips are much more advanced than the H20 chips Nvidia developed specifically for the Chinese market, but the company would only be able to send H200s that are roughly 18 months old, Semafor reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An Nvidia spokesperson told TechCrunch of the development: “We applaud President Trump’s decision to allow America’s chip industry to compete to support high paying jobs and manufacturing in America. Offering H200 to approved commercial customers, vetted by the Department of Commerce, strikes a thoughtful balance that is great for America.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The news report comes a week after U.S. Commerce Secretary Howard Lutnick said the decision on exporting these H200 chips to China was in President Donald Trump’s hands.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The decision to send the chips to China conflicts with Congressional concerns about national security.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pete Ricketts, a Republican senator from Nebraska, and Chris Coons, a Democratic senator from Delaware, introduced a bill on December 4 that would block the export of advanced AI chips to China for more than two years.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The Secure and Feasible Exports Act (SAFE) Chips Act would require the Department of Commerce to deny any export license on advanced AI chips to China for 30 months. It’s unclear when legislators will vote on the proposed bill especially now that the Trump administration has given the green light to sell the H200 chips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Congress has long been clear about sending advanced AI chips to China — on both sides of the aisle — President Trump has waffled on whether or not to allow the exports.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration hit chip companies like Nvidia with licensing requirements to send their chips to China in April before it formally rescinded a Biden administration diffusion rule that would have regulated AI chip exports in May. Over the summer, the U.S. government signaled that companies would be able to start sending chips to China as long as the government got a 15% cut of all revenue, as chips became a bargaining tool in trade talks with China.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, by that point, the market for U.S.-developed chips in China was strained.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In September, China’s internet regulator, the Cyberspace Administration of China, banned domestic companies from buying Nvidia’s chips, leaving companies in the country to rely on less advanced domestic chips from Alibaba and Huawei.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, Trump said that Chinese president Xi Jinping “responded positively” to the latest H200 news in a Truth Social post. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was updated on December 8 when the proposed decision was confirmed. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2219673294.jpg?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Advanced Nvidia AI chips can head back to China after all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Department of Commerce will allow Nvidia to ship H200 chips to China, as originally reported by Semafor, to approved customers in the country. The U.S. will take a 25% cut of these sales, CNBC reported. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;H200 chips are much more advanced than the H20 chips Nvidia developed specifically for the Chinese market, but the company would only be able to send H200s that are roughly 18 months old, Semafor reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An Nvidia spokesperson told TechCrunch of the development: “We applaud President Trump’s decision to allow America’s chip industry to compete to support high paying jobs and manufacturing in America. Offering H200 to approved commercial customers, vetted by the Department of Commerce, strikes a thoughtful balance that is great for America.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The news report comes a week after U.S. Commerce Secretary Howard Lutnick said the decision on exporting these H200 chips to China was in President Donald Trump’s hands.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The decision to send the chips to China conflicts with Congressional concerns about national security.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pete Ricketts, a Republican senator from Nebraska, and Chris Coons, a Democratic senator from Delaware, introduced a bill on December 4 that would block the export of advanced AI chips to China for more than two years.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The Secure and Feasible Exports Act (SAFE) Chips Act would require the Department of Commerce to deny any export license on advanced AI chips to China for 30 months. It’s unclear when legislators will vote on the proposed bill especially now that the Trump administration has given the green light to sell the H200 chips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Congress has long been clear about sending advanced AI chips to China — on both sides of the aisle — President Trump has waffled on whether or not to allow the exports.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration hit chip companies like Nvidia with licensing requirements to send their chips to China in April before it formally rescinded a Biden administration diffusion rule that would have regulated AI chip exports in May. Over the summer, the U.S. government signaled that companies would be able to start sending chips to China as long as the government got a 15% cut of all revenue, as chips became a bargaining tool in trade talks with China.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, by that point, the market for U.S.-developed chips in China was strained.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In September, China’s internet regulator, the Cyberspace Administration of China, banned domestic companies from buying Nvidia’s chips, leaving companies in the country to rely on less advanced domestic chips from Alibaba and Huawei.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, Trump said that Chinese president Xi Jinping “responded positively” to the latest H200 news in a Truth Social post. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was updated on December 8 when the proposed decision was confirmed. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/08/department-of-commerce-may-approve-nvidia-h200-chip-exports-to-china/</guid><pubDate>Mon, 08 Dec 2025 21:40:46 +0000</pubDate></item><item><title>Z.ai debuts open source GLM-4.6V, a native tool-calling vision model for multimodal reasoning (AI | VentureBeat)</title><link>https://venturebeat.com/ai/z-ai-debuts-open-source-glm-4-6v-a-native-tool-calling-vision-model-for</link><description>[unable to retrieve full-text content]&lt;p&gt;Chinese AI startup Zhipu AI aka &lt;a href="https://z.ai/blog/glm-4.6v"&gt;&lt;b&gt;Z.ai has released its GLM-4.6V series&lt;/b&gt;&lt;/a&gt;, a new generation of open-source vision-language models (VLMs) optimized for multimodal reasoning, frontend automation, and high-efficiency deployment. &lt;/p&gt;&lt;p&gt;The release includes two models in &amp;quot;large&amp;quot; and &amp;quot;small&amp;quot; sizes: &lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GLM-4.6V (106B)&lt;/b&gt;, a larger 106-billion parameter model aimed at cloud-scale inference&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GLM-4.6V-Flash (9B)&lt;/b&gt;, a smaller model of only 9 billion parameters designed for low-latency, local applications&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Recall that generally speaking, models with more parameters — or internal settings governing their behavior, i.e. weights and biases — are more powerful, performant, and capable of performing at a higher general level across more varied tasks.&lt;/p&gt;&lt;p&gt;However, smaller models can offer better efficiency for edge or real-time applications where latency and resource constraints are critical.&lt;/p&gt;&lt;p&gt;The defining innovation in this series is the introduction of &lt;b&gt;native function calling&lt;/b&gt; in a vision-language model—enabling direct use of tools such as search, cropping, or chart recognition with visual inputs. &lt;/p&gt;&lt;p&gt;With a 128,000 token context length (equivalent to a 300-page novel&amp;#x27;s worth of text exchanged in a single input/output interaction with the user) and state-of-the-art (SoTA) results across more than 20 benchmarks, the GLM-4.6V series positions itself as a highly competitive alternative to both closed and open-source VLMs. It&amp;#x27;s available in the following formats:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/vlm/glm-4.6v"&gt;API access&lt;/a&gt; via OpenAI-compatible interface&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://chat.z.ai"&gt;Try the demo&lt;/a&gt; on Zhipu’s web interface&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;Download weights&lt;/a&gt; from Hugging Face&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Desktop assistant app available on &lt;a href="https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App"&gt;Hugging Face Spaces&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Licensing and Enterprise Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM‑4.6V and GLM‑4.6V‑Flash are distributed under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT license&lt;/a&gt;, a permissive open-source license that allows free commercial and non-commercial use, modification, redistribution, and local deployment without obligation to open-source derivative works. &lt;/p&gt;&lt;p&gt;This licensing model makes the series suitable for enterprise adoption, including scenarios that require full control over infrastructure, compliance with internal governance, or air-gapped environments.&lt;/p&gt;&lt;p&gt;Model weights and documentation are publicly hosted on &lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;Hugging Face&lt;/a&gt;, with supporting code and tooling available on &lt;a href="https://github.com/zai-org/GLM-V"&gt;GitHub&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The MIT license ensures maximum flexibility for integration into proprietary systems, including internal tools, production pipelines, and edge deployments.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Architecture and Technical Capabilities&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GLM-4.6V models follow a conventional encoder-decoder architecture with significant adaptations for multimodal input. &lt;/p&gt;&lt;p&gt;Both models incorporate a Vision Transformer (ViT) encoder—based on AIMv2-Huge—and an MLP projector to align visual features with a large language model (LLM) decoder. &lt;/p&gt;&lt;p&gt;Video inputs benefit from 3D convolutions and temporal compression, while spatial encoding is handled using 2D-RoPE and bicubic interpolation of absolute positional embeddings.&lt;/p&gt;&lt;p&gt;A key technical feature is the system’s support for arbitrary image resolutions and aspect ratios, including wide panoramic inputs up to 200:1. &lt;/p&gt;&lt;p&gt;In addition to static image and document parsing, GLM-4.6V can ingest temporal sequences of video frames with explicit timestamp tokens, enabling robust temporal reasoning.&lt;/p&gt;&lt;p&gt;On the decoding side, the model supports token generation aligned with function-calling protocols, allowing for structured reasoning across text, image, and tool outputs. This is supported by extended tokenizer vocabulary and output formatting templates to ensure consistent API or agent compatibility.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Native Multimodal Tool Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM-4.6V introduces native multimodal function calling, allowing visual assets—such as screenshots, images, and documents—to be passed directly as parameters to tools. This eliminates the need for intermediate text-only conversions, which have historically introduced information loss and complexity.&lt;/p&gt;&lt;p&gt;The tool invocation mechanism works bi-directionally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Input tools can be passed images or videos directly (e.g., document pages to crop or analyze).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Output tools such as chart renderers or web snapshot utilities return visual data, which GLM-4.6V integrates directly into the reasoning chain.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In practice, this means GLM-4.6V can complete tasks such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Generating structured reports from mixed-format documents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Performing visual audit of candidate images&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Automatically cropping figures from papers during generation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Conducting visual web search and answering multimodal queries&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;High Performance Benchmarks Compared to Other Similar-Sized Models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM-4.6V was evaluated across more than 20 public benchmarks covering general VQA, chart understanding, OCR, STEM reasoning, frontend replication, and multimodal agents. &lt;/p&gt;&lt;p&gt;According to the benchmark chart released by Zhipu AI:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V (106B) achieves SoTA or near-SoTA scores among open-source models of comparable size (106B) on MMBench, MathVista, MMLongBench, ChartQAPro, RefCOCO, TreeBench, and more.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V-Flash (9B) outperforms other lightweight models (e.g., Qwen3-VL-8B, GLM-4.1V-9B) across almost all categories tested.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The 106B model’s 128K-token window allows it to outperform larger models like Step-3 (321B) and Qwen3-VL-235B on long-context document tasks, video summarization, and structured multimodal reasoning.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Example scores from the leaderboard include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;MathVista: 88.2 (GLM-4.6V) vs. 84.6 (GLM-4.5V) vs. 81.4 (Qwen3-VL-8B)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;WebVoyager: 81.0 vs. 68.4 (Qwen3-VL-8B)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ref-L4-test: 88.9 vs. 89.5 (GLM-4.5V), but with better grounding fidelity at 87.7 (Flash) vs. 86.8&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Both models were evaluated using the vLLM inference backend and support SGLang for video-based tasks.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Frontend Automation and Long-Context Workflows&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Zhipu AI emphasized GLM-4.6V’s ability to support frontend development workflows. The model can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Replicate pixel-accurate HTML/CSS/JS from UI screenshots&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Accept natural language editing commands to modify layouts&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Identify and manipulate specific UI components visually&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This capability is integrated into an end-to-end visual programming interface, where the model iterates on layout, design intent, and output code using its native understanding of screen captures.&lt;/p&gt;&lt;p&gt;In long-document scenarios, GLM-4.6V can process up to 128,000 tokens—enabling a single inference pass across:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;150 pages of text (input)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;200 slide decks&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;1-hour videos&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Zhipu AI reported successful use of the model in financial analysis across multi-document corpora and in summarizing full-length sports broadcasts with timestamped event detection.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Training and Reinforcement Learning&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The model was trained using multi-stage pre-training followed by supervised fine-tuning (SFT) and reinforcement learning (RL). Key innovations include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Curriculum Sampling (RLCS): Dynamically adjusts the difficulty of training samples based on model progress&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Multi-domain reward systems: Task-specific verifiers for STEM, chart reasoning, GUI agents, video QA, and spatial grounding&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Function-aware training: Uses structured tags (e.g., &amp;lt;think&amp;gt;, &amp;lt;answer&amp;gt;, &amp;lt;|begin_of_box|&amp;gt;) to align reasoning and answer formatting&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The reinforcement learning pipeline emphasizes verifiable rewards (RLVR) over human feedback (RLHF) for scalability, and avoids KL/entropy losses to stabilize training across multimodal domains&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Pricing (API)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Zhipu AI offers competitive pricing for the GLM-4.6V series, with both the flagship model and its lightweight variant positioned for high accessibility.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V: $0.30 (input) / $0.90 (output) per 1M tokens&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V-Flash: Free&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Compared to major vision-capable and text-first LLMs, GLM-4.6V is among the most cost-efficient for multimodal reasoning at scale. Below is a comparative snapshot of pricing across providers:&lt;/p&gt;&lt;p&gt;&lt;i&gt;USD per 1M tokens — sorted lowest → highest total cost&lt;/i&gt;&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Total Cost&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.11&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.45&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.56&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GLM‑4.6V&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.30&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.90&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$1.20&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/overview/pricing"&gt;Z.AI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (non-reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-chat (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-reasoner (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Plus&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen-Max&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$11.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/pricing"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$11.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$12.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$17.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4 (0709)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$22.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$75.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$90.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs/models-overview"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;&lt;b&gt;Previous Releases: GLM‑4.5 Series and Enterprise Applications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Prior to GLM‑4.6V, Z.ai released the GLM‑4.5 family in mid-2025, establishing the company as a serious contender in open-source LLM development. &lt;/p&gt;&lt;p&gt;The flagship GLM‑4.5 and its smaller sibling GLM‑4.5‑Air both support reasoning, tool use, coding, and agentic behaviors, while offering strong performance across standard benchmarks. &lt;/p&gt;&lt;p&gt;The models introduced dual reasoning modes (“thinking” and “non-thinking”) and could automatically generate complete PowerPoint presentations from a single prompt — a feature positioned for use in enterprise reporting, education, and internal comms workflows. Z.ai also extended the GLM‑4.5 series with additional variants such as GLM‑4.5‑X, AirX, and Flash, targeting ultra-fast inference and low-cost scenarios.&lt;/p&gt;&lt;p&gt;Together, these features position the GLM‑4.5 series as a cost-effective, open, and production-ready alternative for enterprises needing autonomy over model deployment, lifecycle management, and integration pipel&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Ecosystem Implications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GLM-4.6V release represents a notable advance in open-source multimodal AI. While large vision-language models have proliferated over the past year, few offer:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Integrated visual tool usage&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Structured multimodal generation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Agent-oriented memory and decision logic&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Zhipu AI’s emphasis on “closing the loop” from perception to action via native function calling marks a step toward agentic multimodal systems. &lt;/p&gt;&lt;p&gt;The model’s architecture and training pipeline show a continued evolution of the GLM family, positioning it competitively alongside offerings like OpenAI’s GPT-4V and Google DeepMind’s Gemini-VL.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Takeaway for Enterprise Leaders&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With GLM-4.6V, Zhipu AI introduces an open-source VLM capable of native visual tool use, long-context reasoning, and frontend automation. It sets new performance marks among models of similar size and provides a scalable platform for building agentic, multimodal AI systems&lt;!-- --&gt;.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Chinese AI startup Zhipu AI aka &lt;a href="https://z.ai/blog/glm-4.6v"&gt;&lt;b&gt;Z.ai has released its GLM-4.6V series&lt;/b&gt;&lt;/a&gt;, a new generation of open-source vision-language models (VLMs) optimized for multimodal reasoning, frontend automation, and high-efficiency deployment. &lt;/p&gt;&lt;p&gt;The release includes two models in &amp;quot;large&amp;quot; and &amp;quot;small&amp;quot; sizes: &lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GLM-4.6V (106B)&lt;/b&gt;, a larger 106-billion parameter model aimed at cloud-scale inference&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GLM-4.6V-Flash (9B)&lt;/b&gt;, a smaller model of only 9 billion parameters designed for low-latency, local applications&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Recall that generally speaking, models with more parameters — or internal settings governing their behavior, i.e. weights and biases — are more powerful, performant, and capable of performing at a higher general level across more varied tasks.&lt;/p&gt;&lt;p&gt;However, smaller models can offer better efficiency for edge or real-time applications where latency and resource constraints are critical.&lt;/p&gt;&lt;p&gt;The defining innovation in this series is the introduction of &lt;b&gt;native function calling&lt;/b&gt; in a vision-language model—enabling direct use of tools such as search, cropping, or chart recognition with visual inputs. &lt;/p&gt;&lt;p&gt;With a 128,000 token context length (equivalent to a 300-page novel&amp;#x27;s worth of text exchanged in a single input/output interaction with the user) and state-of-the-art (SoTA) results across more than 20 benchmarks, the GLM-4.6V series positions itself as a highly competitive alternative to both closed and open-source VLMs. It&amp;#x27;s available in the following formats:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/vlm/glm-4.6v"&gt;API access&lt;/a&gt; via OpenAI-compatible interface&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://chat.z.ai"&gt;Try the demo&lt;/a&gt; on Zhipu’s web interface&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;Download weights&lt;/a&gt; from Hugging Face&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Desktop assistant app available on &lt;a href="https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App"&gt;Hugging Face Spaces&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Licensing and Enterprise Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM‑4.6V and GLM‑4.6V‑Flash are distributed under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT license&lt;/a&gt;, a permissive open-source license that allows free commercial and non-commercial use, modification, redistribution, and local deployment without obligation to open-source derivative works. &lt;/p&gt;&lt;p&gt;This licensing model makes the series suitable for enterprise adoption, including scenarios that require full control over infrastructure, compliance with internal governance, or air-gapped environments.&lt;/p&gt;&lt;p&gt;Model weights and documentation are publicly hosted on &lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;Hugging Face&lt;/a&gt;, with supporting code and tooling available on &lt;a href="https://github.com/zai-org/GLM-V"&gt;GitHub&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The MIT license ensures maximum flexibility for integration into proprietary systems, including internal tools, production pipelines, and edge deployments.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Architecture and Technical Capabilities&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GLM-4.6V models follow a conventional encoder-decoder architecture with significant adaptations for multimodal input. &lt;/p&gt;&lt;p&gt;Both models incorporate a Vision Transformer (ViT) encoder—based on AIMv2-Huge—and an MLP projector to align visual features with a large language model (LLM) decoder. &lt;/p&gt;&lt;p&gt;Video inputs benefit from 3D convolutions and temporal compression, while spatial encoding is handled using 2D-RoPE and bicubic interpolation of absolute positional embeddings.&lt;/p&gt;&lt;p&gt;A key technical feature is the system’s support for arbitrary image resolutions and aspect ratios, including wide panoramic inputs up to 200:1. &lt;/p&gt;&lt;p&gt;In addition to static image and document parsing, GLM-4.6V can ingest temporal sequences of video frames with explicit timestamp tokens, enabling robust temporal reasoning.&lt;/p&gt;&lt;p&gt;On the decoding side, the model supports token generation aligned with function-calling protocols, allowing for structured reasoning across text, image, and tool outputs. This is supported by extended tokenizer vocabulary and output formatting templates to ensure consistent API or agent compatibility.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Native Multimodal Tool Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM-4.6V introduces native multimodal function calling, allowing visual assets—such as screenshots, images, and documents—to be passed directly as parameters to tools. This eliminates the need for intermediate text-only conversions, which have historically introduced information loss and complexity.&lt;/p&gt;&lt;p&gt;The tool invocation mechanism works bi-directionally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Input tools can be passed images or videos directly (e.g., document pages to crop or analyze).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Output tools such as chart renderers or web snapshot utilities return visual data, which GLM-4.6V integrates directly into the reasoning chain.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In practice, this means GLM-4.6V can complete tasks such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Generating structured reports from mixed-format documents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Performing visual audit of candidate images&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Automatically cropping figures from papers during generation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Conducting visual web search and answering multimodal queries&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;High Performance Benchmarks Compared to Other Similar-Sized Models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM-4.6V was evaluated across more than 20 public benchmarks covering general VQA, chart understanding, OCR, STEM reasoning, frontend replication, and multimodal agents. &lt;/p&gt;&lt;p&gt;According to the benchmark chart released by Zhipu AI:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V (106B) achieves SoTA or near-SoTA scores among open-source models of comparable size (106B) on MMBench, MathVista, MMLongBench, ChartQAPro, RefCOCO, TreeBench, and more.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V-Flash (9B) outperforms other lightweight models (e.g., Qwen3-VL-8B, GLM-4.1V-9B) across almost all categories tested.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The 106B model’s 128K-token window allows it to outperform larger models like Step-3 (321B) and Qwen3-VL-235B on long-context document tasks, video summarization, and structured multimodal reasoning.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Example scores from the leaderboard include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;MathVista: 88.2 (GLM-4.6V) vs. 84.6 (GLM-4.5V) vs. 81.4 (Qwen3-VL-8B)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;WebVoyager: 81.0 vs. 68.4 (Qwen3-VL-8B)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ref-L4-test: 88.9 vs. 89.5 (GLM-4.5V), but with better grounding fidelity at 87.7 (Flash) vs. 86.8&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Both models were evaluated using the vLLM inference backend and support SGLang for video-based tasks.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Frontend Automation and Long-Context Workflows&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Zhipu AI emphasized GLM-4.6V’s ability to support frontend development workflows. The model can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Replicate pixel-accurate HTML/CSS/JS from UI screenshots&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Accept natural language editing commands to modify layouts&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Identify and manipulate specific UI components visually&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This capability is integrated into an end-to-end visual programming interface, where the model iterates on layout, design intent, and output code using its native understanding of screen captures.&lt;/p&gt;&lt;p&gt;In long-document scenarios, GLM-4.6V can process up to 128,000 tokens—enabling a single inference pass across:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;150 pages of text (input)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;200 slide decks&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;1-hour videos&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Zhipu AI reported successful use of the model in financial analysis across multi-document corpora and in summarizing full-length sports broadcasts with timestamped event detection.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Training and Reinforcement Learning&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The model was trained using multi-stage pre-training followed by supervised fine-tuning (SFT) and reinforcement learning (RL). Key innovations include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Curriculum Sampling (RLCS): Dynamically adjusts the difficulty of training samples based on model progress&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Multi-domain reward systems: Task-specific verifiers for STEM, chart reasoning, GUI agents, video QA, and spatial grounding&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Function-aware training: Uses structured tags (e.g., &amp;lt;think&amp;gt;, &amp;lt;answer&amp;gt;, &amp;lt;|begin_of_box|&amp;gt;) to align reasoning and answer formatting&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The reinforcement learning pipeline emphasizes verifiable rewards (RLVR) over human feedback (RLHF) for scalability, and avoids KL/entropy losses to stabilize training across multimodal domains&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Pricing (API)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Zhipu AI offers competitive pricing for the GLM-4.6V series, with both the flagship model and its lightweight variant positioned for high accessibility.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V: $0.30 (input) / $0.90 (output) per 1M tokens&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V-Flash: Free&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Compared to major vision-capable and text-first LLMs, GLM-4.6V is among the most cost-efficient for multimodal reasoning at scale. Below is a comparative snapshot of pricing across providers:&lt;/p&gt;&lt;p&gt;&lt;i&gt;USD per 1M tokens — sorted lowest → highest total cost&lt;/i&gt;&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Total Cost&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.11&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.45&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.56&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GLM‑4.6V&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.30&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.90&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$1.20&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/overview/pricing"&gt;Z.AI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (non-reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-chat (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-reasoner (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Plus&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen-Max&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$11.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/pricing"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$11.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$12.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$17.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4 (0709)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$22.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$75.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$90.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs/models-overview"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;&lt;b&gt;Previous Releases: GLM‑4.5 Series and Enterprise Applications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Prior to GLM‑4.6V, Z.ai released the GLM‑4.5 family in mid-2025, establishing the company as a serious contender in open-source LLM development. &lt;/p&gt;&lt;p&gt;The flagship GLM‑4.5 and its smaller sibling GLM‑4.5‑Air both support reasoning, tool use, coding, and agentic behaviors, while offering strong performance across standard benchmarks. &lt;/p&gt;&lt;p&gt;The models introduced dual reasoning modes (“thinking” and “non-thinking”) and could automatically generate complete PowerPoint presentations from a single prompt — a feature positioned for use in enterprise reporting, education, and internal comms workflows. Z.ai also extended the GLM‑4.5 series with additional variants such as GLM‑4.5‑X, AirX, and Flash, targeting ultra-fast inference and low-cost scenarios.&lt;/p&gt;&lt;p&gt;Together, these features position the GLM‑4.5 series as a cost-effective, open, and production-ready alternative for enterprises needing autonomy over model deployment, lifecycle management, and integration pipel&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Ecosystem Implications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GLM-4.6V release represents a notable advance in open-source multimodal AI. While large vision-language models have proliferated over the past year, few offer:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Integrated visual tool usage&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Structured multimodal generation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Agent-oriented memory and decision logic&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Zhipu AI’s emphasis on “closing the loop” from perception to action via native function calling marks a step toward agentic multimodal systems. &lt;/p&gt;&lt;p&gt;The model’s architecture and training pipeline show a continued evolution of the GLM family, positioning it competitively alongside offerings like OpenAI’s GPT-4V and Google DeepMind’s Gemini-VL.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Takeaway for Enterprise Leaders&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With GLM-4.6V, Zhipu AI introduces an open-source VLM capable of native visual tool use, long-context reasoning, and frontend automation. It sets new performance marks among models of similar size and provides a scalable platform for building agentic, multimodal AI systems&lt;!-- --&gt;.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/z-ai-debuts-open-source-glm-4-6v-a-native-tool-calling-vision-model-for</guid><pubDate>Tue, 09 Dec 2025 01:03:00 +0000</pubDate></item></channel></rss>