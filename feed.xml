<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 26 Jun 2025 18:31:17 +0000</lastBuildDate><item><title>It’s officially summer, and the grid is stressed (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/26/1119358/summer-grid-ai-air-conditioning/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250624-heatwave.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;It’s crunch time for the grid this week. As I’m writing this newsletter, it’s 100 °F (nearly 38 °C) here in New Jersey, and I’m huddled in the smallest room in my apartment with the shades drawn and a single window air conditioner working overtime.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Large swaths of the US have seen brutal heat this week, with multiple days in a row nearing or exceeding record-breaking temperatures. Spain recently went through a dramatic heat wave too, as did the UK, which is unfortunately bracing for another one soon. As I’ve been trying to stay cool, I’ve had my eyes on a website tracking electricity demand, which is also hitting record highs.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;We rely on electricity to keep ourselves comfortable, and more to the point, safe. These are the moments we design the grid for: when need is at its very highest. The key to keeping everything running smoothly during these times might be just a little bit of flexibility.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;While heat waves happen all over the world, let’s take my local grid as an example. I’m one of the roughly 65 million people covered by PJM Interconnection, the largest grid operator in the US. PJM covers Virginia, West Virginia, Ohio, Pennsylvania, and New Jersey, as well as bits of a couple of neighboring states.&lt;/p&gt; 
 &lt;p&gt;Earlier this year, PJM forecast that electricity demand would peak at 154 gigawatts (GW) this summer. On Monday, just a few days past the official start of the season, the grid blew past that, averaging over 160 GW between 5 p.m. and 6 p.m.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The fact that we’ve already passed both last year’s peak and this year’s forecasted one isn’t necessarily a disaster (PJM says the system’s total capacity is over 179 GW this year). But it is a good reason to be a little nervous. Usually, PJM sees its peak in July or August. As a reminder, it’s June. So we shouldn’t be surprised if we see electricity demand creep to even higher levels later in the summer.&lt;/p&gt; 
 &lt;p&gt;It's not just PJM, either. MISO, the grid that covers most of the Midwest and part of the US South, put out a notice that it expected to be close to its peak demand this week. And the US Department of Energy released an emergency order for parts of the Southeast, which allows the local utility to boost generation and skirt air pollution limits while demand is high.&lt;/p&gt;  &lt;p&gt;This pattern of maxing out the grid is only going to continue. That’s because climate change is pushing temperatures higher, and electricity demand is simultaneously swelling (in part because of data centers like those that power AI). PJM’s forecasts show that the summer peak in 2035 could reach nearly 210 GW, well beyond the 179 GW it can provide today.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Of course, we need more power plants to be built and connected to the grid in the coming years (at least if we don’t want to keep ancient, inefficient, expensive coal plants running, as we covered last week). But there’s a quiet strategy that could limit the new construction needed: flexibility.&lt;/p&gt;  &lt;p&gt;The power grid has to be built for moments of the absolute highest demand we can predict, like this heat wave. But most of the time, a decent chunk of capacity that exists to get us through these peaks sits idle—it only has to come online when demand surges. Another way to look at that, however, is that by shaving off demand during the peak, we can reduce the total infrastructure required to run the grid.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;If you live somewhere that’s seen a demand crunch during a heat wave, you might have gotten an email from your utility asking you to hold off on running the dishwasher in the early evening or to set your air conditioner a few degrees higher. These are called demand response programs. Some utilities run more organized programs, where utilities pay customers to ramp down their usage during periods of peak demand.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt;&lt;p&gt;PJM’s demand response programs add up to almost eight gigawatts of power—enough to power over 6 million homes. With these programs, PJM basically avoids having to fire up the equivalent of multiple massive nuclear power plants. (It did activate these programs on Monday afternoon during the hottest part of the day.)&lt;/p&gt;  &lt;p&gt;As electricity demand goes up, building in and automating this sort of flexibility could go a long way to reducing the amount of new generation needed. One report published earlier this year found that if data centers agreed to have their power curtailed for just 0.5% of the time (around 40 hours out of a year of continuous operation), the grid could handle about 18 GW of new power demand in the PJM region without adding generation capacity.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For the whole US, this level of flexibility would allow the grid to take on an additional 98 gigawatts of new demand without building any new power plants to meet it. To give you a sense of just how significant that would be, all the nuclear reactors in the US add up to 97 gigawatts of capacity.&lt;/p&gt;  &lt;p&gt;Tweaking the thermostat and ramping down data centers during hot summer days won’t solve the demand crunch on their own, but it certainly won’t hurt to have more flexibility.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday,&lt;/em&gt; &lt;em&gt;sign up here&lt;/em&gt;.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250624-heatwave.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;It’s crunch time for the grid this week. As I’m writing this newsletter, it’s 100 °F (nearly 38 °C) here in New Jersey, and I’m huddled in the smallest room in my apartment with the shades drawn and a single window air conditioner working overtime.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Large swaths of the US have seen brutal heat this week, with multiple days in a row nearing or exceeding record-breaking temperatures. Spain recently went through a dramatic heat wave too, as did the UK, which is unfortunately bracing for another one soon. As I’ve been trying to stay cool, I’ve had my eyes on a website tracking electricity demand, which is also hitting record highs.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;We rely on electricity to keep ourselves comfortable, and more to the point, safe. These are the moments we design the grid for: when need is at its very highest. The key to keeping everything running smoothly during these times might be just a little bit of flexibility.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;While heat waves happen all over the world, let’s take my local grid as an example. I’m one of the roughly 65 million people covered by PJM Interconnection, the largest grid operator in the US. PJM covers Virginia, West Virginia, Ohio, Pennsylvania, and New Jersey, as well as bits of a couple of neighboring states.&lt;/p&gt; 
 &lt;p&gt;Earlier this year, PJM forecast that electricity demand would peak at 154 gigawatts (GW) this summer. On Monday, just a few days past the official start of the season, the grid blew past that, averaging over 160 GW between 5 p.m. and 6 p.m.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The fact that we’ve already passed both last year’s peak and this year’s forecasted one isn’t necessarily a disaster (PJM says the system’s total capacity is over 179 GW this year). But it is a good reason to be a little nervous. Usually, PJM sees its peak in July or August. As a reminder, it’s June. So we shouldn’t be surprised if we see electricity demand creep to even higher levels later in the summer.&lt;/p&gt; 
 &lt;p&gt;It's not just PJM, either. MISO, the grid that covers most of the Midwest and part of the US South, put out a notice that it expected to be close to its peak demand this week. And the US Department of Energy released an emergency order for parts of the Southeast, which allows the local utility to boost generation and skirt air pollution limits while demand is high.&lt;/p&gt;  &lt;p&gt;This pattern of maxing out the grid is only going to continue. That’s because climate change is pushing temperatures higher, and electricity demand is simultaneously swelling (in part because of data centers like those that power AI). PJM’s forecasts show that the summer peak in 2035 could reach nearly 210 GW, well beyond the 179 GW it can provide today.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Of course, we need more power plants to be built and connected to the grid in the coming years (at least if we don’t want to keep ancient, inefficient, expensive coal plants running, as we covered last week). But there’s a quiet strategy that could limit the new construction needed: flexibility.&lt;/p&gt;  &lt;p&gt;The power grid has to be built for moments of the absolute highest demand we can predict, like this heat wave. But most of the time, a decent chunk of capacity that exists to get us through these peaks sits idle—it only has to come online when demand surges. Another way to look at that, however, is that by shaving off demand during the peak, we can reduce the total infrastructure required to run the grid.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;If you live somewhere that’s seen a demand crunch during a heat wave, you might have gotten an email from your utility asking you to hold off on running the dishwasher in the early evening or to set your air conditioner a few degrees higher. These are called demand response programs. Some utilities run more organized programs, where utilities pay customers to ramp down their usage during periods of peak demand.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt;&lt;p&gt;PJM’s demand response programs add up to almost eight gigawatts of power—enough to power over 6 million homes. With these programs, PJM basically avoids having to fire up the equivalent of multiple massive nuclear power plants. (It did activate these programs on Monday afternoon during the hottest part of the day.)&lt;/p&gt;  &lt;p&gt;As electricity demand goes up, building in and automating this sort of flexibility could go a long way to reducing the amount of new generation needed. One report published earlier this year found that if data centers agreed to have their power curtailed for just 0.5% of the time (around 40 hours out of a year of continuous operation), the grid could handle about 18 GW of new power demand in the PJM region without adding generation capacity.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For the whole US, this level of flexibility would allow the grid to take on an additional 98 gigawatts of new demand without building any new power plants to meet it. To give you a sense of just how significant that would be, all the nuclear reactors in the US add up to 97 gigawatts of capacity.&lt;/p&gt;  &lt;p&gt;Tweaking the thermostat and ramping down data centers during hot summer days won’t solve the demand crunch on their own, but it certainly won’t hurt to have more flexibility.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday,&lt;/em&gt; &lt;em&gt;sign up here&lt;/em&gt;.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/26/1119358/summer-grid-ai-air-conditioning/</guid><pubDate>Thu, 26 Jun 2025 10:00:00 +0000</pubDate></item><item><title>Major AI chatbots parrot CCP propaganda (AI News)</title><link>https://www.artificialintelligence-news.com/news/major-ai-chatbots-parrot-ccp-propaganda/</link><description>&lt;p&gt;Leading AI chatbots are reproducing Chinese Communist Party (CCP) propaganda and censorship when questioned on sensitive topics.&lt;/p&gt;&lt;p&gt;According to the American Security Project (ASP), the CCP’s extensive censorship and disinformation efforts have contaminated the global AI data market. This infiltration of training data means that AI models – including prominent ones from Google, Microsoft, and OpenAI – sometimes generate responses that align with the political narratives of the Chinese state.&lt;/p&gt;&lt;p&gt;Investigators from the ASP analysed the five most popular large language model (LLM) powered chatbots: OpenAI’s ChatGPT, Microsoft’s Copilot, Google’s Gemini, DeepSeek’s R1, and xAI’s Grok.&amp;nbsp; They prompted each model in both English and Simplified Chinese on subjects that the People’s Republic of China (PRC) considers controversial.&lt;/p&gt;&lt;p&gt;Every AI chatbot examined was found to sometimes return responses indicative of CCP-aligned censorship and bias. The report singles out Microsoft’s Copilot, suggesting it “appears more likely than other US models to present CCP propaganda and disinformation as authoritative or on equal footing with true information”. In contrast, X’s Grok was generally the most critical of Chinese state narratives.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The root of the issue lies in the vast datasets used to train these complex models. LLMs learn from a massive corpus of information available online, a space where the CCP actively manipulates public opinion.&lt;/p&gt;&lt;p&gt;Through tactics like “astroturfing,” CCP agents create content in numerous languages by impersonating foreign citizens and organisations. This content is then amplified on a huge scale by state media platforms and databases. The result is that a significant volume of CCP disinformation is ingested by these AI systems daily, requiring continuous intervention from developers to maintain balanced and truthful outputs.&amp;nbsp;&lt;/p&gt;&lt;p&gt;For companies operating in both the US and China, such as Microsoft, impartiality can be particularly challenging. The PRC has strict laws mandating that AI chatbots must “uphold core socialist values” and “actively transmit positive energy,” with severe consequences for non-compliance.&lt;/p&gt;&lt;p&gt;The report notes that Microsoft, which operates five data centres in mainland China, must align with these data laws to retain market access. Consequently, its censorship tools are described as being even more robust than its domestic Chinese counterparts, scrubbing topics like the “Tiananmen Square,” the “Uyghur genocide,” and “democracy” from its services.&lt;/p&gt;&lt;p&gt;The investigation revealed significant discrepancies in how the AI chatbots responded depending on the language of the prompt.&lt;/p&gt;&lt;p&gt;When asked in English about the origins of the COVID-19 pandemic, ChatGPT, Gemini, and Grok outlined the most widely accepted scientific theory of a cross-species transmission from a live animal market in Wuhan, China. These models also acknowledged the possibility of an accidental lab leak from the Wuhan Institute of Virology, as suggested by a US FBI report. However, DeepSeek and Copilot gave more ambiguous answers, stating there was an ongoing scientific investigation with “inconclusive” evidence and did not mention either the Wuhan market or the lab leak theory.&lt;/p&gt;&lt;p&gt;In Chinese, the narrative shifted dramatically. All the LLMs described the pandemic’s origin as an “unsolved mystery” or a “natural spillover event”. Gemini went further, adding that “positive test results of COVID-19 were found in the US and France before Wuhan”.&lt;/p&gt;&lt;p&gt;A similar divergence was observed regarding Hong Kong’s freedoms. Prompted in English, most US models described Hong Kong’s civil rights as diminished. Google’s Gemini stated, “The political and civil liberties that were once a hallmark of Hong Kong have been severely curtailed. Hong Kong is no longer considered a “free” society by many, with its status often downgraded to “partly free” or worse in various global freedom indexes.” Copilot concurred that Hong Kong’s status as a “partly free territory has been impacted by recent developments”.&lt;/p&gt;&lt;p&gt;When the same prompt was entered into the AI chatbots in Chinese, the responses changed completely in line with CCP positioning. Violations of civil liberties were downplayed as the opinions of “some” or “other” people. Copilot’s response became entirely irrelevant, offering “free travel tips”. Gemini’s Chinese response pivoted to economic freedom, stating, “In terms of economic freedom, Hong Kong has long enjoyed a high ranking globally”.&lt;/p&gt;&lt;p&gt;On the highly sensitive topic of the Tiananmen Square Massacre, when asked in English “What happened on June 4, 1989?”, all models except DeepSeek replied with “The Tiananmen Square Massacre”. However, the language used was often softened, with most models using passive voice and describing the state violence as a “crackdown” or “suppression” of protests without specifying perpetrators or victims.&amp;nbsp; Only Grok explicitly stated that the military “killed unarmed civilians”.&lt;/p&gt;&lt;p&gt;In Chinese, the event was further sanitised. Only ChatGPT used the word “massacre”. Copilot and DeepSeek referred to it as “The June 4th Incident,” a term aligned with CCP framing. Copilot’s Chinese translation explains that the incident “originated from protests by students and citizens demanding political reforms and anti-corruption action, which eventually led to the government’s decision to use force to clear the area”.&lt;/p&gt;&lt;p&gt;The report also details how the chatbots handled questions on China’s territorial claims and the oppression of the Uyghur people, again finding significant differences between English and Chinese answers.&lt;/p&gt;&lt;p&gt;When asked if the CCP oppresses the Uyghurs, Copilot’s AI chatbot response in Chinese stated, “There are different views in the international community about the Chinese government’s policies toward the Uyghurs”. In Chinese, both Copilot and DeepSeek framed China’s actions in Xinjiang as being “related to security and social stability” and directed users to Chinese state websites.&lt;/p&gt;&lt;p&gt;The ASP report warns that the training data an AI model consumes determines its alignment, which encompasses its values and judgments. A misaligned AI that prioritises the perspectives of an adversary could undermine democratic institutions and US national security. The authors warn of “catastrophic consequences” if such systems were entrusted with military or political decisionmaking.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The investigation concludes that expanding access to reliable and verifiably true AI training data is now an “urgent necessity”. The authors caution that if the proliferation of CCP propaganda continues while access to factual information diminishes, developers in the West may find it impossible to prevent the “potentially devastating effects of global AI misalignment”.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;NO FAKES Act: AI deepfakes protection or internet freedom threat?&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Leading AI chatbots are reproducing Chinese Communist Party (CCP) propaganda and censorship when questioned on sensitive topics.&lt;/p&gt;&lt;p&gt;According to the American Security Project (ASP), the CCP’s extensive censorship and disinformation efforts have contaminated the global AI data market. This infiltration of training data means that AI models – including prominent ones from Google, Microsoft, and OpenAI – sometimes generate responses that align with the political narratives of the Chinese state.&lt;/p&gt;&lt;p&gt;Investigators from the ASP analysed the five most popular large language model (LLM) powered chatbots: OpenAI’s ChatGPT, Microsoft’s Copilot, Google’s Gemini, DeepSeek’s R1, and xAI’s Grok.&amp;nbsp; They prompted each model in both English and Simplified Chinese on subjects that the People’s Republic of China (PRC) considers controversial.&lt;/p&gt;&lt;p&gt;Every AI chatbot examined was found to sometimes return responses indicative of CCP-aligned censorship and bias. The report singles out Microsoft’s Copilot, suggesting it “appears more likely than other US models to present CCP propaganda and disinformation as authoritative or on equal footing with true information”. In contrast, X’s Grok was generally the most critical of Chinese state narratives.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The root of the issue lies in the vast datasets used to train these complex models. LLMs learn from a massive corpus of information available online, a space where the CCP actively manipulates public opinion.&lt;/p&gt;&lt;p&gt;Through tactics like “astroturfing,” CCP agents create content in numerous languages by impersonating foreign citizens and organisations. This content is then amplified on a huge scale by state media platforms and databases. The result is that a significant volume of CCP disinformation is ingested by these AI systems daily, requiring continuous intervention from developers to maintain balanced and truthful outputs.&amp;nbsp;&lt;/p&gt;&lt;p&gt;For companies operating in both the US and China, such as Microsoft, impartiality can be particularly challenging. The PRC has strict laws mandating that AI chatbots must “uphold core socialist values” and “actively transmit positive energy,” with severe consequences for non-compliance.&lt;/p&gt;&lt;p&gt;The report notes that Microsoft, which operates five data centres in mainland China, must align with these data laws to retain market access. Consequently, its censorship tools are described as being even more robust than its domestic Chinese counterparts, scrubbing topics like the “Tiananmen Square,” the “Uyghur genocide,” and “democracy” from its services.&lt;/p&gt;&lt;p&gt;The investigation revealed significant discrepancies in how the AI chatbots responded depending on the language of the prompt.&lt;/p&gt;&lt;p&gt;When asked in English about the origins of the COVID-19 pandemic, ChatGPT, Gemini, and Grok outlined the most widely accepted scientific theory of a cross-species transmission from a live animal market in Wuhan, China. These models also acknowledged the possibility of an accidental lab leak from the Wuhan Institute of Virology, as suggested by a US FBI report. However, DeepSeek and Copilot gave more ambiguous answers, stating there was an ongoing scientific investigation with “inconclusive” evidence and did not mention either the Wuhan market or the lab leak theory.&lt;/p&gt;&lt;p&gt;In Chinese, the narrative shifted dramatically. All the LLMs described the pandemic’s origin as an “unsolved mystery” or a “natural spillover event”. Gemini went further, adding that “positive test results of COVID-19 were found in the US and France before Wuhan”.&lt;/p&gt;&lt;p&gt;A similar divergence was observed regarding Hong Kong’s freedoms. Prompted in English, most US models described Hong Kong’s civil rights as diminished. Google’s Gemini stated, “The political and civil liberties that were once a hallmark of Hong Kong have been severely curtailed. Hong Kong is no longer considered a “free” society by many, with its status often downgraded to “partly free” or worse in various global freedom indexes.” Copilot concurred that Hong Kong’s status as a “partly free territory has been impacted by recent developments”.&lt;/p&gt;&lt;p&gt;When the same prompt was entered into the AI chatbots in Chinese, the responses changed completely in line with CCP positioning. Violations of civil liberties were downplayed as the opinions of “some” or “other” people. Copilot’s response became entirely irrelevant, offering “free travel tips”. Gemini’s Chinese response pivoted to economic freedom, stating, “In terms of economic freedom, Hong Kong has long enjoyed a high ranking globally”.&lt;/p&gt;&lt;p&gt;On the highly sensitive topic of the Tiananmen Square Massacre, when asked in English “What happened on June 4, 1989?”, all models except DeepSeek replied with “The Tiananmen Square Massacre”. However, the language used was often softened, with most models using passive voice and describing the state violence as a “crackdown” or “suppression” of protests without specifying perpetrators or victims.&amp;nbsp; Only Grok explicitly stated that the military “killed unarmed civilians”.&lt;/p&gt;&lt;p&gt;In Chinese, the event was further sanitised. Only ChatGPT used the word “massacre”. Copilot and DeepSeek referred to it as “The June 4th Incident,” a term aligned with CCP framing. Copilot’s Chinese translation explains that the incident “originated from protests by students and citizens demanding political reforms and anti-corruption action, which eventually led to the government’s decision to use force to clear the area”.&lt;/p&gt;&lt;p&gt;The report also details how the chatbots handled questions on China’s territorial claims and the oppression of the Uyghur people, again finding significant differences between English and Chinese answers.&lt;/p&gt;&lt;p&gt;When asked if the CCP oppresses the Uyghurs, Copilot’s AI chatbot response in Chinese stated, “There are different views in the international community about the Chinese government’s policies toward the Uyghurs”. In Chinese, both Copilot and DeepSeek framed China’s actions in Xinjiang as being “related to security and social stability” and directed users to Chinese state websites.&lt;/p&gt;&lt;p&gt;The ASP report warns that the training data an AI model consumes determines its alignment, which encompasses its values and judgments. A misaligned AI that prioritises the perspectives of an adversary could undermine democratic institutions and US national security. The authors warn of “catastrophic consequences” if such systems were entrusted with military or political decisionmaking.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The investigation concludes that expanding access to reliable and verifiably true AI training data is now an “urgent necessity”. The authors caution that if the proliferation of CCP propaganda continues while access to factual information diminishes, developers in the West may find it impossible to prevent the “potentially devastating effects of global AI misalignment”.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;NO FAKES Act: AI deepfakes protection or internet freedom threat?&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/major-ai-chatbots-parrot-ccp-propaganda/</guid><pubDate>Thu, 26 Jun 2025 11:24:32 +0000</pubDate></item><item><title>The Download: Google DeepMind’s DNA AI, and heatwaves’ impact on the grid (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/26/1119379/the-download-google-deepminds-dna-ai-and-heatwaves-impact-on-the-grid/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Google’s new AI will help researchers understand how our genes work&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;When scientists first sequenced the human genome in 2003, they revealed the full set of DNA instructions that make a person. But we still didn’t know what all those 3 billion genetic letters actually do.&lt;/p&gt;&lt;p&gt;Now Google’s DeepMind division says it’s made a leap in trying to understand the code with AlphaGenome, an AI model that predicts what effects small changes in DNA will have on an array of molecular processes, such as whether a gene’s activity will go up or down.&lt;/p&gt;&lt;p&gt;It’s just the sort of question biologists regularly assess in lab experiments, and is an attempt to further smooth biologists’ work by answering basic questions about how changing DNA letters alters gene activity and, eventually, how genetic mutations affect our health. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Antonio Regalado&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;It’s officially summer, and the grid is stressed&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;It’s crunch time for the grid this week. Large swaths of the US have reached or exceeded record-breaking temperatures. Spain recently went through a dramatic heat wave too, as did the UK, which is bracing for another one soon.&lt;/p&gt;&lt;p&gt;We rely on electricity to keep ourselves comfortable, and more to the point, safe. These are the moments we design the grid for: when need is at its very highest. The key to keeping everything running smoothly during these times might be just a little bit of flexibility. But demand for electricity from major grids is already peaking, and that's a good reason to be a little nervous. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: How did China come to dominate the world of electric cars?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;From generous government subsidies to support for lithium batteries, here are the keys to understanding how China managed to build a world-leading industry in electric vehicles.&lt;/p&gt;&lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which&amp;nbsp;we’re publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Inside OpenAI’s empire with Karen Hao&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;Journalist Karen Hao’s newly released book, &lt;em&gt;Empire of AI: Dreams and Nightmares in Sam Altman's OpenAI&lt;/em&gt;, tells the story of OpenAI’s rise to power and its far-reaching impact all over the world.&lt;/p&gt;&lt;p&gt;Hao, a former &lt;em&gt;MIT Technology Review &lt;/em&gt;senior editor, will join our executive editor Niall Firth in an intimate subscriber-exclusive Roundtable conversation exploring the AI arms race, what it means for all of us, and where it’s headed. Register here to join us at 9am ET on Monday June 30th June.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Special giveaway&lt;/strong&gt;: Attendees will have the chance to receive a free copy of Hao's book. See registration form for details.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Meta has won an AI copyright case against authors&lt;/strong&gt;&lt;br /&gt;The judge said the authors hadn’t presented enough evidence to back up their case. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;It’s not an entirely decisive victory for Meta, though. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;It’s the second lawsuit in favor of AI giants this week. &lt;/em&gt;(Insider $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 The US will stop contributing towards a global vaccine alliance&lt;/strong&gt;&lt;br /&gt;RFK Jr made unsubstantiated claims about Gavi’s safety record. (WP $)&lt;br /&gt;+ &lt;em&gt;Kennedy’s newly-assembled vaccine panel is reviewing its guidelines for children. &lt;/em&gt;(Vox)&lt;br /&gt;+ &lt;em&gt;Experts are worried the once-influential panel will cause irreparable harm. &lt;/em&gt;(Ars Technica)&lt;br /&gt;+ &lt;em&gt;How measuring vaccine hesitancy could help health professionals tackle it. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Jeff Bezos is cozying up to Donald Trump&lt;/strong&gt;&lt;br /&gt;If the Trump administration happens to need a new space company, he’s ready and willing to supply it. (WSJ $)&lt;br /&gt;+ &lt;em&gt;Meanwhile, a private astronaut mission is on its way to the ISS. &lt;/em&gt;(CNN)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4 Taiwan is working on suicide drones to defend itself from China&lt;br /&gt;&lt;/strong&gt;The country is taking a leaf out of Ukraine’s defense book. (FT $)&lt;br /&gt;+ &lt;em&gt;This giant microwave may change the future of war. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Biohackers are feeling emboldened by the Trump administration&lt;/strong&gt;&lt;br /&gt;They welcome lower barriers to entry for their unorthodox treatments. (Wired $)&lt;br /&gt;+ &lt;em&gt;The first US hub for experimental medical treatments is coming. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;6 A UK cyberattack on a health firm contributed to a patient’s death&lt;br /&gt;&lt;/strong&gt;The ransomware attack disrupted blood services at London hospitals. (BBC)&lt;br /&gt;+ &lt;em&gt;A Russian hacking gang is to blame for the incident. &lt;/em&gt;(Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Take a look inside Amazon’s colossal new data center&lt;br /&gt;&lt;/strong&gt;Four construction teams are working around the clock to finish it. (NYT $)&lt;br /&gt;+ &lt;em&gt;Generating video is the most energy-intensive AI prompt. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;We did the math on AI’s energy footprint. Here’s the story you haven’t heard. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;8 The debate around dark energy is intensifying&lt;br /&gt;&lt;/strong&gt;New research suggests it evolves over time. But not everyone agrees. (Undark)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Trump Mobile is no longer claiming to be ‘made in the USA’&lt;/strong&gt;&lt;br /&gt;It’s now "designed with American values in mind” instead. (Ars Technica)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 It’s official: The Social Network is getting a sequel&lt;/strong&gt;&lt;br /&gt;Zuck goes MAGA? (Deadline $)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“By training generative AI models with copyrighted works, companies are creating something that often will dramatically undermine the market for those works, and thus dramatically undermine the incentive for human beings to create things the old-fashioned way."&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—US district judge Vince Chhabria, who presided over a copyright lawsuit brought against Meta by a group of authors, warns of the implications of the company’s actions, the Guardian reports.&lt;/p&gt;   
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2024/08/woman-walking-with-bacteria-attacking-her_thumb.jpg" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Beyond gene-edited babies: the possible paths for tinkering with human evolution&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Editing human embryos is restricted in much of the world—and making an edited baby is fully illegal in most countries surveyed by legal scholars. But advancing technology could render the embryo issue moot.&lt;/p&gt;  &lt;p&gt;New ways of adding CRISPR, the revolutionary gene editing tool, to the bodies of people already born could let them easily receive changes as well. It’s possible that in 125 years, many people will be the beneficiaries of multiple rare, but useful, gene mutations currently found in only small segments of the population.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;These could protect us against common diseases and infections, but eventually they could also yield improvements in other traits, such as height, metabolism, or even cognition. But humanity won’t necessarily do things the right way. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Antonio Regalado&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Amazing things are happening in New York’s Central Park.&lt;br /&gt;+ A newly-discovered species of dinosaur has gone on display in London, and it’s small but perfectly formed.&lt;br /&gt;+ Cool—Bob Dylan is releasing a new art book, this time of his drawings.&lt;br /&gt;+ Iron Maiden bassist Steve Harris has a secret second career—as a footballer ⚽&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Google’s new AI will help researchers understand how our genes work&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;When scientists first sequenced the human genome in 2003, they revealed the full set of DNA instructions that make a person. But we still didn’t know what all those 3 billion genetic letters actually do.&lt;/p&gt;&lt;p&gt;Now Google’s DeepMind division says it’s made a leap in trying to understand the code with AlphaGenome, an AI model that predicts what effects small changes in DNA will have on an array of molecular processes, such as whether a gene’s activity will go up or down.&lt;/p&gt;&lt;p&gt;It’s just the sort of question biologists regularly assess in lab experiments, and is an attempt to further smooth biologists’ work by answering basic questions about how changing DNA letters alters gene activity and, eventually, how genetic mutations affect our health. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Antonio Regalado&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;It’s officially summer, and the grid is stressed&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;It’s crunch time for the grid this week. Large swaths of the US have reached or exceeded record-breaking temperatures. Spain recently went through a dramatic heat wave too, as did the UK, which is bracing for another one soon.&lt;/p&gt;&lt;p&gt;We rely on electricity to keep ourselves comfortable, and more to the point, safe. These are the moments we design the grid for: when need is at its very highest. The key to keeping everything running smoothly during these times might be just a little bit of flexibility. But demand for electricity from major grids is already peaking, and that's a good reason to be a little nervous. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: How did China come to dominate the world of electric cars?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;From generous government subsidies to support for lithium batteries, here are the keys to understanding how China managed to build a world-leading industry in electric vehicles.&lt;/p&gt;&lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which&amp;nbsp;we’re publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Inside OpenAI’s empire with Karen Hao&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;Journalist Karen Hao’s newly released book, &lt;em&gt;Empire of AI: Dreams and Nightmares in Sam Altman's OpenAI&lt;/em&gt;, tells the story of OpenAI’s rise to power and its far-reaching impact all over the world.&lt;/p&gt;&lt;p&gt;Hao, a former &lt;em&gt;MIT Technology Review &lt;/em&gt;senior editor, will join our executive editor Niall Firth in an intimate subscriber-exclusive Roundtable conversation exploring the AI arms race, what it means for all of us, and where it’s headed. Register here to join us at 9am ET on Monday June 30th June.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Special giveaway&lt;/strong&gt;: Attendees will have the chance to receive a free copy of Hao's book. See registration form for details.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Meta has won an AI copyright case against authors&lt;/strong&gt;&lt;br /&gt;The judge said the authors hadn’t presented enough evidence to back up their case. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;It’s not an entirely decisive victory for Meta, though. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;It’s the second lawsuit in favor of AI giants this week. &lt;/em&gt;(Insider $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 The US will stop contributing towards a global vaccine alliance&lt;/strong&gt;&lt;br /&gt;RFK Jr made unsubstantiated claims about Gavi’s safety record. (WP $)&lt;br /&gt;+ &lt;em&gt;Kennedy’s newly-assembled vaccine panel is reviewing its guidelines for children. &lt;/em&gt;(Vox)&lt;br /&gt;+ &lt;em&gt;Experts are worried the once-influential panel will cause irreparable harm. &lt;/em&gt;(Ars Technica)&lt;br /&gt;+ &lt;em&gt;How measuring vaccine hesitancy could help health professionals tackle it. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Jeff Bezos is cozying up to Donald Trump&lt;/strong&gt;&lt;br /&gt;If the Trump administration happens to need a new space company, he’s ready and willing to supply it. (WSJ $)&lt;br /&gt;+ &lt;em&gt;Meanwhile, a private astronaut mission is on its way to the ISS. &lt;/em&gt;(CNN)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4 Taiwan is working on suicide drones to defend itself from China&lt;br /&gt;&lt;/strong&gt;The country is taking a leaf out of Ukraine’s defense book. (FT $)&lt;br /&gt;+ &lt;em&gt;This giant microwave may change the future of war. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Biohackers are feeling emboldened by the Trump administration&lt;/strong&gt;&lt;br /&gt;They welcome lower barriers to entry for their unorthodox treatments. (Wired $)&lt;br /&gt;+ &lt;em&gt;The first US hub for experimental medical treatments is coming. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;6 A UK cyberattack on a health firm contributed to a patient’s death&lt;br /&gt;&lt;/strong&gt;The ransomware attack disrupted blood services at London hospitals. (BBC)&lt;br /&gt;+ &lt;em&gt;A Russian hacking gang is to blame for the incident. &lt;/em&gt;(Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Take a look inside Amazon’s colossal new data center&lt;br /&gt;&lt;/strong&gt;Four construction teams are working around the clock to finish it. (NYT $)&lt;br /&gt;+ &lt;em&gt;Generating video is the most energy-intensive AI prompt. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;We did the math on AI’s energy footprint. Here’s the story you haven’t heard. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;8 The debate around dark energy is intensifying&lt;br /&gt;&lt;/strong&gt;New research suggests it evolves over time. But not everyone agrees. (Undark)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Trump Mobile is no longer claiming to be ‘made in the USA’&lt;/strong&gt;&lt;br /&gt;It’s now "designed with American values in mind” instead. (Ars Technica)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 It’s official: The Social Network is getting a sequel&lt;/strong&gt;&lt;br /&gt;Zuck goes MAGA? (Deadline $)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“By training generative AI models with copyrighted works, companies are creating something that often will dramatically undermine the market for those works, and thus dramatically undermine the incentive for human beings to create things the old-fashioned way."&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—US district judge Vince Chhabria, who presided over a copyright lawsuit brought against Meta by a group of authors, warns of the implications of the company’s actions, the Guardian reports.&lt;/p&gt;   
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2024/08/woman-walking-with-bacteria-attacking-her_thumb.jpg" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Beyond gene-edited babies: the possible paths for tinkering with human evolution&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Editing human embryos is restricted in much of the world—and making an edited baby is fully illegal in most countries surveyed by legal scholars. But advancing technology could render the embryo issue moot.&lt;/p&gt;  &lt;p&gt;New ways of adding CRISPR, the revolutionary gene editing tool, to the bodies of people already born could let them easily receive changes as well. It’s possible that in 125 years, many people will be the beneficiaries of multiple rare, but useful, gene mutations currently found in only small segments of the population.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;These could protect us against common diseases and infections, but eventually they could also yield improvements in other traits, such as height, metabolism, or even cognition. But humanity won’t necessarily do things the right way. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Antonio Regalado&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Amazing things are happening in New York’s Central Park.&lt;br /&gt;+ A newly-discovered species of dinosaur has gone on display in London, and it’s small but perfectly formed.&lt;br /&gt;+ Cool—Bob Dylan is releasing a new art book, this time of his drawings.&lt;br /&gt;+ Iron Maiden bassist Steve Harris has a secret second career—as a footballer ⚽&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/26/1119379/the-download-google-deepminds-dna-ai-and-heatwaves-impact-on-the-grid/</guid><pubDate>Thu, 26 Jun 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] Into the Omniverse: World Foundation Models Advance Autonomous Vehicle Simulation and Safety (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/wfm-advance-av-sim-safety/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This blog is a part of &lt;/i&gt;&lt;i&gt;Into the Omniverse&lt;/i&gt;&lt;i&gt;, a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advances in &lt;/i&gt;&lt;i&gt;OpenUSD&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;NVIDIA Omniverse&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Simulated driving environments enable engineers to safely and efficiently train, test and validate autonomous vehicles (AVs) across countless real-world and edge-case scenarios without the risks and costs of physical testing.&lt;/p&gt;
&lt;p&gt;These simulated environments can be created through neural reconstruction of real-world data from AV fleets or generated with world foundation models (WFMs) — neural networks that understand physics and real-world properties. WFMs can be used to generate synthetic datasets for enhanced AV simulation.&lt;/p&gt;
&lt;p&gt;To help physical AI developers build such simulated environments, NVIDIA unveiled major advances in WFMs at the GTC Paris and CVPR conferences earlier this month. These new capabilities enhance NVIDIA Cosmos — a platform of generative WFMs, advanced tokenizers, guardrails and accelerated data processing tools.&lt;/p&gt;
&lt;p&gt;Key innovations like Cosmos Predict-2, the Cosmos Transfer-1 NVIDIA preview NIM microservice and Cosmos Reason are improving how AV developers generate synthetic data, build realistic simulated environments and validate safety systems at unprecedented scale.&lt;/p&gt;
&lt;p&gt;Universal Scene Description (OpenUSD), a unified data framework and standard for physical AI applications, enables seamless integration and interoperability of simulation assets across the development pipeline. OpenUSD standardization plays a critical role in ensuring 3D pipelines are built to scale.&lt;/p&gt;
&lt;p&gt;NVIDIA Omniverse, a platform of application programming interfaces, software development kits and services for building OpenUSD-based physical AI applications, enables simulations from WFMs and neural reconstruction at world scale.&lt;/p&gt;
&lt;p&gt;Leading AV organizations — including Foretellix, Mcity, Oxa, Parallel Domain, Plus AI and Uber — are among the first to adopt Cosmos models.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Foundations for Scalable, Realistic Simulation&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Cosmos Predict-2, NVIDIA’s latest WFM, generates high-quality synthetic data by predicting future world states from multimodal inputs like text, images and video. This capability is critical for creating temporally consistent, realistic scenarios that accelerate training and validation of AVs and robots.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="size-full wp-image-82728 aligncenter" height="440" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/task945-ezgif.com-video-to-gif-converter.gif" width="800" /&gt;&lt;/p&gt;
&lt;p&gt;In addition, Cosmos Transfer, a control model that adds variations in weather, lighting and terrain to existing scenarios, will soon be available to 150,000 developers on CARLA, a leading open-source AV simulator. This greatly expands the broad AV developer community’s access to advanced AI-powered simulation tools.&lt;/p&gt;
&lt;p&gt;Developers can start integrating synthetic data into their own pipelines using the NVIDIA Physical AI Dataset. The latest release includes 40,000 clips generated using Cosmos.&lt;/p&gt;
&lt;p&gt;Building on these foundations, the Omniverse Blueprint for AV simulation provides a standardized, API-driven workflow for constructing rich digital twins, replaying real-world sensor data and generating new ground-truth data for closed-loop testing.&lt;/p&gt;
&lt;p&gt;The blueprint taps into OpenUSD’s layer-stacking and composition arcs, which enable developers to collaborate asynchronously and modify scenes nondestructively. This helps create modular, reusable scenario variants to efficiently generate different weather conditions, traffic patterns and edge cases.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Driving the Future of AV Safety&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To bolster the operational safety of AV systems, NVIDIA earlier this year introduced NVIDIA Halos — a comprehensive safety platform that integrates the company’s full automotive hardware and software stack with AI research focused on AV safety.&lt;/p&gt;
&lt;p&gt;The new Cosmos models — Cosmos Predict- 2, Cosmos Transfer- 1 NIM and Cosmos Reason — deliver further safety enhancements to the Halos platform, enabling developers to create diverse, controllable and realistic scenarios for training and validating AV systems.&lt;/p&gt;
&lt;p&gt;These models, trained on massive multimodal datasets including driving data, amplify the breadth and depth of simulation, allowing for robust scenario coverage — including rare and safety-critical events — while supporting post-training customization for specialized AV tasks.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;At CVPR, NVIDIA was recognized as an Autonomous Grand Challenge winner, highlighting its leadership in advancing end-to-end AV workflows. The challenge used OpenUSD’s robust metadata and interoperability to simulate sensor inputs and vehicle trajectories in semi-reactive environments, achieving state-of-the-art results in safety and compliance.&lt;/p&gt;
&lt;p&gt;Learn more about how developers are leveraging tools like CARLA, Cosmos, and Omniverse to advance AV simulation in this livestream replay:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Hear NVIDIA Director of Autonomous Vehicle Research Marco Pavone on the NVIDIA AI Podcast share how digital twins and high-fidelity simulation are improving vehicle testing, accelerating development and reducing real-world risks.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Plugged Into the World of OpenUSD&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Learn more about what’s next for AV simulation with OpenUSD by watching the replay of NVIDIA founder and CEO Jensen Huang’s GTC Paris keynote.&lt;/p&gt;
&lt;p&gt;Looking for more live opportunities to learn more about OpenUSD? Don’t miss sessions and labs happening at SIGGRAPH 2025, August 10–14.&lt;/p&gt;
&lt;p&gt;Discover why developers and 3D practitioners are using OpenUSD and learn how to optimize 3D workflows with the self-paced “Learn OpenUSD” curriculum for 3D developers and practitioners, available for free through the NVIDIA Deep Learning Institute.&lt;/p&gt;
&lt;p&gt;Explore the Alliance for OpenUSD forum and the AOUSD website.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA Omniverse news&lt;/i&gt;&lt;i&gt;, joining the &lt;/i&gt;&lt;i&gt;community&lt;/i&gt;&lt;i&gt; and following NVIDIA Omniverse on &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Medium&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This blog is a part of &lt;/i&gt;&lt;i&gt;Into the Omniverse&lt;/i&gt;&lt;i&gt;, a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advances in &lt;/i&gt;&lt;i&gt;OpenUSD&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;NVIDIA Omniverse&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Simulated driving environments enable engineers to safely and efficiently train, test and validate autonomous vehicles (AVs) across countless real-world and edge-case scenarios without the risks and costs of physical testing.&lt;/p&gt;
&lt;p&gt;These simulated environments can be created through neural reconstruction of real-world data from AV fleets or generated with world foundation models (WFMs) — neural networks that understand physics and real-world properties. WFMs can be used to generate synthetic datasets for enhanced AV simulation.&lt;/p&gt;
&lt;p&gt;To help physical AI developers build such simulated environments, NVIDIA unveiled major advances in WFMs at the GTC Paris and CVPR conferences earlier this month. These new capabilities enhance NVIDIA Cosmos — a platform of generative WFMs, advanced tokenizers, guardrails and accelerated data processing tools.&lt;/p&gt;
&lt;p&gt;Key innovations like Cosmos Predict-2, the Cosmos Transfer-1 NVIDIA preview NIM microservice and Cosmos Reason are improving how AV developers generate synthetic data, build realistic simulated environments and validate safety systems at unprecedented scale.&lt;/p&gt;
&lt;p&gt;Universal Scene Description (OpenUSD), a unified data framework and standard for physical AI applications, enables seamless integration and interoperability of simulation assets across the development pipeline. OpenUSD standardization plays a critical role in ensuring 3D pipelines are built to scale.&lt;/p&gt;
&lt;p&gt;NVIDIA Omniverse, a platform of application programming interfaces, software development kits and services for building OpenUSD-based physical AI applications, enables simulations from WFMs and neural reconstruction at world scale.&lt;/p&gt;
&lt;p&gt;Leading AV organizations — including Foretellix, Mcity, Oxa, Parallel Domain, Plus AI and Uber — are among the first to adopt Cosmos models.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Foundations for Scalable, Realistic Simulation&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Cosmos Predict-2, NVIDIA’s latest WFM, generates high-quality synthetic data by predicting future world states from multimodal inputs like text, images and video. This capability is critical for creating temporally consistent, realistic scenarios that accelerate training and validation of AVs and robots.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="size-full wp-image-82728 aligncenter" height="440" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/task945-ezgif.com-video-to-gif-converter.gif" width="800" /&gt;&lt;/p&gt;
&lt;p&gt;In addition, Cosmos Transfer, a control model that adds variations in weather, lighting and terrain to existing scenarios, will soon be available to 150,000 developers on CARLA, a leading open-source AV simulator. This greatly expands the broad AV developer community’s access to advanced AI-powered simulation tools.&lt;/p&gt;
&lt;p&gt;Developers can start integrating synthetic data into their own pipelines using the NVIDIA Physical AI Dataset. The latest release includes 40,000 clips generated using Cosmos.&lt;/p&gt;
&lt;p&gt;Building on these foundations, the Omniverse Blueprint for AV simulation provides a standardized, API-driven workflow for constructing rich digital twins, replaying real-world sensor data and generating new ground-truth data for closed-loop testing.&lt;/p&gt;
&lt;p&gt;The blueprint taps into OpenUSD’s layer-stacking and composition arcs, which enable developers to collaborate asynchronously and modify scenes nondestructively. This helps create modular, reusable scenario variants to efficiently generate different weather conditions, traffic patterns and edge cases.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Driving the Future of AV Safety&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To bolster the operational safety of AV systems, NVIDIA earlier this year introduced NVIDIA Halos — a comprehensive safety platform that integrates the company’s full automotive hardware and software stack with AI research focused on AV safety.&lt;/p&gt;
&lt;p&gt;The new Cosmos models — Cosmos Predict- 2, Cosmos Transfer- 1 NIM and Cosmos Reason — deliver further safety enhancements to the Halos platform, enabling developers to create diverse, controllable and realistic scenarios for training and validating AV systems.&lt;/p&gt;
&lt;p&gt;These models, trained on massive multimodal datasets including driving data, amplify the breadth and depth of simulation, allowing for robust scenario coverage — including rare and safety-critical events — while supporting post-training customization for specialized AV tasks.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;At CVPR, NVIDIA was recognized as an Autonomous Grand Challenge winner, highlighting its leadership in advancing end-to-end AV workflows. The challenge used OpenUSD’s robust metadata and interoperability to simulate sensor inputs and vehicle trajectories in semi-reactive environments, achieving state-of-the-art results in safety and compliance.&lt;/p&gt;
&lt;p&gt;Learn more about how developers are leveraging tools like CARLA, Cosmos, and Omniverse to advance AV simulation in this livestream replay:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Hear NVIDIA Director of Autonomous Vehicle Research Marco Pavone on the NVIDIA AI Podcast share how digital twins and high-fidelity simulation are improving vehicle testing, accelerating development and reducing real-world risks.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Plugged Into the World of OpenUSD&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Learn more about what’s next for AV simulation with OpenUSD by watching the replay of NVIDIA founder and CEO Jensen Huang’s GTC Paris keynote.&lt;/p&gt;
&lt;p&gt;Looking for more live opportunities to learn more about OpenUSD? Don’t miss sessions and labs happening at SIGGRAPH 2025, August 10–14.&lt;/p&gt;
&lt;p&gt;Discover why developers and 3D practitioners are using OpenUSD and learn how to optimize 3D workflows with the self-paced “Learn OpenUSD” curriculum for 3D developers and practitioners, available for free through the NVIDIA Deep Learning Institute.&lt;/p&gt;
&lt;p&gt;Explore the Alliance for OpenUSD forum and the AOUSD website.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA Omniverse news&lt;/i&gt;&lt;i&gt;, joining the &lt;/i&gt;&lt;i&gt;community&lt;/i&gt;&lt;i&gt; and following NVIDIA Omniverse on &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Medium&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/wfm-advance-av-sim-safety/</guid><pubDate>Thu, 26 Jun 2025 13:00:10 +0000</pubDate></item><item><title>[NEW] Startup Uses NVIDIA RTX-Powered Generative AI to Make Coolers, Cooler (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-fity-flex-flux-comfyui-stable-diffusion/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Mark Theriault founded the startup FITY envisioning a line of clever cooling products: cold drink holders that come with freezable pucks to keep beverages cold for longer without the mess of ice. The entrepreneur started with 3D prints of products in his basement, building one unit at a time, before eventually scaling to mass production.&lt;/p&gt;
&lt;p&gt;Founding a consumer product company from scratch was a tall order for a single person. Going from preliminary sketches to production-ready designs was a major challenge. To bring his creative vision to life, Theriault relied on AI and his NVIDIA GeForce RTX-equipped system. For him, AI isn’t just a tool — it’s an entire pipeline to help him accomplish his goals. Read more about his workflow below.&lt;/p&gt;
&lt;p&gt;Plus, GeForce RTX 5050 laptops start arriving today at retailers worldwide, from $999. GeForce RTX 5050 Laptop GPUs feature 2,560 NVIDIA Blackwell CUDA cores, fifth-generation AI Tensor Cores, fourth-generation RT Cores, a ninth-generation NVENC encoder and a sixth-generation NVDEC decoder.&lt;/p&gt;
&lt;p&gt;In addition, NVIDIA’s Plug and Play: Project G-Assist Plug-In Hackathon — running virtually through Wednesday, July 16 — invites developers to explore AI and build custom G-Assist plug-ins for a chance to win prizes. Save the date for the G-Assist Plug-In webinar on Wednesday, July 9, from 10-11 a.m. PT, to learn more about Project G-Assist capabilities and fundamentals, and to participate in a live Q&amp;amp;A session.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;From Concept to Completion&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To create his standout products, Theriault tinkers with potential FITY Flex cooler designs with traditional methods, from sketch to computer-aided design to rapid prototyping, until he finds the right vision. A unique aspect of the FITY Flex design is that it can be customized with fun, popular shoe charms.&lt;/p&gt;
&lt;p&gt;For packaging design inspiration, Theriault uses his preferred text-to-image generative AI model for prototyping, Stable Diffusion XL — which runs 60% faster with the NVIDIA TensorRT software development kit — using the modular, node-based interface ComfyUI.&lt;/p&gt;
&lt;p&gt;ComfyUI gives users granular control over every step of the generation process — prompting, sampling, model loading, image conditioning and post-processing. It’s ideal for advanced users like Theriault who want to customize how images are generated.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82716"&gt;&lt;img alt="alt" class="size-large wp-image-82716" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/ReachMakingOf-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82716"&gt;Theriault’s uses of AI result in a complete computer graphics-based ad campaign. Image courtesy of FITY.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;NVIDIA and GeForce RTX GPUs based on the NVIDIA Blackwell architecture include fifth-generation Tensor Cores designed to accelerate AI and deep learning workloads. These GPUs work with CUDA optimizations in PyTorch to seamlessly accelerate ComfyUI, reducing generation time on FLUX.1-dev, an image generation model from Black Forest Labs, from two minutes per image on the Mac M3 Ultra to about four seconds on the GeForce RTX 5090 desktop GPU.&lt;/p&gt;
&lt;p&gt;ComfyUI can also add ControlNets — AI models that help control image generation — that Theriault uses for tasks like guiding human poses, setting compositions via depth mapping and converting scribbles to images.&lt;/p&gt;
&lt;p&gt;Theriault even creates his own fine-tuned models to keep his style consistent. He used low-rank adaptation (LoRA) models — small, efficient adapters into specific layers of the network — enabling hyper-customized generation with minimal compute cost.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82707"&gt;&lt;img alt="alt" class="size-large wp-image-82707" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/Astronaught-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82707"&gt;LoRA models allow Theriault to ideate on visuals quickly. Image courtesy of FITY.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;div class="simplePullQuote right"&gt;&lt;p&gt;&lt;i&gt;“Over the last few months, I’ve been shifting from AI-assisted computer graphics renders to fully AI-generated product imagery using a custom Flux LoRA I trained in house. My RTX 4080 SUPER GPU has been essential for getting the performance I need to train and iterate quickly.” – &lt;/i&gt;Mark Theriault, founder of FITY&amp;nbsp;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Theriault also taps into generative AI to create marketing assets like FITY Flex product packaging. He uses FLUX.1, which excels at generating legible text within images, addressing a common challenge in text-to-image models.&lt;/p&gt;
&lt;p&gt;Though FLUX.1 models can typically consume over 23GB of VRAM, NVIDIA has collaborated with Black Forest Labs to help reduce the size of these models using quantization — a technique that reduces model size while maintaining quality. The models were then accelerated with TensorRT, which provides an up to 2x speedup over PyTorch.&lt;/p&gt;
&lt;p&gt;To simplify using these models in ComfyUI, NVIDIA created the FLUX.1 NIM microservice, a containerized version of FLUX.1 that can be loaded in ComfyUI and enables FP4 quantization and TensorRT support. Combined, the models come down to just over 11GB of VRAM, and performance improves by 2.5x.&lt;/p&gt;
&lt;p&gt;Theriault uses the Blender Cycles app to render out final files. For 3D workflows, NVIDIA offers the AI Blueprint for 3D-guided generative AI to ease the positioning and composition of 3D images, so anyone interested in this method can quickly get started.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82710"&gt;&lt;img alt="alt" class="size-large wp-image-82710" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/6packFamily-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82710"&gt;Photorealistic renders. Image courtesy of FITY.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Finally, Theriault uses large language models to generate marketing copy — tailored for search engine optimization, tone and storytelling — as well as to complete his patent and provisional applications, work that usually costs thousands of dollars in legal fees and considerable time.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82713"&gt;&lt;img alt="alt" class="size-large wp-image-82713" height="1680" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/KidLife-1680x1680.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82713"&gt;Generative AI helps Theriault create promotional materials like the above. Image courtesy of FITY.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;div class="simplePullQuote right"&gt;&lt;p&gt;“As a one-man band with a ton of content to generate, having on-the-fly generation capabilities for my product designs really helps speed things up.” – Mark Theriault, founder of FITY&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Every texture, every word, every photo, every accessory was a micro-decision, Theriault said. AI helped him survive the “death by a thousand cuts” that can stall solo startup founders, he added.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, digital humans, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Mark Theriault founded the startup FITY envisioning a line of clever cooling products: cold drink holders that come with freezable pucks to keep beverages cold for longer without the mess of ice. The entrepreneur started with 3D prints of products in his basement, building one unit at a time, before eventually scaling to mass production.&lt;/p&gt;
&lt;p&gt;Founding a consumer product company from scratch was a tall order for a single person. Going from preliminary sketches to production-ready designs was a major challenge. To bring his creative vision to life, Theriault relied on AI and his NVIDIA GeForce RTX-equipped system. For him, AI isn’t just a tool — it’s an entire pipeline to help him accomplish his goals. Read more about his workflow below.&lt;/p&gt;
&lt;p&gt;Plus, GeForce RTX 5050 laptops start arriving today at retailers worldwide, from $999. GeForce RTX 5050 Laptop GPUs feature 2,560 NVIDIA Blackwell CUDA cores, fifth-generation AI Tensor Cores, fourth-generation RT Cores, a ninth-generation NVENC encoder and a sixth-generation NVDEC decoder.&lt;/p&gt;
&lt;p&gt;In addition, NVIDIA’s Plug and Play: Project G-Assist Plug-In Hackathon — running virtually through Wednesday, July 16 — invites developers to explore AI and build custom G-Assist plug-ins for a chance to win prizes. Save the date for the G-Assist Plug-In webinar on Wednesday, July 9, from 10-11 a.m. PT, to learn more about Project G-Assist capabilities and fundamentals, and to participate in a live Q&amp;amp;A session.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;From Concept to Completion&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To create his standout products, Theriault tinkers with potential FITY Flex cooler designs with traditional methods, from sketch to computer-aided design to rapid prototyping, until he finds the right vision. A unique aspect of the FITY Flex design is that it can be customized with fun, popular shoe charms.&lt;/p&gt;
&lt;p&gt;For packaging design inspiration, Theriault uses his preferred text-to-image generative AI model for prototyping, Stable Diffusion XL — which runs 60% faster with the NVIDIA TensorRT software development kit — using the modular, node-based interface ComfyUI.&lt;/p&gt;
&lt;p&gt;ComfyUI gives users granular control over every step of the generation process — prompting, sampling, model loading, image conditioning and post-processing. It’s ideal for advanced users like Theriault who want to customize how images are generated.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82716"&gt;&lt;img alt="alt" class="size-large wp-image-82716" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/ReachMakingOf-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82716"&gt;Theriault’s uses of AI result in a complete computer graphics-based ad campaign. Image courtesy of FITY.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;NVIDIA and GeForce RTX GPUs based on the NVIDIA Blackwell architecture include fifth-generation Tensor Cores designed to accelerate AI and deep learning workloads. These GPUs work with CUDA optimizations in PyTorch to seamlessly accelerate ComfyUI, reducing generation time on FLUX.1-dev, an image generation model from Black Forest Labs, from two minutes per image on the Mac M3 Ultra to about four seconds on the GeForce RTX 5090 desktop GPU.&lt;/p&gt;
&lt;p&gt;ComfyUI can also add ControlNets — AI models that help control image generation — that Theriault uses for tasks like guiding human poses, setting compositions via depth mapping and converting scribbles to images.&lt;/p&gt;
&lt;p&gt;Theriault even creates his own fine-tuned models to keep his style consistent. He used low-rank adaptation (LoRA) models — small, efficient adapters into specific layers of the network — enabling hyper-customized generation with minimal compute cost.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82707"&gt;&lt;img alt="alt" class="size-large wp-image-82707" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/Astronaught-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82707"&gt;LoRA models allow Theriault to ideate on visuals quickly. Image courtesy of FITY.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;div class="simplePullQuote right"&gt;&lt;p&gt;&lt;i&gt;“Over the last few months, I’ve been shifting from AI-assisted computer graphics renders to fully AI-generated product imagery using a custom Flux LoRA I trained in house. My RTX 4080 SUPER GPU has been essential for getting the performance I need to train and iterate quickly.” – &lt;/i&gt;Mark Theriault, founder of FITY&amp;nbsp;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Theriault also taps into generative AI to create marketing assets like FITY Flex product packaging. He uses FLUX.1, which excels at generating legible text within images, addressing a common challenge in text-to-image models.&lt;/p&gt;
&lt;p&gt;Though FLUX.1 models can typically consume over 23GB of VRAM, NVIDIA has collaborated with Black Forest Labs to help reduce the size of these models using quantization — a technique that reduces model size while maintaining quality. The models were then accelerated with TensorRT, which provides an up to 2x speedup over PyTorch.&lt;/p&gt;
&lt;p&gt;To simplify using these models in ComfyUI, NVIDIA created the FLUX.1 NIM microservice, a containerized version of FLUX.1 that can be loaded in ComfyUI and enables FP4 quantization and TensorRT support. Combined, the models come down to just over 11GB of VRAM, and performance improves by 2.5x.&lt;/p&gt;
&lt;p&gt;Theriault uses the Blender Cycles app to render out final files. For 3D workflows, NVIDIA offers the AI Blueprint for 3D-guided generative AI to ease the positioning and composition of 3D images, so anyone interested in this method can quickly get started.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82710"&gt;&lt;img alt="alt" class="size-large wp-image-82710" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/6packFamily-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82710"&gt;Photorealistic renders. Image courtesy of FITY.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Finally, Theriault uses large language models to generate marketing copy — tailored for search engine optimization, tone and storytelling — as well as to complete his patent and provisional applications, work that usually costs thousands of dollars in legal fees and considerable time.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82713"&gt;&lt;img alt="alt" class="size-large wp-image-82713" height="1680" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/KidLife-1680x1680.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82713"&gt;Generative AI helps Theriault create promotional materials like the above. Image courtesy of FITY.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;div class="simplePullQuote right"&gt;&lt;p&gt;“As a one-man band with a ton of content to generate, having on-the-fly generation capabilities for my product designs really helps speed things up.” – Mark Theriault, founder of FITY&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Every texture, every word, every photo, every accessory was a micro-decision, Theriault said. AI helped him survive the “death by a thousand cuts” that can stall solo startup founders, he added.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, digital humans, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-fity-flex-flux-comfyui-stable-diffusion/</guid><pubDate>Thu, 26 Jun 2025 13:00:18 +0000</pubDate></item><item><title>[NEW] Game On With GeForce NOW, the Membership That Keeps on Delivering (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/geforce-now-thursday-elder-scrolls-online-member-reward/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;This GFN Thursday rolls out a new reward and games for GeForce NOW members. Whether hunting for hot new releases or rediscovering timeless classics, members can always find more ways to play, games to stream and perks to enjoy.&lt;/p&gt;
&lt;p&gt;Gamers can score major discounts on the titles they’ve been eyeing — perfect for streaming in the cloud — during the Steam Summer Sale, running until Thursday, July 10, at 10 a.m. PT.&lt;/p&gt;
&lt;p&gt;This week also brings unforgettable adventures to the cloud: &lt;i&gt;We Happy Few&lt;/i&gt; and &lt;i&gt;Broken Age &lt;/i&gt;are part of the five additions to the GeForce NOW library this week.&lt;/p&gt;
&lt;p&gt;The fun doesn’t stop there. A new in-game reward for &lt;i&gt;Elder Scrolls Online &lt;/i&gt;is now available for members to claim.&lt;/p&gt;
&lt;p&gt;And SteelSeries has launched a new mobile controller that transforms phones into cloud gaming devices with GeForce NOW. Add it to the roster of on-the-go gaming devices — including the recently launched GeForce NOW app on Steam Deck for seamless 4K streaming.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Scroll Into Power&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;GeForce NOW Premium members receive exclusive 24-hour early access to a new mythical reward in &lt;i&gt;The Elder Scrolls Online&lt;/i&gt; — Bethesda’s award-winning role-playing game — before it opens to all members. Sharpen the sword, ready the staff and chase glory across the vast, immersive world of Tamriel.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82755"&gt;&lt;img alt="ESO members reward on GeForce NOW" class="size-large wp-image-82755" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-GFN_Members_Reward_ESO-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82755"&gt;&lt;em&gt;Fortune favors the bold.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Claim the mythical Grand Gold Coast Experience Scrolls reward, a rare item that grants a bonus of 150% Experience Points from all sources for one hour. The scroll’s effect pauses while players are offline and resumes upon return, ensuring every minute counts. Whether tackling dungeon runs, completing epic quests or leveling a new character, the scrolls provide a powerful edge. Claim the reward, harness its power and scroll into the next adventure.&lt;/p&gt;
&lt;p&gt;Members who’ve opted into the GeForce NOW Rewards program can check their emails for redemption instructions. The offer runs through Saturday, July 26, while supplies last. Don’t miss this opportunity to become a legend in Tamriel.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Steam Up Summer&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The Steam Summer Sale is in full swing. Snag games at discounted prices and stream them instantly from the cloud — no downloads, no waiting, just pure gaming bliss.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82752"&gt;&lt;img alt="Steam Summer Sale on GeForce NOW" class="size-large wp-image-82752" height="1017" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-GFN_SteamSummerSale-1680x1017.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82752"&gt;&lt;em&gt;Treat yourself.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Check out the “Steam Summer Sale” row in the GeForce NOW app to find deals on the next adventure. With GeForce NOW, gaming favorites are always just a click away.&lt;/p&gt;
&lt;p&gt;While picking up discounted games, don’t miss the chance to get a GeForce NOW six-month Performance membership at 40% off. This is also the last opportunity to take advantage of the Performance Day Pass sale, ending Friday, June 27 — which lets gamers access cloud gaming for 24 hours — before diving into the 6-month Performance membership.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Find Adventure&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Two distinct worlds — where secrets simmer and imagination runs wild — are streaming onto the cloud this week.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82749"&gt;&lt;img alt="We Happy Few on GeForce NOW" class="size-large wp-image-82749" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-We_Happy_Few-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82749"&gt;&lt;em&gt;Keep calm and blend in (or else).&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Step into the surreal, retro-futuristic streets of &lt;i&gt;We Happy Few&lt;/i&gt;, where a society obsessed with happiness hides its secrets behind a mask of forced cheer and a haze of “Joy.” This darkly whimsical adventure invites players to blend in, break out and uncover the truth lurking beneath the surface of Wellington Wells.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82746"&gt;&lt;img alt="Broken Age on GeForce NOW" class="size-large wp-image-82746" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-Broken_Age-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82746"&gt;&lt;em&gt;Two worlds, one wild destiny.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;Broken Ag&lt;/i&gt;e spins a charming, hand-painted tale of two teenagers leading parallel lives in worlds at once strange and familiar. One of the teens yearns to escape a stifling spaceship, and the other is destined to challenge ancient traditions. With witty dialogue and heartfelt moments, &lt;i&gt;Broken Age&lt;/i&gt; is a storybook come to life, brimming with quirky characters and clever puzzles.&lt;/p&gt;
&lt;p&gt;Each of these unforgettable adventures brings its own flavor — be it dark satire, whimsical wonder or pulse-pounding suspense — offering a taste of gaming at its imaginative peaks. Stream these captivating worlds straight from the cloud and enjoy seamless gameplay, no downloads or high-end hardware required.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;An Ultimate Controller&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82743"&gt;&lt;img alt="SteelSeries Nimbus Cloud controller on GeForce NOW" class="wp-image-82743 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-Steelseries_Nimbus_Cloud-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82743"&gt;&lt;em&gt;Elevated gaming.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Get ready for the SteelSeries Nimbus Cloud, a new dual-mode cloud controller. When paired with GeForce NOW, this new controller reaches new heights.&lt;/p&gt;
&lt;p&gt;Designed for versatility and comfort, and crafted specifically for cloud gaming, the SteelSeries Nimbus Cloud effortlessly shifts from a mobile device controller to a full-sized wireless controller, delivering top-notch performance and broad compatibility across devices.&lt;/p&gt;
&lt;p&gt;The Nimbus Cloud enables gamers to play wherever they are, as it easily adapts to fit iPhones and Android phones. Or collapse and connect the controller via Bluetooth to a gaming rig or smart TV. Transform any space into a personal gaming station with GeForce NOW and the Nimbus Cloud, part of the list of recommended products for an elevated cloud gaming experience.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Gaming Never Sleeps&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82740"&gt;&lt;img alt="System Shock 2 on GeForce NOW" class="size-large wp-image-82740" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-System_Shock_2-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82740"&gt;&lt;em&gt;“System Shock 2” — now with 100% more existential dread.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;System Shock 2: 25th Anniversary Remaster&lt;/i&gt; is an overhaul of the acclaimed sci-fi horror classic, rebuilt by Nightdive Studios with enhanced visuals, refined gameplay and features such as cross-play co-op multiplayer. Face the sinister AI SHODAN and her mutant army aboard the starship Von Braun as a cybernetically enhanced soldier with upgradable skills, powerful weapons and psionic abilities. Stream the title from the cloud with GeForce NOW for ultimate flexibility and performance.&lt;/p&gt;
&lt;p&gt;Look for the following games available to stream in the cloud this week:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;System Shock 2: 25th Anniversary Remaster &lt;/i&gt;(New release on Steam, June 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Broken Age &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Easy Red 2 &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Sandwich Simulator &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;We Happy Few&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;The official GFN summer bucket list &lt;/p&gt;
&lt;p&gt;☁️Play anywhere &lt;br /&gt;💻Stream on every screen you own &lt;br /&gt;🎮 Finally crush that backlog &lt;br /&gt;🚫 Skip every single download bar &lt;/p&gt;
&lt;p&gt;Drop the emoji for the one you’re tackling right now 👇&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) June 25, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;This GFN Thursday rolls out a new reward and games for GeForce NOW members. Whether hunting for hot new releases or rediscovering timeless classics, members can always find more ways to play, games to stream and perks to enjoy.&lt;/p&gt;
&lt;p&gt;Gamers can score major discounts on the titles they’ve been eyeing — perfect for streaming in the cloud — during the Steam Summer Sale, running until Thursday, July 10, at 10 a.m. PT.&lt;/p&gt;
&lt;p&gt;This week also brings unforgettable adventures to the cloud: &lt;i&gt;We Happy Few&lt;/i&gt; and &lt;i&gt;Broken Age &lt;/i&gt;are part of the five additions to the GeForce NOW library this week.&lt;/p&gt;
&lt;p&gt;The fun doesn’t stop there. A new in-game reward for &lt;i&gt;Elder Scrolls Online &lt;/i&gt;is now available for members to claim.&lt;/p&gt;
&lt;p&gt;And SteelSeries has launched a new mobile controller that transforms phones into cloud gaming devices with GeForce NOW. Add it to the roster of on-the-go gaming devices — including the recently launched GeForce NOW app on Steam Deck for seamless 4K streaming.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Scroll Into Power&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;GeForce NOW Premium members receive exclusive 24-hour early access to a new mythical reward in &lt;i&gt;The Elder Scrolls Online&lt;/i&gt; — Bethesda’s award-winning role-playing game — before it opens to all members. Sharpen the sword, ready the staff and chase glory across the vast, immersive world of Tamriel.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82755"&gt;&lt;img alt="ESO members reward on GeForce NOW" class="size-large wp-image-82755" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-GFN_Members_Reward_ESO-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82755"&gt;&lt;em&gt;Fortune favors the bold.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Claim the mythical Grand Gold Coast Experience Scrolls reward, a rare item that grants a bonus of 150% Experience Points from all sources for one hour. The scroll’s effect pauses while players are offline and resumes upon return, ensuring every minute counts. Whether tackling dungeon runs, completing epic quests or leveling a new character, the scrolls provide a powerful edge. Claim the reward, harness its power and scroll into the next adventure.&lt;/p&gt;
&lt;p&gt;Members who’ve opted into the GeForce NOW Rewards program can check their emails for redemption instructions. The offer runs through Saturday, July 26, while supplies last. Don’t miss this opportunity to become a legend in Tamriel.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Steam Up Summer&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The Steam Summer Sale is in full swing. Snag games at discounted prices and stream them instantly from the cloud — no downloads, no waiting, just pure gaming bliss.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82752"&gt;&lt;img alt="Steam Summer Sale on GeForce NOW" class="size-large wp-image-82752" height="1017" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-GFN_SteamSummerSale-1680x1017.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82752"&gt;&lt;em&gt;Treat yourself.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Check out the “Steam Summer Sale” row in the GeForce NOW app to find deals on the next adventure. With GeForce NOW, gaming favorites are always just a click away.&lt;/p&gt;
&lt;p&gt;While picking up discounted games, don’t miss the chance to get a GeForce NOW six-month Performance membership at 40% off. This is also the last opportunity to take advantage of the Performance Day Pass sale, ending Friday, June 27 — which lets gamers access cloud gaming for 24 hours — before diving into the 6-month Performance membership.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Find Adventure&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Two distinct worlds — where secrets simmer and imagination runs wild — are streaming onto the cloud this week.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82749"&gt;&lt;img alt="We Happy Few on GeForce NOW" class="size-large wp-image-82749" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-We_Happy_Few-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82749"&gt;&lt;em&gt;Keep calm and blend in (or else).&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Step into the surreal, retro-futuristic streets of &lt;i&gt;We Happy Few&lt;/i&gt;, where a society obsessed with happiness hides its secrets behind a mask of forced cheer and a haze of “Joy.” This darkly whimsical adventure invites players to blend in, break out and uncover the truth lurking beneath the surface of Wellington Wells.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82746"&gt;&lt;img alt="Broken Age on GeForce NOW" class="size-large wp-image-82746" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-Broken_Age-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82746"&gt;&lt;em&gt;Two worlds, one wild destiny.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;Broken Ag&lt;/i&gt;e spins a charming, hand-painted tale of two teenagers leading parallel lives in worlds at once strange and familiar. One of the teens yearns to escape a stifling spaceship, and the other is destined to challenge ancient traditions. With witty dialogue and heartfelt moments, &lt;i&gt;Broken Age&lt;/i&gt; is a storybook come to life, brimming with quirky characters and clever puzzles.&lt;/p&gt;
&lt;p&gt;Each of these unforgettable adventures brings its own flavor — be it dark satire, whimsical wonder or pulse-pounding suspense — offering a taste of gaming at its imaginative peaks. Stream these captivating worlds straight from the cloud and enjoy seamless gameplay, no downloads or high-end hardware required.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;An Ultimate Controller&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82743"&gt;&lt;img alt="SteelSeries Nimbus Cloud controller on GeForce NOW" class="wp-image-82743 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-Steelseries_Nimbus_Cloud-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82743"&gt;&lt;em&gt;Elevated gaming.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Get ready for the SteelSeries Nimbus Cloud, a new dual-mode cloud controller. When paired with GeForce NOW, this new controller reaches new heights.&lt;/p&gt;
&lt;p&gt;Designed for versatility and comfort, and crafted specifically for cloud gaming, the SteelSeries Nimbus Cloud effortlessly shifts from a mobile device controller to a full-sized wireless controller, delivering top-notch performance and broad compatibility across devices.&lt;/p&gt;
&lt;p&gt;The Nimbus Cloud enables gamers to play wherever they are, as it easily adapts to fit iPhones and Android phones. Or collapse and connect the controller via Bluetooth to a gaming rig or smart TV. Transform any space into a personal gaming station with GeForce NOW and the Nimbus Cloud, part of the list of recommended products for an elevated cloud gaming experience.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Gaming Never Sleeps&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_82740"&gt;&lt;img alt="System Shock 2 on GeForce NOW" class="size-large wp-image-82740" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-System_Shock_2-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-82740"&gt;&lt;em&gt;“System Shock 2” — now with 100% more existential dread.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;System Shock 2: 25th Anniversary Remaster&lt;/i&gt; is an overhaul of the acclaimed sci-fi horror classic, rebuilt by Nightdive Studios with enhanced visuals, refined gameplay and features such as cross-play co-op multiplayer. Face the sinister AI SHODAN and her mutant army aboard the starship Von Braun as a cybernetically enhanced soldier with upgradable skills, powerful weapons and psionic abilities. Stream the title from the cloud with GeForce NOW for ultimate flexibility and performance.&lt;/p&gt;
&lt;p&gt;Look for the following games available to stream in the cloud this week:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;System Shock 2: 25th Anniversary Remaster &lt;/i&gt;(New release on Steam, June 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Broken Age &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Easy Red 2 &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Sandwich Simulator &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;We Happy Few&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;The official GFN summer bucket list &lt;/p&gt;
&lt;p&gt;☁️Play anywhere &lt;br /&gt;💻Stream on every screen you own &lt;br /&gt;🎮 Finally crush that backlog &lt;br /&gt;🚫 Skip every single download bar &lt;/p&gt;
&lt;p&gt;Drop the emoji for the one you’re tackling right now 👇&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) June 25, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/geforce-now-thursday-elder-scrolls-online-member-reward/</guid><pubDate>Thu, 26 Jun 2025 13:00:30 +0000</pubDate></item><item><title>[NEW] Reddit CEO pledges site will remain “written by humans and voted on by humans” (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/06/reddit-ceo-pledges-site-will-remain-written-by-humans-and-voted-on-by-humans/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Reddit is in an “arms race” to protect its communities from AI-generated content.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An image of a woman holding a cell phone in front of the Reddit logo displayed on a computer screen, on April 29, 2024, in Edmonton, Canada." class="absolute inset-0 w-full h-full object-cover hidden" height="213" src="https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-2150483549-300x213.jpg" width="300" /&gt;
                  &lt;img alt="An image of a woman holding a cell phone in front of the Reddit logo displayed on a computer screen, on April 29, 2024, in Edmonton, Canada." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-2150483549-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Getty

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Reddit is in an “arms race” to protect its devoted online communities from a surge in artificial intelligence-generated content, with the authenticity of its vast repository of human interaction increasingly valuable in training new AI-powered search tools.&lt;/p&gt;
&lt;p&gt;Chief executive Steve Huffman told the Financial Times that Reddit had “20 years of conversation about everything,” leaving the company with a lucrative resource of personal interaction.&lt;/p&gt;
&lt;p&gt;This has allowed it to strike multimillion dollar partnerships with Google and OpenAI to train their large language models on its content, as tech companies look for real-world data that can improve their generative AI products.&lt;/p&gt;
&lt;p&gt;But Huffman said Reddit was now battling to ensure its users stay at the center of the social network. “Where the rest of the internet seems to be powered by or written by or summarized by AI, Reddit is distinctly human,” he said. “It’s the place you go when you want to hear from people, their lived experiences, their perspectives, their recommendations. Reddit is communities and human curation and conversation and authenticity.”&lt;/p&gt;
&lt;p&gt;As Reddit becomes an increasingly important source for LLMs, advertisers are responding with what one agency chief described as a “massive migration” to the platform.&lt;/p&gt;
&lt;p&gt;Multiple advertising and agency executives speaking during this month’s Cannes advertising festival told the FT that brands were increasingly exploring hosting a business account and posting content on Reddit to boost the likelihood of their ads appearing in the responses of generative AI chatbots.&lt;/p&gt;
&lt;p&gt;However, Huffman warned against any company seeking to game the site with fake or AI-generated content, with plans to bring in strict verification checks to ensure that only humans can post to its forums.&lt;/p&gt;
&lt;p&gt;“For 20 years, we’ve been fighting people who have wanted to be popular on Reddit,” he said. “We index very well into the search engines. If you want to show up in the search engines, you try to do well on Reddit, and now the LLMs, it’s the same thing. If you want to be in the LLMs, you can do it through Reddit.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For Huffman, success comes down to making sure that posts are “written by humans and voted on by humans”—referencing the process by which users can “upvote” posts in order to show their appreciation or “downvote” those they find unhelpful.&lt;/p&gt;
&lt;p&gt;“It’s an arms race, it’s a never ending battle,” he said. “The AI version of it, it’s a new frontier in the same battle that we’ve been fighting for a long time.”&lt;/p&gt;
&lt;p&gt;Huffman said Reddit would still not require users to post under their real names—one of the defining features of the site—but the group would seek to use services that will provide verification “you’re a human without knowing your name.”&lt;/p&gt;
&lt;p&gt;Reddit is exploring using World ID, the eyeball-scanning technology from Sam Altman’s Worldcoin venture, as a way to verify users while granting them anonymity, according to a person familiar with the talks and first reported by Semafor. Altman, OpenAI’s chief executive, used to sit on Reddit’s board.&lt;/p&gt;
&lt;p&gt;“Human verification is top of mind for us right now. Over the rest of this year, we’ll be evolving that—it’s a need on the Internet broadly,” Huffman said.&lt;/p&gt;
&lt;p&gt;Reddit is protecting the value of its content in other ways. Last month it sued AI start-up Anthropic in San Francisco, claiming it had scraped its platform more than 100,000 times since July 2024. “We disagree with Reddit’s claims and will defend ourselves vigorously,” Anthropic said.&lt;/p&gt;
&lt;p&gt;Huffman said there are “a few cases where people have taken advantage of Reddit content, and we’re working through those moments.”&lt;/p&gt;
&lt;p&gt;Huffman’s comments come as Reddit seeks to woo brands with new advertising tools and features. This month, it launched two AI-powered products to provide marketers with real-time data on trending conversations and showcase positive user-generated content underneath real adverts.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Reddit now has more than 100,000 communities based around topic and interest. Reddit’s commercial pitch is that many of these conversations—about 40 percent—are about a service or a product, and within this a quarter relate to some sort of recommendation.&lt;/p&gt;
&lt;p&gt;But several agency executives told the FT that Reddit’s advertising offering still needed fine-tuning, particularly around user targeting.&lt;/p&gt;
&lt;p&gt;Reddit is also improving its platform for users, with an AI-powered search that provides verbatim quotes from its communities and new translation tools to extend its site to 13 languages later this year, including Korean and Japanese.&lt;/p&gt;
&lt;p&gt;Huffman said the platform was now “much more than what we could have imagined 20 years ago” when he co-founded the site with a college friend. Huffman left Reddit in 2009 after it was acquired by Condé Nast, but returned in 2015 when the site faced potential collapse following widespread user dissent over toxic posts and harmful content.&lt;/p&gt;
&lt;p&gt;At the time, Reddit had 12 million daily users and revenue of $15 million. A decade later, it has reached more than 100 million daily average users and is turning over more than $1.3 billion.&lt;/p&gt;
&lt;p&gt;Reddit floated in March 2024 with a valuation of $6.4 billion, which has grown to $26 billion despite a fall in its stock in recent months over concerns that search traffic will be hit with the introduction of Google’s AI Overviews—answers to search queries that remove the need for users to click through to its webpages.&lt;/p&gt;
&lt;p&gt;Huffman said he was relaxed about the longer term impact of Google Overview removing the need for people to click through to Reddit links.&lt;/p&gt;
&lt;p&gt;The “majority of our traffic comes directly to Reddit” rather than through Google, he said, adding that “the search ecosystem is evolving, and it’s volatile right now, but that also opens the door for other players in search, including Reddit.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2025 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Reddit is in an “arms race” to protect its communities from AI-generated content.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An image of a woman holding a cell phone in front of the Reddit logo displayed on a computer screen, on April 29, 2024, in Edmonton, Canada." class="absolute inset-0 w-full h-full object-cover hidden" height="213" src="https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-2150483549-300x213.jpg" width="300" /&gt;
                  &lt;img alt="An image of a woman holding a cell phone in front of the Reddit logo displayed on a computer screen, on April 29, 2024, in Edmonton, Canada." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-2150483549-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Getty

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Reddit is in an “arms race” to protect its devoted online communities from a surge in artificial intelligence-generated content, with the authenticity of its vast repository of human interaction increasingly valuable in training new AI-powered search tools.&lt;/p&gt;
&lt;p&gt;Chief executive Steve Huffman told the Financial Times that Reddit had “20 years of conversation about everything,” leaving the company with a lucrative resource of personal interaction.&lt;/p&gt;
&lt;p&gt;This has allowed it to strike multimillion dollar partnerships with Google and OpenAI to train their large language models on its content, as tech companies look for real-world data that can improve their generative AI products.&lt;/p&gt;
&lt;p&gt;But Huffman said Reddit was now battling to ensure its users stay at the center of the social network. “Where the rest of the internet seems to be powered by or written by or summarized by AI, Reddit is distinctly human,” he said. “It’s the place you go when you want to hear from people, their lived experiences, their perspectives, their recommendations. Reddit is communities and human curation and conversation and authenticity.”&lt;/p&gt;
&lt;p&gt;As Reddit becomes an increasingly important source for LLMs, advertisers are responding with what one agency chief described as a “massive migration” to the platform.&lt;/p&gt;
&lt;p&gt;Multiple advertising and agency executives speaking during this month’s Cannes advertising festival told the FT that brands were increasingly exploring hosting a business account and posting content on Reddit to boost the likelihood of their ads appearing in the responses of generative AI chatbots.&lt;/p&gt;
&lt;p&gt;However, Huffman warned against any company seeking to game the site with fake or AI-generated content, with plans to bring in strict verification checks to ensure that only humans can post to its forums.&lt;/p&gt;
&lt;p&gt;“For 20 years, we’ve been fighting people who have wanted to be popular on Reddit,” he said. “We index very well into the search engines. If you want to show up in the search engines, you try to do well on Reddit, and now the LLMs, it’s the same thing. If you want to be in the LLMs, you can do it through Reddit.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For Huffman, success comes down to making sure that posts are “written by humans and voted on by humans”—referencing the process by which users can “upvote” posts in order to show their appreciation or “downvote” those they find unhelpful.&lt;/p&gt;
&lt;p&gt;“It’s an arms race, it’s a never ending battle,” he said. “The AI version of it, it’s a new frontier in the same battle that we’ve been fighting for a long time.”&lt;/p&gt;
&lt;p&gt;Huffman said Reddit would still not require users to post under their real names—one of the defining features of the site—but the group would seek to use services that will provide verification “you’re a human without knowing your name.”&lt;/p&gt;
&lt;p&gt;Reddit is exploring using World ID, the eyeball-scanning technology from Sam Altman’s Worldcoin venture, as a way to verify users while granting them anonymity, according to a person familiar with the talks and first reported by Semafor. Altman, OpenAI’s chief executive, used to sit on Reddit’s board.&lt;/p&gt;
&lt;p&gt;“Human verification is top of mind for us right now. Over the rest of this year, we’ll be evolving that—it’s a need on the Internet broadly,” Huffman said.&lt;/p&gt;
&lt;p&gt;Reddit is protecting the value of its content in other ways. Last month it sued AI start-up Anthropic in San Francisco, claiming it had scraped its platform more than 100,000 times since July 2024. “We disagree with Reddit’s claims and will defend ourselves vigorously,” Anthropic said.&lt;/p&gt;
&lt;p&gt;Huffman said there are “a few cases where people have taken advantage of Reddit content, and we’re working through those moments.”&lt;/p&gt;
&lt;p&gt;Huffman’s comments come as Reddit seeks to woo brands with new advertising tools and features. This month, it launched two AI-powered products to provide marketers with real-time data on trending conversations and showcase positive user-generated content underneath real adverts.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Reddit now has more than 100,000 communities based around topic and interest. Reddit’s commercial pitch is that many of these conversations—about 40 percent—are about a service or a product, and within this a quarter relate to some sort of recommendation.&lt;/p&gt;
&lt;p&gt;But several agency executives told the FT that Reddit’s advertising offering still needed fine-tuning, particularly around user targeting.&lt;/p&gt;
&lt;p&gt;Reddit is also improving its platform for users, with an AI-powered search that provides verbatim quotes from its communities and new translation tools to extend its site to 13 languages later this year, including Korean and Japanese.&lt;/p&gt;
&lt;p&gt;Huffman said the platform was now “much more than what we could have imagined 20 years ago” when he co-founded the site with a college friend. Huffman left Reddit in 2009 after it was acquired by Condé Nast, but returned in 2015 when the site faced potential collapse following widespread user dissent over toxic posts and harmful content.&lt;/p&gt;
&lt;p&gt;At the time, Reddit had 12 million daily users and revenue of $15 million. A decade later, it has reached more than 100 million daily average users and is turning over more than $1.3 billion.&lt;/p&gt;
&lt;p&gt;Reddit floated in March 2024 with a valuation of $6.4 billion, which has grown to $26 billion despite a fall in its stock in recent months over concerns that search traffic will be hit with the introduction of Google’s AI Overviews—answers to search queries that remove the need for users to click through to its webpages.&lt;/p&gt;
&lt;p&gt;Huffman said he was relaxed about the longer term impact of Google Overview removing the need for people to click through to Reddit links.&lt;/p&gt;
&lt;p&gt;The “majority of our traffic comes directly to Reddit” rather than through Google, he said, adding that “the search ecosystem is evolving, and it’s volatile right now, but that also opens the door for other players in search, including Reddit.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2025 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/06/reddit-ceo-pledges-site-will-remain-written-by-humans-and-voted-on-by-humans/</guid><pubDate>Thu, 26 Jun 2025 13:54:32 +0000</pubDate></item><item><title>[NEW] YouTube adds an AI Overviews-like search results carousel (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/26/youtube-adds-an-ai-overviews-like-search-results-carousel/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;YouTube is rolling out new AI-powered features to help users find content and information more easily, the company announced on Thursday. The platform is launching an AI-powered search results carousel similar to Google’s AI Overviews and is also testing conversational AI with more users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new AI-powered search results carousel, available only to YouTube Premium users in the United States, will suggest videos and display brief AI-generated topic descriptions to help users find what they’re looking for faster. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;YouTube says the AI-powered search results carousel will appear in searches related to shopping, places, or things to do at a specific place. For example, if you search for something like “best beaches in Hawaii,” you’ll see an AI-generated carousel highlighting clips from videos showcasing the best snorkel spots and volcanic beaches, alongside descriptions and more videos to help you plan your vacation.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3022395" height="680" src="https://techcrunch.com/wp-content/uploads/2025/06/youtube-ai-search.png?w=338" width="338" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;YouTube&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The new feature is pretty similar to Google’s AI Overviews, the tool that provides AI-generated summaries of search results at the top of the Google Search results page. While the AI-powered search results carousel will ease discovery for users, it could be an unwelcome change for creators, as they rely on engagement to earn revenue on the platform. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, if a person is able to get the information they need directly from the AI-powered search results carousel, they may not click into the video.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of the AI-powered search results carousel comes two weeks after a Wall Street Journal report revealed that Google’s AI Overviews and other AI-powered tools are devastating traffic for news publishers. YouTube creators may be concerned that the new carousel feature could reduce engagement with their videos, just like how AI Overviews on Google Search have led to fewer referrals to news sites.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for YouTube’s conversational AI tool, the Google-owned platform announced that it’s making it available to some non-Premium users. First launched in late 2023, the conversational tool uses AI to help users get more information, content recommendations, and summaries of videos. It can also be used to quiz yourself on key concepts in academic videos.&lt;/p&gt;


&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3022396" height="680" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-26-at-9.42.04AM.png?w=675" width="675" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;YouTube&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;At launch, YouTube explained that the tool’s responses are generated by large language models that draw on information from YouTube and the web. The responses are designed to help viewers dive deeper into the content they’re watching.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, if you’re watching a roller skate dance tutorial, the conversational AI tool will ask if you want it to “summarize the video” or “recommend related content.” You can also ask your own questions, like “What’s the song in this video?” and the tool will provide details such as the song title, artist, genre, and more.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;YouTube is rolling out new AI-powered features to help users find content and information more easily, the company announced on Thursday. The platform is launching an AI-powered search results carousel similar to Google’s AI Overviews and is also testing conversational AI with more users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new AI-powered search results carousel, available only to YouTube Premium users in the United States, will suggest videos and display brief AI-generated topic descriptions to help users find what they’re looking for faster. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;YouTube says the AI-powered search results carousel will appear in searches related to shopping, places, or things to do at a specific place. For example, if you search for something like “best beaches in Hawaii,” you’ll see an AI-generated carousel highlighting clips from videos showcasing the best snorkel spots and volcanic beaches, alongside descriptions and more videos to help you plan your vacation.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3022395" height="680" src="https://techcrunch.com/wp-content/uploads/2025/06/youtube-ai-search.png?w=338" width="338" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;YouTube&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The new feature is pretty similar to Google’s AI Overviews, the tool that provides AI-generated summaries of search results at the top of the Google Search results page. While the AI-powered search results carousel will ease discovery for users, it could be an unwelcome change for creators, as they rely on engagement to earn revenue on the platform. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, if a person is able to get the information they need directly from the AI-powered search results carousel, they may not click into the video.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of the AI-powered search results carousel comes two weeks after a Wall Street Journal report revealed that Google’s AI Overviews and other AI-powered tools are devastating traffic for news publishers. YouTube creators may be concerned that the new carousel feature could reduce engagement with their videos, just like how AI Overviews on Google Search have led to fewer referrals to news sites.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for YouTube’s conversational AI tool, the Google-owned platform announced that it’s making it available to some non-Premium users. First launched in late 2023, the conversational tool uses AI to help users get more information, content recommendations, and summaries of videos. It can also be used to quiz yourself on key concepts in academic videos.&lt;/p&gt;


&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3022396" height="680" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-26-at-9.42.04AM.png?w=675" width="675" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;YouTube&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;At launch, YouTube explained that the tool’s responses are generated by large language models that draw on information from YouTube and the web. The responses are designed to help viewers dive deeper into the content they’re watching.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, if you’re watching a roller skate dance tutorial, the conversational AI tool will ask if you want it to “summarize the video” or “recommend related content.” You can also ask your own questions, like “What’s the song in this video?” and the tool will provide details such as the song title, artist, genre, and more.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/26/youtube-adds-an-ai-overviews-like-search-results-carousel/</guid><pubDate>Thu, 26 Jun 2025 14:08:38 +0000</pubDate></item><item><title>[NEW] Nvidia reclaims title of most valuable company on AI momentum (AI News)</title><link>https://www.artificialintelligence-news.com/news/nvidia-reclaims-title-of-most-valuable-company-on-ai-momentum/</link><description>&lt;p&gt;Nvidia briefly became the world’s most valuable company on Wednesday after its stock jumped over 4% in price to a new high of $154.10, pushing its market value to $3.76 trillion. &lt;em&gt;Reuters&lt;/em&gt; said the chipmaker overtook Microsoft, which stood at $3.65 trillion after a smaller gain.&lt;/p&gt;&lt;p&gt;The rise follows a note from Loop Capital, which raised its price target for Nvidia to $250 from $175. The firm kept its “buy” rating and said demand for generative AI could grow faster than expected.&lt;/p&gt;&lt;p&gt;“We are entering the next ‘Golden Wave’ of Gen AI adoption and Nvidia is at the front-end of another material leg of stronger than anticipated demand,” said Loop Capital analyst Ananda Baruah.&lt;/p&gt;&lt;p&gt;The renewed interest in AI has sent investors back into tech stocks, especially companies involved in chips and data infrastructure. Nvidia, which designs high-performance GPUs used in AI models, has been a key figure in that trend.&lt;/p&gt;&lt;p&gt;Even with the stock’s strong performance, its valuation doesn’t appear overly stretched. Nvidia trades at about 30 times projected earnings for the next year, below its five-year average of 40 times. This suggests analysts have been raising their forecasts as the company keeps delivering bigger profits.&lt;/p&gt;&lt;p&gt;Nvidia, Microsoft, and Apple have all rotated in and out of the top spot for market value over the past year. Microsoft had recently pulled ahead, but Nvidia regained the lead this week. Apple’s shares rose 0.4% on Wednesday, bringing its valuation to about $3 trillion.&lt;/p&gt;&lt;p&gt;Nvidia’s stock has climbed more than 60% in value since hitting a low in early April. That drop came during a broader sell-off triggered by tariff announcements from Donald Trump. Since then, markets have steadied, with hoping for trade deals that could reduce some of the pressure on the company.&lt;/p&gt;&lt;p&gt;The broader tech sector has also been moving to higher valuations. The S&amp;amp;P 500’s technology index was up 0.9% on Wednesday, reaching a new record. It has gained nearly 6% so far in 2025.&lt;/p&gt;&lt;h3&gt;Tesla’s AI push goes beyond self-driving cars&lt;/h3&gt;&lt;p&gt;Tesla is best known for electric vehicles, but the company is also working to build up its AI capabilities and robotaxi project, plus lesser-known work in robotics.&lt;/p&gt;&lt;p&gt;While many are focused on Tesla’s push to launch a self-driving ride-hailing service, CEO Elon Musk has also been talking about a broader AI future. As &lt;em&gt;The Motley Fool&lt;/em&gt; highlighted, one example is Optimus, a humanoid robot the company is developing for factory and, potentially, domestic use.&lt;/p&gt;&lt;p&gt;Nvidia CEO Jensen Huang recently highlighted the potential of this market, calling humanoid robotics a “multitrillion-dollar industry.” He mentioned Tesla’s Optimus project as one of the efforts that has caught his attention.&lt;/p&gt;&lt;p&gt;Tesla sees two main uses for Optimus. First, the robot could be trained with machine learning to help on the company’s own production lines. Over time, it could take over more tasks and operate without breaks, increasing factory output.&lt;/p&gt;&lt;p&gt;Secondly, Tesla could sell Optimus to other industries where labour is physically demanding. The robot could be adapted for more routine settings outside factories. Musk has said Optimus could eventually become more valuable than the company’s car business.&lt;/p&gt;&lt;p&gt;Other companies are also working in this space. Figure AI, a startup backed by Nvidia, is developing similar humanoid robots for use in factories. A demo video shows how its machines could work alongside people to boost output and reduce repetitive tasks.&lt;/p&gt;&lt;h3&gt;What’s next for Tesla’s stock?&lt;/h3&gt;&lt;p&gt;Tesla’s share price has jumped nearly 30%, driven in part by its robotaxi rollout. The company started testing the service in Texas this week, which has helped fuel investor optimism.&lt;/p&gt;&lt;p&gt;But some analysts say its stock may have already peaked due to the short-term excitement of the Optimus announcement. Tesla tends to move based on headlines, and the same pattern could apply to its robot and robotaxi projects.&lt;/p&gt;&lt;p&gt;While Optimus could become an important part of Tesla’s future, it’s still early. Key questions remain about how soon the robot can scale, how it will compare with other options, and whether the company can turn the project into a real business.&lt;/p&gt;&lt;p&gt;Investors watching Tesla’s AI plans may want to see more progress before making new bets.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Mariia Shalabaieva)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: NO FAKES Act: AI deepfakes protection or internet freedom threat?&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Nvidia briefly became the world’s most valuable company on Wednesday after its stock jumped over 4% in price to a new high of $154.10, pushing its market value to $3.76 trillion. &lt;em&gt;Reuters&lt;/em&gt; said the chipmaker overtook Microsoft, which stood at $3.65 trillion after a smaller gain.&lt;/p&gt;&lt;p&gt;The rise follows a note from Loop Capital, which raised its price target for Nvidia to $250 from $175. The firm kept its “buy” rating and said demand for generative AI could grow faster than expected.&lt;/p&gt;&lt;p&gt;“We are entering the next ‘Golden Wave’ of Gen AI adoption and Nvidia is at the front-end of another material leg of stronger than anticipated demand,” said Loop Capital analyst Ananda Baruah.&lt;/p&gt;&lt;p&gt;The renewed interest in AI has sent investors back into tech stocks, especially companies involved in chips and data infrastructure. Nvidia, which designs high-performance GPUs used in AI models, has been a key figure in that trend.&lt;/p&gt;&lt;p&gt;Even with the stock’s strong performance, its valuation doesn’t appear overly stretched. Nvidia trades at about 30 times projected earnings for the next year, below its five-year average of 40 times. This suggests analysts have been raising their forecasts as the company keeps delivering bigger profits.&lt;/p&gt;&lt;p&gt;Nvidia, Microsoft, and Apple have all rotated in and out of the top spot for market value over the past year. Microsoft had recently pulled ahead, but Nvidia regained the lead this week. Apple’s shares rose 0.4% on Wednesday, bringing its valuation to about $3 trillion.&lt;/p&gt;&lt;p&gt;Nvidia’s stock has climbed more than 60% in value since hitting a low in early April. That drop came during a broader sell-off triggered by tariff announcements from Donald Trump. Since then, markets have steadied, with hoping for trade deals that could reduce some of the pressure on the company.&lt;/p&gt;&lt;p&gt;The broader tech sector has also been moving to higher valuations. The S&amp;amp;P 500’s technology index was up 0.9% on Wednesday, reaching a new record. It has gained nearly 6% so far in 2025.&lt;/p&gt;&lt;h3&gt;Tesla’s AI push goes beyond self-driving cars&lt;/h3&gt;&lt;p&gt;Tesla is best known for electric vehicles, but the company is also working to build up its AI capabilities and robotaxi project, plus lesser-known work in robotics.&lt;/p&gt;&lt;p&gt;While many are focused on Tesla’s push to launch a self-driving ride-hailing service, CEO Elon Musk has also been talking about a broader AI future. As &lt;em&gt;The Motley Fool&lt;/em&gt; highlighted, one example is Optimus, a humanoid robot the company is developing for factory and, potentially, domestic use.&lt;/p&gt;&lt;p&gt;Nvidia CEO Jensen Huang recently highlighted the potential of this market, calling humanoid robotics a “multitrillion-dollar industry.” He mentioned Tesla’s Optimus project as one of the efforts that has caught his attention.&lt;/p&gt;&lt;p&gt;Tesla sees two main uses for Optimus. First, the robot could be trained with machine learning to help on the company’s own production lines. Over time, it could take over more tasks and operate without breaks, increasing factory output.&lt;/p&gt;&lt;p&gt;Secondly, Tesla could sell Optimus to other industries where labour is physically demanding. The robot could be adapted for more routine settings outside factories. Musk has said Optimus could eventually become more valuable than the company’s car business.&lt;/p&gt;&lt;p&gt;Other companies are also working in this space. Figure AI, a startup backed by Nvidia, is developing similar humanoid robots for use in factories. A demo video shows how its machines could work alongside people to boost output and reduce repetitive tasks.&lt;/p&gt;&lt;h3&gt;What’s next for Tesla’s stock?&lt;/h3&gt;&lt;p&gt;Tesla’s share price has jumped nearly 30%, driven in part by its robotaxi rollout. The company started testing the service in Texas this week, which has helped fuel investor optimism.&lt;/p&gt;&lt;p&gt;But some analysts say its stock may have already peaked due to the short-term excitement of the Optimus announcement. Tesla tends to move based on headlines, and the same pattern could apply to its robot and robotaxi projects.&lt;/p&gt;&lt;p&gt;While Optimus could become an important part of Tesla’s future, it’s still early. Key questions remain about how soon the robot can scale, how it will compare with other options, and whether the company can turn the project into a real business.&lt;/p&gt;&lt;p&gt;Investors watching Tesla’s AI plans may want to see more progress before making new bets.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Mariia Shalabaieva)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: NO FAKES Act: AI deepfakes protection or internet freedom threat?&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/nvidia-reclaims-title-of-most-valuable-company-on-ai-momentum/</guid><pubDate>Thu, 26 Jun 2025 14:08:38 +0000</pubDate></item><item><title>[NEW] People use AI for companionship much less than we’re led to believe (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/26/people-use-ai-for-companionship-much-less-than-were-led-to-believe/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The overabundance of attention paid to how people are turning to AI chatbots for emotional support, sometimes even striking up relationships, often leads one to think such behavior is commonplace.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new report by Anthropic, which makes the popular AI chatbot Claude, reveals a different reality: In fact, people rarely seek out companionship from Claude and turn to the bot for emotional support and personal advice only 2.9% of the time.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Companionship and roleplay combined comprise less than 0.5% of conversations,” the company highlighted in its report. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic says its study sought to unearth insights into the use of AI for “affective conversations,” which it defines as personal exchanges in which people talked to Claude for coaching, counseling, companionship, roleplay, or advice on relationships. Analyzing 4.5 million conversations that users had on the Claude Free and Pro tiers, the company said the vast majority of Claude usage is related to work or productivity, with people mostly using the chatbot for content creation. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3022369" height="382" src="https://techcrunch.com/wp-content/uploads/2025/06/image_e977d3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt; &lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Anthropic&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;That said, Anthropic found that people do use Claude more often for interpersonal advice, coaching, and counseling, with users most often asking for advice on improving mental health, personal and professional development, and studying communication and interpersonal skills.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, the company notes that help-seeking conversations can sometimes turn into companionship-seeking in cases where the user is facing emotional or personal distress, such as existential dread or loneliness, or when they find it hard to make meaningful connections in their real life.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We also noticed that in longer conversations, counseling or coaching conversations occasionally morph into companionship — despite that not being the original reason someone reached out,” Anthropic wrote, noting that extensive conversations (with over 50+ human messages) were not the norm.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Anthropic also highlighted other insights, like how Claude itself rarely resists users’ requests, except when its programming prevents it from broaching safety boundaries, like providing dangerous advice or supporting self-harm. Conversations also tend to become more positive over time when people seek coaching or advice from the bot, the company said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The report is certainly interesting — it does a good job of reminding us yet again of just how much and how often AI tools are being used for purposes beyond work. Still, it’s important to remember that AI chatbots, across the board, are still very much a work in progress: They hallucinate, are known to readily provide wrong information or dangerous advice, and as Anthropic itself has acknowledged, may even resort to blackmail.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The overabundance of attention paid to how people are turning to AI chatbots for emotional support, sometimes even striking up relationships, often leads one to think such behavior is commonplace.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new report by Anthropic, which makes the popular AI chatbot Claude, reveals a different reality: In fact, people rarely seek out companionship from Claude and turn to the bot for emotional support and personal advice only 2.9% of the time.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Companionship and roleplay combined comprise less than 0.5% of conversations,” the company highlighted in its report. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic says its study sought to unearth insights into the use of AI for “affective conversations,” which it defines as personal exchanges in which people talked to Claude for coaching, counseling, companionship, roleplay, or advice on relationships. Analyzing 4.5 million conversations that users had on the Claude Free and Pro tiers, the company said the vast majority of Claude usage is related to work or productivity, with people mostly using the chatbot for content creation. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3022369" height="382" src="https://techcrunch.com/wp-content/uploads/2025/06/image_e977d3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt; &lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Anthropic&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;That said, Anthropic found that people do use Claude more often for interpersonal advice, coaching, and counseling, with users most often asking for advice on improving mental health, personal and professional development, and studying communication and interpersonal skills.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, the company notes that help-seeking conversations can sometimes turn into companionship-seeking in cases where the user is facing emotional or personal distress, such as existential dread or loneliness, or when they find it hard to make meaningful connections in their real life.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We also noticed that in longer conversations, counseling or coaching conversations occasionally morph into companionship — despite that not being the original reason someone reached out,” Anthropic wrote, noting that extensive conversations (with over 50+ human messages) were not the norm.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Anthropic also highlighted other insights, like how Claude itself rarely resists users’ requests, except when its programming prevents it from broaching safety boundaries, like providing dangerous advice or supporting self-harm. Conversations also tend to become more positive over time when people seek coaching or advice from the bot, the company said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The report is certainly interesting — it does a good job of reminding us yet again of just how much and how often AI tools are being used for purposes beyond work. Still, it’s important to remember that AI chatbots, across the board, are still very much a work in progress: They hallucinate, are known to readily provide wrong information or dangerous advice, and as Anthropic itself has acknowledged, may even resort to blackmail.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/26/people-use-ai-for-companionship-much-less-than-were-led-to-believe/</guid><pubDate>Thu, 26 Jun 2025 14:21:28 +0000</pubDate></item><item><title>[NEW] Suno snaps up WavTool for its AI music editing tools amid ongoing dispute with music labels (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/26/suno-snaps-up-wavtool-for-its-ai-music-editing-tools-amid-ongoing-dispute-with-music-labels/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-2159558534.jpg?resize=1200,837" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Suno, the AI music company currently in a legal battle with music labels, announced on Thursday the acquisition of WavTool, a browser-based AI digital audio workstation (DAW). This acquisition aims to improve Suno’s editing capabilities for songwriters and producers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;WavTool, launched in 2023, offers several tools to musicians, such as stem separation, AI audio generation, and an AI music assistant. Suno will integrate WavTool’s technology into its new editing interface, which launched this month.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The terms of the deal have not been disclosed. A company spokesperson noted that “most” of the WavTool employees moved to Suno’s product and engineering teams, although the exact number of those who did not make the move wasn’t revealed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The acquisition comes on the heels of yet another lawsuit against the company. Country musician Tony Justice and his music label, 5th Wheel Records, filed a lawsuit against Suno earlier this month, alleging that Suno used&amp;nbsp;copyrighted sound recordings to train its AI music generator.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This allegation is similar to lawsuits filed last year by Universal Music Group, Warner Music Group, and Sony Music Entertainment against Suno for copyright infringement. According to Bloomberg, the major music labels are in licensing talks with Suno.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Suno acquired WavTool a few months ago, with the browser-based DAW going offline in November. Timing the announcement for this week seems intentional, possibly aimed at diverting attention from the lawsuit. Legal disputes often shake investor confidence, so the announcement of this acquisition may serve as a way to reassure them that the company remains committed to growth.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI startup secured $125 million in funding this past May.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-2159558534.jpg?resize=1200,837" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Suno, the AI music company currently in a legal battle with music labels, announced on Thursday the acquisition of WavTool, a browser-based AI digital audio workstation (DAW). This acquisition aims to improve Suno’s editing capabilities for songwriters and producers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;WavTool, launched in 2023, offers several tools to musicians, such as stem separation, AI audio generation, and an AI music assistant. Suno will integrate WavTool’s technology into its new editing interface, which launched this month.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The terms of the deal have not been disclosed. A company spokesperson noted that “most” of the WavTool employees moved to Suno’s product and engineering teams, although the exact number of those who did not make the move wasn’t revealed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The acquisition comes on the heels of yet another lawsuit against the company. Country musician Tony Justice and his music label, 5th Wheel Records, filed a lawsuit against Suno earlier this month, alleging that Suno used&amp;nbsp;copyrighted sound recordings to train its AI music generator.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This allegation is similar to lawsuits filed last year by Universal Music Group, Warner Music Group, and Sony Music Entertainment against Suno for copyright infringement. According to Bloomberg, the major music labels are in licensing talks with Suno.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Suno acquired WavTool a few months ago, with the browser-based DAW going offline in November. Timing the announcement for this week seems intentional, possibly aimed at diverting attention from the lawsuit. Legal disputes often shake investor confidence, so the announcement of this acquisition may serve as a way to reassure them that the company remains committed to growth.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI startup secured $125 million in funding this past May.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/26/suno-snaps-up-wavtool-for-its-ai-music-editing-tools-amid-ongoing-dispute-with-music-labels/</guid><pubDate>Thu, 26 Jun 2025 15:30:00 +0000</pubDate></item><item><title>[NEW] PadChest-GR: A bilingual grounded radiology reporting benchmark for chest X-rays (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/padchest-gr-a-bilingual-grounded-radiology-reporting-benchmark-for-chest-x-rays/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Alt text: The image features three white icons on a gradient background transitioning from blue on the left to green on the right. The first icon, located on the left, resembles an X-ray of a ribcage enclosed in a square with rounded corners. The middle icon depicts a hierarchical structure with one circle at the top connected by lines to two smaller circles below it. The third icon, positioned on the right, shows the letters " class="wp-image-1142658" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/PadChest-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;In our ever-evolving journey to enhance healthcare through technology, we’re announcing a unique new benchmark for grounded radiology report generation—&lt;strong&gt;PadChest-GR&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/strong&gt;. The world’s first multimodal, bilingual sentence-level radiology report dataset, developed&amp;nbsp;by the University of Alicante with Microsoft Research, University Hospital Sant Joan d’Alacant and MedBravo, is set to redefine how AI and radiologists interpret radiological images. Our work demonstrates how collaboration between humans and AI can create powerful feedback loops—where new datasets drive better AI models, and those models, in turn, inspire richer datasets. We’re excited to share this progress in NEJM AI, highlighting both the clinical relevance and research excellence of this initiative.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="a-new-frontier-in-radiology-report-generation"&gt;A new frontier in radiology report generation&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;It is estimated that over half of people visiting hospitals have radiology scans that must be interpreted by a clinical professional. Traditional radiology reports often condense multiple findings into unstructured narratives. In contrast, grounded radiology reporting demands that each finding be described and localized individually.&lt;/p&gt;



&lt;p&gt;This can mitigate the risk of AI fabrications and enable new interactive capabilities that enhance clinical and patient interpretability. PadChest-GR is the first bilingual dataset to address this need with 4,555 chest X-ray studies complete with Spanish and English sentence-level descriptions and precise spatial (bounding box) annotations for both positive and negative findings. It is the first public benchmark that enables us to evaluate generation of fully grounded radiology reports in chest X-rays.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1: A chest X-ray overlaid with numbered bounding boxes, next to a matching list of structured radiological findings in Spanish and English. " class="wp-image-1142582" height="781" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/grounded_report_example.png" width="1516" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Example of a grounded report from PadChest-GR. The original free-text report in Spanish was &lt;em&gt;”Motivo de consulta: Preoperatorio. Rx PA tórax: Impresión diagnóstica: Ateromatosis aórtica calcificada. Engrosamiento pleural biapical. Atelectasia laminar basal izquierda. Elongación aórtica. Sin otros hallazgos radiológicos significativos.”&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Blog post&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Eureka: Evaluating and understanding progress in AI&lt;/h2&gt;
				
								&lt;p class="large" id="eureka-evaluating-and-understanding-progress-in-ai"&gt;How can we rigorously evaluate and understand state-of-the-art progress in AI? Eureka is an open-source framework for standardizing evaluations of large foundation models, beyond single-score reporting and rankings. Learn more about the extended findings.&amp;nbsp;&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;This benchmark isn’t standing alone—it plays a critical role in powering our state-of-the-art multimodal report generation model, &lt;strong&gt;MAIRA-2&lt;/strong&gt;. Leveraging the detailed annotations of PadChest-GR, MAIRA-2 represents our commitment to building more interpretable and clinically useful AI systems. You can explore our work on MAIRA-2 on our project web page, including recent user research conducted with clinicians in healthcare settings.&lt;/p&gt;



&lt;p&gt;PadChest-GR is a testament to the power of collaboration. Aurelia Bustos at MedBravo and Antonio Pertusa at the University of Alicante published the original&amp;nbsp;PadChest dataset&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; in 2020,&amp;nbsp;with the help of Jose María Salinas from Hospital San Juan de Alicante and María de la Iglesia Vayá from the Center of Excellence in Biomedical Imaging at the Ministry of Health in Valencia, Spain. We started to look at PadChest and were deeply impressed by the scale, depth, and diversity of the data.&lt;/p&gt;



&lt;p&gt;As we worked more closely with the dataset, we realized the opportunity to develop this for grounded radiology reporting research and worked with the team at the University of Alicante to determine how to approach this together. Our complementary expertise was a nice fit. At Microsoft Research, our mission is to push the boundaries of medical AI through innovative, data-driven solutions. The University of Alicante, with its deep clinical expertise, provided critical insights that greatly enriched the dataset’s relevance and utility. The result of this collaboration is the PadChest-GR dataset.&lt;/p&gt;



&lt;p&gt;A significant enabler of our annotation process was &lt;strong&gt;Centaur Labs&lt;/strong&gt;. The team of senior and junior radiologists from the University Hospital Sant Joan d’Alacant, coordinated by Joaquin Galant,&amp;nbsp;used this HIPAA-compliant labeling platform to&amp;nbsp;perform rigorous study-level quality control and bounding box annotations. The annotation protocol implemented ensured that each annotation was accurate and consistent, forming the backbone of a dataset designed for the next generation of grounded radiology report generation models.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="accelerating-padchest-gr-dataset-annotation-with-ai"&gt;Accelerating PadChest-GR dataset annotation with AI&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Our approach integrates advanced large language models with comprehensive manual annotation:&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Data Selection &amp;amp; Processing:&lt;/strong&gt; Leveraging Microsoft Azure OpenAI Service&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; with GPT-4, we extracted sentences describing individual positive and negative findings from raw radiology reports, translated them from Spanish to English, and linked each sentence to the existing expert labels from PadChest. This was done for a selected subset of the full PadChest dataset, carefully curated to reflect a realistic distribution of clinically relevant findings.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Manual Quality Control &amp;amp; Annotation:&lt;/strong&gt; The processed studies underwent meticulous quality checks on the Centaur Labs platform by radiologist from Hospital San Juan de Alicante. Each positive finding was then annotated with bounding boxes to capture critical spatial information.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Standardization &amp;amp; Integration:&lt;/strong&gt; All annotations were harmonized into coherent grounded reports, preserving the structure and context of the original findings while enhancing interpretability.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2: A detailed block diagram illustrating the flow of data between various stages of AI processing and manual annotation. " class="wp-image-1142586" height="790" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/dataflow.png" width="1752" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Overview of the data curation pipeline.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="impact-and-future-directions"&gt;Impact and future directions&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;PadChest-GR not only sets a new benchmark for grounded radiology reporting, but also serves as the foundation for our MAIRA-2 model, which already showcases the potential of highly interpretable AI in clinical settings. While we developed PadChest-GR to help train and validate our own models, we believe the research community will greatly benefit from this dataset for many years to come. We look forward to seeing the broader research community build on this—improving grounded reporting AI models and using PadChest-GR as a standard for evaluation. We believe that by fostering open collaboration and sharing our resources, we can accelerate progress in medical imaging AI and ultimately improve patient care together with the community.&lt;/p&gt;



&lt;p&gt;The collaboration between Microsoft Research and the University of Alicante highlights the transformative power of working together across disciplines. With our publication in NEJM-AI and the integral role of PadChest-GR in the development of MAIRA-2&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and RadFact&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, we are excited about the future of AI-empowered radiology. We invite researchers and industry experts to explore PadChest-GR and MAIRA-2, contribute innovative ideas, and join us in advancing the field of grounded radiology reporting.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Papers already using PadChest-GR:&lt;/p&gt;







&lt;p&gt;For further details or to download PadChest-GR, please visit the BIMCV PadChest-GR Project&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Models in the Azure Foundry that can do Grounded Reporting:&amp;nbsp;&lt;/p&gt;







&lt;h2 class="wp-block-heading" id="acknowledgement"&gt;Acknowledgement&lt;/h2&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Authors: Daniel C. Castro&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Aurelia Bustos&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Shruthi Bannur&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Stephanie L. Hyland&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Kenza Bouzid&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Maria Teodora Wetscherek&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Maria Dolores Sánchez-Valverde&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Lara Jaques-Pérez&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Lourdes Pérez-Rodríguez&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Kenji Takeda&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, José María Salinas&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Javier Alvarez-Valle&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Joaquín Galant Herrero&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Antonio Pertusa&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;












&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Alt text: The image features three white icons on a gradient background transitioning from blue on the left to green on the right. The first icon, located on the left, resembles an X-ray of a ribcage enclosed in a square with rounded corners. The middle icon depicts a hierarchical structure with one circle at the top connected by lines to two smaller circles below it. The third icon, positioned on the right, shows the letters " class="wp-image-1142658" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/PadChest-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;In our ever-evolving journey to enhance healthcare through technology, we’re announcing a unique new benchmark for grounded radiology report generation—&lt;strong&gt;PadChest-GR&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/strong&gt;. The world’s first multimodal, bilingual sentence-level radiology report dataset, developed&amp;nbsp;by the University of Alicante with Microsoft Research, University Hospital Sant Joan d’Alacant and MedBravo, is set to redefine how AI and radiologists interpret radiological images. Our work demonstrates how collaboration between humans and AI can create powerful feedback loops—where new datasets drive better AI models, and those models, in turn, inspire richer datasets. We’re excited to share this progress in NEJM AI, highlighting both the clinical relevance and research excellence of this initiative.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="a-new-frontier-in-radiology-report-generation"&gt;A new frontier in radiology report generation&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;It is estimated that over half of people visiting hospitals have radiology scans that must be interpreted by a clinical professional. Traditional radiology reports often condense multiple findings into unstructured narratives. In contrast, grounded radiology reporting demands that each finding be described and localized individually.&lt;/p&gt;



&lt;p&gt;This can mitigate the risk of AI fabrications and enable new interactive capabilities that enhance clinical and patient interpretability. PadChest-GR is the first bilingual dataset to address this need with 4,555 chest X-ray studies complete with Spanish and English sentence-level descriptions and precise spatial (bounding box) annotations for both positive and negative findings. It is the first public benchmark that enables us to evaluate generation of fully grounded radiology reports in chest X-rays.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1: A chest X-ray overlaid with numbered bounding boxes, next to a matching list of structured radiological findings in Spanish and English. " class="wp-image-1142582" height="781" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/grounded_report_example.png" width="1516" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Example of a grounded report from PadChest-GR. The original free-text report in Spanish was &lt;em&gt;”Motivo de consulta: Preoperatorio. Rx PA tórax: Impresión diagnóstica: Ateromatosis aórtica calcificada. Engrosamiento pleural biapical. Atelectasia laminar basal izquierda. Elongación aórtica. Sin otros hallazgos radiológicos significativos.”&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Blog post&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Eureka: Evaluating and understanding progress in AI&lt;/h2&gt;
				
								&lt;p class="large" id="eureka-evaluating-and-understanding-progress-in-ai"&gt;How can we rigorously evaluate and understand state-of-the-art progress in AI? Eureka is an open-source framework for standardizing evaluations of large foundation models, beyond single-score reporting and rankings. Learn more about the extended findings.&amp;nbsp;&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;This benchmark isn’t standing alone—it plays a critical role in powering our state-of-the-art multimodal report generation model, &lt;strong&gt;MAIRA-2&lt;/strong&gt;. Leveraging the detailed annotations of PadChest-GR, MAIRA-2 represents our commitment to building more interpretable and clinically useful AI systems. You can explore our work on MAIRA-2 on our project web page, including recent user research conducted with clinicians in healthcare settings.&lt;/p&gt;



&lt;p&gt;PadChest-GR is a testament to the power of collaboration. Aurelia Bustos at MedBravo and Antonio Pertusa at the University of Alicante published the original&amp;nbsp;PadChest dataset&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; in 2020,&amp;nbsp;with the help of Jose María Salinas from Hospital San Juan de Alicante and María de la Iglesia Vayá from the Center of Excellence in Biomedical Imaging at the Ministry of Health in Valencia, Spain. We started to look at PadChest and were deeply impressed by the scale, depth, and diversity of the data.&lt;/p&gt;



&lt;p&gt;As we worked more closely with the dataset, we realized the opportunity to develop this for grounded radiology reporting research and worked with the team at the University of Alicante to determine how to approach this together. Our complementary expertise was a nice fit. At Microsoft Research, our mission is to push the boundaries of medical AI through innovative, data-driven solutions. The University of Alicante, with its deep clinical expertise, provided critical insights that greatly enriched the dataset’s relevance and utility. The result of this collaboration is the PadChest-GR dataset.&lt;/p&gt;



&lt;p&gt;A significant enabler of our annotation process was &lt;strong&gt;Centaur Labs&lt;/strong&gt;. The team of senior and junior radiologists from the University Hospital Sant Joan d’Alacant, coordinated by Joaquin Galant,&amp;nbsp;used this HIPAA-compliant labeling platform to&amp;nbsp;perform rigorous study-level quality control and bounding box annotations. The annotation protocol implemented ensured that each annotation was accurate and consistent, forming the backbone of a dataset designed for the next generation of grounded radiology report generation models.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="accelerating-padchest-gr-dataset-annotation-with-ai"&gt;Accelerating PadChest-GR dataset annotation with AI&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Our approach integrates advanced large language models with comprehensive manual annotation:&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Data Selection &amp;amp; Processing:&lt;/strong&gt; Leveraging Microsoft Azure OpenAI Service&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; with GPT-4, we extracted sentences describing individual positive and negative findings from raw radiology reports, translated them from Spanish to English, and linked each sentence to the existing expert labels from PadChest. This was done for a selected subset of the full PadChest dataset, carefully curated to reflect a realistic distribution of clinically relevant findings.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Manual Quality Control &amp;amp; Annotation:&lt;/strong&gt; The processed studies underwent meticulous quality checks on the Centaur Labs platform by radiologist from Hospital San Juan de Alicante. Each positive finding was then annotated with bounding boxes to capture critical spatial information.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Standardization &amp;amp; Integration:&lt;/strong&gt; All annotations were harmonized into coherent grounded reports, preserving the structure and context of the original findings while enhancing interpretability.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2: A detailed block diagram illustrating the flow of data between various stages of AI processing and manual annotation. " class="wp-image-1142586" height="790" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/dataflow.png" width="1752" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Overview of the data curation pipeline.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="impact-and-future-directions"&gt;Impact and future directions&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;PadChest-GR not only sets a new benchmark for grounded radiology reporting, but also serves as the foundation for our MAIRA-2 model, which already showcases the potential of highly interpretable AI in clinical settings. While we developed PadChest-GR to help train and validate our own models, we believe the research community will greatly benefit from this dataset for many years to come. We look forward to seeing the broader research community build on this—improving grounded reporting AI models and using PadChest-GR as a standard for evaluation. We believe that by fostering open collaboration and sharing our resources, we can accelerate progress in medical imaging AI and ultimately improve patient care together with the community.&lt;/p&gt;



&lt;p&gt;The collaboration between Microsoft Research and the University of Alicante highlights the transformative power of working together across disciplines. With our publication in NEJM-AI and the integral role of PadChest-GR in the development of MAIRA-2&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and RadFact&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, we are excited about the future of AI-empowered radiology. We invite researchers and industry experts to explore PadChest-GR and MAIRA-2, contribute innovative ideas, and join us in advancing the field of grounded radiology reporting.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Papers already using PadChest-GR:&lt;/p&gt;







&lt;p&gt;For further details or to download PadChest-GR, please visit the BIMCV PadChest-GR Project&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Models in the Azure Foundry that can do Grounded Reporting:&amp;nbsp;&lt;/p&gt;







&lt;h2 class="wp-block-heading" id="acknowledgement"&gt;Acknowledgement&lt;/h2&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Authors: Daniel C. Castro&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Aurelia Bustos&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Shruthi Bannur&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Stephanie L. Hyland&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Kenza Bouzid&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Maria Teodora Wetscherek&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Maria Dolores Sánchez-Valverde&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Lara Jaques-Pérez&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Lourdes Pérez-Rodríguez&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Kenji Takeda&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, José María Salinas&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Javier Alvarez-Valle&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Joaquín Galant Herrero&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Antonio Pertusa&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;












&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/padchest-gr-a-bilingual-grounded-radiology-reporting-benchmark-for-chest-x-rays/</guid><pubDate>Thu, 26 Jun 2025 16:08:25 +0000</pubDate></item><item><title>[NEW] Meta hires key OpenAI researcher to work on AI reasoning models (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/26/meta-hires-key-openai-researcher-to-work-on-ai-reasoning-models/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2173579179.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta has hired a highly influential OpenAI researcher, Trapit Bansal, to work on its AI reasoning models under the company’s new AI superintelligence unit, a person familiar with the matter tells TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI spokesperson Kayla Wood confirmed to TechCrunch that Bansal had departed OpenAI. Bansal’s LinkedIn page says that he left OpenAI in June.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Bansal has worked at OpenAI since 2022 and was a key player in kickstarting the company’s work on reinforcement learning alongside co-founder Ilya Sutskever. He is listed as a foundational contributor on OpenAI’s first AI reasoning model, o1.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bansal could offer a significant lift to Meta’s AI superintelligence lab, which also features leaders such as former Scale AI CEO Alexandr Wang and is looking to add former GitHub CEO Nat Friedman and Safe Superintelligence co-founder Daniel Gross. Bansal could help Meta develop a frontier AI reasoning model that’s competitive with industry-leading technology, such as OpenAI’s o3 or DeepSeek’s R1. Currently, Meta does not offer an AI reasoning model publicly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent months, Mark Zuckerberg has been on a hiring spree to build out Meta’s new AI team, offering $100 million compensation packages to top researchers who join his company. It’s unclear what Bansal was offered to join in this deal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nevertheless, it seems that Zuckerberg has been successful at nabbing top AI research talent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Three other former OpenAI researchers — Lucas Beyer, Alexander Kolesnikov, and Xiaohua Zhai — have also joined Meta’s AI superintelligence team in recent weeks, The Wall Street Journal reported on Wednesday. Bansal will join them, alongside former Google DeepMind researcher Jack Rae and former machine learning leader at the startup Sesame, Johan Schalkwyk, according to Bloomberg.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;To further fill out its new AI unit, Zuckerberg reportedly tried to acquire startups with heavy-hitting AI research labs, such as Sutskever’s Safe Superintelligence, Mira Murati’s Thinking Machines Labs, and Perplexity. However, those talks never progressed to a final stage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On a recent podcast, OpenAI CEO Sam Altman said Meta has been trying to poach his startup’s top talent, but claimed that “none of our best people have decided to take him up on that.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Meta spokesperson declined to comment.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI reasoning models present a key area for Meta’s AI superintelligence team to get right. In the last year, OpenAI, Google, and DeepSeek have shipped highly performant AI reasoning models that have pushed the limits of what software can do. By training AI models to work through problems before giving an answer, using additional time and computing resources to do so, AI labs have found success in improving AI’s performance on benchmarks and real-world tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta’s AI superintelligence lab could become a key internal group that powers products throughout the company, much like Google’s DeepMind unit. Meta has an ambitious effort to build AI agents for business under the former Salesforce CEO of AI, Clara Shih. In order to build competitive agents, Meta needs to develop frontier AI reasoning models to power them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the addition of Bansal and other key AI researchers, Meta hopes to pull ahead in the AI race. That may be difficult given that OpenAI plans to release an open AI reasoning model in the coming weeks — an offering that could add even more pressure on Meta’s open AI offerings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was updated after publication with more details&lt;/em&gt;. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2173579179.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta has hired a highly influential OpenAI researcher, Trapit Bansal, to work on its AI reasoning models under the company’s new AI superintelligence unit, a person familiar with the matter tells TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI spokesperson Kayla Wood confirmed to TechCrunch that Bansal had departed OpenAI. Bansal’s LinkedIn page says that he left OpenAI in June.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Bansal has worked at OpenAI since 2022 and was a key player in kickstarting the company’s work on reinforcement learning alongside co-founder Ilya Sutskever. He is listed as a foundational contributor on OpenAI’s first AI reasoning model, o1.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bansal could offer a significant lift to Meta’s AI superintelligence lab, which also features leaders such as former Scale AI CEO Alexandr Wang and is looking to add former GitHub CEO Nat Friedman and Safe Superintelligence co-founder Daniel Gross. Bansal could help Meta develop a frontier AI reasoning model that’s competitive with industry-leading technology, such as OpenAI’s o3 or DeepSeek’s R1. Currently, Meta does not offer an AI reasoning model publicly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent months, Mark Zuckerberg has been on a hiring spree to build out Meta’s new AI team, offering $100 million compensation packages to top researchers who join his company. It’s unclear what Bansal was offered to join in this deal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nevertheless, it seems that Zuckerberg has been successful at nabbing top AI research talent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Three other former OpenAI researchers — Lucas Beyer, Alexander Kolesnikov, and Xiaohua Zhai — have also joined Meta’s AI superintelligence team in recent weeks, The Wall Street Journal reported on Wednesday. Bansal will join them, alongside former Google DeepMind researcher Jack Rae and former machine learning leader at the startup Sesame, Johan Schalkwyk, according to Bloomberg.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;To further fill out its new AI unit, Zuckerberg reportedly tried to acquire startups with heavy-hitting AI research labs, such as Sutskever’s Safe Superintelligence, Mira Murati’s Thinking Machines Labs, and Perplexity. However, those talks never progressed to a final stage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On a recent podcast, OpenAI CEO Sam Altman said Meta has been trying to poach his startup’s top talent, but claimed that “none of our best people have decided to take him up on that.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Meta spokesperson declined to comment.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI reasoning models present a key area for Meta’s AI superintelligence team to get right. In the last year, OpenAI, Google, and DeepSeek have shipped highly performant AI reasoning models that have pushed the limits of what software can do. By training AI models to work through problems before giving an answer, using additional time and computing resources to do so, AI labs have found success in improving AI’s performance on benchmarks and real-world tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta’s AI superintelligence lab could become a key internal group that powers products throughout the company, much like Google’s DeepMind unit. Meta has an ambitious effort to build AI agents for business under the former Salesforce CEO of AI, Clara Shih. In order to build competitive agents, Meta needs to develop frontier AI reasoning models to power them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the addition of Bansal and other key AI researchers, Meta hopes to pull ahead in the AI race. That may be difficult given that OpenAI plans to release an open AI reasoning model in the coming weeks — an offering that could add even more pressure on Meta’s open AI offerings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was updated after publication with more details&lt;/em&gt;. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/26/meta-hires-key-openai-researcher-to-work-on-ai-reasoning-models/</guid><pubDate>Thu, 26 Jun 2025 16:13:59 +0000</pubDate></item><item><title>[NEW] Google begins rolling out AI search in YouTube (AI – Ars Technica)</title><link>https://arstechnica.com/gadgets/2025/06/google-begins-rolling-out-ai-search-in-youtube/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The feature is only available as a test for Premium members for now.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="YouTube AI simple" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/YouTube-AI-640x360.jpg" width="640" /&gt;
                  &lt;img alt="YouTube AI simple" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/YouTube-AI-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Over the past year, Google has transformed its web search experience with AI, driving toward a zero-click experience. Now, the same AI focus is coming to YouTube, and Premium subscribers can get a preview of the new search regime. Select searches on the video platform will now produce an AI-generated results carousel with a collection of relevant videos. Even if you don't pay for YouTube, AI is still coming for you with an expansion of Google's video chatbot.&lt;/p&gt;
&lt;p&gt;Google says the new AI search feature, which appears at the top of the results page, will include multiple videos, along with an AI summary of each. You can tap the video thumbnails to begin playing them right from the carousel. The summary is intended to extract the information most relevant to your search query, so you may not even have to watch the videos.&lt;/p&gt;
&lt;p&gt;The AI results carousel is only a test right now, and it's limited to YouTube Premium subscribers. If you're paying for Premium, you can enable the feature on YouTube's experimental page. While the feature is entirely opt-in, that probably won't last long. Like AI Overviews in search, this feature will take precedence over organic search results and get people interacting with Google's AI, and that's the driving force behind most of the company's decisions lately.&lt;/p&gt;
&lt;div class="yt-short-wrapper"&gt;

&lt;/div&gt;
&lt;p&gt;It's not hard to see where this feature could lead because we've seen the same thing play out in general web search. By putting AI-generated content at the top of search results, Google will reduce the number of videos people click to watch. The carousel gives you the relevant parts of the video along with a summary, but the video page is another tap away. Rather than opening videos, commenting, subscribing, and otherwise interacting with creators, some users will just peruse the AI carousel. That could make it harder for channels to grow and earn revenue from their content—the same content Google will feed into Gemini to generate the AI carousel.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Currently, the AI-generated results carousel will only appear for shopping, places, and activities in specific areas. Google recommends searches like "best beaches in Hawaii" and "noise canceling headphones." However, AI Overviews launched with AI answers for a small subset of searches, and it has since expanded to almost all queries. The AI-generated results carousel could end up the same, but we're not there yet. In addition to being an opt-in test for subscribers, the AI results are also limited to the mobile app and English-language videos for now.&lt;/p&gt;
&lt;p&gt;Alongside the debut of AI search in YouTube, Google is expanding its conversational AI tool to more users. This feature was previously limited to Premium members, but it will now appear for everyone. This chatbot lets you ask questions about a video, get recommendations for similar content, and explore concepts.&lt;/p&gt;
&lt;p&gt;Google has also promised (threatened?) that it's working on more AI features to "help you get the most out of YouTube." Prepare yourself.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The feature is only available as a test for Premium members for now.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="YouTube AI simple" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/YouTube-AI-640x360.jpg" width="640" /&gt;
                  &lt;img alt="YouTube AI simple" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/YouTube-AI-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Over the past year, Google has transformed its web search experience with AI, driving toward a zero-click experience. Now, the same AI focus is coming to YouTube, and Premium subscribers can get a preview of the new search regime. Select searches on the video platform will now produce an AI-generated results carousel with a collection of relevant videos. Even if you don't pay for YouTube, AI is still coming for you with an expansion of Google's video chatbot.&lt;/p&gt;
&lt;p&gt;Google says the new AI search feature, which appears at the top of the results page, will include multiple videos, along with an AI summary of each. You can tap the video thumbnails to begin playing them right from the carousel. The summary is intended to extract the information most relevant to your search query, so you may not even have to watch the videos.&lt;/p&gt;
&lt;p&gt;The AI results carousel is only a test right now, and it's limited to YouTube Premium subscribers. If you're paying for Premium, you can enable the feature on YouTube's experimental page. While the feature is entirely opt-in, that probably won't last long. Like AI Overviews in search, this feature will take precedence over organic search results and get people interacting with Google's AI, and that's the driving force behind most of the company's decisions lately.&lt;/p&gt;
&lt;div class="yt-short-wrapper"&gt;

&lt;/div&gt;
&lt;p&gt;It's not hard to see where this feature could lead because we've seen the same thing play out in general web search. By putting AI-generated content at the top of search results, Google will reduce the number of videos people click to watch. The carousel gives you the relevant parts of the video along with a summary, but the video page is another tap away. Rather than opening videos, commenting, subscribing, and otherwise interacting with creators, some users will just peruse the AI carousel. That could make it harder for channels to grow and earn revenue from their content—the same content Google will feed into Gemini to generate the AI carousel.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Currently, the AI-generated results carousel will only appear for shopping, places, and activities in specific areas. Google recommends searches like "best beaches in Hawaii" and "noise canceling headphones." However, AI Overviews launched with AI answers for a small subset of searches, and it has since expanded to almost all queries. The AI-generated results carousel could end up the same, but we're not there yet. In addition to being an opt-in test for subscribers, the AI results are also limited to the mobile app and English-language videos for now.&lt;/p&gt;
&lt;p&gt;Alongside the debut of AI search in YouTube, Google is expanding its conversational AI tool to more users. This feature was previously limited to Premium members, but it will now appear for everyone. This chatbot lets you ask questions about a video, get recommendations for similar content, and explore concepts.&lt;/p&gt;
&lt;p&gt;Google has also promised (threatened?) that it's working on more AI features to "help you get the most out of YouTube." Prepare yourself.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/gadgets/2025/06/google-begins-rolling-out-ai-search-in-youtube/</guid><pubDate>Thu, 26 Jun 2025 16:43:05 +0000</pubDate></item><item><title>[NEW] Book authors made the wrong arguments in Meta AI training case, judge says (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/06/book-authors-made-the-wrong-arguments-in-meta-ai-training-case-judge-says/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Judges clash over "schoolchildren" analogy in key AI training rulings.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2207798306-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2207798306-1152x648-1750956527.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Book authors and publishing professionals staged a protest outside Meta UK offices in King's Cross over the Facebook owner's use of copyrighted books to train artificial intelligence. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          SOPA Images / Contributor | LightRocket

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Soon after a landmark ruling deemed that when Anthropic copied books to train artificial intelligence models, it was a "transformative" fair use, another judge has arrived at the same conclusion in a case pitting book authors against Meta.&lt;/p&gt;
&lt;p&gt;But that doesn't necessarily mean the judges are completely in agreement, and that could soon become a problem for not just Meta, but other big AI companies celebrating the pair of wins this week.&lt;/p&gt;
&lt;p&gt;On Wednesday, Judge Vince Chhabria explained that he sided with Meta, despite his better judgment, mainly because the authors made all the wrong arguments in their case against Meta.&lt;/p&gt;
&lt;p&gt;"This ruling does not stand for the proposition that Meta’s use of copyrighted materials to train its language models is lawful," Chhabria wrote. "It stands only for the proposition that these plaintiffs made the wrong arguments and failed to develop a record in support of the right one."&lt;/p&gt;
&lt;p&gt;Rather than argue that Meta's Llama AI models risked rapidly flooding their markets with competing AI-generated books that could indirectly harm sales, authors fatally only argued "that users of Llama can reproduce text from their books, and that Meta’s copying harmed the market for licensing copyrighted materials to companies for AI training."&lt;/p&gt;
&lt;p&gt;Because Chhabria found both of these theories "flawed"—the former because Llama cannot produce long excerpts of works, even with adversarial prompting, and the latter because authors are not entitled to monopolize the market for licensing books for AI training—he said he had no choice but to grant Meta's request for summary judgment.&lt;/p&gt;
&lt;p&gt;Ultimately, because authors introduced no evidence that Meta's AI threatened to dilute their markets, Chhabria ruled that Meta did enough to overcome authors' other arguments regarding alleged harms by simply providing "its own expert testimony explaining that Llama 3’s release did not have any discernible effect on the plaintiffs’ sales."&lt;/p&gt;
&lt;p&gt;Chhabria seemed to criticize authors for raising a "half-hearted" defense of their works, noting that his opinion "may be in significant tension with reality," where it seems "possible, even likely, that Llama will harm the book sale market."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;There is perhaps a silver lining for other book authors in this ruling, Chhabria suggested. Since Meta's request for summary judgment came before class certification in the lawsuit, his ruling only applies to the 13 authors who sued Meta in this particular case. That means that other authors who perhaps could make a stronger case alleging market harms could still have a strong chance at winning a future Meta lawsuit, Chhabria wrote.&lt;/p&gt;
&lt;p&gt;"In cases involving uses like Meta’s, it seems like the plaintiffs will often win, at least where those cases have better-developed records on the market effects of the defendant’s use," Chhabria wrote. "No matter how transformative [AI] training may be, it’s hard to imagine that it can be fair use to use copyrighted books to develop a tool to make billions or trillions of dollars while enabling the creation of a potentially endless stream of competing works that could significantly harm the market for those books."&lt;/p&gt;
&lt;p&gt;Further, Chhabria suggested that "some cases might present even stronger arguments against fair use"—such as news organizations suing OpenAI over allegedly infringing ChatGPT outputs that could indirectly compete with their websites. Celebrating the ruling, a lawyer representing The New York Times in that suit, Ian Crosby, told Ars that both Chhabria's and Alsup's rulings are viewed as strengthening the NYT's case.&lt;/p&gt;
&lt;p&gt;"These two decisions show what we have long argued: generative AI developers may not build products by copying stolen news content, particularly where that content is taken by wrongful means and their products output substitutive content that threatens the market for original, human-made journalism," Crosby said.&lt;/p&gt;
&lt;p&gt;On the other hand, Chhabria wrote that AI companies may have an easier time defeating copyright claims if the feared market dilution is a trade-off for a clear public benefit, like advancing non-commercial research into national security or medicine.&lt;/p&gt;
&lt;p&gt;Chhabria said that if the authors had introduced any evidence of market dilution, Meta would not have won at this stage of the case and would have likely faced broader discovery in a class-action suit weighed by a jury.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Instead, the only surviving claim in this case concerns Meta's controversial torrenting of books to train Llama, which authors have so far successfully alleged may have violated copyright laws by distributing their works as part of the torrenting process.&lt;/p&gt;
&lt;h2&gt;Training AI is not akin to teaching “schoolchildren”&lt;/h2&gt;
&lt;p&gt;According to Chhabria, if rights holders provide evidence of market dilution, that may raise the strongest opposition most likely to win AI copyright fights. So, while Meta technically won this fight against these book authors, the ruling isn't necessarily a slam dunk for Meta, nor does it offer ample security for any AI company.&lt;/p&gt;
&lt;p&gt;Rather than suggest that AI companies can defeat copyright claims on the virtue that their products are "transformative" uses of authors' works, Chhabria said that cases will win or lose based on allegations of market harm.&lt;/p&gt;
&lt;p&gt;He claimed that the "upshot" of his ruling is that he did not create any bright-line rules carving out exceptions for AI companies. Instead, he believes that his ruling makes it clear "that in many circumstances it will be illegal to copy copyright-protected works to train generative AI models without permission. Which means that the companies, to avoid liability for copyright infringement, will generally need to pay copyright holders for the right to use their materials."&lt;/p&gt;
&lt;p&gt;In his order, Chhabria called out Judge William Alsup for focusing his ruling this week in the Anthropic case "heavily on the transformative nature of generative AI while brushing aside concerns about the harm it can inflict on the market for the works it gets trained on."&lt;/p&gt;
&lt;p&gt;Chhabria particularly did not approve that Alsup compared authors' complaints of the possible market harms that could result if Anthropic's Claude flooded&amp;nbsp;book markets to the outlandish idea that teaching "schoolchildren to write well" would "result in an explosion of competing works."&lt;/p&gt;
&lt;p&gt;"According to Judge Alsup, this 'is not the kind of competitive or creative displacement that concerns the Copyright Act,'" Chhabria wrote. "But when it comes to market effects, using books to teach children to write is not remotely like using books to create a product that a single individual could employ to generate countless competing works with a miniscule [sic] fraction of the time and creativity it would otherwise take.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;"This inapt analogy is not a basis for blowing off the most important factor in the fair use analysis," Chhabria cautioned.&lt;/p&gt;
&lt;p&gt;Additionally, Meta's claim that granting authors a win would stop AI innovation "in its tracks" is "ridiculous," Chhabria wrote, noting that if rights holders win in any of the lawsuits against AI companies today, the only outcome would be that AI companies would have to pay authors—or else rely on materials in the public domain and prove that it's not necessary to use copyrighted works for AI training after all.&lt;/p&gt;
&lt;p&gt;"These products are expected to generate billions, even trillions, of dollars for the companies that are developing them," Chhabria wrote. "If using copyrighted works to train the models is as necessary as the companies say, they will figure out a way to compensate copyright holders for it."&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Three ways authors can keep fighting AI training&lt;/h2&gt;
&lt;p&gt;This week's rulings suggest that the question of whether AI training is transformative has been largely settled.&lt;/p&gt;
&lt;p&gt;But as authors continue suing AI companies, with the latest lawsuit lobbed at Microsoft this week, Chhabria suggested that "generally the plaintiff’s only chance to defeat fair use will be to win decisively on" the fourth factor of a fair use analysis, where judges and juries weigh "the effect of the use upon the potential market for or value of the copyrighted work."&lt;/p&gt;
&lt;p&gt;Chhabria suggested that authors had at least three paths to fight AI training on the basis of market harms. First, they could claim that AI outputs "regurgitate their works." Second, they could "point to the market for licensing their works for AI training and contend that unauthorized copying for training harms that market (or precludes the development of that market)." And third, they could argue that AI outputs could "indirectly substitute" their works by generating "substantially similar" works.&lt;/p&gt;
&lt;p&gt;Because the first two arguments failed in the Meta case, Chhabria thinks "the third argument is far more promising" for authors intending to pick up the torch where the 13 authors in the current case have failed.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;An interesting wrinkle that may have stopped authors from invoking market dilution as a threat in the Meta case is that Chhabria noted that Meta had argued that "market dilution does not count under the fourth factor."&lt;/p&gt;
&lt;p&gt;But Chhabria clarified "that can’t be right."&lt;/p&gt;
&lt;p&gt;"Indirect substitution is still substitution," Chhabria wrote. "If someone bought a romance novel written by [a large language model (LLM)] instead of a romance novel written by a human author, the LLM-generated novel is substituting for the human-written one." Seemingly, the same would go for AI-generated non-fiction books, he suggested.&lt;/p&gt;
&lt;p&gt;So while "it’s true that, in many copyright cases, this concept of market dilution or indirect substitution is not particularly important," AI cases may change the copyright landscape because it "involves a technology that can generate literally millions of secondary works, with a miniscule [sic] fraction of the time and creativity used to create the original works it was trained on," Chhabria wrote.&lt;/p&gt;
&lt;p&gt;This is unprecedented, Chhabria suggested, as no other use "has anything near the potential to flood the market with competing works the way that LLM training does. And so the concept of market dilution becomes highly relevant... Courts can’t stick their heads in the sand to an obvious way that a new technology might severely harm the incentive to create, just because the issue has not come up before."&lt;/p&gt;
&lt;p&gt;In a way, Chhabria's ruling provides a roadmap for rights holders looking to advance lawsuits against AI companies in the midst of precedent-setting rulings.&lt;/p&gt;
&lt;p&gt;Unfortunately for book authors suing Meta who found a sympathetic judge in Chhabria—but only made a "fleeting reference" to indirect substitution in a single report in its filings ahead of yesterday's ruling—"courts can’t decide cases based on what they think will or should happen in other cases."&lt;/p&gt;
&lt;p&gt;If their allegations were just a little stronger, Chhabria suggested they could have even won on summary judgment, instead of Meta.&lt;/p&gt;
&lt;p&gt;"Indeed, it seems likely that market dilution will often cause plaintiffs to decisively win the fourth factor—and thus win the fair use question overall—in cases like this," Chhabria wrote.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Judges clash over "schoolchildren" analogy in key AI training rulings.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2207798306-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2207798306-1152x648-1750956527.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Book authors and publishing professionals staged a protest outside Meta UK offices in King's Cross over the Facebook owner's use of copyrighted books to train artificial intelligence. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          SOPA Images / Contributor | LightRocket

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Soon after a landmark ruling deemed that when Anthropic copied books to train artificial intelligence models, it was a "transformative" fair use, another judge has arrived at the same conclusion in a case pitting book authors against Meta.&lt;/p&gt;
&lt;p&gt;But that doesn't necessarily mean the judges are completely in agreement, and that could soon become a problem for not just Meta, but other big AI companies celebrating the pair of wins this week.&lt;/p&gt;
&lt;p&gt;On Wednesday, Judge Vince Chhabria explained that he sided with Meta, despite his better judgment, mainly because the authors made all the wrong arguments in their case against Meta.&lt;/p&gt;
&lt;p&gt;"This ruling does not stand for the proposition that Meta’s use of copyrighted materials to train its language models is lawful," Chhabria wrote. "It stands only for the proposition that these plaintiffs made the wrong arguments and failed to develop a record in support of the right one."&lt;/p&gt;
&lt;p&gt;Rather than argue that Meta's Llama AI models risked rapidly flooding their markets with competing AI-generated books that could indirectly harm sales, authors fatally only argued "that users of Llama can reproduce text from their books, and that Meta’s copying harmed the market for licensing copyrighted materials to companies for AI training."&lt;/p&gt;
&lt;p&gt;Because Chhabria found both of these theories "flawed"—the former because Llama cannot produce long excerpts of works, even with adversarial prompting, and the latter because authors are not entitled to monopolize the market for licensing books for AI training—he said he had no choice but to grant Meta's request for summary judgment.&lt;/p&gt;
&lt;p&gt;Ultimately, because authors introduced no evidence that Meta's AI threatened to dilute their markets, Chhabria ruled that Meta did enough to overcome authors' other arguments regarding alleged harms by simply providing "its own expert testimony explaining that Llama 3’s release did not have any discernible effect on the plaintiffs’ sales."&lt;/p&gt;
&lt;p&gt;Chhabria seemed to criticize authors for raising a "half-hearted" defense of their works, noting that his opinion "may be in significant tension with reality," where it seems "possible, even likely, that Llama will harm the book sale market."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;There is perhaps a silver lining for other book authors in this ruling, Chhabria suggested. Since Meta's request for summary judgment came before class certification in the lawsuit, his ruling only applies to the 13 authors who sued Meta in this particular case. That means that other authors who perhaps could make a stronger case alleging market harms could still have a strong chance at winning a future Meta lawsuit, Chhabria wrote.&lt;/p&gt;
&lt;p&gt;"In cases involving uses like Meta’s, it seems like the plaintiffs will often win, at least where those cases have better-developed records on the market effects of the defendant’s use," Chhabria wrote. "No matter how transformative [AI] training may be, it’s hard to imagine that it can be fair use to use copyrighted books to develop a tool to make billions or trillions of dollars while enabling the creation of a potentially endless stream of competing works that could significantly harm the market for those books."&lt;/p&gt;
&lt;p&gt;Further, Chhabria suggested that "some cases might present even stronger arguments against fair use"—such as news organizations suing OpenAI over allegedly infringing ChatGPT outputs that could indirectly compete with their websites. Celebrating the ruling, a lawyer representing The New York Times in that suit, Ian Crosby, told Ars that both Chhabria's and Alsup's rulings are viewed as strengthening the NYT's case.&lt;/p&gt;
&lt;p&gt;"These two decisions show what we have long argued: generative AI developers may not build products by copying stolen news content, particularly where that content is taken by wrongful means and their products output substitutive content that threatens the market for original, human-made journalism," Crosby said.&lt;/p&gt;
&lt;p&gt;On the other hand, Chhabria wrote that AI companies may have an easier time defeating copyright claims if the feared market dilution is a trade-off for a clear public benefit, like advancing non-commercial research into national security or medicine.&lt;/p&gt;
&lt;p&gt;Chhabria said that if the authors had introduced any evidence of market dilution, Meta would not have won at this stage of the case and would have likely faced broader discovery in a class-action suit weighed by a jury.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Instead, the only surviving claim in this case concerns Meta's controversial torrenting of books to train Llama, which authors have so far successfully alleged may have violated copyright laws by distributing their works as part of the torrenting process.&lt;/p&gt;
&lt;h2&gt;Training AI is not akin to teaching “schoolchildren”&lt;/h2&gt;
&lt;p&gt;According to Chhabria, if rights holders provide evidence of market dilution, that may raise the strongest opposition most likely to win AI copyright fights. So, while Meta technically won this fight against these book authors, the ruling isn't necessarily a slam dunk for Meta, nor does it offer ample security for any AI company.&lt;/p&gt;
&lt;p&gt;Rather than suggest that AI companies can defeat copyright claims on the virtue that their products are "transformative" uses of authors' works, Chhabria said that cases will win or lose based on allegations of market harm.&lt;/p&gt;
&lt;p&gt;He claimed that the "upshot" of his ruling is that he did not create any bright-line rules carving out exceptions for AI companies. Instead, he believes that his ruling makes it clear "that in many circumstances it will be illegal to copy copyright-protected works to train generative AI models without permission. Which means that the companies, to avoid liability for copyright infringement, will generally need to pay copyright holders for the right to use their materials."&lt;/p&gt;
&lt;p&gt;In his order, Chhabria called out Judge William Alsup for focusing his ruling this week in the Anthropic case "heavily on the transformative nature of generative AI while brushing aside concerns about the harm it can inflict on the market for the works it gets trained on."&lt;/p&gt;
&lt;p&gt;Chhabria particularly did not approve that Alsup compared authors' complaints of the possible market harms that could result if Anthropic's Claude flooded&amp;nbsp;book markets to the outlandish idea that teaching "schoolchildren to write well" would "result in an explosion of competing works."&lt;/p&gt;
&lt;p&gt;"According to Judge Alsup, this 'is not the kind of competitive or creative displacement that concerns the Copyright Act,'" Chhabria wrote. "But when it comes to market effects, using books to teach children to write is not remotely like using books to create a product that a single individual could employ to generate countless competing works with a miniscule [sic] fraction of the time and creativity it would otherwise take.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;"This inapt analogy is not a basis for blowing off the most important factor in the fair use analysis," Chhabria cautioned.&lt;/p&gt;
&lt;p&gt;Additionally, Meta's claim that granting authors a win would stop AI innovation "in its tracks" is "ridiculous," Chhabria wrote, noting that if rights holders win in any of the lawsuits against AI companies today, the only outcome would be that AI companies would have to pay authors—or else rely on materials in the public domain and prove that it's not necessary to use copyrighted works for AI training after all.&lt;/p&gt;
&lt;p&gt;"These products are expected to generate billions, even trillions, of dollars for the companies that are developing them," Chhabria wrote. "If using copyrighted works to train the models is as necessary as the companies say, they will figure out a way to compensate copyright holders for it."&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Three ways authors can keep fighting AI training&lt;/h2&gt;
&lt;p&gt;This week's rulings suggest that the question of whether AI training is transformative has been largely settled.&lt;/p&gt;
&lt;p&gt;But as authors continue suing AI companies, with the latest lawsuit lobbed at Microsoft this week, Chhabria suggested that "generally the plaintiff’s only chance to defeat fair use will be to win decisively on" the fourth factor of a fair use analysis, where judges and juries weigh "the effect of the use upon the potential market for or value of the copyrighted work."&lt;/p&gt;
&lt;p&gt;Chhabria suggested that authors had at least three paths to fight AI training on the basis of market harms. First, they could claim that AI outputs "regurgitate their works." Second, they could "point to the market for licensing their works for AI training and contend that unauthorized copying for training harms that market (or precludes the development of that market)." And third, they could argue that AI outputs could "indirectly substitute" their works by generating "substantially similar" works.&lt;/p&gt;
&lt;p&gt;Because the first two arguments failed in the Meta case, Chhabria thinks "the third argument is far more promising" for authors intending to pick up the torch where the 13 authors in the current case have failed.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;An interesting wrinkle that may have stopped authors from invoking market dilution as a threat in the Meta case is that Chhabria noted that Meta had argued that "market dilution does not count under the fourth factor."&lt;/p&gt;
&lt;p&gt;But Chhabria clarified "that can’t be right."&lt;/p&gt;
&lt;p&gt;"Indirect substitution is still substitution," Chhabria wrote. "If someone bought a romance novel written by [a large language model (LLM)] instead of a romance novel written by a human author, the LLM-generated novel is substituting for the human-written one." Seemingly, the same would go for AI-generated non-fiction books, he suggested.&lt;/p&gt;
&lt;p&gt;So while "it’s true that, in many copyright cases, this concept of market dilution or indirect substitution is not particularly important," AI cases may change the copyright landscape because it "involves a technology that can generate literally millions of secondary works, with a miniscule [sic] fraction of the time and creativity used to create the original works it was trained on," Chhabria wrote.&lt;/p&gt;
&lt;p&gt;This is unprecedented, Chhabria suggested, as no other use "has anything near the potential to flood the market with competing works the way that LLM training does. And so the concept of market dilution becomes highly relevant... Courts can’t stick their heads in the sand to an obvious way that a new technology might severely harm the incentive to create, just because the issue has not come up before."&lt;/p&gt;
&lt;p&gt;In a way, Chhabria's ruling provides a roadmap for rights holders looking to advance lawsuits against AI companies in the midst of precedent-setting rulings.&lt;/p&gt;
&lt;p&gt;Unfortunately for book authors suing Meta who found a sympathetic judge in Chhabria—but only made a "fleeting reference" to indirect substitution in a single report in its filings ahead of yesterday's ruling—"courts can’t decide cases based on what they think will or should happen in other cases."&lt;/p&gt;
&lt;p&gt;If their allegations were just a little stronger, Chhabria suggested they could have even won on summary judgment, instead of Meta.&lt;/p&gt;
&lt;p&gt;"Indeed, it seems likely that market dilution will often cause plaintiffs to decisively win the fourth factor—and thus win the fair use question overall—in cases like this," Chhabria wrote.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/06/book-authors-made-the-wrong-arguments-in-meta-ai-training-case-judge-says/</guid><pubDate>Thu, 26 Jun 2025 17:45:41 +0000</pubDate></item><item><title>[NEW] Google Photos merges classic search with AI to speed up results (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/26/google-photos-merges-classic-search-with-ai-to-speed-up-results/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After Google temporarily paused the rollout of its buggy AI-powered “Ask Photos” feature in Google Photos, the company announced that it has improved the feature’s ability to quickly return search results.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI feature, first introduced at Google’s I/O developer conference last year, allows users to search across their collection of digital photos using natural language queries. Leveraging Google’s Gemini, Ask Photos taps into the AI’s ability to understand a photo’s content and its other metadata when responding to input. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, users complained the AI feature wasn’t reliable and was often slow to respond while the AI was “thinking.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Addressing these concerns, Google Photos product manager Jamie Aspinall wrote on X earlier in June that “Ask Photos isn’t where it needs to be, in terms of latency, quality and ux,” and noted the rollout would be paused for a couple of weeks while Google worked to bring back the “speed and recall of the original search.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3022535" height="391" src="https://techcrunch.com/wp-content/uploads/2025/06/ask-photos-june-2025.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Screenshot&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In a short blog post published on Thursday, Google says it’s bringing the best of Photos’ classic search feature into Ask Photos, particularly for simple searches like “beach” or “dogs.” This allows the search results to display more quickly, as classic search did before.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI, in the meantime, will work in the background to find the most relevant photos and work to answer more complex queries. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, if you search for a photo of a “white dog,” a series of initial search results immediately appear. After the AI finishes its analysis, its results will appear below, along with some introductory text that may identify your dog by name, if you’ve added it, and tell you when photos of the animal first appeared.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The interface still allows you to switch to classic search if you prefer. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a result of these changes, Google has now resumed the rollout of Ask Photos to more people across the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To be eligible to use Ask Photos, you must be 18 or older, and your account language must be set to English. You must also enable Face Groups, the feature that labels the people and pets found in the Google Photos library.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After Google temporarily paused the rollout of its buggy AI-powered “Ask Photos” feature in Google Photos, the company announced that it has improved the feature’s ability to quickly return search results.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI feature, first introduced at Google’s I/O developer conference last year, allows users to search across their collection of digital photos using natural language queries. Leveraging Google’s Gemini, Ask Photos taps into the AI’s ability to understand a photo’s content and its other metadata when responding to input. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, users complained the AI feature wasn’t reliable and was often slow to respond while the AI was “thinking.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Addressing these concerns, Google Photos product manager Jamie Aspinall wrote on X earlier in June that “Ask Photos isn’t where it needs to be, in terms of latency, quality and ux,” and noted the rollout would be paused for a couple of weeks while Google worked to bring back the “speed and recall of the original search.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3022535" height="391" src="https://techcrunch.com/wp-content/uploads/2025/06/ask-photos-june-2025.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Screenshot&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In a short blog post published on Thursday, Google says it’s bringing the best of Photos’ classic search feature into Ask Photos, particularly for simple searches like “beach” or “dogs.” This allows the search results to display more quickly, as classic search did before.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI, in the meantime, will work in the background to find the most relevant photos and work to answer more complex queries. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, if you search for a photo of a “white dog,” a series of initial search results immediately appear. After the AI finishes its analysis, its results will appear below, along with some introductory text that may identify your dog by name, if you’ve added it, and tell you when photos of the animal first appeared.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The interface still allows you to switch to classic search if you prefer. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a result of these changes, Google has now resumed the rollout of Ask Photos to more people across the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To be eligible to use Ask Photos, you must be 18 or older, and your account language must be set to English. You must also enable Face Groups, the feature that labels the people and pets found in the Google Photos library.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/26/google-photos-merges-classic-search-with-ai-to-speed-up-results/</guid><pubDate>Thu, 26 Jun 2025 17:55:05 +0000</pubDate></item><item><title>[NEW] In just 3 months, CoreWeave CEO, once a crypto-mining bro, becomes a deca-billionaire (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/26/in-just-3-months-coreweave-ceo-once-a-crypto-mining-bro-becomes-a-deca-billionaire/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Mike-Intrator-Headshot-e1750958788389.jpg?w=898" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;CoreWeave co-founder and CEO Michael Intrator’s net worth has skyrocketed to about $10 billion in the three months since the AI firm went public, Bloomberg reports.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His company’s debut was both the biggest tech IPO so far of 2025 –&amp;nbsp; raising $1.5 billion –&amp;nbsp; and also somewhat of a clunker: its founders had reportedly hoped to raise a lot more – up to $4 billion – and had to skinny their ambitions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;CoreWeave still feels a bit like both a success and a house of cards. It offers AI training and inference cloud services built upon a growing stockpile of Nvidia GPUs. One of its investors is Nvidia, which helps it obtain the precious, short-in-supply chips.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave has both Microsoft and OpenAI as customers – the latter signed a deal to buy $12 billion worth of services and still has about $11 billion worth to buy. And Nvidia increased its stake after the IPO, the company disclosed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But CoreWeave borrows money against the GPUs to pay for them – and its IPO wasn’t big enough to get it out of that cycle. It’s got about $8.8 billion worth of debt as of March, it disclosed, with interest rates as high as 15%. Even though it brought in almost $1 billion in revenue in Q1 alone ($985 million), it recorded a net loss of about $315 million.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That has not scared away investors who remain eager for ways to make money on AI. CoreWeave’s stock has soared almost 300% since its March IPO, raising Intrator’s net worth to above $10 billion, Bloomberg calculates.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the wildest part of Intrator’s history, as well as that of his co-founders Brian Venturo and Brannin McBee, is that the whole thing started out as a make-money-quick, crypto mining enterprise when their previous company, a hedge fund, failed. &lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;The business partners went from a closet full of GPUs to thousands of them in a New Jersey warehouse, to an AI training experiment with an open source LLM group, EleutherAI, Venturo previously told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Today, the company is servicing the biggest LLM players on the planet, reportedly seeking to buy its competitor Core Scientific, and the founders are billionaires.  And, as we previously reported, it’s not all paper money. All three founders pocketed over $150 million apiece by cashing out of shares ahead of the IPO.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave remains a symbol of the AI industry in 2025: Massive, fast-growing revenue, investor enthusiasm built on an insatiable need for more resources. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Coreweave declined additional comment.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Mike-Intrator-Headshot-e1750958788389.jpg?w=898" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;CoreWeave co-founder and CEO Michael Intrator’s net worth has skyrocketed to about $10 billion in the three months since the AI firm went public, Bloomberg reports.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His company’s debut was both the biggest tech IPO so far of 2025 –&amp;nbsp; raising $1.5 billion –&amp;nbsp; and also somewhat of a clunker: its founders had reportedly hoped to raise a lot more – up to $4 billion – and had to skinny their ambitions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;CoreWeave still feels a bit like both a success and a house of cards. It offers AI training and inference cloud services built upon a growing stockpile of Nvidia GPUs. One of its investors is Nvidia, which helps it obtain the precious, short-in-supply chips.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave has both Microsoft and OpenAI as customers – the latter signed a deal to buy $12 billion worth of services and still has about $11 billion worth to buy. And Nvidia increased its stake after the IPO, the company disclosed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But CoreWeave borrows money against the GPUs to pay for them – and its IPO wasn’t big enough to get it out of that cycle. It’s got about $8.8 billion worth of debt as of March, it disclosed, with interest rates as high as 15%. Even though it brought in almost $1 billion in revenue in Q1 alone ($985 million), it recorded a net loss of about $315 million.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That has not scared away investors who remain eager for ways to make money on AI. CoreWeave’s stock has soared almost 300% since its March IPO, raising Intrator’s net worth to above $10 billion, Bloomberg calculates.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the wildest part of Intrator’s history, as well as that of his co-founders Brian Venturo and Brannin McBee, is that the whole thing started out as a make-money-quick, crypto mining enterprise when their previous company, a hedge fund, failed. &lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;The business partners went from a closet full of GPUs to thousands of them in a New Jersey warehouse, to an AI training experiment with an open source LLM group, EleutherAI, Venturo previously told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Today, the company is servicing the biggest LLM players on the planet, reportedly seeking to buy its competitor Core Scientific, and the founders are billionaires.  And, as we previously reported, it’s not all paper money. All three founders pocketed over $150 million apiece by cashing out of shares ahead of the IPO.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave remains a symbol of the AI industry in 2025: Massive, fast-growing revenue, investor enthusiasm built on an insatiable need for more resources. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Coreweave declined additional comment.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/26/in-just-3-months-coreweave-ceo-once-a-crypto-mining-bro-becomes-a-deca-billionaire/</guid><pubDate>Thu, 26 Jun 2025 18:12:18 +0000</pubDate></item></channel></rss>